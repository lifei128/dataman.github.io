<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[spark参数优化]]></title>
    <url>%2F2019%2F02%2F14%2Fspark%E5%B8%B8%E8%A7%84%E4%BC%98%E5%8C%96%E7%82%B9%2F</url>
    <content type="text"><![CDATA[spark.default.parallelism默认的并发数spark中有partition的概念（和slice是同一个概念，在spark1.2中官网已经做出了说明），一般每个partition对应一个task。在我的测试过程中，如果没有设置spark.default.parallelism参数，spark计算出来的partition非常巨大，与我的cores非常不搭。我在两台机器上（8cores 2 +6g 2）上，spark计算出来的partition达到2.8万个，也就是2.9万个tasks，每个task完成时间都是几毫秒或者零点几毫秒，执行起来非常缓慢。在我尝试设置了 spark.default.parallelism 后，任务数减少到10，执行一次计算过程从minute降到20second。 问题: reduce task数目不合适 解决方案：需要根据实际情况调整默认配置，调整方式是修改参数spark.default.parallelism。通常的，reduce数目设置为core数目的2-3倍。数量太大，造成很多小任务，增加启动任务的开销；数目太小，任务运行缓慢。所以要合理修改reduce的task数目即spark.default.parallelism 参考：https://blog.csdn.net/bbaiggey/article/details/51984753 spark.files.maxPartitionBytes = 128 M（默认）代表着rdd的一个分区能存放数据的最大字节数，如果一个400m的文件，只分了两个区，则在action时会发生错误。当一个spark应用程序执行时，生成spark.context，同时会生成两个参数，由上面得到的spark.default.parallelism推导出这两个参数的值12sc.defaultParallelism = spark.default.parallelismsc.defaultMinPartitions = min(spark.default.parallelism,2) 当sc.defaultParallelism和sc.defaultMinPartitions最终确认后，就可以推算rdd的分区数了。 spark.local.dir这个看起来很简单，就是Spark用于写中间数据，如RDD Cache，Shuffle，Spill等数据的位置，那么有什么可以注意的呢。 首先，最基本的当然是我们可以配置多个路径（用逗号分隔）到多个磁盘上增加整体IO带宽，这个大家都知道。 其次，目前的实现中，Spark是通过对文件名采用hash算法分布到多个路径下的目录中去，如果你的存储设备有快有慢，比如SSD+HDD混合使用，那么你可以通过在SSD上配置更多的目录路径来增大它被Spark使用的比例，从而更好地利用SSD的IO带宽能力。当然这只是一种变通的方法，终极解决方案还是应该像目前HDFS的实现方向一样，让Spark能够感知具体的存储设备类型，针对性的使用。 需要注意的是，在Spark 1.0 以后，SPARK_LOCAL_DIRS(Standalone, Mesos) or LOCAL_DIRS (YARN)参数会覆盖这个配置。比如Spark On YARN的时候，Spark Executor的本地路径依赖于Yarn的配置，而不取决于这个参数。 spark.shuffle.consolidateFiles为true问题：map|reduce数量大，造成shuffle小文件数目多解决方案： 通过设置spark.shuffle.consolidateFiles为true，来合并shuffle中间文件，此时文件数为reduce tasks数目； spark.serializer问题：序列化时间长、结果大 解决方案： spark默认使用JDK 自带的ObjectOutputStream，这种方式产生的结果大、CPU处理时间长，可以通过设置spark.serializer为org.apache.spark.serializer.KeyoSerializer。 另外如果结果已经很大，那就最好使用广播变量方式了，结果你懂得。 mapPartition问题：单条记录消耗大 解决方案： 使用mapPartition替换map，mapPartition是对每个Partition进行计算，而map是对partition中的每条记录进行计算； collect输出大量结果时速度慢解决方案： collect源码中是把所有的结果以一个Array的方式放在内存中，可以直接输出到分布式的文件系统，然后查看文件系统中的内容； spark.speculation问题: 任务执行速度倾斜 解决方案： 如果数据倾斜，一般是partition key取得不好，可以考虑其他的并行处理方式，并在中间加上aggregation操作；如果是Worker倾斜，例如在某些Worker上的executor执行缓慢，可以通过设置spark.speculation=true 把那些持续慢的节点去掉； repartition问题: 通过多步骤的RDD操作后有很多空任务或者小任务产生 解决方案： 使用coalesce或者repartition去减少RDD中partition数量； spark.streaming.concurrentJobs问题:Spark Streaming吞吐量不高 可以设置spark.streaming.concurrentJobs Spark Streaming 运行速度突然下降了，经常会有任务延迟和阻塞解决方案： 这是因为我们设置job启动interval时间间隔太短了，导致每次job在指定时间无法正常执行完成，换句话说就是创建的windows窗口时间间隔太密集了； 使用KryoSerializer进行序列化使用KryoSerializer序列化的好处默认情况，spark使用的是java的序列化机制，ObjectOutputStream / ObjectInputStream，对象输入输出流机制，来进行序列化。 该序列化的好处是方便使用，但必须实现Serializable接口，缺点是效率低，速度慢，序列化后的占用空间大 KryoSerializer序列化机制，效率高，速度快，占用空间小（只有java序列化的1/10），可以减少网络传输 使用方法1234//配置使用KryoSerializer进行序列化conf.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)//（为了使序列化效果达到最优）注册自定义的类型使KryoSerializer序列化 .registerKryoClasses(new Class[]&#123;ExtractSession.class,FilterCount.class,SessionDetail.class,Task.class,Top10Session.class,Top10.class,VisitAggr.class&#125;); 使用KryoSerializer序列化的场景 算子函数中使用到的外部变量，使用KryoSerializer后，可以优化网络传输效率，优化集群中内存的占用和消耗持久化Rdd,优化内存占用，task过程中创建对象，减少GC次数shuffle过程，优化网络的传输性能 将每个task中都使用的大的外部变量作为广播变量没有使用广播变量的缺点 默认情况，task使用到了外部变量，每个task都会获取一份外部变量的副本，会占用不必要的内存消耗，导致在Rdd持久化时不能写入到内存，只能持久化到磁盘中，增加了IO读写操作。 同时，在task创建对象时，内存不足，进行频繁的GC操作，降低效率 使用广播变量的好处 广播变量不是每个task保存一份，而是每个executor保存一份。 广播变量初始化时，在Driver上生成一份副本，task运行时需要用到广播变量中的数据，首次使用会在本地的Executor对应的BlockManager中尝试获取变量副本；如果本地没有，那么就会从Driver远程拉取变量副本，并保存到本地的BlockManager中；此后这个Executor中的task使用到的数据都从本地的BlockManager中直接获取。 Executor中的BlockManager除了从远程的Driver中拉取变量副本，也可能从其他节点的BlockManager中拉取数据，距离越近越好。 将rdd进行持久化持久化的原则 Rdd的架构重构和优化 尽量复用Rdd，差不多的Rdd进行抽象为一个公共的Rdd，供后面使用 公共Rdd一定要进行持久化 对应对次计算和使用的Rdd，一定要进行持久化 持久化是可以序列化的 首先采用纯内存的持久化方式，如果出现OOM异常，则采用纯内存+序列化的方法，如果依然存在OOM异常，使用内存+磁盘，以及内存+磁盘+序列化的方法 为了数据的高可靠性，而且内存充足时，可以使用双副本机制进行持久化 持久化的代码实现 .persist(StorageLevel.MEMORY_ONLY()) 持久化等级 StorageLevel.MEMORY_ONLY() 纯内存 等效于 .cache() 序列化的：后缀带有_SER 如：StorageLevel.MEMORY_ONLY_SER() 内存+序列化 后缀带有_DISK 表示磁盘，如：MEMORY_AND_DISK() 内存+磁盘 后缀带有_2表示副本数，如：MEMORY_AND_DISK_2() 内存+磁盘且副本数为2 开启map端输出文件的合并机制为什么要开启map端输出文件的合并机制 默认情况下，map端的每个task会为reduce端的每个task生成一个输出文件，reduce段的每个task拉取map端每个task生成的相应文件 开启后，map端只会在并行执行的task生成reduce端task数目的文件，下一批map端的task执行时，会复用首次生成的文件 如何开启12//开启map端输出文件的合并机制conf.set(&quot;spark.shuffle.consolidateFiles&quot;, &quot;true&quot;); 调节map端内存缓冲区为什么要调节map端内存缓冲区默认情况下，shuffle的map task,输出的文件到内存缓冲区，当内存缓冲区满了，才会溢写spill操作到磁盘，如果该缓冲区比较小，而map端输出文件又比较大，会频繁的出现溢写到磁盘，影响性能。如何调整12//设置map 端内存缓冲区大小（默认32k）conf.set(&quot;spark.shuffle.file.buffer&quot;, &quot;64k&quot;); 调节reduce端内存占比为什么要调节reduce端内存占比reduce task 在进行汇聚，聚合等操作时，实际上使用的是自己对应的executor内存，默认情况下executor分配给reduce进行聚合的内存比例是0.2，如果拉取的文件比较大，会频繁溢写到本地磁盘，影响性能。12//设置reduce端内存占比conf.set(&quot;spark.shuffle.memoryFraction&quot;, &quot;0.4&quot;); 修改shuffle管理器有哪些shuffle管理器HashShuffleManager：1.2.x版本前的默认选择SortShuffleManager：1.2.x版本之后的默认选择，会对每个task要处理的数据进行排序；同时，可以避免像 HashShuffleManager那么默认去创建多份磁盘文件，而是一个task只会写入一个磁盘文件，不同reduce task需要的的数据使用offset来进行划分。tungsten-sort（钨丝）：1.5.x之后的出现，和SortShuffleManager相似，但是它本事实现了一套内存管理机制，性能有了很大的提高，而且避免了shuffle过程中产生大量的OOM、GC等相关问题。 如何选择如果不需要排序，建议使用HashShuffleManager以提高性能如果需要排序，建议使用SortShuffleManager如果不需要排序，但是希望每个task输出的文件都合并到一个文件中，可以去调节bypassMergeThreshold这个阀值（默认为200），因为在合并文件的时候会进行排序，所以应该让该阀值大于reduce task数量。如果需要排序，而且版本在1.5.x或者更高，可以尝试使用 tungsten-sort 在项目中如何使用12345//设置spark shuffle manager (hash,sort,tungsten-sort)conf.set(&quot;spark.shuffle.manager&quot;, &quot;tungsten-sort&quot;); //设置文件合并的阀值conf.set(&quot;spark.shuffle.sort.bypassMergeThreshold&quot;, &quot;550&quot;); spark.reducer.maxSizeInFlight调节reduce端缓冲区大小避免OOM异常 为什么要调节reduce端缓冲区大小 对于map端不断产生的数据，reduce端会不断拉取一部分数据放入到缓冲区，进行聚合处理； 当map端数据特别大时，reduce端的task拉取数据是可能全部的缓冲区都满了，此时进行reduce聚合处理时创建大量的对象，导致OOM异常； 如何调节reduce端缓冲区大小 当由于以上的原型导致OOM异常出现是，可以通过减小reduce端缓冲区大小来避免OOM异常的出现 但是如果在内存充足的情况下，可以适当增大reduce端缓冲区大小，从而减少reduce端拉取数据的次数，提供性能。12//调节reduce端缓存的大小(默认48M)conf.set(&quot;spark.reducer.maxSizeInFlight&quot;, &quot;24&quot;); 解决JVM GC导致的shuffle文件拉取失败问题描述 下一个stage的task去拉取上一个stage的task的输出文件时，如果正好上一个stage正处在full gc的情况下（所有线程后停止运行），它们之间是通过netty进行通信的，就会出现很长时间拉取不到数据，此时就会报shuffle file not found的错误；但是下一个stage又重新提交task就不会出现问题了。 如何解决 调节最大尝试拉取次数：spark.shuffle.io.maxRetries 默认为3次 调节每次拉取最大的等待时长：spark.shuffle.io.retryWait 默认为5秒12345//调节拉取文件的最大尝试次数(默认3次)conf.set(&quot;spark.shuffle.io.maxRetries&quot;, &quot;60&quot;); //调节每次拉取数据时最大等待时长(默认为5s)conf.set(&quot;spark.shuffle.io.retryWait&quot;, &quot;5s&quot;); yarn-cluster模式的JVM内存溢出无法执行的问题 问题描述 有些spark作业，在yarn-client模式下是可以运行的，但在yarn-cluster模式下，会报出JVM的PermGen(永久代)的内存溢出，OOM. 出现以上原因是：yarn-client模式下，driver运行在本地机器上，spark使用的JVM的PermGen的配置，是本地的默认配置128M； 但在yarn-cluster模式下，driver运行在集群的某个节点上，spark使用的JVM的PermGen是没有经过默认配置的，默认是82M，故有时会出现PermGen Out of Memory error log. 如何处理在spark-submit脚本中设置PermGen--conf spark.driver.extraJavaOptions=&quot;-XX:PermSize=128M -XX:MaxPermSize=256M&quot;(最小128M，最大256M) 如果使用spark sql，sql中使用大量的or语句，可能会报出jvm stack overflow,jvm栈内存溢出，此时可以把复杂的sql简化为多个简单的sql进行处理即可。 checkpoint的使用checkpoint的作用 默认持久化的Rdd会保存到内存或磁盘中，下次使用该Rdd时直接冲缓存中获取，不需要重新计算；如果内存或者磁盘中文件丢失，再次使用该Rdd时需要重新进行。 如果将持久化的Rdd进行checkpoint处理，会把内存写入到hdfs文件系统中，此时如果再次使用持久化的Rdd，但文件丢失后，会从hdfs中获取Rdd并重新进行缓存。 如何使用 首先设置checkpoint目录12//设置checkpoint目录javaSparkContext.checkpointFile(&quot;hdfs://hadoop-senior.ibeifeng.com:8020/user/yanglin/spark/checkpoint/UserVisitSessionAnalyzeSpark&quot;); 将缓存后的Rdd进行checkpoint处理12//将缓存后的Rdd进行checkpointsessionRowPairRdd.checkpoint(); 参考：https://blog.csdn.net/stark_summer/article/details/42981201https://www.cnblogs.com/lifeone/p/6428885.htmlhttps://www.cnblogs.com/lifeone/p/6434840.htmlhttps://www.jianshu.com/p/da7d5edfb61f]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark jvm优化]]></title>
    <url>%2F2019%2F02%2F14%2Fspark-jvm%E4%BC%98%E5%8C%96%E7%82%B9%2F</url>
    <content type="text"><![CDATA[关于JVM内存的深入知识在这里不赘述，请大家自行对相关知识进行补充。好，说回Spark，运行Spark作业的时候，JVM对会对Spark作业产生什么影响呢？答案很简单，如果数据量过大，一定会导致JVM内存不足。在Spark作业运行时，会创建出来大量的对象，每一次将对象放入JVM时，首先将创建的对象都放入到eden区域和其中一个survivor区域中；当eden区域和一个survivor区域放满了以后，这个时候会触发minor gc，把不再使用的对象全部清除，而剩余的对象放入另外一个servivor区域中。JVM中默认的eden，survivor1，survivor2的内存占比为8:1:1。当存活的对象在一个servivor中放不下的时候，就会将这些对象移动到老年代。如果JVM的内存不够大的话，就会频繁的触发minor gc，这样会导致一些短生命周期的对象进入到老年代，老年代的对象不断的囤积，最终触发full gc。一次full gc会使得所有其他程序暂停很长时间。最终严重影响我们的Spark的性能和运行速度。 降低cache操作的内存占比spark中，堆内存又被划分成了两块儿，一块儿是专门用来给RDD的cache、persist操作进行RDD数据缓存用的；另外一块儿，就是我们刚才所说的，用来给spark算子函数的运行使用的，存放函数中自己创建的对象。默认情况下，给RDD cache操作的内存占比是0.6，即60%的内存都给了cache操作了。但是问题是，如果某些情况下cache占用的内存并不需要占用那么大，这个时候可以将其内存占比适当降低。怎么判断在什么时候调整RDD cache的内存占用比呢？其实通过Spark监控平台就可以看到Spark作业的运行情况了，如果发现task频繁的gc，就可以去调整cache的内存占用比了。通过SparkConf.set(&quot;spark.storage.memoryFraction&quot;,&quot;0.3&quot;)来设定。0.6 -&gt; 0.5 -&gt; 0.4 -&gt; 0.2大家可以自己去调，然后观察spark作业的运行统计针对该情况，大家可以 在 spark webui观察。yarn 去运行的话，那么就通过yarn的界面，去查看你的spark作业的 运行统计。很简单， 大家一层一层点击进去就好。 可以看到每个stage 的运行情况。 包括每个task的运行时间统计。gc时间。 如果发现gc太贫乏。时间太长，那么就可以适当调整这个比例。 降低cache操作的内存占比，大不了用persist 操作。选择一部分缓存的rdd数据写入磁盘，或者序列化的的方式。配合kryo序列化类。减少rdd缓存的内存占用，降低cache操作内存占比。 对应的 算子函数的内存占比，就提升了。这个时候，可能就减少minor gc 的频率，同时减少full gc的频率，对性能的 提升 有一定帮助的。 堆外内存和连接等待时长的调整其实这两个参数主要是为了解决一些Spark作业运行时候出现的一些错误信息而进行调整的 堆外内存a) 问题提出有时候，如果你的spark作业处理的数据量特别特别大，几亿数据量；然后spark作业一运行就会出现类似shuffle file cannot find，executor、task lost，out of memory（内存溢出）等这样的错误。这是因为可能是说executor的堆外内存不太够用，导致executor在运行的过程中，可能会内存溢出；然后可能导致后续的stage的task在运行的时候，可能要从一些executor中去拉取shuffle map output文件，但是executor可能已经挂掉了，关联的blockmanager也没有了；所以可能会报shuffle output file not found；resubmitting task；executor lost 这样的错误；最终导致spark作业彻底崩溃。 上述情况下，就可以去考虑调节一下executor的堆外内存。也许就可以避免报错；此外，有时，堆外内存调节的比较大的时候，对于性能来说，也会带来一定的提升。b) 解决方案：--conf spark.yarn.executor.memoryOverhead=2048在spark-submit脚本里面添加如上配置。默认情况下，这个堆外内存上限大概是300多M；我们通常项目中真正处理大数据的时候，这里都会出现问题导致spark作业反复崩溃无法运行；此时就会去调节这个参数，到至少1G或者更大的内存。通常这个参数调节上去以后，就会避免掉某些OOM的异常问题，同时呢，会让整体spark作业的性能，得到较大的提升。 连接等待时长的调整a) 问题提出：由于JVM内存过小，导致频繁的Minor gc，有时候更会触犯full gc，一旦出发full gc；此时所有程序暂停，导致无法建立网络连接；spark默认的网络连接的超时时长是60s；如果卡住60s都无法建立连接的话，那么就宣告失败了。碰到一种情况，有时候报错信息会出现一串类似file id not found，file lost的错误。这种情况下，很有可能是task需要处理的那份数据的executor在正在进行gc。所以拉取数据的时候，建立不了连接。然后超过默认60s以后，直接宣告失败。几次都拉取不到数据的话，可能会导致spark作业的崩溃。也可能会导致DAGScheduler，反复提交几次stage。TaskScheduler，反复提交几次task。大大延长我们的spark作业的运行时间。b) 解决方案：--conf spark.core.connection.ack.wait.timeout=300在spark-submit脚本中添加如上参数，调节这个值比较大以后，通常来说，可以避免部分的偶尔出现的某某文件拉取失败，某某文件lost掉的错误。 查看gcspark-env.sh: JAVA_OPTS=” -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps” https://www.cnblogs.com/jcchoiling/p/6494652.html参考:https://www.jianshu.com/p/e4557bf9186bhttps://www.cnblogs.com/lifeone/p/6434356.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDD]]></title>
    <url>%2F2019%2F02%2F14%2FRDD%2F</url>
    <content type="text"><![CDATA[特点Resillient Distributed Dataset，即弹性分布式数据集RDD的内部属性 通过RDD的内部属性，用户可以获取相应的元数据信息。通过这些信息可以支持更复杂的算法或优化。 1）分区列表：通过分区列表可以找到一个RDD中包含的所有分区及其所在地址。 2）计算每个分片的函数：通过函数可以对每个数据块进行RDD需要进行的用户自定义函数运算。 3）对父RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖。 4）可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。 5）可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。(存储的是一个表，可以将处理的分区“本地化”) 分区— partitions计算函数— computer(p,context)依赖— dependencies()分区策略(Pair RDD)– partitioner()本地性策略— preferredLocations(p) 12345678910//只计算一次 protected def getPartitions: Array[Partition] //对一个分片进行计算，得出一个可遍历的结果 def compute(split: Partition, context: TaskContext): Iterator[T] //只计算一次，计算RDD对父RDD的依赖 protected def getDependencies: Seq[Dependency[_]] = deps //可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce @transient val partitioner: Option[Partitioner] = None //可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置 protected def getPreferredLocations(split: Partition): Seq[String] = Nil 1）RDD的特点 1）创建：只能通过转换 ( transformation ，如map/filter/groupBy/join 等，区别于动作 action) 从两种数据源中创建 RDD 1 ）稳定存储中的数据； 2 ）其他 RDD。 2）只读：状态不可变，不能修改。 3）分区：支持使 RDD 中的元素根据那个 key 来分区 ( partitioning ) ，保存到多个结点上。还原时只会重新计算丢失分区的数据，而不会影响整个系统。 4）路径：在 RDD 中叫世族或血统 ( lineage ) ，即 RDD 有充足的信息关于它是如何从其他 RDD 产生而来的。 5）持久化：支持将会被重用的 RDD 缓存 ( 如 in-memory 或溢出到磁盘 )。 6）延迟计算： Spark 也会延迟计算 RDD ，使其能够将转换管道化 (pipeline transformation)。 7）操作：丰富的转换（transformation）和动作 ( action ) ， count/reduce/collect/save 等。 执行了多少次transformation操作，RDD都不会真正执行运算（记录lineage），只有当action操作被执行时，运算才会触发。https://blog.csdn.net/guohecang/article/details/51736572 本地性策略一直在用spark但是没有考虑过优化的看过来。分布式计算系统的精粹在于移动计算而非移动数据，但是在实际的计算过程中，总存在着移动数据的情况，除非是在集群的所有节点上都保存数据的副本。移动数据，将数据从一个节点移动到另一个节点进行计算，不但消耗了网络IO，也消耗了磁盘IO，降低了整个计算的效率。为了提高数据的本地性，除了优化算法（也就是修改spark内存，难度有点高），就是合理设置数据的副本。设置数据的副本，这需要通过配置参数并长期观察运行状态才能获取的一个经验值。Spark中的数据本地性有三种： PROCESS_LOCAL：进程本地化，代码和数据在同一个进程中，也就是在同一个executor中；计算数据的task由executor执行，数据在executor的BlockManager中；性能最好NODE_LOCAL：节点本地化，代码和数据在同一个节点中；比如说，数据作为一个HDFS block块，就在节点上，而task在节点上某个executor中运行；或者是，数据和task在一个节点上的不同executor中；数据需要在进程间进行传输NO_PREF：对于task来说，数据从哪里获取都一样，没有好坏之分,比如从数据库中获取数据RACK_LOCAL：机架本地化，数据和task在一个机架的两个节点上；数据需要通过网络在节点之间进行传输ANY：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差 通常读取数据PROCESS_LOCAL&gt;NODE_LOCAL&gt;ANY，尽量使数据以PROCESS_LOCAL或NODE_LOCAL方式读取。其中PROCESS_LOCAL还和cache有关，如果RDD经常用的话将该RDD cache到内存中，注意，由于cache是lazy的，所以必须通过一个action的触发，才能真正的将该RDD cache到内存中。 如何配置Locality呢？可以统一采用spark.locality.wait来设置（例如设置5000ms）。当然可以分别设置spark.locality.wait.process、spark.locality.wait.node、spark.locality.wait.rack等；一般的具体设置是Locality优先级越高则可以设置越高的等待超时时间。12new SparkConf() .set(&quot;spark.locality.wait&quot;, &quot;10&quot;) 如果数据是PROCESS_LOCAL，但是此时并没有空闲的Core来运行我们的Task，此时Task就要等待，例如等待3000ms，3000ms内如果能够运行待运行的Task则直接运行，如果超过了3000ms，此时数据本地性就要退而求其次采用NODE_LOCAL的方式。同样的道理，NODE_LOCAL也会有等待的超时时间，以此类推…对于ANY的情况，默认情况状态下性能会非常低下，此时强烈建议使用Tachyon，例如在百度云上，为了确保计算速度，就在计算集群和存储集群之间加入了Tachyon,通过Tachyon来从远程抓取数据，而Spark基于Tachyon来进行计算，这就更好的满足了数据本地性。 数据本地性任务分配的源码在 taskSetManager.scala 。https://blog.csdn.net/oitebody/article/details/80137721数据本地性的副作用https://www.jianshu.com/p/a1d0824053d8 参考:https://www.cnblogs.com/yourarebest/p/5122372.htmlhttps://www.cnblogs.com/jackie2016/p/5643100.htmlhttp://blog.sina.com.cn/s/blog_9ca9623b0102w8pc.htmlhttps://bit1129.iteye.com/blog/2186084https://www.jianshu.com/p/a1d0824053d8https://blog.csdn.net/u013939918/article/details/60897191]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark算子]]></title>
    <url>%2F2019%2F02%2F13%2Fspark%E7%AE%97%E5%AD%90%2F</url>
    <content type="text"><![CDATA[sortByKey该函数会对原始RDD中的数据进行Shuffle操作，从而实现排序。这个函数中,传入两个參数,ascending表示是升序还是降序,默认true表示升序.第二个參数是运行排序使用的partition的个数,默认是当前RDD的partition个数. groupByKey、reduceByKey与sortByKeygroupByKey把相同的key的数据分组到一个集合序列当中： [(“hello”,1), (“world”,1), (“hello”,1), (“fly”,1), (“hello”,1), (“world”,1)] –&gt; [(“hello”,(1,1,1)),(“word”,(1,1)),(“fly”,(1))] reduceByKey把相同的key的数据聚合到一起并进行相应的计算： [(“hello”,1), (“world”,1), (“hello”,1), (“fly”,1), (“hello”,1), (“world”,1)] add–&gt; [(“hello”,3）,(“word”,2),(“fly”,1)] sortByKey按key的大小排序，默认为升序排序： [(3,”hello”）,(2,”word”),(1,”fly”)] –&gt; [(1,”fly”)，(2,”word”)，(3,”hello”)] 123456789101112131415161718192021222324252627282930313233343536373839 from pyspark import SparkConf, SparkContextfrom operator import addconf = SparkConf()sc = SparkContext(conf=conf)def func_by_key(): data = [ &quot;hello world&quot;, &quot;hello fly&quot;, &quot;hello world&quot;, &quot;hello fly&quot;, &quot;hello fly&quot;, &quot;hello fly&quot; ] data_rdd = sc.parallelize(data) word_rdd = data_rdd.flatMap(lambda s: s.split(&quot; &quot;)).map(lambda x: (x, 1)) group_by_key_rdd = word_rdd.groupByKey() print(&quot;groupByKey:&#123;&#125;&quot;.format(group_by_key_rdd.mapValues(list).collect())) print(&quot;groupByKey mapValues(len):&#123;&#125;&quot;.format( group_by_key_rdd.mapValues(len).collect() )) reduce_by_key_rdd = word_rdd.reduceByKey(add) print(&quot;reduceByKey:&#123;&#125;&quot;.format(reduce_by_key_rdd.collect())) print(&quot;sortByKey:&#123;&#125;&quot;.format(reduce_by_key_rdd.map( lambda x: (x[1], x[0]) ).sortByKey().map(lambda x: (x[0], x[1])).collect()))func_by_key()sc.stop()&quot;&quot;&quot;result：groupByKey:[(&apos;fly&apos;, [1, 1, 1, 1]), (&apos;world&apos;, [1, 1]), (&apos;hello&apos;, [1, 1, 1, 1, 1, 1])]groupByKey mapValues(len):[(&apos;fly&apos;, 4), (&apos;world&apos;, 2), (&apos;hello&apos;, 6)]reduceByKey:[(&apos;fly&apos;, 4), (&apos;world&apos;, 2), (&apos;hello&apos;, 6)]sortByKey:[(2, &apos;world&apos;), (4, &apos;fly&apos;), (6, &apos;hello&apos;)]&quot;&quot;&quot; 从结果可以看出，groupByKey对分组后的每个key的value做mapValues(len)后的结果与reduceByKey的结果一致，即：如果分组后要对每一个key所对应的值进行操作则应直接用reduceByKey；sortByKey是按key排序，如果要对value排序，可以交换key与value的位置，再排序。https://www.cnblogs.com/FG123/p/9746830.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive 排序与分发的各种By]]></title>
    <url>%2F2019%2F02%2F13%2Fhive-%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[排序与分发的各种By与传统关系型数据库最大的区别就是处理数据的能力这种能力最大的体现就是排序与分发的原理order by 是全局排序，只有一个reduce，数据量多时速度慢sort by 是随机分发到一个reduce然后reduce内部排序，一般不会单独使用;，也就是说保证了局部有序（每个reducer出来的数据是有序的，但是不能保证所有的数据是有序的，除非只有一个reducer）好处是：执行了局部排序之后可以为接下去的全局排序提高不少的效率（其实就是做一次归并排序就可以做到全局排序了）distribute by 是根据 distribute by 的字段把相应的记录分发到那个reduce cluster by是distribute by + sort by的简写 group by是对检索结果的保留行进行单纯分组，一般总爱和聚合函数一块用例如AVG（），COUNT（），max（），main（）等一块用。distribute by不可以。 partition by虽然也具有分组功能，但同时也具有其他的功能,一般和窗口函数一起使用。]]></content>
  </entry>
  <entry>
    <title><![CDATA[beeline]]></title>
    <url>%2F2019%2F02%2F13%2Fbeeline%2F</url>
    <content type="text"><![CDATA[1beeline -u &quot;jdbc:hive2://xxxxx;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=datahiveserver2_zk&quot; -n username -p &quot;password&quot; --color=true --silent=false --showHeader=false --outputformat=csv 格式参数:–outputformat[table/vertical/csv/tsv/dsv/csv2/tsv2]为了便于输出结果解析，建议把输出格式设置成普通文本，否则输出格式默认为table参考：https://my.oschina.net/guol/blog/875875]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[层次桥接表]]></title>
    <url>%2F2019%2F01%2F29%2F%E5%B1%82%E6%AC%A1%E6%A1%A5%E6%8E%A5%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[初版： 扁平化: 层次桥接表 下钻 上钻er图：]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支架表]]></title>
    <url>%2F2019%2F01%2F28%2F%E6%94%AF%E6%9E%B6%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[支架表看起来和雪花模式有些类似。采用支架表的原因是：1、支架表中的维度没有基本维度表中的分析价值大，使用也不是很频繁。2、其次如果有维度在基本维度中有很大的冗余(基本维度记录中都含有重复字段)，那么将不常用的放到支架表中可以节省空间。备注：可以使用支架表，但是只可偶尔为之，不要常用。如果您的设计包含大量的支架表，就应该拉响警报。您可能会陷入过度规范化设计的麻烦之中。]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里onedata]]></title>
    <url>%2F2019%2F01%2F28%2F%E9%98%BF%E9%87%8Conedata%2F</url>
    <content type="text"><![CDATA[OneData体系分为：1、数据规范定义体系2、数据模型规范设计3、ETL规范研发以及支撑整个体系从方法到实施的工具体系。 落地实现数据规范定义将此前个性化的数据指标进行规范定义，抽象成：原子指标、时间周期、其他修饰词等三个要素。例如，以往业务方提出的需求是：最近7天的成交。而实际上，这个指标在规范定义中，应该结构化分解成为： 原子指标（支付订单金额 ）+修饰词-时间周期（最近7天）+修饰词-卖家类型（淘宝） 上文只有抽象指标的方法，应该还包括字段规范，表名规范派生指标的定义也很重要，因为数据运营人员经常会查最近7，最近一月的指标，每次都从基础事实表中，查询的量会很大。 数据模型架构 理解为数据仓库的分层设计 将数据分为ODS（操作数据）层、CDM（公共维度模型）层、ADS（应用数据）层。 其中：ODS层主要功能 这里先介绍一下阿里云数加大数据计算服务MaxCompute，是一种快速、完全托管的TB/PB级数据仓库解决方案，适用于多种数据处理场景，如日志分析，数据仓库，机器学习，个性化推荐和生物信息等。 同步：结构化数据增量或全量同步到数加MaxCompute（原ODPS）；结构化：非结构化(日志)结构化处理并存储到MaxCompute（原ODPS）；累积历史、清洗：根据数据业务需求及稽核和审计要求保存历史数据、数据清洗； CDM层主要功能 CDM层又细分为DWD层和DWS层，分别是明细宽表层和公共汇总数据层，采取维度模型方法基础，更多采用一些维度退化手法，减少事实表和维度表的关联，容易维度到事实表强化明细事实表的易用性；同时在汇总数据层，加强指标的维度退化，采取更多宽表化的手段构建公共指标数据层，提升公共指标的复用性，减少重复的加工。ADS层主要功能 个性化指标加工：不公用性；复杂性（指数型、比值型、排名型指标）基于应用的数据组装：大宽表集市、横表转纵表、趋势指标串 其模型架构图如上图，阿里通过构建全域的公共层数据，极大的控制了数据规模的增长趋势，同时在整体的数据研发效率，成本节约、性能改进方面都有不错的结果。 研发流程和工具落地实现将OneData体系贯穿于整个研发流程的每个环节中，并通过研发工具来进行保障。 实施效果1、数据标准统一：数据指标口径一致，各种场景下看到的数据一致性得到保障2、支撑多个业务，极大扩展性：服务了集团内部45个BU的业务，满足不同业务的个性化需求3、统一数据服务：建立了统一的数据服务层，其中离线数据日均调用次数超过22亿；实时数据调用日均超过11亿4、计算、存储成本：指标口径复用性强，将原本30000多个指标精简到3000个；模型分层、粒度清晰，数据表从之前的25000张精简到不超过3000张。5、研发成本：通过数据分域、模型分层，强调工程师之间的分工和协作，不再需要从头到尾每个细节都了解一遍，节省了工程师的时间和精力。参考：https://yq.aliyun.com/articles/67011 明细宽表层-面向业务建模过程： &emsp;&emsp;事务型事实宽表，周期性快照事实宽表，累计快照事实宽表公共汇总款表层-面向分析主题建模: &emsp;&emsp;流量，订单，商品，用户 上面两层都是基于公共维度表的基础上 维度退化手法指标维度退化大宽表集市、横表转纵表、趋势指标串个性化指标加工：不公用性；复杂性（指数型、比值型、排名型指标）]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive使用和优化]]></title>
    <url>%2F2019%2F01%2F28%2Fhive%E4%BD%BF%E7%94%A8%E5%92%8C%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[使用complex type array 和 map 类型创建表时指定类型和分隔符 create table test1(f1 array,f2 map&lt;string,int&gt;)ROW FORMAT DELIMITEDCOLLECTION ITEMS TERMINATED BY ‘|’MAP KEYS TERMINATED BY ‘:’;文件中按照表定义的分隔符来分割，字段的分隔符默认为^A，可以在建表时通过 FIELDS TERMINATED BY ‘^A’ 指定 cat /home/hadoop/hivetest1.txt 1|2|3^Aa:1|b:211|12|13^Aaa:1|bb:2把文件 load 到表中 load data local inpath ‘/home/hadoop/hivetest1.txt’ into table test1;在查询中使用array 和 map 类型 select f1[0],f2[‘aa’] from test1;select * from test1 where f1[0]=11 and f2[‘aa’]=1;处理json字符串 create table test(json_str string);test表的json_str字段的内容为： {“a”:”1”,”b”:”2”,”c”:[“aa”,”11”,{“aaa”:”111”,”bbb”:”222”}]} select get_json_object(json_str,’$.b’) from test;select get_json_object(json_str,’$.c[0]’) from test;select get_json_object(json_str,’$.c[2].aaa’) from test;动态分区 set hive.exec.dynamic.partition=true;set hive.exec.dynamic.partition.mode=nonstrict;另外，如果默认值不够大，根据需要调大以下三个参数，否则会报错 set hive.exec.max.dynamic.partitions=2000set hive.exec.max.dynamic.partitions.pernode=1000set hive.exec.max.created.files=110000;这样，从另外一个表把数据导入到目标表时，就会自动按照指定字段分区。例如： insert overwrite table A partition(reportdate) select * from B;表A要先创建，并且以reportdate作为分区的字段。在上面sql中不用指定reportdate的值（如果是非动态分区则需要指定）。表B最后一个字段作为分区的字段，会自动根据最后一个字段的值自动分区。 map join 优化将hive.auto.convert.join设置为true set hive.auto.convert.join=true;设置进行mapjoin的小表的阈值（即当小表的大小 少于 一定该阈值时 hive 会做 mapjoin 优化，如果不设置，则采用默认值） set hive.mapjoin.smalltable.filesize=25000000;hive.mapjoin.bucket.cache.size=100每一个key有多少个value值缓存在内存中hive.mapjoin.cache.numrows=25000有多少行cache在内存中 insert into/overwrite多表插入的优化通过对原表扫描一遍，插入到不同的目的表中 from table insert overwrite table dest1 select where col=condition ; insert overwrite table dest2 select where col=condition ; insert overwrite table dest3 select * where col=condition ;对limit语句的优化 hive.limit.optimize.enable=falsehive.limit.row.max.size=100000hive.limit.optimize.limit.file=10hive.limit.optimize.fetch.max=50000对reduce个数的设置hive默认是根据输入的大小来设置，每个reducer处理的数据量是1G。如果有10G的输入数据，则hive自动生成10个reducer。默认情况下一个reducer处理1G的数据，这样一个reducer处理的数据量太大，可以改小一些。 hiive.exec.reducers.bytes.per.reducer=1Ghive.exec.reducers.max=999mapred.reduce.tasks数据倾斜优化有编译器的优化和运行期的优化 hive.groupby.skewindata=falsehive.optimize.skewjoin.compiletime=falsehive.optimize.skewjoin=falsehive.skewjoin.key=100000文件压缩对中间结果的压缩和最终结果的压缩 hive.exec.compress.output=falsehive.exec.compress.intermediate=false并行执行一个hive sql语句编译成多个MR作业，没有依赖关系的作业可以并行执行。 hive.exec.parallel=falsehive.exec.parallel.thread.number=8合并结果的小文件对于hive的结果中是小文件的，会再起一个MR作业合并小文件。 hive.merge.mapfiles=truehive.merge.mapredfiles=falsehive.merge.size.per.task=256000000hive.merge.smallfiles.avgsize=16000000推断执行 hive.mapred.reduce.tasks.speculative.execution=true设置reduce个数显示设置reduce的个数，或者每个reduce处理的数据的大小（默认1G的值很多时候有些大，可以设置小一些，同时reduce的内存也要相应小一些，提高并行度）。 mapred.reduce.tasks=-1hive.exec.reducers.bytes.per.reducer=1000000000（1G）hive.exec.reducers.max=999map端聚合 hive.map.aggr=truegroup by语句时现在map聚合一次，减少传输到reduce的数据，就是mapreduce的combiner。默认是打开的。 列剪裁列剪裁优化，只对需要的列进行处理，忽略其他的列，减少数据的处理量。默认是打开的。 hive.optimize.cp=true本地模式一些sql处理的数据量比较少，或者计算量比较少，可以在本地运行而不是MR作业运行，这样性能会更好些，也节约hadoop集群的资源。 hive.exec.mode.local.auto=falsehive.fetch.task.conversion=minimal测试模式 hive.test.mode=falsehive.test.mode.prefix=test_hive.test.mode.samplefreq=32hive.test.mode.nosamplelist=table1,table2,table3用于测试，通过采样减少输入的数据，结果表前面加前缀“test_” 严格模式 hive.mapred.mode=nonstrict通过严格模式，使一些不严格的用法不通过，防止潜在的错误。No partition being picked up for a query.Comparing bigints and strings.Comparing bigints and doubles.Orderby without limit. jvm重用JVM重用是hadoop调优参数的内容，对hive的性能具有非常大的 影响，特别是对于很难避免小文件的场景或者task特别多的场景，这类场景大多数执行时间都很短。hadoop默认配置是使用派生JVM来执行map和 reduce任务的，这是jvm的启动过程可能会造成相当大的开销，尤其是执行的job包含有成千上万个task任务的情况。 JVM重用可以使得JVM实例在同一个JOB中重新使用N次，N的值可以在Hadoop的mapre-site.xml文件中进行设置 1mapred.job.reuse.jvm.num.tasks 也可在hive的执行设置：1set mapred.job.reuse.jvm.num.tasks=10; JVM的一个缺点是，开启JVM重用将会一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡“的job中有几个 reduce task 执行的时间要比其他reduce task消耗的时间多得多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。 hive参数优化之默认启用本地模式启动hive本地模式参数，一般建议将其设置为true，即时刻启用：12hive (chavin)&gt; set hive.exec.mode.local.auto; hive.exec.mode.local.auto=false 推测执行相关配置1234567891011hive (default)&gt; set mapred.map.tasks.speculative.execution;mapred.map.tasks.speculative.execution=truehive (default)&gt; set mapred.reduce.tasks.speculative.execution;mapred.reduce.tasks.speculative.execution=truehive (default)&gt; set hive.mapred.reduce.tasks.speculative.execution;hive.mapred.reduce.tasks.speculative.execution=true 单个mapreduce中运行多个group by参数hive.multigroupby.singlemr控制师徒将查询中的多个group by组装到单个mapreduce任务中。如果启用这个优化，那么需要一组常用的group by键： 聚合优化：启用参数：hive.map.aggr=true,默认开启 参数hive.fetch.task.conversion的调优：默认值：hive.fetch.task.conversion=minimal 建议值：set hive.fetch.task.conversion=more;参考：https://www.cnblogs.com/duanxingxing/p/4535842.html]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink vs spark]]></title>
    <url>%2F2019%2F01%2F28%2Fflink-vs-spark%2F</url>
    <content type="text"><![CDATA[概念在 spark 中， 批处理使用 RDD， 流处理使用 DStream（内部使用RDD实现）， 所有底层统一都使用 RDD的抽象实现在 flink 中， 批处理使用 Dataset， 流处理使用 DataStreams， 听起来类似 RDD 和 DStreams， 但其实不是， 不同点在于 1、flink 中的 Dataset 代表着执行计划， 而spark 的 RDD 仅仅是一个 java 对象，spark中的dataframes 才有执行计划， flink 和 spark 两者最基础的东西， Dataset 和 RDD， 是不同的。 一个是经过优化器优化的， 一个没有。 flink 中的 Dataset 类似 spark 中经过优化的 Dataframe 概念， spark 1.6 中引入的dataset（跟 flink 中的 Dataset 重名了，两者类似） 最终应该会代替RDD的抽象吧。 2、在 spark 中， DStream 和 Dataframe 等都是基于 RDD 的封装， 然而 flink 中的 Dataset 和 DataStream 则是独立实现的， 尽管两者间尽量保持相同的 API， 但是你很难统一起来， 至少没有 spark 中那样优雅， 这个大方向， flink 能不能做到就难说了。我们不能统一 DataSet 和 DataStreams， 尽管 flink 有和 spark 相同的抽象，但是内部实现是不同的。 内存管理spark 1.5 之前， spark 一直都是使用 java jvm 堆来保存对象， 虽然有利于启动项目， 但是经常会产生 OOM 和 gc 问题， 所以 1.5 开始， spark 开始引入 自己管理内存的 tungsten 项目。 Flink 从第一天起就自己管理内存， spark 就是从这学的吧， 不仅保存数据使用 binary data， 而且可以直接在binary data 上进行操作。 spark 1.5 开始也可以直接在binary data 进行 dataframe API 提供的操作了。 自己管理内存， 直接使用分配的binary data而不是JVM objects 可以得到更高的性能 和 更好的资源利用。 流式处理在 spark 的眼里， streaming 是特殊的 batch， 在 flink 眼里， batch 是特殊的 streaming， 主要的区别在于1、实时 vs 准实时 ，flink 提供 event 事件级别的延迟， 可以认为是实时的， 类似于 storm 的模型， 而 spark 中， 是微批处理， 不能提供事件级别的延迟， 我们可以称之为准实时。2、flink 则可以灵活的支持窗口， 支持带有事件时间的窗口（Window）操作是flink 的亮点， 你可以选择使用处理时间还是事件时间进行窗口操作， 这种灵活性是 spark 所不如的。3、spark 来自于 Map/Reduce 时代， 崇尚 运算追着数据走， 数据可以是内存中的数组， 也可以是磁盘中的文件， 可以进行很好的容错。Flink 的模型中， 数据是追着运算走的， 算子位于节点上， 数据从中流过， 类似于 akka-streams 中的概念。]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[状态计算]]></title>
    <url>%2F2019%2F01%2F28%2F%E7%8A%B6%E6%80%81%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[spark流计算的数据是以窗口的形式，源源不断的流过来的。如果每个窗口之间的数据都有联系的话，那么就需要对前一个窗口的数据做状态管理。spark有提供了两种模型来达到这样的功能，一个是updateStateByKey，另一个是mapWithState ，后者属于Spark1.6之后的版本特性，性能是前者的数十倍。基本的wordcount123456789101112131415161718192021222324252627282930313233343536package com.scala.testimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;object WC &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() //程序在运行时receiver会独占一个线程,所以streaming程序至少要两个线程,防止starvation scenario conf.setAppName(&quot;WordCount&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) //所有流功能的主要入口 val smc : StreamingContext = new StreamingContext(sc, Seconds(5)) //指定从TCP源数据流的离散流,接收到的每一行数据都是一行文本 val lines : ReceiverInputDStream[String] = smc.socketTextStream(&quot;localhost&quot;,6666) //将接收到的文本压平,转换,聚合 val dStream : DStream[(String, Int)] = lines.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_) dStream.print() // Spark Streaming 只有建立在启动时才会执行计算，在它已经开始之后，并没有真正地处理// -------------------------- //启动计算 smc.start(); //等待计算终止 smc.awaitTermination(); //true 会把内部的sparkcontext同时停止 //false 只会停止streamingcontext 不会停sparkcontext smc.stop(true); &#125;&#125; updateStateByKey1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.scala.testimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.util.parsing.json.JSON/** * * https://blog.csdn.net/cyony/article/details/79653357 * nc -lk 6666 * 样例数据 * &#123;&quot;name&quot;:&quot;cyony1&quot;,&quot;score&quot;:&quot;90&quot;,&quot;sex&quot;:&quot;1&quot;&#125; * * &#123;&quot;name&quot;:&quot;cyony2&quot;,&quot;score&quot;:&quot;76&quot;,&quot;sex&quot;:&quot;0&quot;&#125; * * updateStateByKey这种模型，每次窗口触发，都会将两个RDD执行cogroup操作，，非常的耗时。而且checkpoint dir也会很大 * */object WC_stateful &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() //程序在运行时receiver会独占一个线程,所以streaming程序至少要两个线程,防止starvation scenario conf.setAppName(&quot;WordCount&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) //所有流功能的主要入口 val smc : StreamingContext = new StreamingContext(sc, Seconds(5)) //定义checkpoint目录 smc.checkpoint(&quot;./wc_stateful&quot;) //指定从TCP源数据流的离散流,接收到的每一行数据都是一行文本 val lines : ReceiverInputDStream[String] = smc.socketTextStream(&quot;localhost&quot;,6666) //定义updateStateByKey更新函数 val updateFunc = (currentValue:Seq[Int],preValue:Option[Int]) =&gt; &#123; Some(currentValue.sum + preValue.getOrElse(0)) &#125; //将接收到的文本压平,转换,聚合 lines.map(JSON.parseFull(_).get.asInstanceOf[Map[String,String]]) .map(map =&gt; (map.get(&quot;sex&quot;).get.toInt,map.get(&quot;score&quot;).get.toInt)) .reduceByKey(_+_) .updateStateByKey(updateFunc) .print()// dStream.print() // Spark Streaming 只有建立在启动时才会执行计算，在它已经开始之后，并没有真正地处理// -------------------------- //启动计算 smc.start(); //等待计算终止 smc.awaitTermination(); //true 会把内部的sparkcontext同时停止 //false 只会停止streamingcontext 不会停sparkcontext smc.stop(true); &#125;&#125; mapWithState123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package com.scala.testimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.streaming.&#123;Seconds, State, StateSpec, StreamingContext&#125;import org.apache.spark.streaming.dstream.ReceiverInputDStreamimport scala.util.parsing.json.JSON/** * * https://blog.csdn.net/cyony/article/details/79653357 * nc -lk 6666 * 样例数据 * &#123;&quot;name&quot;:&quot;cyony1&quot;,&quot;score&quot;:&quot;90&quot;,&quot;sex&quot;:&quot;1&quot;&#125; * * &#123;&quot;name&quot;:&quot;cyony2&quot;,&quot;score&quot;:&quot;76&quot;,&quot;sex&quot;:&quot;0&quot;&#125; * * 如果当前窗口期没有新的数据过来，mapstate方式是根本不会触发状态更新操作的，但是updateState方式就会触发更新操作。 * 这个和他的模型原理有关，进一步佐证了updateState方式会每次都执行cogroup操作RDD，生成新的RDD。 * * https://www.jianshu.com/p/1463bc1d81b5 * https://blog.csdn.net/zangdaiyang1991/article/details/84099722 * http://www.cnblogs.com/DT-Spark/articles/5616560.html * */object WC_mapWithState &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() //程序在运行时receiver会独占一个线程,所以streaming程序至少要两个线程,防止starvation scenario conf.setAppName(&quot;WordCount&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) //所有流功能的主要入口 val smc : StreamingContext = new StreamingContext(sc, Seconds(5)) //定义checkpoint目录 smc.checkpoint(&quot;./wc_mapWithState&quot;) //指定从TCP源数据流的离散流,接收到的每一行数据都是一行文本 val lines : ReceiverInputDStream[String] = smc.socketTextStream(&quot;localhost&quot;,6666) //定义MapWithState更新函数 val mappingFun = (sex: Int, score: Option[Int], state: State[Int]) =&gt; &#123; val sum = score.getOrElse(0) + state.getOption().getOrElse(0) state.update(sum) (sex, sum) &#125; //将接收到的文本压平,转换,聚合 lines.map(JSON.parseFull(_).get.asInstanceOf[Map[String, String]]) .map(map =&gt; (map.get(&quot;sex&quot;).get.toInt, map.get(&quot;score&quot;).get.toInt)).reduceByKey(_ + _) .mapWithState(StateSpec.function(mappingFun)) .print() // dStream.print() // Spark Streaming 只有建立在启动时才会执行计算，在它已经开始之后，并没有真正地处理 // -------------------------- //启动计算 smc.start(); //等待计算终止 smc.awaitTermination(); //true 会把内部的sparkcontext同时停止 //false 只会停止streamingcontext 不会停sparkcontext smc.stop(true); &#125;&#125;]]></content>
      <categories>
        <category>spark编程</category>
      </categories>
      <tags>
        <tag>spark编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[幂等操作]]></title>
    <url>%2F2019%2F01%2F28%2F%E5%B9%82%E7%AD%89%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[最近很多人都在谈论幂等性，好吧，这回我也来聊聊这个话题，光看着俩字，一开始的确有点一头雾水，语文不好嘛，词太专业嘛，对吧 现如今我们的系统大多拆分为分布式SOA，或者微服务，一套系统中包含了多个子系统服务，而一个子系统服务往往会去调用另一个服务，而服务调用服务无非就是使用RPC通信或者restful，既然是通信，那么就有可能再服务器处理完毕后返回结果的时候挂掉，这个时候用户端发现很久没有反应，那么就会多次点击按钮，这样请求有多次，那么处理数据的结果是否要统一呢？那是肯定的！尤其再支付场景。幂等性：就是用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用。举个最简单的例子，那就是支付，用户购买商品使用约支付，支付扣款成功，但是返回结果的时候网络异常，此时钱已经扣了，用户再次点击按钮，此时会进行第二次扣款，返回结果成功，用户查询余额返发现多扣钱了，流水记录也变成了两条．．．在以前的单应用系统中，我们只需要把数据操作放入事务中即可，发生错误立即回滚，但是再响应客户端的时候也有可能出现网络中断或者异常等等。在增删改查4个操作中，尤为注意就是增加或者修改，查询对于结果是不会有改变的，删除只会进行一次，用户多次点击产生的结果一样修改在大多场景下结果一样增加在重复提交的场景下会出现那么如何设计接口才能做到幂等呢？ 方法一、单次支付请求，也就是直接支付了，不需要额外的数据库操作了，这个时候发起异步请求创建一个唯一的ticketId，就是门票，这张门票只能使用一次就作废，具体步骤如下：异步请求获取门票调用支付，传入门票根据门票ID查询此次操作是否存在，如果存在则表示该操作已经执行过，直接返回结果；如果不存在，支付扣款，保存结果返回结果到客户端如果步骤4通信失败，用户再次发起请求，那么最终结果还是一样的 方法二、分布式环境下各个服务相互调用这边就要举例我们的系统了，我们支付的时候先要扣款，然后更新订单，这个地方就涉及到了订单服务以及支付服务了。用户调用支付，扣款成功后，更新对应订单状态，然后再保存流水。而在这个地方就没必要使用门票ticketId了，因为会比较闲的麻烦（支付状态：未支付，已支付）步骤：1、查询订单支付状态2、如果已经支付，直接返回结果3、如果未支付，则支付扣款并且保存流水4、返回支付结果如果步骤4通信失败，用户再次发起请求，那么最终结果还是一样的对于做过支付的朋友，幂等，也可以称之为冲正，保证客户端与服务端的交易一致性，避免多次扣款。 最后来看一下我们的订单流程，虽然不是很复杂，但是最后在支付环境是一定要实现幂等性的 可以简单理解成upsert操作，有则更新，无则插入参考https://www.cnblogs.com/leechenxiang/p/6626629.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming如何应对 Exactly once 语义（kafka）]]></title>
    <url>%2F2019%2F01%2F28%2FSpark-Streaming%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9-Exactly-once-%E8%AF%AD%E4%B9%89%2F</url>
    <content type="text"><![CDATA[Spark Streaming（以下简写SS）Exactly once语义（以下简写EO） 首先EO表示可以精准控制到某一条记录，但由于SS是基于rdd和batch的，所以SS的EO可以认为是针对一个批次的的精准控制(控制各个批次间是否重复和漏读)。涉及到三部分都保证 exactly once 的语义。1、(数据源)上游是否EO到SS2、(数据处理)SS作为整体是否保证了EO3、(数据存储)SS是否将数据EO地写出到了下游 涉及到 上游是否EO到SS对于 接收数据，主要取决于上游数据源的特性。例如，从 HDFS 这类支持容错的文件系统中读取文件，能够直接支持 Exactly-once 语义。如果上游消息系统支持 ACK（如RabbitMQ），我们就可以结合 Spark 的 Write Ahead Log 特性来实现 At-least-once 语义。对于非可靠的数据接收器（如 socketTextStream），当 Worker 或 Driver 节点发生故障时就会产生数据丢失，提供的语义也是未知的。而 Kafka 消息系统是基于偏移量（Offset）的，它的 Direct API 可以提供 Exactly-once 语义。 官方在创建 DirectKafkaInputStream(Kafka direct api)时只需要输入消费 Kafka 的 From Offset，然后其自行获取本次消费的 End Offset，也就是当前最新的 Offset。保存的 Offset 是本批次的 End Offset，下次消费从上次的 End Offset 开始消费。 当程序宕机或重启任务后，这其中存在一些问题。如果在数据处理完成前存储 Offset，则可能存在作业处理数据失败与作业宕机等情况，重启后会无法追溯上次处理的数据导致数据出现丢失。如果在数据处理完成后存储 Offset，但是存储 Offset 过程中发生失败或作业宕机等情况，则在重启后会重复消费上次已经消费过的数据。 而且此时又无法保证重启后消费的数据与宕机前的数据量相同数据相当，这又会引入另外一个问题，如果是基于聚合统计指标作更新操作，这会带来无法判断上次数据是否已经更新成功。 参考文章中给出的解决方案是，保证在创建 DirectKafkaInputStream 可以同时输入 From Offset 与 End Offset，并且我们在存储 Kafka Offset 的时候保存了每个批次的起始Offset 与结束 Offset。这样的设计使得后面用户在后面对于第一个批次的数据处理非常灵活可变：1、如果用户直接忽略第一个批次的数据，那此时保证的是 at most once 的语义，因为我们无法获知重启前的最后一个批次数据操作是否有成功完成。2、如果用户依照原有逻辑处理第一个批次的数据，不对其做去重操作，那此时保证的是 at least once 的语义，最终结果中可能存在重复数据；3、最后如果用户想要实现 exactly once，muise spark core 提供了根据topic、partition 与 offset 生成 UID 的功能。只要确保两个批次消费的 Offset 相同，则最终生成的 UID 也相同，用户可以根据此 UID 作为判断上个批次数据是否有存储成功的依据。下面简单的给出了重启后第一个批次操作的行为。 参考:http://www.10tiao.com/html/522/201809/2651425155/1.html实际上如果是聚合操作，完全可以引入状态计算，而不需要修改源码。或者将Offset，存储到redis中，每次存redis中读取,见其他文章https://blog.csdn.net/qq_32252917/article/details/78827126 。本文只对EO做讨论，状态计算在其他文章做介绍。EO只要明确保证能拿到上次成功结束的offset就可以了(保证数据零丢失)，至于后面是否会被重复计算部分，可以根据业务做不同的处理(根据输出做幂等设计)。 控制offset实际上是解决数据丢失如下的主要场景：SS在使用Receiver收到数据时(非Kafka direct api)，通过Driver的调度，Executor开始计算数据的时候如果Driver突然奔溃（导致Executor会被Kill掉），此时Executor会被Kill掉，那么Executor中的数据就会丢失，此时就必须通过例如WAL机制让所有的数据通过类似HDFS的方式进行安全性容错处理，从而解决Executor被Kill掉后导致数据丢失可以通过WAL机制恢复回来。此时数据可以零丢失，但并不能保证Exactly Once，如果Receiver接收且保存起来后没来得及更新updateOffsets时，就会导致数据被重复处理。所以本文讨论的EO是用户自己能精准控制offset而非交给框架去处理。 SS处理数据是否保证了EO在使用 Spark RDD 对数据进行 转换或汇总 时，我们可以天然获得 Exactly-once 语义，因为 RDD 本身就是一种具备容错性、不变性、以及计算确定性的数据结构。只要数据来源是可用的，且处理过程中没有副作用（Side effect），我们就能一直得到相同的计算结果。 SS内部的实现机制是spark core基于RDD模型的，RDD为保证计算过程中数据不丢失使用了checkpoint机制，也就是说其计算逻辑是RDD的变换过程，也就是DAG，可以在计算过程中的任何一个阶段（也就是这个阶段的RDD）上使用checkpoint方法，就可以保证当后续计算失败，可以从这个checkpoint重新算起，使得计算延续下去。当Spark Streaming场景下，其天然会进行batch操作，也就是说kafka过来的数据，每秒（一个固定batch的时间周期）会对当前kafka中的数据产生一个RDD，那么后续计算就是在这个RDD上进行的。只需要在kafkaRDD这个位置合理使用了checkpoint（这一点在前面已经讲过，可以保证）就能保证SS内部的Exactly once。 SS是否将数据EO地写出到了下游结果输出 默认符合 At-least-once 语义，因为 foreachRDD 方法可能会因为 Worker 节点失效而执行多次，从而重复写入外部存储。我们有两种方式解决这一问题，幂等更新和事务更新。下面我们将深入探讨这两种方式。参考：https://blog.csdn.net/qq_32252917/article/details/78827126首先输出操作是具有At-least Once语义的，也就是说SS可以保证需要输出的数据一定会输出出去，只不过由于失败等原因可能会输出多次。那么如何保证Exactly once？第一种“鸵鸟做法”，就是期望下游（数据）具有幂等特性。比如相同数据写 hdfs 同一个文件，这本身就是幂等操作，保证了多次操作最终获取的值还是相同；HBase、ElasticSearch 与 redis 等都能够实现幂等操作。对于关系型数据库的操作一般都是能够支持事务性操作。第二种使用事务更新，简要代码如下：1234567dstream.foreachRDD &#123; (rdd, time) =&gt; rdd.foreachPartition &#123; partitionIterator =&gt; val partitionId = TaskContext.get.partitionId() val uniqueId = generateUniqueId(time.milliseconds, partitionId) // use this uniqueId to transactionally commit the data in partitionIterator &#125;&#125; 这样保证同一个partition要么一起更新成功，要么一起失败，通过uniqueId来标识这一次的更新，这就要求下游支持事务机制。如果不采用幂等或者事务。可以采用如下方案，除了数据主动(重启服务)出错外，还会遇到如下问题。关于Spark Streaming数据输出多次重写及解决方案： 为什么会有这个问题，因为SparkStreaming在计算的时候基于SparkCore，SparkCore天生会做以下事情导致SparkStreaming的结果（部分）重复输出: 1.Task重试； 2.慢任务推测； 3.Stage重复； 4.Job重试；会导致数据的丢失。对应的解决方案： 1.一个任务失败就是job 失败，设置spark.task.maxFailures次数为1； 2.设置spark.speculation为关闭状态（因为慢任务推测其实非常消耗性能，所以关闭后可以显著的提高Spark Streaming处理性能） 3.Spark streaming on kafka的话，假如job失败后可以设置kafka的auto.offset.reset为largest的方式会自动恢复job的执行。参考：https://zybuluo.com/marlin/note/486917http://spark.apache.org/docs/latest/streaming-programming-guide.html#semantics-of-output-operationshttps://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka/]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive小文件合并]]></title>
    <url>%2F2019%2F01%2F26%2FHive%E5%B0%8F%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6%2F</url>
    <content type="text"><![CDATA[HDFS中的文件、目录和块都映射为一个对象，存储在NameNode服务器内存中，通常占用150个字节。 如果有1千万个文件，就需要消耗大约3G的内存空间。如果是10亿个文件呢，简直不可想象。所以我们要了解一下,hadoop 处理小文件的各种方案，然后选择一种适合的方案来解决本的小文件问题。 此外，HDFS读写小文件时也会更加耗时，因为每次都需要从NameNode获取元信息，并与对应的DataNode建立连接。对于MapReduce程序来说，小文件还会增加Mapper的个数，job作为一个独立的jvm实例，每个job只处理很少的数据，其开启和停止的开销可能会大大超过实际的任务处理时间，浪费了大量的调度时间。当然，这个问题可以通过使用CombinedInputFile和JVM重用来解决. 参考:https://blog.csdn.net/WYpersist/article/details/80043816 输入文件合并输入合并。即在Map前合并小文件。这个方法即可以解决之前小文件数太多，导致mapper数太多的问题；还可以防止输出小文件合数太多的问题（因为mr只有map时，mapper数就是输出的文件个数）。文件合并失效，且job只有map时，map的个数就是文件个数；通过控制map大小控制map个数，以控制输出文件个数。set hive.hadoop.supports.splittable.combineinputformat=true; 开关set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 执行Map前进行小文件合并set mapred.max.split.size=2048000000; 2G 每个Map最大输入大小set mapred.min.split.size.per.node=2048000000; 一个节点上split的至少的大小 ，决定了多个data node上的文件是否需要合并set mapred.min.split.size.per.rack=2048000000; 一个交换机下split的至少的大小，决定了多个交换机上的文件是否需要合并MR-Job 默认的输入格式 FileInputFormat 为每一个小文件生成一个切片。CombineFileInputFormat 通过将多个“小文件”合并为一个”切片”（在形成切片的过程中也考虑同一节点、同一机架的数据本地性），让每一个 Mapper 任务可以处理更多的数据，从而提高 MR 任务的执行速度。 解读：CombineFileInputFormat类：https://www.cnblogs.com/skyl/p/4754999.html 输出文件合并输出合并。即在输出结果的时候合并小文件动态分区好用，但是会产生很多小文件。原因就在于，假设初始有N个mapper,最后生成了m个分区，最终会有多少个文件生成呢？答案是N*m,是的，每一个mapper会生成m个文件，就是每个分区都会对应一个文件，这样的话你算一下。所以小文件就会成倍的产生。怎么解决这个问题，通常处理方式也是像上面那样，让数据尽量聚到少量reducer里面。但是有时候虽然动态分区不会产生reducer,但是也就意味着最后没有进行文件合并,我们也可以用distribute by rand()这句来保证数据聚类到相同的reducer。参考：https://www.iteblog.com/archives/1533.html 但是如果是多层分区呢，且二层分区数据量差异很大，虽然也可以使用上面的方式但仍然有可能不均匀，此时需要扩展，采用如下方式distribute by if(productType in (&#39;h&#39;,&#39;f&#39;,&#39;t&#39;),floor(rand() * 10),1) 其他合并方式，配置参数set hive.merg.xxxx在文件合并 和 压缩 并存时会失效，即只对text或者seq文件生效，对压缩格式如orc等会不适用，就有些鸡肋参考：https://meihuakaile.github.io/2018/10/19/hive%E5%B0%8F%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6/ 上面的方式只是解决使用和规避问题，如果是已经有很多小文件，那么只有压缩一条路可走了，问题就转变为如何更快更优的解决各种格式的压缩问题了。]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk]]></title>
    <url>%2F2019%2F01%2F26%2Fawk%2F</url>
    <content type="text"><![CDATA[awk是行处理器: 相比较屏幕处理的优点，在处理庞大文件时不会出现内存溢出或是处理缓慢的问题，通常用来格式化文本信息awk处理过程: 依次对每一行进行处理，然后输出 awk命令形式:awk [-F|-f|-v] ‘BEGIN{} // {command1; command2} END{}’ file [-F|-f|-v] 大参数，-F指定分隔符，-f调用脚本，-v定义变量 var=value‘ ‘ 引用代码块BEGIN 初始化代码块，在对每一行进行处理之前，初始化代码，主要是引用全局变量，设置FS分隔符// 匹配代码块，可以是字符串或正则表达式{} 命令代码块，包含一条或多条命令； 多条命令使用分号分隔END 结尾代码块，在对每一行进行处理之后再执行的代码块，主要是进行最终计算或输出结尾摘要信息特殊要点: $0 表示整个当前行$1 每行第一个字段 NF 字段数量变量；即每行的字段个数，Number FieldNR 每行的记录号，多文件记录递增；即行号 Number Record FNR 与NR类似，不过多文件记录不递增，每个文件都从1开始 \t 制表符\n 换行符 FS BEGIN时定义分隔符RS输入的记录分隔符， 默认为换行符(即文本是按一行一行输入) ~ 匹配，与==相比不是精确比较!~ 不匹配，不精确比较 == 等于，必须全部相等，精确比较!= 不等于，精确比较 &amp;&amp; 逻辑与|| 逻辑或 + 匹配时表示1个或1个以上 /[0-9][0-9]+/ 两个或两个以上数字/[0-9][0-9]*/ 一个或一个以上数字 FILENAME 文件名 OFS输出字段分隔符， 默认也是空格，可以改为制表符等ORS 输出的记录分隔符，默认为换行符,即处理结果也是一行一行输出到屏幕 -F’[:#/]’ 定义三个分隔符 IF语句 必须用在{}中，且比较内容用()扩起来 awk -F: ‘{if($1~/mail/) print $1}’ /etc/passwd //简写 awk -F: ‘{if($1~/mail/) {print $1}}’ /etc/passwd //全写 awk -F: ‘{if($1~/mail/) {print $1} else {print $2}}’ /etc/passwd //if…else… 原文链接 : http://blog.chinaunix.net/uid-23302288-id-3785105.html]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell统计]]></title>
    <url>%2F2019%2F01%2F26%2Fshell%E7%BB%9F%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[统计 列出当天访问次数最多的IP前20个命令：cut -d- -f 1 /usr/local/apache2/logs/access_log |uniq -c | sort -rn | head -20 查看当天有多少个IP访问：awk ‘{print $1}’ log_file|sort|uniq|wc -l 查看某一个页面被访问的次数;grep “/index.php” log_file | wc -l 查看每一个IP访问了多少个页面：awk ‘{++S[$1]} END {for (a in S) print a,S[a]}’ log_file 将每个IP访问的页面数进行从小到大排序：awk ‘{++S[$1]} END {for (a in S) print S[a],a}’ log_file | sort -n 查看某一个IP访问了哪些页面：grep ^111.111.111.111 log_file| awk ‘{print $1,$7}’ 去掉搜索引擎统计当天的页面：awk ‘{print $12,$1}’ log_file | grep ^\”Mozilla | awk ‘{print $2}’ |sort | uniq | wc -l 查看2009年6月21日14时这一个小时内有多少IP访问：awk ‘{print $4,$1}’ log_file | grep 21/Jun/2009:14 | awk ‘{print $2}’| sort | uniq | wc -l 统计访问日志里每个ip访问次数[root@qunar logs]# cat a.sh#!/bin/bash#将28/Jan/2015全天的访问日志放到a.txt文本cat access.log |sed -rn ‘/28\/Jan\/2015/p’ &gt; a.txt #统计a.txt里面有多少个ip访问cat a.txt |awk ‘{print $1}’|sort |uniq &gt; ipnum.txt#通过shell统计每个ip访问次数for i in `cat ipnum.txt`doiptj=`cat access.log |grep $i | grep -v 400 |wc -l`echo “ip地址”$i”在2015-01-28日全天(24小时)累计成功请求”$iptj”次，平均每分钟请求次数为：”$(($iptj/1440)) &gt;&gt; result.txtdone 把100天前的文件打包并且删除find [path] -type f -mtime +100 -exec tar rvf tmp.tar –remove-files {} \; 查找find . -name ‘.sh’ | xargs grep -in ‘48 ‘ 查找所有”.h”文件find /PATH -name “*.h” 查找所有”.h”文件中的含有”helloworld”字符串的文件find /PATH -name “.h” -exec grep -in “helloworld” {} \;find /PATH -name “.h” | xargs grep -in “helloworld” 查找所有”.h”和”.c”文件中的含有”helloworld”字符串的文件find /PATH /( -name “.h” -or -name “.c” /) -exec grep -in “helloworld” {} \; 查找非备份文件中的含有”helloworld”字符串的文件find /PATH /( -not -name “*~” /) -exec grep -in “helloworld” {} \;注：/PATH为查找路径，默认为当前路径。带-exec参数时必须以\;结尾，否则会提示“find: 遗漏“-exec”的参数”。 运维 lsof |grep deleted注：这个deleted表示该已经删除了的文件，但是文件句柄未释放,这个命令会把所有的未释放文件句柄的进程列出来 swap使用排序 1for i in $( cd /proc;ls |grep &quot;^[0-9]&quot;|awk &apos; $0 &gt;100&apos;) ;do sudo awk &apos;/^Swap:/&#123;a=a+$2&#125;END&#123;print &apos;&quot;$i&quot;&apos;,a/1024&quot;M&quot;&#125;&apos; /proc/$i/smaps 2&gt;/dev/null ; done | sort -k2nr | head -10 curl -O下载 正则bizType=([^&amp;]+)bizType=(.*?)[&amp;|$]]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Top*]]></title>
    <url>%2F2019%2F01%2F26%2FTop%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sed操作]]></title>
    <url>%2F2019%2F01%2F26%2Fsed%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[一次性给文件多行加注释在vim 视图模式下：2,5 s/^/#/或者直接使用sed，命令如下：sed -i ‘2,5s/^/#/‘ filename 注释取消反之，将2~5行带#注释取消：：2,5 s/^#//或者sed -i ‘2,5s/^#//‘ filename 去掉空行sed -i ‘/^$/d’ df.txt 将每一行拖尾的“空白字符”（空格，制表符）删除sed ‘s/ *$//‘ df.txt &gt;cwm.txt 将每一行中的前导和拖尾的空白字符删除sed ‘s/^ //;s/ $//‘ df.txt &gt;cwm.txt vi下全文替换:%s/1/2/g 全文替换“1,20” ：表示从第1行到20行；“%” ：表示整个文件，同“1,$”；“. ,$” ：从当前行到文件尾；]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有效电话号码]]></title>
    <url>%2F2019%2F01%2F26%2F%E6%9C%89%E6%95%88%E7%94%B5%E8%AF%9D%E5%8F%B7%E7%A0%81%2F</url>
    <content type="text"><![CDATA[给定一个包含电话号码列表（一行一个电话号码）的文本文件 file.txt，写一个 bash 脚本输出所有有效的电话号码。 你可以假设一个有效的电话号码必须满足以下两种格式： (xxx) xxx-xxxx 或 xxx-xxx-xxxx。（x 表示一个数字） 你也可以假设每行前后没有多余的空格字符。 示例: 假设 file.txt 内容如下： 987-123-4567123 456 7890(123) 456-7890你的脚本应当输出下列有效的电话号码： 987-123-4567(123) 456-7890 1egrep -o &quot;(^[0-9]&#123;3&#125;-[0-9]&#123;3&#125;-[0-9]&#123;4&#125;$)|(^\([0-9][0-9][0-9]\)\s[0-9]&#123;3&#125;-[0-9]&#123;4&#125;$)&quot; file.txt]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[转置文件]]></title>
    <url>%2F2019%2F01%2F26%2F%E8%BD%AC%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[给定一个文件 file.txt，转置它的内容。 你可以假设每行列数相同，并且每个字段由 ‘ ‘ 分隔. 示例: 假设 file.txt 文件内容如下： name agealice 21ryan 30应当输出： name alice ryanage 21 30 1cat file.txt | awk &apos;&#123;for(i=1;i&lt;=NF;i++)&#123;if(NR==1)&#123;res[i]=$i&#125;else&#123;res[i]=res[i]&quot; &quot;$i&#125;&#125;&#125;END&#123;for(i=1;i&lt;=NF;i++)&#123;print res[i]&#125;&#125;&apos; awk是一行一行处理数据，if(NR==1){res[i]=$i}优先初始化第一行的res[]数组，长度为第一行的元素格式，后面每行在对应位置元素后面追加。]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第十行]]></title>
    <url>%2F2019%2F01%2F26%2F%E7%AC%AC%E5%8D%81%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[给定一个文本文件 file.txt，请只打印这个文件中的第十行。 示例: 假设 file.txt 有如下内容： Line 1Line 2Line 3Line 4Line 5Line 6Line 7Line 8Line 9Line 10你的脚本应当显示第十行： Line 10说明: 如果文件少于十行，你应当输出什么？ 至少有三种不同的解法，请尝试尽可能多的方法来解题。 12tail -n +10 file.txt | head -n 1awk &apos;NR==10&#123;print $0&#125;&apos; file.txt]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计词频]]></title>
    <url>%2F2019%2F01%2F26%2F%E7%BB%9F%E8%AE%A1%E8%AF%8D%E9%A2%91%2F</url>
    <content type="text"><![CDATA[cat words.txt | tr -s &quot; &quot; &quot;\n&quot; |sort |uniq -c | sort -r -n | awk &#39;{print $2,$1}&#39;tr -s 将重复出现字符串压缩为一个字符串“ “ “\n” 将空格替换为换行uniq 命令删除文件中的重复行。uniq 命令读取由 InFile 参数指定的标准输入或文件。该命令首先比较相邻的行，然后除去第二行和该行的后续副本。重复的行一定相邻。（所以一定要在发出 uniq 命令之前，请使用 sort 命令使所有重复行相邻。）sort -n按照数值排序sort -r降序排序sort file.txt | uniq -c -c或–count在每列旁边显示该行重复出现的次数 另:删除字符asdtr -d ‘asd’删除空行tr -s ‘\n’sort -u 去重,如果只有sort是不会去重sort file.txt | uniq -u -u或——unique：仅显示出一次的行列；只显示单一行sort file.txt | uniq -d -d或–repeated：仅显示重复出现的行列]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[left semi join]]></title>
    <url>%2F2019%2F01%2F25%2Fhive-LeftSemiJoin%2F</url>
    <content type="text"><![CDATA[参考:https://blog.csdn.net/happyrocking/article/details/79885071]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[join倾斜]]></title>
    <url>%2F2019%2F01%2F25%2Fhive-join%E5%80%BE%E6%96%9C%2F</url>
    <content type="text"><![CDATA[map join使用 map join 在 map 端就先进行 join ，免得到reduce 时卡住 Auto Map Join还记得原理中提到的物理优化器？Physical Optimizer么？它的其中一个功能就是把Join优化成Auto Map Join 图上左边是优化前的，右边是优化后的 优化过程是把Join作业前面加上一个条件选择器ConditionalTask和一个分支。左边的分支是MapJoin，右边的分支是Common Join(Reduce Join) 看看左边的分支是不是和我们上上一张图很像？ 这个时候，我们在执行的时候，就由这个Conditional Task 进行实时路径选择，遇到小于25兆走左边，大于25兆走右边。所谓，男的走左边，女的走右边，人妖走中间。 在比较新版的Hive中，Auto Mapjoin是默认开启的。如果没有开启，可以使用一个开关， set hive.auto.convert.join=true 开启。当然，Join也会遇到和上面的Group By一样的倾斜问题。 Ｈive 也可以通过像Group By一样两道作业的模式单独处理一行或者多行倾斜的数据。即采用下面的方式。 hive.optimize.skewjoinhive 中设定set hive.optimize.skewjoin = true;set hive.skewjoin.key = skew_key_threshold （default = 100000）可以就按官方默认的1个reduce 只处理1G 的算法，那么skew_key_threshold= 1G/平均行长.或者默认直接设成250000000 (差不多算平均行长4个字节) 其原理是就在Reduce Join过程，把超过十万条的倾斜键的行写到文件里，回头再起一道Join单行的Map Join作业来单独收拾它们。最后把结果取并集就是了 对full outer join无效。 其他方法如果A表关联B表在某个key上倾斜，B是码表，那么可以将B表放大1000倍，然后对A表的key字段加上一个1000以内hash后缀，然后和放大后的b表关联，可以解决问题。 参考:https://blog.csdn.net/lw_ghy/article/details/51469753]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[group by数据倾斜]]></title>
    <url>%2F2019%2F01%2F25%2Fhive_groupby%E5%80%BE%E6%96%9C%2F</url>
    <content type="text"><![CDATA[group by数据倾斜倾斜原因：select count(distinct name) from user时 使用distinct会将所有的name值都shuffle到一个reducer里面。特别的有select uid, count(distinct name) from user group by uid; 即count distinct + （group by）的情况。 优化：（1）主要是把count distinct改变成group by。改变上面的sql为select uid, count(name) from (select uid, name from user group by uid, name)t group by uid.（2）给group by 字段加随机数打散，聚合，之后把随机数去掉，再次聚合（有点类似下面的参数SET hive.groupby.skewindata=true;）1234567891011121314151617181920212223242526272829select split(uid, &apos;_&apos;)[0] uid, sum(names) from( select concat_ws(&apos;-&apos;, uid, substr(rand()*10, 1, 1)) uid, count(name) names from ( select uid, name ---此处去重，且不会倾斜 from user group by uid, name )a group by concat_ws(&apos;-&apos;, uid, substr(rand()*10, 1, 1)))bgroup by split(uid, &apos;_&apos;)[0]等同于下面select split(uid, &apos;_&apos;)[0] uid, sum(names) from( select uid,count(name) names from ( select concat_ws(&apos;-&apos;, uid, substr(rand()*10, 1, 1)) uid, name from ( select uid, name from user group by uid, name )a ) c group by uid )bgroup by split(uid, &apos;_&apos;)[0] (3)SET hive.groupby.skewindata=true; 当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，该Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作. set hive.map.aggr=true; 在mapper端部分聚合，相当于Combiner 。而Map-Side聚合，一般在聚合函数sum,count时使用。 无论你使用Map端，或者两道作业。其原理都是通过部分聚合来来减少数据量。能不能部分聚合，部分聚合能不能有效减少数据量，通常与UDAF，也就是聚合函数有关。也就是只对代数聚合函数有效，对整体聚合函数无效。所谓代数聚合函数，就是由部分结果可以汇总出整体结果的函数，如count，sum。 所谓整体聚合函数，就是无法由部分结果汇总出整体结果的函数，如avg，mean。 比如，sum, count，知道部分结果可以加和得到最终结果。 而对于，mean，avg，知道部分数据的中位数或者平均数，是求不出整体数据的中位数和平均数的。 set hive.groupby.mapaggr.checkinterval=100000；–这个是group的键对应的记录条数超过这个值则会进行分拆,值根据具体数据量设置。hive.map.aggr.hash.min.reduction=0.5(默认)预先取100000条数据聚合,如果聚合后的条数/100000&gt;0.5，则不再聚合 使用Hive的过程中，我们习惯性用set hive.groupby.skewindata=true来避免因数据倾斜造成的计算效率问题，但是每个设置都是把双刃剑，最近调研了下相关问题，现总结如下： 从下表可以看出，skewindata配置真正发生作用，只会在以下三种情况下，能够将1个job转化为2个job：select count distinct … from …select a,count() from … group by a 只针对单列有效select count(),count(distinct …) from 此处没有group by如下sql会报错select count(*),count(distinct …) from … group by a 此处有group by需要改为select a,sum(1),count(distinct …) from … group by a 参考：https://meihuakaile.github.io/2018/10/19/hive%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/https://blog.csdn.net/dxl342/article/details/77886577https://blog.csdn.net/lw_ghy/article/details/51469753]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark 二次排序]]></title>
    <url>%2F2018%2F02%2F13%2Fspark-%E4%BA%8C%E6%AC%A1%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[数据：40 2040 1040 3040 530 3030 2030 1030 4050 2050 5050 1050 601234567891011121314151617181920212223242526272829303132333435363738394041package com.scala.test.core.secondsortimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object SecondarySort &#123; def main(args:Array[String]): Unit =&#123; val conf = new SparkConf().setAppName(&quot;SecondarySort&quot;).setMaster(&quot;local[*]&quot;) val sc = new SparkContext(conf) val line = sc.textFile(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/secondarysort/sort.txt&quot;) val data = line.map(x =&gt; &#123; val line : Array[String] = x.split(&quot; &quot;) (line(0),line(1)) &#125;) val rdd = data.groupByKey().sortByKey()//todo groupbykey和sortByKey// (40,CompactBuffer(20, 10, 30, 5))// (50,CompactBuffer(20, 50, 10, 60))// (30,CompactBuffer(30, 20, 10, 40)) //todo 之前多条现在变成3条了,改变了原来的条数// val result = rdd.map(item =&gt; (item._1, item._2.toList.sortWith(_.toInt&lt;_.toInt)))//todo values转list并排序 //todo 不改变原来的条数 val result = rdd.flatMap(item =&gt; &#123; val list = item._2.toList.sortWith(_.toInt&lt;_.toInt) list.map(x =&gt; (item._1,x)) &#125;)// 保存// result.saveAsTextFile(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/secondarysort/out&quot;)// 打印 result.collect().foreach(println) sc.stop() &#125;&#125;``` 上面的方式，groupbykey之后把相同的key聚合在一起排序，实际上是内存排序，这种方法可能导致归约器耗尽内存。如果数量很少可以用。和mapreduce的setGroupingComparatorClass方式不同,mapreduce在reduce中没有排序操作，是用框架中的排序进行。 使用repartitionAndSortWithinPartitions： package com.scala.test.core.secondsort import org.apache.spark.{HashPartitioner, Partitioner, SparkConf, SparkContext} object SecondarySortRepartition { def main(args: Array[String]): Unit = { implicit val caseInsensitiveOrdering = new Ordering[Int] { override def compare(a: Int, b: Int) = b.compareTo(a) } import org.apache.spark.Partitioner class KeyBasePartitioner(partitions: Int) extends Partitioner { require(partitions &gt;= 0, s&quot;Number of partitions ($partitions) cannot be negative.&quot;) override def numPartitions: Int = partitions override def getPartition(key: Any): Int = { val k = key.asInstanceOf[SecondarySort] Math.abs(k.one.hashCode() % numPartitions) } } val conf = new SparkConf().setAppName(&quot;SecondarySortRepartition&quot;).setMaster(&quot;local[*]&quot;) val sc = new SparkContext(conf) val line = sc.textFile(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/secondarysort/sort.txt&quot;) class SecondarySort(val one :Int,val two : Int) extends Ordered[SecondarySort] with Serializable { override def compare(that: SecondarySort): Int = { if(this.one-that.one != 0){ this.one-that.one }else{ this.two-that.two } } } line.map(x =&gt; { val xy=x.split(&quot; &quot;) (new SecondarySort(Integer.parseInt(xy(0)),Integer.parseInt(xy(1)).toInt),x) } ) .repartitionAndSortWithinPartitions(new KeyBasePartitioner(3)) .saveAsTextFile(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/secondarysort/out&quot;) // .foreach(println) sc.stop() }}`当spark的分区数大于线程数时，spark仍会按照一个一个分区单独处理，而不会像MapReduce设置setGroupingComparatorClass。可以通过设置.setMaster(“local[2]”)和new KeyBasePartitioner(3)来验证。 spark 1.2之后引入了一个高质量的算子 repartitionAndSortWithinPartitions?。该算子为spark的Shuffle增加了sort。假如，后面再跟mapPartitions算子的话，其算子就是针对已经按照key排序的分区，这就有点像mr的意思了。与groupbykey不同的是，数据不会一次装入内存，而是使用迭代器一次一条记录从磁盘加载。这种方式最小化了内存压力。 定义partitioner的方式可以参考https://www.bbsmax.com/A/LPdoVrl253/repartitionAndSortWithinPartitions算子比先分区在排序效率高https://blog.csdn.net/luofazha2012/article/details/80587128 sortbykey VS repartitionAndSortWithinPartitionshttps://blog.csdn.net/wyqwilliam/article/details/81627603]]></content>
      <categories>
        <category>spark编程</category>
      </categories>
      <tags>
        <tag>spark编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark wordcount]]></title>
    <url>%2F2018%2F02%2F12%2Fspark-wordcount%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718package com.scala.test.coreimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object WordCount &#123; def main(args : Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(&quot;wordCount&quot;).setMaster(&quot;local[*]&quot;) val sc = new SparkContext(conf) val line = sc.textFile(&quot;/Users/lifei/Desktop/ab/a.txt&quot;);//todo wholeTextFiles 可以读取目录// 直接打印// line.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collect().foreach(println)// 保存到文件 line.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;/Users/lifei/Desktop/ab/lala&quot;) sc.stop(); &#125;&#125;]]></content>
      <categories>
        <category>spark编程</category>
      </categories>
      <tags>
        <tag>spark编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive级联报表查询]]></title>
    <url>%2F2018%2F02%2F12%2Fhive%E7%BA%A7%E8%81%94%E6%8A%A5%E8%A1%A8%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021A,2015-01,5A,2015-01,15B,2015-01,5A,2015-01,8B,2015-01,25A,2015-01,5C,2015-01,10C,2015-01,20A,2015-02,4A,2015-02,6C,2015-02,30C,2015-02,10B,2015-02,10B,2015-02,5A,2015-03,14A,2015-03,6B,2015-03,20B,2015-03,25C,2015-03,10C,2015-03,20``` create table t_access_times(username string,month string,counts int)row format delimited fields terminated by ‘,’;1![upload successful](/images/pasted-52.png) – 窗口分析函数– 窗口的定义：– 针对窗口中数据的操作– 哪些数据select uid, month, amount,sum (amount) over(partition by uid order by month rows between unbounded preceding and current row) as accumulatefrom t_access_amount; `参考：https://www.cnblogs.com/arjenlee/p/9692312.html#auto_id_81]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive模型]]></title>
    <url>%2F2018%2F02%2F12%2Fhive%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Jobtracker是hadoop1.x中的组件，它的功能相当于： Resourcemanager+MRAppMaster TaskTracker 相当于： Nodemanager + yarnchild hive里面有两种服务模式一种是cli模式，一种是hiveserver2，分别对应的启动入口 cli：hive-cli/org.apache.hadoop.hive.cli.CliDriver.javahiveserver2：hiveservice/org.apache.hive.service.server.HiveServer2.java,直接用debug或者run运行调试 hive sql 的执行顺序from… where…. select…group by… having … order by… 源码 compiler包括parser解释器和SemanticAnalyzer语义解析器。 无论使用CLI、Thrift Server、JDBC还是自定义的提交工具，最终的HQL都会传给Driver实例，执行Driver.run()方法。从这种设计也可以看出，如果您要开发一套自定义的Hive作业提交工具，最好的方式是引用Driver实例，调用相关方法进行开发。 而Driver.run()方法，获得了这样一个HQL，则会执行两个重要的步骤：编译和执行，即Driver.complie()和Driver.execute()。对于Driver.comile()来说，其实就是调用parse和optimizer包中的相关模块，执行语法解析、语义分析、优化；对于Driver.run()来说，其实就是调用exec包中的相关模块，将解析后的执行计划执行，如果解析后的结果是一个查询计划，那么通常的作法就是提交一系列的MapReduce作业。 以查询的执行为例，整个Hive的流程是非常简单的一条直线，由上到下进行。 Join操作左边为小表应该将条目少的表/子查询放在 Join 操作符的左边原因是在 Join 操作的 Reduce 阶段，位于 Join 操作符左边的表的内容会被加载进内存，将条目少的表放在左边，可以有效减少发生 OOM 错误的几率 参考：https://blog.csdn.net/dante_003/article/details/73789910https://www.jianshu.com/p/892cc8985c9chttps://blog.csdn.net/wf1982/article/details/9122543https://blog.csdn.net/wzq6578702/article/details/71250081]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[join]]></title>
    <url>%2F2018%2F02%2F11%2Fjoin%2F</url>
    <content type="text"><![CDATA[测试数据user.txt (用户id,用户名)1 用户12 用户23 用户3 more post.txt (用户id,帖子id,标题)1 1 贴子11 2 贴子22 3 帖子34 4 贴子45 5 贴子55 6 贴子65 7 贴子7123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149package com.qunar.mr.join;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.ArrayList;import java.util.List;/** * * user.txt 用户表(用户id,用户名) * * post.txt 帖子表(用户id,帖子id,标题) */public class Join &#123;// 类型 U表示用户,P表示帖子 public static class UserMap extends Mapper&lt;Object, Text,Text,Text&gt; &#123; public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; String[] line = value.toString().split(&quot;,&quot;); context.write(new Text(line[0]),new Text(&quot;U,&quot;+value.toString())); &#125; &#125; public static class PostMap extends Mapper&lt;Object, Text,Text,Text&gt; &#123; public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; String[] line = value.toString().split(&quot;,&quot;); context.write(new Text(line[0]),new Text(&quot;P,&quot;+value.toString())); &#125; &#125; public static class Reduce extends Reducer&lt;Text,Text,Text,Text&gt; &#123; private List&lt;String&gt; users = new ArrayList&lt;String&gt;(); private List&lt;String&gt; posts = new ArrayList&lt;String&gt;(); private String joinType; @Override protected void setup(Context context) throws IOException, InterruptedException &#123; super.setup(context); joinType = context.getConfiguration().get(&quot;joinType&quot;); &#125; public void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; //todo users.clear(); posts.clear(); for (Text v:values) &#123; if (v.toString().contains(&quot;U,&quot;))&#123; users.add(v.toString().substring(2)); &#125; else &#123; posts.add(v.toString().substring(2)); &#125; &#125; if (joinType.equals(&quot;innerJoin&quot;))&#123; if (users.size() &gt; 0 &amp;&amp; posts.size() &gt;0)&#123; for (String user:users)&#123; for (String post:posts)&#123; context.write(new Text(user),new Text(post)); &#125; &#125; &#125; &#125; if (joinType.equals(&quot;leftOuter&quot;))&#123; for (String user:users) &#123; if (posts.size() &gt; 0)&#123;//todo for (String post:posts) &#123; context.write(new Text(user),new Text(post)); &#125; &#125; else &#123; context.write(new Text(user),new Text(&quot;,,&quot;)); &#125; &#125; &#125; if (joinType.equals(&quot;rightOuter&quot;))&#123; for (String post:posts) &#123; if (users.size() &gt; 0)&#123; for (String user:users) &#123; context.write(new Text(user),new Text(post)); &#125; &#125; else &#123; context.write(new Text(&quot;,&quot;),new Text(post)); &#125; &#125; &#125; if (joinType.equals(&quot;allOuter&quot;))&#123; if (users.size() &gt; 0)&#123; for (String user:users) &#123; if (posts.size() &gt; 0)&#123; for (String post:posts) &#123; context.write(new Text(user),new Text(post)); &#125; &#125; else &#123; context.write(new Text(user),new Text(&quot;,,&quot;)); &#125; &#125; &#125; else &#123; for (String post:posts) &#123; context.write(new Text(&quot;,&quot;),new Text(post)); &#125; &#125; &#125; if (joinType.equals(&quot;anti&quot;))&#123; if (users.size() == 0 || posts.size() == 0)&#123;//todo for (String user:users) &#123; context.write(new Text(user),new Text(&quot;,,&quot;)); &#125; for (String post:posts) &#123; context.write(new Text(&quot;,&quot;),new Text(post)); &#125; &#125; &#125; &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;Join&quot;); job.setJarByClass(Join.class); //设置连接类型 //innerJoin,leftOuter,rightOuter,allOuter,anti job.getConfiguration().set(&quot;joinType&quot;,&quot;allOuter&quot;);// job.setMapperClass(Map.class); //todo 不能单独设置 job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); //todo MultipleInputs.addInputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/join/in/user.txt&quot;), TextInputFormat.class,UserMap.class); MultipleInputs.addInputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/join/in/post.txt&quot;), TextInputFormat.class,PostMap.class); Path outpath = new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/join/out&quot;); FileSystem fs = outpath.getFileSystem(job.getConfiguration()); if (fs.exists(outpath))&#123; fs.delete(outpath,true); &#125; FileOutputFormat.setOutputPath(job,outpath); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;]]></content>
      <categories>
        <category>mapreduce编程</category>
      </categories>
      <tags>
        <tag>mapreduce编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[倒排]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%80%92%E6%8E%92%2F</url>
    <content type="text"><![CDATA[题意hdfs 上有三个文件，内容下上面左面框中所示。右框中为处理完成后的结果文件。倒排索引（Inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档检索系统中最常用的数据结构。通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。倒排索引主要由两个部分组成：“单词词典”和“倒排文件”这个任务与传统的倒排索引任务不同的地方是加上了每个文件中的频数。 实现思路首先关注结果中有文件名称，这个我们有两种方式处理：1、自定义InputFormat，在其中的自定义RecordReader中，直接通过InputSplit得到Path，继而得到FileName;2、在Mapper中，通过上下文可以取到Split，也可以得到fileName。这个任务中我们使用第二种方式，得到filename.在mapper中，得到filename 及 word，封装到一个自定义keu中。value 使用IntWritable。在map 中直接输出值为1的IntWritable对象。对进入reduce函数中的key进行分组控制，要求按word相同的进入同一次reduce调用。所以需要自定义GroupingComparator。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160package com.qunar.mr.invertedsort;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.*;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import org.apache.log4j.Logger;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import java.util.LinkedHashMap;/** * http://blog.itpub.net/30066956/viewspace-2120238/ */public class InvertedSort &#123; static class WordKey implements WritableComparable&lt;WordKey&gt; &#123; private String fileName; private String word; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(fileName); out.writeUTF(word); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.fileName = in.readUTF(); this.word = in.readUTF(); &#125; @Override public int compareTo(WordKey key) &#123; int r = word.compareTo(key.word); if(r==0) r = fileName.compareTo(key.fileName); return r; &#125; public String getFileName() &#123; return fileName; &#125; public void setFileName(String fileName) &#123; this.fileName = fileName; &#125; public String getWord() &#123; return word; &#125; public void setWord(String word) &#123; this.word = word; &#125; &#125; public static class IndexInvertedMapper extends Mapper&lt;LongWritable, Text,WordKey, IntWritable&gt; &#123; private WordKey newKey = new WordKey(); private IntWritable ONE = new IntWritable(1); private String fileName ; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; newKey.setFileName(fileName); String words [] = value.toString().split(&quot; &quot;); for(String w:words)&#123; newKey.setWord(w); context.write(newKey, ONE); &#125; &#125; @Override protected void setup(Context context) throws IOException, InterruptedException &#123; FileSplit inputSplit = (FileSplit) context.getInputSplit(); fileName = inputSplit.getPath().getName(); &#125; &#125; public static class IndexInvertedReducer extends Reducer&lt;WordKey,IntWritable,Text,Text&gt; &#123; private Text outputKey = new Text(); @Override protected void reduce(WordKey key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; outputKey.set(key.getWord()); LinkedHashMap&lt;String,Integer&gt; map = new LinkedHashMap&lt;String,Integer&gt;(); for(IntWritable v :values)&#123; if(map.containsKey(key.getFileName()))&#123; map.put(key.getFileName(), map.get(key.getFileName())+ v.get()); &#125; else&#123; map.put(key.getFileName(), v.get()); &#125; &#125; StringBuilder sb = new StringBuilder(); sb.append(&quot;&#123;&quot;); for(String k: map.keySet())&#123; sb.append(&quot;(&quot;).append(k).append(&quot;,&quot;).append(map.get(k)).append(&quot;)&quot;).append(&quot;,&quot;); &#125; sb.deleteCharAt(sb.length()-1).append(&quot;&#125;&quot;); context.write(outputKey, new Text(sb.toString())); &#125; &#125; public static class IndexInvertedGroupingComparator extends WritableComparator &#123; Logger log = Logger.getLogger(getClass()); public IndexInvertedGroupingComparator()&#123; super(WordKey.class,true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; WordKey key1 = (WordKey) a; WordKey key2 = (WordKey) b; log.info(&quot;==============key1.getWord().compareTo(key2.getWord()):&quot;+key1.getWord().compareTo(key2.getWord())); return key1.getWord().compareTo(key2.getWord()); &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;IndexInvertedJob&quot;); job.setJarByClass(InvertedSort.class); Path in = new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/invertedsort/in&quot;); Path out = new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/invertedsort/out&quot;); FileSystem.get(conf).delete(out,true); FileInputFormat.setInputPaths(job, in); FileOutputFormat.setOutputPath(job, out); job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); job.setMapperClass(IndexInvertedMapper.class); job.setMapOutputKeyClass(WordKey.class); job.setMapOutputValueClass(IntWritable.class); job.setReducerClass(IndexInvertedReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); job.setGroupingComparatorClass(IndexInvertedGroupingComparator.class); System.exit(job.waitForCompletion(true)? 0 : 1); &#125;&#125; 参考:http://blog.itpub.net/30066956/viewspace-2120238/]]></content>
      <categories>
        <category>mapreduce编程</category>
      </categories>
      <tags>
        <tag>mapreduce编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二次排序]]></title>
    <url>%2F2018%2F02%2F11%2F%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[输入文件 sort.txt 内容为 40 20 40 10 40 30 40 5 30 30 30 20 30 10 30 40 50 20 50 50 50 10 50 60 输出文件的内容（从小到大排序）如下 30 10 30 20 30 30 30 40 -------- 40 5 40 10 40 20 40 30 -------- 50 10 50 20 50 50 50 60 从输出的结果可以看出Key实现了从小到大的排序，同时相同Key的Value也实现了从小到大的排序，这就是二次排序的结果 在本例中要比较两次。先按照第一字段排序，然后再对第一字段相同的按照第二字段排序。根据这一点，我们可以构造一个复合类IntPair ，它有两个字段，先利用分区对第一字段排序，再利用分区内的比较对第二字段排序。二次排序的流程分为以下几步。 1、自定义 key 2、自定义分区 3、Key的比较类 4、定义分组类函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159 package com.qunar.mr.secondarysort;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.*;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Partitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import java.util.*;public class SecondarySort &#123;// 1、自定义 key public static class IntPair implements WritableComparable&lt;IntPair&gt; &#123; private int first = 0; private int second = 0; /** * Read the two integers. * Encoded as: MIN_VALUE -&amp;gt; 0, 0 -&amp;gt; -MIN_VALUE, MAX_VALUE-&amp;gt; -1 */ @Override public void readFields(DataInput in) throws IOException &#123; first = in.readInt(); second = in.readInt(); &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(first); out.writeInt(second); &#125; @Override public int hashCode() &#123; return first * 157 + second; &#125; @Override public boolean equals(Object right) &#123; if (right instanceof IntPair) &#123; IntPair r = (IntPair) right; return r.first == first &amp;&amp; r.second == second; &#125; else &#123; return false; &#125; &#125; @Override public int compareTo(IntPair o) &#123; if (first != o.first) &#123; return first &lt; o.first ? -1 : 1; &#125; else if (second != o.second) &#123; return second &lt; o.second ? -1 : 1; &#125; else &#123; return 0; &#125; &#125; /** * Set the left and right values. */ public void set(int left, int right) &#123; first = left; second = right; &#125; public int getFirst() &#123; return first; &#125; public int getSecond() &#123; return second; &#125; &#125;// 2、自定义分区 public static class FirstPartitioner extends Partitioner&lt;IntPair,IntWritable&gt; &#123; @Override public int getPartition(IntPair key, IntWritable value, int numPartitions) &#123; return Math.abs(key.getFirst() * 127) % numPartitions; &#125; &#125;// 3、Key的比较类 public static class FirstGroupingComparator implements RawComparator&lt;IntPair&gt; &#123; @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123; return WritableComparator.compareBytes(b1, s1, Integer.SIZE/8, b2, s2, Integer.SIZE/8); &#125; @Override public int compare(IntPair o1, IntPair o2) &#123; int l = o1.getFirst(); int r = o2.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); &#125; &#125; public static class MapClass extends Mapper&lt;Object, Text,IntPair, IntWritable&gt;&#123; private final IntPair key = new IntPair(); private final IntWritable value = new IntWritable(); @Override public void map(Object inKey, Text inValue, Context context) throws IOException, InterruptedException &#123; StringTokenizer itr = new StringTokenizer(inValue.toString()); int left = 0; int right = 0; if (itr.hasMoreTokens()) &#123; left = Integer.parseInt(itr.nextToken()); if (itr.hasMoreTokens()) &#123; right = Integer.parseInt(itr.nextToken()); &#125; key.set(left, right); value.set(right); context.write(key, value); &#125; &#125; &#125; public static class Reduce extends Reducer&lt;IntPair,IntWritable,Text,IntWritable&gt;&#123; private static final Text SEPARATOR = new Text(&quot;------------------------------------------------&quot;); private final Text first = new Text(); public void reduce(IntPair key,Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; context.write(SEPARATOR, null); first.set(Integer.toString(key.getFirst())); for(IntWritable value: values) &#123; context.write(first, value); &#125; &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, &quot;secondary sort&quot;); job.setJarByClass(SecondarySort.class); job.setMapperClass(MapClass.class); job.setReducerClass(Reduce.class); // group and partition by the first int in the pair job.setPartitionerClass(FirstPartitioner.class); job.setGroupingComparatorClass(FirstGroupingComparator.class); // the map output is IntPair, IntWritable job.setMapOutputKeyClass(IntPair.class); job.setMapOutputValueClass(IntWritable.class); // the reduce output is Text, IntWritable job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/secondarysort/sort.txt&quot;)); FileOutputFormat.setOutputPath(job, new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/secondarysort/out&quot;)); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;]]></content>
      <categories>
        <category>mapreduce编程</category>
      </categories>
      <tags>
        <tag>mapreduce编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全排序]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%85%A8%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[错误写法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.qunar.mr.datasort;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;/** * http://www.aboutyun.com/thread-7046-1-1.html * * 这个实例仅仅要求对输入数据进行排序，熟悉MapReduce过程的读者会很快想到在MapReduce过程中就有排序，是否可以利用这个默认的排序，而不需要自己再实现具体的排序呢？ * 答案是肯定的。 * * 但是在使用之前首先需要了解它的默认排序规则。它是按照key值进行排序的， * 如果key为封装int的IntWritable类型，那么MapReduce按照数字大小对key排序， * 如果key为封装为String的Text类型，那么MapReduce按照字典顺序对字符串排序。 * * 了解了这个细节，我们就知道应该使用封装int的IntWritable型数据结构了。 * 也就是在map中将读入的数据转化成IntWritable型，然后作为key值输出（value任意）。 * reduce拿到&lt;key，value-list&gt;之后，将输入的key作为value输出，并根据value-list中元素的个数决定输出的次数。 * 输出的key（即代码中的linenum）是一个全局变量，它统计当前key的位次。 * * 需要注意的是这个程序中没有配置Combiner，也就是在MapReduce过程中不使用Combiner。 * 这主要是因为使用map和reduce就已经能够完成任务了。 * * * https://blog.csdn.net/evo_steven/article/details/17139123 */public class DataSort &#123; public static class Map extends Mapper&lt;Object, Text, IntWritable,Text&gt; &#123;// private static IntWritable data = null;//todo 此处不能这样写，否则会报错 public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; String line = value.toString(); context.write(new IntWritable(Integer.parseInt(line)),new Text(&quot;&quot;)); &#125; &#125; public static class Reduce extends Reducer&lt;IntWritable,Text,IntWritable,IntWritable&gt; &#123;// 每个reduce中也只是单独的全局变量，并非整个集群的全局变量，// 一旦加入job.setNumReduceTasks(2);就会有两个文件，会出现错误结果 private static IntWritable count = new IntWritable(1); //todo IntWritable public void reduce(IntWritable key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException &#123; for (Text val:values) &#123; context.write(count,key); count = new IntWritable(count.get() + 1); &#125; &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;DataSort&quot;); job.setJarByClass(DataSort.class); job.setNumReduceTasks(2); //todo 如果有此处，实际环境中会报错 job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(IntWritable.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/datasort/in&quot;)); FileOutputFormat.setOutputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/datasort/out&quot;)); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 有问题的做法，缺少setPartitionerClass设计模式中的全排序实现思路是 两次JOB，第一次做分区，第二次做排序，也用到了setPartitionerClass，reduce只负责输出Partitinoner的作用除了快速找到key对应的reducer，更重要的一点是：这个Partitioner控制了排序的总体有序！ 正确做法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package com.qunar.mr.datasort;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.*;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.partition.InputSampler;import org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner;import java.io.IOException;public class DataSortNew &#123; public static class Map extends Mapper&lt;Text, Text, Text,IntWritable&gt; &#123; public void map(Text key,Text value,Context context) throws IOException, InterruptedException &#123; context.write(key,new IntWritable(Integer.parseInt(key.toString())));//todo key是Text,value也是key &#125; &#125; public static class Reduce extends Reducer&lt;Text,IntWritable,IntWritable, NullWritable&gt; &#123;//todo 此处的key也一定要跟着改为Text public void reduce(Text key,Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; for (IntWritable val:values) &#123; context.write(val,NullWritable.get()); &#125; &#125; &#125; public static class KeyComparator extends WritableComparator &#123; protected KeyComparator() &#123; super(Text.class, true); &#125; @Override public int compare(WritableComparable w1, WritableComparable w2) &#123; int v1 = Integer.parseInt(w1.toString()); int v2 = Integer.parseInt(w2.toString()); return v1 - v2; &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); conf.set(&quot;mapreduce.totalorderpartitioner.naturalorder&quot;, &quot;false&quot;); Job job = Job.getInstance(conf,&quot;DataSortNew&quot;); job.setJarByClass(DataSortNew.class); FileInputFormat.addInputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/datasort/in&quot;)); FileOutputFormat.setOutputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/datasort/out&quot;)); job.setInputFormatClass(KeyValueTextInputFormat.class); job.setSortComparatorClass(KeyComparator.class);//todo job.setNumReduceTasks(100); //todo job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); job.setOutputValueClass(NullWritable.class);// 在新版本的Hadoop中，内置了三个采样器： SplitSampler，RandomSampler和IntervalSampler。这三个采样器都是InputSampler类的静态内部类，并且都实现了InputSampler类的内部接口Sample// https://flyingdutchman.iteye.com/blog/1878962// 0.01-----------------每个样本被抽到的概率// 1000------------------样本数// 100--------------------分区数 String partitionPath=&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/datasort/ss/sampler&quot;; TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(partitionPath)); InputSampler.RandomSampler&lt;Text,Text&gt; sampler =new InputSampler.RandomSampler&lt;&gt;(0.01,1000,100); InputSampler.writePartitionFile(job,sampler); job.setPartitionerClass(TotalOrderPartitioner.class);//todo job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 参考:https://www.iteblog.com/archives/2146.htmlhttps://www.iteblog.com/archives/2147.htmlhttps://flyingdutchman.iteye.com/blog/1878962]]></content>
      <categories>
        <category>mapreduce编程</category>
      </categories>
      <tags>
        <tag>mapreduce编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[去重]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%8E%BB%E9%87%8D%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.qunar.mr.removeduplicate;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;/** * http://www.aboutyun.com/thread-7041-1-1.html */public class RemoveDuplicate &#123; public static class Map extends Mapper&lt;Object,Text, Text,Text&gt;&#123; private static Text line = new Text(); public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123;//todo line = value; context.write(line,new Text(&quot;&quot;)); &#125; &#125; public static class Reduce extends Reducer&lt;Text,Text,Text,Text&gt;&#123; public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException &#123;//todo context.write(key,new Text(&quot;&quot;)); &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;//todo Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;RemoveDuplicate&quot;);//todo job.setJarByClass(RemoveDuplicate.class); job.setMapperClass(Map.class); job.setCombinerClass(Reduce.class);//todo job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.addInputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/removeduplicate/in&quot;)); FileOutputFormat.setOutputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/removeduplicate/out&quot;)); System.exit(job.waitForCompletion(true) ? 0: 1); &#125;&#125;]]></content>
      <categories>
        <category>mapreduce编程</category>
      </categories>
      <tags>
        <tag>mapreduce编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordcount]]></title>
    <url>%2F2018%2F02%2F11%2Fwordcount%2F</url>
    <content type="text"><![CDATA[最基本的，需要手动写出。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.qunar.mr.wordcount;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.StringTokenizer;/** * StringTokenizer * https://www.cnblogs.com/gnivor/p/4509268.html * * * http://hadoop.apache.org/docs/r1.0.4/cn/mapred_tutorial.html#Map%2FReduce+-+%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2 * http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Payload * */public class WordCount &#123; public static class TokenizerMapper extends Mapper&lt;Object, Text,Text, IntWritable&gt;&#123; private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; StringTokenizer itr = new StringTokenizer(value.toString());//StringTokenizer while (itr.hasMoreTokens())&#123; word.set(itr.nextToken()); context.write(word,one); &#125; &#125; &#125; public static class IntSumReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt;&#123; private IntWritable result = new IntWritable(); public void reduce(Text key,Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val:values) &#123; sum += val.get(); &#125; result.set(sum); context.write(key,result); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;word count&quot;); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job,new Path(&quot;/Users/lifei/Desktop/ab/a.txt&quot;)); FileOutputFormat.setOutputPath(job,new Path(&quot;/Users/lifei/Desktop/ab_out&quot;)); System.exit(job.waitForCompletion(true)? 0 : 1); &#125;&#125;]]></content>
      <categories>
        <category>mapreduce编程</category>
      </categories>
      <tags>
        <tag>mapreduce编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hfile格式]]></title>
    <url>%2F2017%2F02%2F18%2FHfile%2F</url>
    <content type="text"><![CDATA[HBase的数据以KeyValue(Cell)的形式顺序的存储在HFile中，在MemStore的Flush过程中生成HFile，由于MemStore中存储的Cell遵循相同的排列顺序，因而Flush过程是顺序写，我们直到磁盘的顺序写性能很高，因为不需要不停的移动磁盘指针。 HFile参考BigTable的SSTable和Hadoop的TFile实现，从HBase开始到现在，HFile经历了三个版本，其中V2在0.92引入，V3在0.98引入。 V1V1的HFile由多个Data Block、Meta Block、FileInfo、Data Index、Meta Index、Trailer组成。 Data Block是HBase的最小存储单元，在前文中提到的BlockCache就是基于Data Block的缓存的。一个Data Block由一个魔数和一系列的KeyValue(Cell)组成，魔数是一个随机的数字，用于表示这是一个Data Block类型，以快速监测这个Data Block的格式，防止数据的破坏。Data Block的大小可以在创建Column Family时设置(HColumnDescriptor.setBlockSize())，默认值是64KB，大号的Block有利于顺序Scan，小号Block利于随机查询，因而需要权衡。 Meta块是可选的 FileInfo是固定长度的块，它纪录了文件的一些Meta信息，例如：AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY等。 Data Index和Meta Index纪录了每个Data块和Meta块的其实点、未压缩时大小、Key(起始RowKey？)等。 Trailer纪录了FileInfo、Data Index、Meta Index块的起始位置，Data Index和Meta Index索引的数量等。其中FileInfo和Trailer是固定长度的。 KeyValueHFile里面的每个KeyValue对就是一个简单的byte数组。但是这个byte数组里面包含了很多项，并且有固定的结构。我们来看看里面的具体结构： 开始是两个固定长度的数值，分别表示Key的长度和Value的长度。紧接着是Key，开始是固定长度的数值，表示RowKey的长度，紧接着是 RowKey，然后是固定长度的数值，表示Family的长度，然后是Family，接着是Qualifier，然后是两个固定长度的数值，表示Time Stamp和Key Type（Put/Delete）。Value部分没有这么复杂的结构，就是纯粹的二进制数据了。随着HFile版本迁移，KeyValue(Cell)的格式并未发生太多变化，只是在V3版本，尾部添加了一个可选的Tag数组。 HFileV1版本的在实际使用过程中发现它占用内存多，并且Bloom File和Block Index会变的很大，而引起启动时间变长。其中每个HFile的Bloom Filter可以增长到100MB，这在查询时会引起性能问题，因为每次查询时需要加载并查询Bloom Filter，100MB的Bloom Filer会引起很大的延迟；另一个，Block Index在一个HRegionServer可能会增长到总共6GB，HRegionServer在启动时需要先加载所有这些Block Index，因而增加了启动时间。为了解决这些问题，在0.92版本中引入HFileV2版本： V2在这个版本中，Block Index和Bloom Filter添加到了Data Block中间，而这种设计同时也减少了写的内存使用量；另外，为了提升启动速度，在这个版本中还引入了延迟读的功能，即在HFile真正被使用时才对其进行解析。 对HFileV2格式具体分析，它是一个多层的类B+树索引，采用这种设计，可以实现查找不需要读取整个文件： Data Block中的Cell都是升序排列，每个block都有它自己的Leaf-Index，每个Block的最后一个Key被放入Intermediate-Index中，Root-Index指向Intermediate-Index。在HFile的末尾还有Bloom Filter用于快速定位那么没有在某个Data Block中的Row；TimeRange信息用于给那些使用时间查询的参考。在HFile打开时，这些索引信息都被加载并保存在内存中，以增加以后的读取性能。https://blog.csdn.net/pun_c/article/details/46841625 V3FileV3版本基本和V2版本相比，并没有太大的改变，它在KeyValue(Cell)层面上添加了Tag数组的支持；并在FileInfo结构中添加了和Tag相关的两个字段。 参考：https://www.cnblogs.com/ios1988/p/6266767.html]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RegionServer的故障恢复]]></title>
    <url>%2F2017%2F02%2F18%2FRegionServer%E7%9A%84%E6%95%85%E9%9A%9C%E6%81%A2%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[我们知道，RegionServer的相关信息保存在ZK中，在RegionServer启动的时候，会在Zookeeper中创建对应的临时节点。RegionServer通过Socket和Zookeeper建立session会话，RegionServer会周期性地向Zookeeper发送ping消息包，以此说明自己还处于存活状态。而Zookeeper收到ping包后，则会更新对应session的超时时间。当Zookeeper超过session超时时间还未收到RegionServer的ping包，则Zookeeper会认为该RegionServer出现故障，ZK会将该RegionServer对应的临时节点删除，并通知Master，Master收到RegionServer挂掉的信息后就会启动数据恢复的流程。Master启动数据恢复流程后，其实主要的流程如下：RegionServer宕机—》ZK检测到RegionServer异常—》Master启动数据恢复—》Hlog切分—》Region重新分配—》Hlog重放—》恢复完成并提供服务故障恢复有3中模式，下面就一一来介绍。 Log Splitting在最开始的恢复流程中，Hlog的整个切分过程都由于Master来执行，如下图所示： a、将待切分的日志文件夹进行重命名，防止RegionServer未真的宕机而持续写入Hlogb、Master启动读取线程读取Hlog的数据，并将不同RegionServer的日志写入到不同的内存buffer中c、针对每个buffer，Master会启动对应的写线程将不同Region的buffer数据写入到HDFS中，对应的路径 为/hbase/table_name/region/recoverd.edits/.tmp。d、Master重新将宕机的RegionServer中的Rgion分配到正常的RegionServer中，对应的RegionServer读取Region的数据，会发现该region目录下的recoverd.edits目录以及相关的日志，然后RegionServer重放对应的Hlog日志，从而实现对应Region数据的恢复。从上面的步骤中，我们可以看出Hlog的切分一直都是master在干活，效率比较低。设想，如果集群中有多台RegionServer在同一时间宕机，会是什么情况？串行修复，肯定异常慢，因为只有master一个人在干Hlog切分的活。因此，为了提高效率，开发了Distributed Log Splitting架构。post-script:/hbase/table_name/region/为hbase表在hdfs上的实际目录 Distributed Log Splitting顾名思义，Distributed Log Splitting是LogSplitting的分布式实现，分布式就不是master一个人在干活了，而是充分使用各个RegionServer上的资源，利用多个RegionServer来并行切分Hlog，提高切分的效率。如下图所示： 上图的操作顺序如下：a、Master将要切分的日志发布到Zookeeper节点上（/hbase/splitWAL），每个Hlog日志一个任务，任务的初始状态为TASK_UNASSIGNEDb、在Master发布Hlog任务后，RegionServer会采用竞争方式认领对应的任务（先查看任务的状态，如果是TASK_UNASSIGNED，就将该任务状态修改为TASK_OWNED）c、RegionServer取得任务后会让对应的HLogSplitter线程处理Hlog的切分，切分的时候读取出Hlog的对，然后写入不同的Region buffer的内存中。d、RegionServer启动对应写线程，将Region buffer的数据写入到HDFS中，路径为/hbase/table/region/seqenceid.temp，seqenceid是一个日志中该Region对应的最大sequenceid，如果日志切分成功，而RegionServer会将对应的ZK节点的任务修改为TASK_DONE，如果切分失败，则会将任务修改为TASK_ERR。e、如果任务是TASK_ERR状态，则Master会重新发布该任务，继续由RegionServer竞争任务，并做切分处理。f、Master重新将宕机的RegionServer中的Rgion分配到正常的RegionServer中，对应的RegionServer读取Region的数据，将该region目录下的一系列的seqenceid.temp进行从小到大进行重放，从而实现对应Region数据的恢复。从上面的步骤中，我们可以看出Distributed Log Splitting采用分布式的方式，使用多台RegionServer做Hlog的切分工作，确实能提高效率。正常故障恢复可以降低到分钟级别。但是这种方式有个弊端是会产生很多小文件（切分的Hlog数 宕机的RegionServer上的Region数）。比如一个RegionServer有20个Region，有50个Hlog，那么产生的小文件数量为20*50=1000个。如果集群中有多台RegionServer宕机的情况，小文件更是会成倍增加，恢复的过程还是会比较慢。由次诞生了Distributed Log Replay模式。 Distributed Log ReplayDistributed Log Replay和Distributed Log Splitting的不同是先将宕机RegionServer上的Region分配给正常的RgionServer，并将该Region标记为recovering。再使用Distributed Log Splitting类似的方式进行Hlog切分，不同的是，RegionServer将Hlog切分到对应Region buffer后，并不写HDFS，而是直接进行重放。这样可以减少将大量的文件写入HDFS中，大大减少了HDFS的IO消耗。如下图所示：]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Region的拆分]]></title>
    <url>%2F2017%2F02%2F18%2FRegion%E7%9A%84%E6%8B%86%E5%88%86%2F</url>
    <content type="text"><![CDATA[Hbase Region的三种拆分策略Hbase Region的拆分策略有比较多，比如除了3种默认过的策略，还有DelimitedKeyPrefixRegionSplitPolicy、KeyPrefixRegionSplitPolicy、DisableSplitPolicy等策略，这里只介绍3种默认的策略。分别是ConstantSizeRegionSplitPolicy策略、IncreasingToUpperBoundRegionSplitPolicy策略和SteppingSplitPolicy策略。 ConstantSizeRegionSplitPolicyConstantSizeRegionSplitPolicy策略是0.94版本之前的默认拆分策略，这个策略的拆分规则是：当region大小达到hbase.hregion.max.filesize（默认10G）后拆分。这种拆分策略对于小表不太友好，按照默认的设置，如果1个表的Hfile小于10G就一直不会拆分。注意10G是压缩后的大小，如果使用了压缩的话。如果1个表一直不拆分，访问量小也不会有问题，但是如果这个表访问量比较大的话，就比较容易出现性能问题。这个时候只能手工进行拆分。还是很不方便。 IncreasingToUpperBoundRegionSplitPolicyIncreasingToUpperBoundRegionSplitPolicy策略是Hbase的0.94~2.0版本默认的拆分策略，这个策略相较于ConstantSizeRegionSplitPolicy策略做了一些优化，该策略的算法为：min(maxFileSize,r^2flushSize )，最大为maxFileSize 。从这个算是我们可以得出maxFileSize为10G,flushsize为128M的情况下，可以计算出Region的分裂情况如下：第一次拆分大小为：min(10G，11128M)=128M第二次拆分大小为：min(10G，33128M)=1152M第三次拆分大小为：min(10G，55128M)=3200M第四次拆分大小为：min(10G，77128M)=6272M第五次拆分大小为：min(10G，99128M)=10G第五次拆分大小为：min(10G，1111*128M)=10G从上面的计算我们可以看到这种策略能够自适应大表和小表，但是这种策略会导致小表产生比较多的小region，对于小表还是不是很完美。 SteppingSplitPolicySteppingSplitPolicy是在Hbase 2.0版本后的默认策略，，拆分规则为：If region=1 then: flush size 2 else: MaxRegionFileSize。还是以flushsize为128M、maxFileSize为10G场景为列，计算出Region的分裂情况如下：第一次拆分大小为：2128M=256M第二次拆分大小为：10G从上面的计算我们可以看出，这种策略兼顾了ConstantSizeRegionSplitPolicy策略和IncreasingToUpperBoundRegionSplitPolicy策略，对于小表也肯呢个比较好的适配。Hbase Region拆分的详细流程Hbase的详细拆分流程图如下： 从上图我们可以看出Region切分的详细流程如下：第1步、会在ZK的/hbase/region-in-transition/region-name下创建一个znode，并设置状态为SPLITTING第2步、master通过watch节点检测到Region状态的变化，并修改内存中Region状态的变化第3步、RegionServer在父Region的目录下创建一个名称 为.splits的子目录第4步、RegionServer关闭父Region，强制将数据刷新到磁盘，并这个Region标记为offline的状态。此时，落到这个Region的请求都会返回NotServingRegionException这个错误第5步、RegionServer在.splits创建daughterA和daughterB，并在文件夹中创建对应的reference文件，指向父Region的Region文件第6步、RegionServer在HDFS中创建daughterA和daughterB的Region目录，并将reference文件移动到对应的Region目录中第7步、在.META.表中设置父Region为offline状态，不再提供服务，并将父Region的daughterA和daughterB的Region添加到.META.表中，已表名父Region被拆分成了daughterA和daughterB两个Region第8步、RegionServer并行开启两个子Region，并正式提供对外写服务第9步、RegionSever将daughterA和daughterB添加到.META.表中，这样就可以从.META.找到子Region，并可以对子Region进行访问了第10步、RegionServr修改/hbase/region-in-transition/region-name的znode的状态为SPLIT transition(过渡，转变，变迁)Transaction(事务)备注：为了减少对业务的影响，Region的拆分并不涉及到数据迁移的操作，而只是创建了对父Region的指向。只有在做大合并的时候，才会将数据进行迁移。那么通过reference文件如何才能查找到对应的数据呢？如下图所示： 根据文件名来判断是否是reference文件由于reference文件的命名规则为前半部分为父Region对应的File的文件名，后半部分是父Region的名称，因此读取的时候也根据前半部分和后半部分来识别根据reference文件的内容来确定扫描的范围，reference的内容包含两部分，一部分是切分点splitkey，另一部分是boolean类型的变量（true或者false）。如果为true则扫描文件的上半部分，false则扫描文件的下半部分接下来确定了扫描的文件，以及文件的扫描范围，那就按照正常的文件检索了]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase读请求分析]]></title>
    <url>%2F2017%2F02%2F18%2FHBase%E8%AF%BB%E8%AF%B7%E6%B1%82%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文主要是基于HBase的0.98.8版本的实现。HBase能提供实时计算服务主要原因是由其架构和底层的数据结构决定的，即由LSM-Tree(Log-Structured Merge-Tree) + HTable(region分区) + Cache决定——客户端可以直接定位到要查数据所在的HRegion server服务器，然后直接在服务器的一个region上查找要匹配的数据，并且这些数据部分是经过cache缓存的。 客户端读请求HBase为客户端提供了的读请求API主要有两个，get和scan。其中，get是通过指定单个的rowKey，获取其对应的value值。而scan是指定startRow和stopRow两个边界rowKey来指定范围rowKey值，获取它们对应的value值。 首先我们知道HBase会根据算法把一个表划分为多个Region，然后HMaster会把这几个Region分配到不同的节点中去，因此，当我们查找一个表的数据时，有可能要把请求发送到不同的节点，再获取其查询结果。但是要如何根据get指定的rowKey到指定的节点上查询呢（scan的流程相差不大）？HBase依靠hbase:meta这个系统表实现，事实上hbase:meta表和普通的HBase表一样，都是要分配到某个节点上，如果这个节点出现故障就会转移到另外可用的节点上，这样就可以防止某个节点出现故障导致整个HBase服务不可用。 HBase要求客户端的读请求需要自行定位到所属的Region的节点上，然后把请求的rowKey以及Region一并发送到这个节点。 我们来看看客户端的读请求执行过程： get由于hbase:meta表在存放在某台可用的节点机器上，因此我们需要定位究竟是哪台节点。这个过程步骤如下： HBase使用zookeeper来存放hbase:meta表的位置，这样客户端就需要先到zookeeper查询hbase:meta表在哪个节点上，如图中的1。然后在根据读请求的rowKey发送请求到这个节点上，节点根据hbase:meta表查找到rowKey所属Region的节点，然后返回给客户端，即图中的2，3。客户端再真正把读请求发送到指定的HRegionServer上，如图中的4，然后HRegionServer再执行具体读请求的查询操作，最后把结果返回到客户端上 要注意的是，客户端会缓存每次hbase:meta表返回的结果缓存到本地上，这样就可以防止每次查询都要经过步骤1，2进行查询，导致过多的网络I/O操作。在上面的第2个过程中，节点是如何根据rowKey通过查询hbase:meta表得到所属的Region呢？hbase:meta表中保存了每个Region的startRow，这样可以查找小于rowKey里的最大startRow值即可确定Region，这个过程在HRegion.getClosestRowBefore()代码实现。 scan 对于scan来说，其实流程和get的查询相差不大，由于scan请求包含startRow和stopRow两个范围，因此可能需要查询多个Region。具体步骤如下： 会先发送startRow作为指定rowKey到hbase:meta表上请求Region信息，返回的Region信息会包含这个Region的startKey和endKey然后客户端再根据Regioin的范围判断。即如果stopRow&lt;=endKey，则只需要查询这个Region即可。否则，需要首先把startRow和endKey作为一次的请求发送到这个Region的节点上，然后再把endKey作为新的startRow去查询hbase:meta，然后重复步骤1 这样经过多次的迭代，即可把scan请求对应为多个Region的查询。 服务端读请求当客户端把读请求发送到对应的HRegionServer上时，服务端就会开始具体的查询了。实际上，客户端发送的读请求会包含Region信息，以及rowKey范围（如果是get，则只有一个rowKey），另外还有客户端指定的其它查询参数，包括columnFamily，timestamp，filter过滤器等。因此HRegionServer首先会简单地找出对应的HRegion对象来执行具体的读请求。另外，无论是get或者scan请求，HRegionServer都会转化成scan请求来对待，其实get请求就是一个特殊的scan请求，它的范围只有一个指定的rowKey。 对于HRegion是如何执行读请求的查询，首先要对HRegionServer内部有一个了解。 集中关注一下HRegionServer的内部结构。HRegion是HBase表经过Sharding后的一部分，每个HRegion里面可能有多个Store，每个Store就是一个Column Family的对象，同一Column Family允许有多个列，这些列的数据都必须存储到一个文件里（但有可能存在多个这样的文件），存储格式为HFile，因此每个Store都有多个StoreFile对象管理Column Family的列数据。另外注意到每个Store内部都有一个MemStore对象。MemStore是HBase用来缓存写请求数据的内存结构，由于HBase的写请求都会写MemStore，然后再写HLog（HBase的WriteAheadLog的实现，存储在HDFS里），这样写请求才算提交状态，才对读请求为可见。MemStore可以让我们从内存中读取最近写请求提交的数据，而不必从HLog中读取，因此避免了I/O操作。另外，图里没有画出的是BlockCache，这个是HRegionServer内部的单例，主要用于HFile读取时缓存HBlock，后面将会继续详述。 弄清楚HRegion的结构之后，可以知道，在某个存在读写请求的时刻里，HRegion包含表数据的地方有MemStore、HLog、HFile、BlockCache。由于MemStore的数据与HLog是等同的，因此对于每个读请求，HRegion需要查询MemStore、BlockCache和HFile这三个部分。 KeyValue 还有一个要注意的是，HBase对于数据KeyValue的定义，来看看下图KeyValue的二进制格式: 一个KeyValue只包含一个rowKey和一个对应的Value值，也就是说，如果一个rowKey在有多个列值，则会有多个KeyValue与其对应。上图的绿色部分中，第二个空格Row就是用于定义的rowKey，在HBase的内部里，对于整个KeyValue来说，包含Column Family、Column Qualifier、TimeStamp和KeyType的整个绿色Key部分才是KeyValue的key值。黄色的Value部分就是保存的列值。在查询流程里，可能会出现对比用于定义的rowKey，或者对比整个绿色部分的key值。 查询流程 为了提高查询速度，MemStore和HFile内部都是经过排序的数据，因此，在实际查询的过程中可以使用多路归并的方法进行查询。整个查询过程如下图所示： HRegion提供了Scanner接口方便解耦HRegion与MemStore、BlockCache和HFile这三个部分的查询过程。其中RegionScannerImpl负责HRegion内部查询的过程，StoreScanner则是负责HStore内部查询的过程，由于可能有多个HStore，则会有对应多个StoreScanner，MemStoreScanner则负责HStore内部的MemStore数据结构查询，StoreFileScanner则负责HStore内部多个HFile的查询。这几个部分分别用KeyValueHeap连接起来，KeyValueHeap实现了多路归并的查询逻辑。 多路归并的逻辑大致如下：首先初始化各个Scanner，这个过程需要Scanner根据要查询的KeyValue数据kv，进行一次seek查询，seek查询的结果是一个大于或等于查询值kv的peek值，然后KeyValueHeap使用优先级队列PriorityQueue保存各个Scanner，这个优先级队列会通过比较各个Scanner的peek值来进行排序，这个优先级队列的第一个元素就是这些scanner里查询kv的结果，如果这个值刚好等于kv，即成功找到，如果大于则表明不存在。如果需要查询多个值则重新进行一次seek查询，再获取优先级队列的第一个元素。可以类别优先级队列里的scanner就是多路归并的各个有序队列，这样就很容易理解算法。 另外，在StoreScanner的初始化过程中，为了减少进行多路归并的Scanner数，提高查询速度，会对Scanner进行一次简单的过滤。MemStoreScanner，会根据查询kv值的时间戳判断是否在TimeRange，以及TTL来判断。而StoreFileScanner则会利用TimeRange（包括TTL）、KeyRange、BloomFilter判断kv值是否在HFile中。BloomFilter特性通过计算kv的哈希值来快速判断是否存在，但有一定的失败几率。 从KeyValueHeap的多路归并算法逻辑可以看到，Scanner关键是如何通过seek查询更新对应的peek值。接下来主要分析一下MemStoreScanner和StoreFileScanner是如何通过seek查询来更新peek值的。 MemStoreScannerMemStoreScanner直接引用MemStore内部的两个主要的数据结构：kvset和snapshot。这两个数据结构都是保存KeyValue数据的KeyValueSkipListSet。KeyValueSkipListSet是HBase单独实现的一个专门保存KeyValue的SkipList，内部用ConcurrentNavigableMap封装实现了高效并发的有序队列。kvset是当HBase写入数据KeyValue的缓存，而snapshot则是指快照，这是MemStore大小超过阀值，需要把kvset数据写入HFile的时候生成的副本，这样当flush这份副本到HFile里的时候，减少对写请求的影响。 MemStoreScanner的seek过程比较简单，代码流程如下：123456789101112131415161718192021222324252627public synchronized KeyValue peek() &#123; //DebugPrint.println(&quot; MS@&quot; + hashCode() + &quot; peek = &quot; + getLowest()); return theNext;&#125; public synchronized boolean seek(KeyValue key) &#123; if (key == null) &#123; close(); return false; &#125; //tailSet返回的是大于等于key的集合 kvsetIt = kvsetAtCreation.tailSet(key).iterator(); snapshotIt = snapshotAtCreation.tailSet(key).iterator(); kvsetItRow = null; snapshotItRow = null; return seekInSubLists(key);&#125; private synchronized boolean seekInSubLists(KeyValue key)&#123; //getNext从迭代器获取符合的KeyValue值 kvsetNextRow = getNext(kvsetIt); snapshotNextRow = getNext(snapshotIt); // Calculate the next value theNext = getLowest(kvsetNextRow, snapshotNextRow); // has data return (theNext != null);&#125; 上面代码主要实现逻辑比较简单，kvsetAtCreation和snapshotAtCreation两个数据结构（引用MemStore的kvset和snapshot），通过比较KeyValue的绿色key部分，来查找大于等于的集合，然后getNext需要通过比较MVCC版本号以及rowKey部分找出集合里符合KeyValue的值，最后再从两个集合里面找出最小的那个KeyValue，同样也是类似多路合并的思想，这样的结果就是theNext的值，也就是peek返回的值。 这里的MVCC版本号是HBase提高读写并发事务，并维持一致性的实现。 StoreFileScannerStoreFileScanner由于涉及到BlockCache缓存，HFile读写，Bloom filter等特性，逻辑实现较为复杂。StoreFileScanner会首先利用HFile记录的时间戳范围、rowKey范围、Bloom filter来判断给定的KeyValue是否存在HFile中，如果不存在则直接跳过HFile的查找。然后会StoreFileScanner的seek操作就要涉及到HFile的查找过程。HFile采用HBlock作为文件操作的基本单位，有不同类型的HBlock，包括DataBlock（保存KeyValue），IndexBlock（保存索引以及对应的rowKey）、BloomMeta（Bloom filter相关）等。HFile采用类似B+树的多级索引算法，优化在数据量大的情况下查找一个指定的KeyValue的速度。在多级索引查找过程中，读取HBlock都会尝试从BlockCache缓存读取指定Block，尽量避免I/O操作；由于所有的Block的rowKey都是有序的，IndexBlock都是采用二分查找算法来得到下一个Block的offset，有助于提高查找速度。接下来是较为详细的实现解析。 HFile HBase的HFile目前有三个不同的版本，本文主要研究HFile v2的布局。首先来看看它的文件布局格式： HFile V2有四部分：”Scanned block”、”Non-scanned block”、”Load-on-open”和Trailer部分（详细请参考这里）。HFile的初始化记载过程中，默认把Trailer和”Load-on-open”部分加载到内存中。首先会加载位于文件末尾的Trailer，包含着HFile的版本、索引数、压缩/非压缩大小、”Load-on-open”部分的偏移等文件信息。从Trailer获得”Load-on-open”部分的偏移开始加载”Load-on-open”部分，这部分主要包括RootDataIndex、midkey、MetaIndex（V2起不再存储Bloom信息，但仍支持）、FileInfo以及Bloom filter metadata。RootDataIndex就是根索引Block，midKey是用于HFile在split的时候快速找到处于中间Key的值。FileInfo包括了LastKey、平均Key长度等信息，BloomFilterMetaData就是Bloom filter的rootIndexBlock。 对于多级索引的算法，是按照数据量大小来确定究竟需要查找多少个IndexBlock的。 在数据量较少的情况下，可以直接从RootDataIndex直接定位DataBlock，也就是RootDataIndex-&gt;DataBlock； 当数据量较大时，索引的数量超过RootDataIndex的大小，因此就要建立多级索引，索引的级数与数据量大小有关，其路径为Root-&gt;Intermediate(0…N)-&gt;Leaf(0…1)-&gt;DataBlock，除了Root和DataBlock必定会有之外，Intermediate会有0到N个（取决于数据量大小），Leaf会有0到1个； Bloom filter只采用一级索引，也就是只会有BloomFilterMetaData-&gt;BloomBlock这样的路径。 HFile保证在IndexBlock中的rowKey有序，这样在查找rowKey对应的索引中，采用的是二分查找算法，稍微有点不同的是，这里的二分查找并不是一定要找到对应的rowKey，而是要找到刚好小于等于这个rowKey的索引，这是由于这个rowKey都是IndexBlock或者DataBlock里最小值。 二分查找到最后确定DataBlock的时候，就需要顺序遍历DataBlock里的KeyValue值，由于每个DataBlock大小约为64kb（这里大约是由于DataBlock是按照KeyValue顺序写入，当超过64KB的时候才会停止写这个DataBlock，另外压缩也会有影响），因此遍历比较的时候不会有太大性能影响。 BlockCache在以上的HFile查找过程中，需要大量I/O读取HBlock，因此HBase提供了BlockCache缓存机制，专门优化读性能。BlockCache是以HBlock为基本单位进行缓存的内存结构，经过多次的改进与性能测试，HBase官方推荐的一种做法是按照HBlock的类型进行分类缓存。事实上，由于在读请求里，对于IndexBlock的请求命中率几乎可以达到100%，而DataBlock由于业务的不确定性，可能会有巨大的命中率差距，因此把这两种类型的Block进行分类缓存，其中IndexBlock会缓存到LruBlockCache中，而DataBlock则缓存到BucketCache中。 LruBlockCache是利用LRU算法+优先级比重对HBlock进行缓存。优先级比重是HBase对HBlock进行优先级划分，分为SINGLE、MULTI、MEMORY三个等级，刚开始缓存为SINGLE等级，如果后面有读请求命中则升级到MULTI等级，MEMORY等级是用户指定HBase表设置的。进行淘汰的时候，会尽可能地按照SINGLE、MULTI、MEMORY的优先级来进行淘汰。在我们的线上集群中，几台HRegionServer的LruBlockCache命中率几乎为100%。 BucketCache同样也是利用LRU算法对HBlock进行缓存。但不同的是，由于它是针对DataBlock进行缓存，如果在随机读较多的情景下，很容易造成大量GC，这个在我们集群中曾经造成过严重的问题，因此可以把它缓存的介质指定为offheap（也支持SSD的File方式），这样可以有效减轻GC带来的影响。BucketCache还有一个优点，就是它把缓存大小分为很多个尺寸，每个尺寸都有对应一个槽，这些槽可以互相转换，这对于经过压缩的DataBlock来说能够大大提高内存利用率。 多级索引+二分查找的做法不但有助于提高读性能，允许动态读取索引Block，减轻读取HFile内存初始化时的压力。BlockCache的使用也能够尽可能地利用好内存进行缓存，避免产生I/O。另外，由于多个HFile是并行查找的，如果对多个小的HFile进行compact操作，可以避免减少多个HFile的读操作。 总结 HBase使用了多种不同的内存结构、多级索引、二分查找等各种方法提高读请求性能，另外把HFile以HBlock为基本单位进行读写，减轻I/O的同时，也能够动态按照需要进行Block读取，减轻内存压力。 参考：https://blog.csdn.net/pun_c/article/details/46841625https://www.cnblogs.com/ulysses-you/p/10072883.html#_label2]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase写入逻辑]]></title>
    <url>%2F2017%2F02%2F18%2FHbase%E5%86%99%E5%85%A5%E9%80%BB%E8%BE%91%2F</url>
    <content type="text"><![CDATA[写入逻辑HBase仅仅支持行级别的事务一致性。本文主要探讨一下HBase的写请求流程。主要基于0.98.8版本号的实现Hbase的写逻辑涉及到写内存、写log、刷盘等操作从上图可以看出氛围3步骤：第1步：Client获取数据写入的Region所在的RegionServer第2步：请求写Hlog第3步：请求写MemStore只有当写Hlog和写MemStore都成功了才算请求写入完成。MemStore后续会逐渐刷到HDFS中。备注：Hlog存储在HDFS，当RegionServer出现异常，需要使用Hlog来恢复数据。 client写请求HBase提供的Java client API是以HTable为主要接口，相应当中的HBase表。写请求API主要为HTable.put（write和update）、HTable.delete等。 以HTable.put为样例，首先来看看客户端是怎么把请求发送到HRegionServer的。 每一个put请求表示一个KeyValue数据，考虑到client有大量的数据须要写入到HBase表，HTable.put默认是会把每一个put请求都放到本地缓存中去，当本地缓存大小超过阀值（默觉得2MB）的时候，就要请求刷新，即把这些put请求发送到指定的HRegionServer中去，这里是利用线程池并发发送多个put请求到不同的HRegionServer。 但假设多个请求都是同一个HRegionServer，甚至是同一个HRegion，则可能造成对服务端造成压力，为了避免发生这样的情况，clientAPI会对写请求做了并发数限制，主要是针对put请求须要发送到的HRegionServer和HRegion来进行限制。详细实如今AsyncProcess中。 主要參数设定为：123hbase.client.max.total.tasks 客户端最大并发写请求数。默觉得100hbase.client.max.perserver.tasks 客户端每一个HRegionServer的最大并发写请求数。默觉得2hbase.client.max.perregion.tasks 客户端每一个HRegion最大并发写请求数。默觉得1 为了提高I/O效率。AsyncProcess会合并同一个HRegion相应的put请求，然后再一次把这些同样HRegion的put请求发送到指定HRegionServer上去。另外AsyncProcess也提供了各种同步的方法，如waitUntilDone等，方便某些场景下必须对请求进行同步处理。每一个put和读请求一样。都是要通过訪问hbase:meta表来查找指定的HRegionServer和HRegion，这个流程和读请求一致。 服务端写请求当client把写请求发送到服务端时。服务端就要開始运行写请求操作。HRegionServer把写请求转发到指定的HRegion运行，HRegion每次操作都是以批量写请求为单位进行处理的。主要流程实如今HRegion.doMiniBatchMutation,大致例如以下： 应用了zookeeper的分布式事务锁的方式：1.获取region锁 有了这个锁的region，想要获取这个有锁的region的进程必须等待2.依次获取各行的锁：拿到每一行的锁3.先将拿到锁的一行数据，写入到memstore中，4.释放写入完毕的这一行数据的行锁5.写数据到wal中6.Memstore和wal都写入以后，释放region锁 为什么数据通过wal已经写入到hdfs中了，还要通过memstore再将数据往 内存中写一次呢？答：是为了数据的写入和读取,还有就是wal主要是为了回滚。为什么会有回滚操作机制？有可能写入到内存成功，但是没有成功的写入到hdfs中，比如hdfs的主机挂掉了，没有写入成功，就不能正常的释放锁，因此就会产生异常，然后启动回滚的操作，将内存的数据也删除回滚就会可能出现脏读的情况：刷新的速度比释放锁的速度块 ，刷新了就把内存中的数据，立马写入到hdfs中 这个可能大概是千万毫秒分子1先写memstore再写wal的优缺点？优点：这条数据没有写入到hdfs中，用户就可以访问了====habse速度快的原因缺点：脏读 Hbase的不同版本，menstore和wal写入的顺序是有差异的1.0以前的老版本：先写wal，写完了以后再写memstore新版本：memstore和wal是并行同时写数据，但是由于memstore’是将数据写入内存中，而wal是将数据写入hdfs中，因此memstore写数据的速度要块，因此memstore先于wal结束，释放行锁 先写memstore再写wal的优缺点？优点：这条数据没有写入到hdfs中，用户就可以访问了====habse速度快的原因缺点：脏读为什么会有回滚操作机制？有可能写入到内存成功，但是没有成功的写入到hdfs中，比如hdfs的主机挂掉了，没有写入成功，就不能正常的释放锁，因此就会产生异常，然后启动回滚的操作，将内存的数据也删除回滚就会可能出现脏读的情况：刷新的速度比释放锁的速度块 ，刷新了就把内存中的数据，立马写入到hdfs中 这个可能大概是千万毫秒分子1 MemStore刷盘为了提高Hbase的写入性能，当写请求写入MemStore后，不会立即刷盘生成storefile。而是会等到一定的时候进行刷盘的操作。具体是哪些场景会触发刷盘的操作呢？总结成如下的几个场景： 全局内存控制这个全局的参数是控制内存整体的使用情况，当所有memstore占整个heap的最大比例的时候，会触发刷盘的操作。这个参数是hbase.regionserver.global.memstore.upperLimit，默认为整个heap内存的40%。但这并不意味着全局内存触发的刷盘操作会将所有的MemStore都进行输盘，而是通过另外一个参数hbase.regionserver.global.memstore.lowerLimit来控制，默认是整个heap内存的35%。当flush到所有memstore占整个heap内存的比率为35%的时候，就停止刷盘。这么做主要是为了减少刷盘对业务带来的影响，实现平滑系统负载的目的。 MemStore达到上限当MemStore的大小达到hbase.hregion.memstore.flush.size大小的时候会触发刷盘，默认128M大小 RegionServer的Hlog数量达到上限前面说到Hlog为了保证Hbase数据的一致性，那么如果Hlog太多的话，会导致故障恢复的时间太长，因此Hbase会对Hlog的最大个数做限制。当达到Hlog的最大个数的时候，会强制刷盘。这个参数是hase.regionserver.max.logs，默认是32个。 手工触发可以通过hbase shell或者java api手工触发flush的操作。 关闭RegionServer触发在正常关闭RegionServer会触发刷盘的操作，全部数据刷盘后就不需要再使用Hlog恢复数据。 Region使用HLOG恢复完数据后触发当RegionServer出现故障的时候，其上面的Region会迁移到其他正常的RegionServer上，在恢复完Region的数据后，会触发刷盘，当刷盘完成后才会提供给业务访问。 每一次Memstore的flush，会为每一个CF创建一个新的storeFile。。 在读方面相对来说就会简单一些：HBase首先检查请求的数据是否在Memstore，不在的话就到HFile中查找，最终返回merged的一个结果给用户。 每次Memstore Flush，会为每个CF都创建一个新的HFile（storeFile）。这样，不同CF中数据量的不均衡将会导致产生过多HFile：当其中一个CF的Memstore达到阈值flush时，所有其他CF的也会被flush。如上所述，太频繁的flush以及过多的HFile将会影响集群性能。 Compaction MemStore频繁的Flush就会创建大量的storeFile(Hfile)。这样HBase在检索的时候，就不得不读取大量的storeFile，读性能会受很大影响。为预防打开过多HFile及避免读性能恶化，HBase有专门的HFile合并处理(HFile Compaction Process)。HBase会周期性的合并数个小HFile为一个大的HFile。明显的，有Memstore Flush产生的HFile越多，集群系统就要做更多的合并操作(额外负载)。更糟糕的是：Compaction处理是跟集群上的其他请求并行进行的。当HBase不能够跟上Compaction的时候(同样有阈值设置项)，会在RS上出现“写阻塞”。像上面说到的，这是最最不希望的。 提示：严重关切RS上Compaction Queue 的size。要在其引起问题前，阻止其持续增大。Compaction的主要目的，是为了减少同一个Region同一个ColumnFamily下面的小文件数目，从而提升读取的性能。 小合并（MinorCompaction）由前面的刷盘部分的介绍，我们知道MemStore会将数据刷到磁盘，生产StoreFile，因此势必产生很多的小问题，对于Hbase的读取，如果要扫描大量的小文件，会导致性能很差，因此需要将这些小文件合并成大一点的文件。因此所谓的小合并，就是把多个小的StoreFile组合在一起，形成一个较大的StoreFile，通常是累积到3个Store File后执行。通过参数hbase.hstore,compactionThreadhold配置。小合并的大致步骤为： 分别读取出待合并的StoreFile文件的KeyValues，并顺序地写入到位于./tmp目录下的临时文件中 将临时文件移动到对应的Region目录中 将合并的输入文件路径和输出路径封装成KeyValues写入WAL日志，并打上compaction标记，最后强制自行sync 将对应region数据目录下的合并的输入文件全部删除，合并完成这种小合并一般速度很快，对业务的影响也比较小。本质上，小合并就是使用短时间的IO消耗以及带宽消耗换取后续查询的低延迟。小范围的Compaction。有最少和最大文件数目限制。通常会选择一些连续时间范围的小文件进行合并====小合并,Minor Compaction选取文件时，遵循一定的算法。 大合并（MajorCompaction）所谓的大合并，就是将一个Region下的所有StoreFile合并成一个StoreFile文件，在大合并的过程中，之前删除的行和过期的版本都会被删除，拆分的母Region的数据也会迁移到拆分后的子Region上。大合并一般一周做一次，控制参数为hbase.hregion.majorcompaction。大合并的影响一般比较大，尽量避免统一时间多个Region进行合并，因此Hbase通过一些参数来进行控制，用于防止多个Region同时进行大合并。该参数为： hbase.hregion.majorcompaction.jitter具体算法为：123hbase.hregion.majorcompaction参数的值乘于一个随机分数，这个随机分数不能超过hbase.hregion.majorcompaction.jitter的值。hbase.hregion.majorcompaction.jitter的值默认为0.5。通过hbase.hregion.majorcompaction参数的值加上或减去hbase.hregion.majorcompaction参数的值乘于一个随机分数的值就确定下一次大合并的时间区间。用户如果想禁用major compaction，只需要将参数hbase.hregion.majorcompaction设为0。建议禁用。 涉及该Region该ColumnFamily下面的所有的HFile文件。删除的数据。===大合并 需要删除的数据包含哪些数据？1.ttl 带有有效期的数据，有效期过来之后，不能被访问，但是仍然存在2.Put数据：时间戳版本数据数量大于vsrsion可存在的数量3.需要删除的数据，将数据标记为删除 区别Major和minor合并的区别有哪些？1.合并范围不同小范围：在一定时间内，将多个小文件合并在一起，合并的是一部分发 hfile大范围：将所有的hfile合并2.功能不同小范围：只进行整理大范围：删除只会在大合并的时候执行操作， 一般情况下，不会做大合并！12Region是HBase中分布式存储和负载均衡的最小单元。不同Region分布到不同RegionServer上，但并不是存储的最小单元。Region由一个或者多个Store组成，每个store保存一个columns family，每个Strore又由一个memStore和0至多个StoreFile 组成。memStore存储在内存中， StoreFile存储在HDFS上。 防止误区：开启大合并，整个hbase的所有的hfile要合并，但是并不是所有的hfile合并成一个hfile，而是一个列族(store)当中的所有的storefile合并成一个storefile，所有的列族开始合并操作,major合并之后，一个store只有一个storeFile文件，会对store的所有数据进行重写，有较大的性能消耗 很少做大合并，为什么？工程量过大如果一定要大合并呢？对于某一张表的某些列族，手动执行大合并，不建议开启大合并！！！ 为什么不建议开启大合并？因为在执行大合并的时候，整个hbase的hfile都在参与合并，整个hbase会锁住，此时读数据和写数据都不行为什么可以做到不执行大合并操作来达到增加空间呢？因为hbase的底层是hdfs，如果空间不够，可以任意的增加硬盘，增加服务器 参考：https://www.cnblogs.com/liguangsunls/p/6792705.htmlhttps://blog.csdn.net/qq_41919284/article/details/81676636https://blog.csdn.net/xiao_jun_0820/article/details/26580247]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase中的zookeeper]]></title>
    <url>%2F2017%2F02%2F18%2Fhbase%E4%B8%AD%E7%9A%84zookeeper%2F</url>
    <content type="text"><![CDATA[ZooKeeper：协调者ZooKeeper为HBase集群提供协调服务 它管理着HMaster和HRegionServer的状态(available/alive等)，并且会在它们宕机时通知给HMaster，从而HMaster可以实现HMaster之间的failover 对宕机的HRegionServer中的HRegion集合的修复(将它们分配给其他的HRegionServer)。ZooKeeper集群本身使用一致性协议(PAXOS协议)保证每个节点状态的一致性。 通过Zoopkeeper存储元数据的统一入口地址 How The Components Work TogetherZooKeeper协调集群所有节点的共享信息，在HMaster和HRegionServer连接到ZooKeeper后创建Ephemeral节点，并使用Heartbeat机制维持这个节点的存活状态，如果某个Ephemeral节点实效，则HMaster会收到通知，并做相应的处理。 另外，HMaster通过监听ZooKeeper中的Ephemeral节点(默认：/hbase/rs/*)来监控HRegionServer的加入和宕机。在第一个HMaster连接到ZooKeeper时会创建Ephemeral节点(默认：/hbasae/master)来表示Active的HMaster，其后加进来的HMaster则监听该Ephemeral节点，如果当前Active的HMaster宕机，则该节点消失，因而其他HMaster得到通知，而将自身转换成Active的HMaster，在变为Active的HMaster之前，它会创建在/hbase/back-masters/下创建自己的Ephemeral节点。 HBase中的AssignmentManagerAssignmentManager模块是HBase中一个非常重要的模块，Assignment Manager（之后简称AM）负责了HBase中所有region的Assign，UnAssign，以及split/merge过程中region状态变化的管理等等。在HBase-0.90之前，AM的状态全部存在内存中，自从HBASE-2485之后，AM把状态持久化到了Zookeeper上。在此基础上，社区对AM又修复了大量的bug和优化（见此文章），最终形成了用在HBase-1.x版本上的这个AM。 老Assignment Mananger的问题相信深度使用过HBase的人一般都会被Region RIT的状态困扰过，长时间的region in transition状态简直令人抓狂。 除了一些确实是由于Region无法被RegionServer open的case，大部分的RIT，都是AM本身的问题引起的。总结一下HBase-1.x版本中AM的问题，主要有以下几点： region状态变化复杂这张图很好地展示了region在open过程中参与的组件和状态变化。可以看到，多达7个组件会参与region状态的变化。并且在region open的过程中多达20多个步骤！越复杂的逻辑意味着越容易出bug region状态多处缓存region的状态会缓存在多个地方，Master中RegionStates会保存Region的状态，Meta表中会保存region的状态，Zookeeper上也会保存region的状态，要保持这三者完全同步是一件很困难的事情。同时，Master和RegionServer都会修改Meta表的状态和Zookeeper的状态，非常容易导致状态的混乱。如果出现不一致，到底以哪里的状态为准？每一个region的transition流程都是各自为政，各自有各自的处理方法 重度依赖Zookeeper在老的AM中，region状态的通知完全通过Zookeeper。比如说RegionServer打开了一个region，它会在Zookeeper把这个region的RIT节点改成OPEN状态，而不去直接通知Master。Master会在Zookeeper上watch这个RIT节点，通过Zookeeper的通知机制来通知Master这个region已经发生变化。Master再根据Zookeeper上读取出来的新状态进行一定的操作。严重依赖Zookeeper的通知机制导致了region的上线/下线的速度存在了一定的瓶颈。特别是在region比较多的时候，Zookeeper的通知会出现严重的滞后现象。 正是这些问题的存在，导致AM的问题频发。 Assignment Mananger V2在这个设计中，它摒弃了Zookeeper这个持久化的存储，一些region transition过程中的中间状态无法被保存。因此，在此基础上，社区又更进了一步，提出了Assignment Mananger V2在这个方案。在这个方案中，仍然摒弃了Zookeeper参与Assignment的整个过程。但是，它引入了ProcedureV2这个持久化存储来保存Region transition中的各个状态，保证在master重启时，之前的assing/unassign，split等任务能够从中断点重新执行。具体的来说，AMv2方案中，主要的改进有以下几点： Procedure V2Master中会有许多复杂的管理工作，比如说建表，region的transition。这些工作往往涉及到非常多的步骤，如果master在做中间某个步骤的时候宕机了，这个任务就会永远停留在了中间状态（RIT因为之前有Zookeeper做持久化因此会继续从某个状态开始执行）。比如说在enable/disable table时，如果master宕机了，可能表就停留在了enabling/disabling状态。需要一些外部的手段进行恢复。那么从本质上来说，ProcedureV2提供了一个持久化的手段（通过ProcedureWAL，一种类似RegionServer中WAL的日志持久化到HDFS上），使master在宕机后能够继续之前未完成的任务继续完成。同时，ProcedureV2提供了非常丰富的状态转换并支持回滚执行，即使执行到某一个步骤出错，master也可以按照用户的逻辑对之前的步骤进行回滚。比如建表到某一个步骤失败了，而之前已经在HDFS中创建了一些新region的文件夹，那么ProcedureV2在rollback的时候，可以把这些残留删除掉。Procedure中提供了两种Procedure框架，顺序执行和状态机，同时支持在执行过程中插入subProcedure，从而能够支持非常丰富的执行流程。在AMv2中，所有的Assign，UnAssign，TableCreate等等流程，都是基于Procedure实现的。 去除Zookeeper依赖有了Procedure V2之后，所有的状态都可以持久化在Procedure中，Procedure中每次的状态变化，都能够持久化到ProcedureWAL中，因此数据不会丢失，宕机后也能恢复。同时，AMv2中region的状态扭转（OPENING，OPEN，CLOSING，CLOSE等）都会由Master记录在Meta表中，不需要Zookeeper做持久化。再者，之前的AM使用的Zookeeper watch机制通知master region状态的改变，而现在每当RegionServer Open或者close一个region后，都会直接发送RPC给master汇报，因此也不需要Zookeeper来做状态的通知。综合以上原因，Zookeeper已经在AMv2中没有了存在的必要。 减少状态冲突的可能性之前我说过，在之前的AM中，region的状态会同时存在于meta表，Zookeeper和master的内存状态。同时Master和regionserver都会去修改Zookeeper和meta表，维护状态统一的代价非常高，非常容易出bug。而在AMv2中，只有master才能去修改meta表。并在region整个transition中做为一个“权威”存在，如果regionserver汇报上来的region状态与master看到的不一致，则master会命令RegionServer abort。Region的状态，都以master内存中保存的RegionStates为准。 除了上述这些优化，AMv2中还有许多其他的优化。比如说AMv2依赖Procedure V2提供的一套locking机制，保证了对于一个实体，如一张表，一个region或者一个RegionServer同一时刻只有一个Procedure在执行。同时，在需要往RegionServer发送命令，如发送open，close等命令时，AMv2实现了一个RemoteProcedureDispatcher来对这些请求做batch，批量把对应服务器的指令一起发送等等。在代码结构上，之前处理相应region状态的代码散落在AssignmentManager这个类的各个地方，而在AMv2中，每个对应的操作，都有对应的Procedure实现，如AssignProcedure，DisableTableProcedure，SplitTableRegionProcedure等等。这样下来，使AssignmentManager这个之前杂乱的类变的清晰简单，代码量从之前的4000多行减到了2000行左右。 AssignProcedure讲解一下在AMv2中，一个region是怎么assign给一个RegionServer，并在对应的RS上Open的。 REGION_TRANSITION_QUEUEAssign开始时的状态。在这个状态时，Procedure会对region状态做一些改变和存储，并丢到AssignmentManager的assign queue中。对于单独region的assign，AssignmentManager会把他们group起来，再通过LoadBalancer分配相应的服务器。当这一步骤完成后，Procedure会把自己标为REGION_TRANSITION_DISPATCH，然后看是否已经分配服务器，如果还没有被分配服务器的话，则会停止继续执行，等待被唤醒。 REGION_TRANSITION_DISPATCH当AssignmentManager为这个region分配好服务器时，Procedure就会被唤醒。或者Procedure在执行完REGION_TRANSITION_QUEUE状态时master宕机，Procedure被恢复后，也会进入此步骤执行。所以在此步骤下，Procedure会先检查一下是否分配好了服务器，如果没有，则把状态转移回REGION_TRANSITION_QUEUE，否则的话，则把这个region交给RemoteProcedureDispatcher，发送RPC给对应的RegionServer来open这个region。同样的，RemoteProcedureDispatcher也会对相应的指令做一个batch，批量把一批region open的命令发送给某一台服务器。当命令发送完成之后，Procedure又会进入休眠状态，等待RegionServer成功OPen这个region后，唤醒这个Procedure REGION_TRANSITION_FINISH当有RegionServer汇报了此region被打开后，会把Procedure的状态置为此状态，并唤醒Procedure执行。此时，AssignProcedure会做一些状态改变的工作，并修改meta表，把meta表中这个region的位置指向对应的RegionServer。至此，region assign的工作全部完成。 AMv2中提供了一个Web页面（Master页面中的‘Procedures&amp;Locks’链接）来展示当前正在执行的Procedure和持有的锁。其实通过log，我们也可以看到Assign的整个过程。 总结Assignment Mananger V2依赖Procedure V2实现了一套清晰明了的region transition机制。去除了Zookeeper依赖，减少了region状态冲突的可能性。整体上来看，代码的可读性更强，出了问题也更好查错。对于解决之前AM中的一系列“顽疾”，AMv2做了很好的尝试，也是一个非常好的方向。AMv2之所以能保持简洁高效的一个重要原因就是重度依赖了Procedure V2，把一些复杂的逻辑都转移到了Procedure V2中。但是这样做的问题是：一旦ProcedureWAL出现了损坏，或者Procedure本身存在bug，这个后果就是灾难性的。 此时并不是说zookeep在hbase中已经没用了，原数据存储，Hmaster的failover还是需要zk的。 参考:https://yq.aliyun.com/articles/601096]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Region寻址]]></title>
    <url>%2F2017%2F02%2F18%2FRegion%E5%AF%BB%E5%9D%80%2F</url>
    <content type="text"><![CDATA[老的Region寻址方式在Hbase 0.96版本以前，Hbase有两个特殊的表，分别是-ROOT-表和.META.表，其中-ROOT-的位置存储在ZooKeeper中，-ROOT-本身存储了 .META. Table的RegionInfo信息，并且-ROOT-不会分裂，只有一个region。而.META.表可以被切分成多个region 。读取的流程如下图所示： 1234第1步：client请求ZK获得-ROOT-所在的RegionServer地址 第2步：client请求-ROOT-所在的RS地址，获取.META.表的地址，client会将-ROOT-的相关信息cache下来，以便下一次快速访问 第3步：client请求 .META.表的RS地址，获取访问数据所在RegionServer的地址，client会将.META.的相关信息cache下来，以便下一次快速访问 第4步：client请求访问数据所在RegionServer的地址，获取对应的数据 从上面的路径我们可以看出，用户需要3次请求才能直到用户Table真正的位置，然后第四次请求开始获取真正的数据,这在一定程序带来了性能的下降。在0.96之前使用3层设计的主要原因是考虑到元数据可能需要很大。但是真正集群运行，元数据的大小其实很容易计算出来。在BigTable的论文中，每行METADATA数据存储大小为1KB左右，如果按照一个Region为128M的计算，3层设计可以支持的Region个数为2^34个，采用2层设计可以支持2^17（131072）。那么2层设计的情况下一个 集群可以存储4P的数据。这仅仅是一个Region只有128M的情况下。如果是10G呢? 因此，通过计算，其实2层设计就可以满足集群的需求。因此在0.96版本以后就去掉了-ROOT-表了。 post-script:如果一个region的大小为128m,那么就说明.META.的大小也为128M。每行METADATA数据存储大小为1KB左右，那么.META.表就可以存储131072 KB = 128M =2^17个region. 此时按照每个region 128m大小，131072*128M = 16P，不知道为什么4P？应该是主本+三副本，16P/4=4P 3层则是-ROOT-表128M，其中每1kb的数据存储一张.META.表，总可以存128*128张.meta表即2^34张 新的Region寻址方式如上面的计算，2层结构其实完全能满足业务的需求，因此0.96版本以后将-ROOT-表去掉了。如下图所示： 访问路径变成了3步：123第1步：Client请求ZK获取.META.所在的RegionServer的地址。 第2步：Client请求.META.所在的RegionServer获取访问数据所在的RegionServer地址，client会将.META.的相关信息cache下来，以便下一次快速访问。 第3步：Client请求数据所在的RegionServer，获取所需要的数据。 这个Meta Table如以前的-ROOT- Table一样是不可split的.总结去掉-ROOT-的原因有如下2点：其一：提高性能其二：2层结构已经足以满足集群的需求这里还有一个问题需要说明，那就是Client会缓存.META.的数据，用来加快访问，既然有缓存，那它什么时候更新？如果.META.更新了，比如Region1不在RerverServer2上了，被转移到了RerverServer3上。client的缓存没有更新会有什么情况？其实，Client的元数据缓存不更新，当.META.的数据发生更新。如上面的例子，由于Region1的位置发生了变化，Client再次根据缓存去访问的时候，会出现错误，当出现异常达到重试次数后就会去.META.所在的RegionServer获取最新的数据，如果.META.所在的RegionServer也变了，Client就会去ZK上获取.META.所在的RegionServer的最新地址。 参考：https://blog.csdn.net/qq_36421826/article/details/82701677https://www.cnblogs.com/qcloud1001/p/7615526.htmlhttps://www.cnblogs.com/ios1988/p/6266767.htmlhttps://www.cnblogs.com/cenzhongman/p/7271761.html]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KMP]]></title>
    <url>%2F2017%2F02%2F14%2FKMP%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637package bluebridgecup.array.kmp;public class Kmp &#123; public static void main(String[] args) &#123; int[] next = new int[14]; char[] p = &quot;abcdabdefgabca&quot;.toCharArray(); getNext(p,next); for (int n:next) &#123; System.out.println(n); &#125; &#125; static void getNext(char[] p,int next[])&#123; int nLen = p.length; next[0] = -1;//todo 初始化为-1，所以 int k = -1; int j = 0;// next[j]代表[0, j - 1]区段中最长相同真前后缀的长度 while (j &lt; nLen - 1)&#123; //j //针对数组p,p[k]表示前项,p[j]表示后项;因为k=-1,j=0 //注:k==-1表示未找到k前缀与k后缀相等,首次分析可先忽略// 细心的朋友会问if语句中k == -1存在的意义是何？第一，程序刚运行时，k是被初始为-1，直接进行P[i] == P[k]判断无疑会边界溢出；// 第二，else语句中k = next[k]，k是不断后退的，若k在后退中被赋值为-1（也就是j = next[0]），在P[i] == P[k]判断也会边界溢出。// 综上两点，其意义就是为了特殊边界判断。 if (k &gt; -1)System.out.println(p[j]+&quot;***&quot;+p[k]+&quot;$$$&quot;+j+&quot;,&quot;+k); if (k == -1 || p[j] == p[k])&#123;//匹配成功 ++j; ++k; next[j] = k;//todo 首次是next[1] = 0;next中存的是索引;next[0] = -1是初始化的，不用再计算 &#125; else &#123; //p[j]与p[k]失配,则继续递归计算前缀p[next[k]] k = next[k];//todo next[0]=-1是初始化的，该出逻辑是判断k的取值为next数组的前一项，前一项如果为-1，则k=-1,那么下一次循环会进入两一个逻辑 &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[递归]]></title>
    <url>%2F2017%2F02%2F14%2F%E8%93%9D%E6%A1%A5%E6%9D%AF-%E5%BE%AA%E7%8E%AF%2F</url>
    <content type="text"><![CDATA[任何循环都可以改为递归，关键是发现逻辑“相似性”和递归出口。有些语言没有循环只有递归，如lisp和clojure。拓展尾递归。 数组求和12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package bluebridgecup;public class A01_SumArray &#123; public static int f1(int[] arr) &#123; int sum = 0; for (int i = 0; i &lt; arr.length; i++) &#123; sum += arr[i]; &#125; return sum; &#125; //**************************线性递归************************** public static int f2(int[] arr,int start) &#123; if (start == arr.length) //递归出口 return 0; return arr[start] + f2(arr,start+1); &#125; public static int f3_2(int[] arr,int end) &#123; if (end &lt; 0) //递归出口 return 0; return f3_2(arr,end-1) + arr[end]; &#125; //**************************二分递归************************** public static int f4(int[] arr,int start,int end) &#123; if (start &gt; end) return 0;//递归出口 int mid = (start + end)/2; return f4(arr,start,mid-1) + arr[mid] + f4(arr,mid+1,end);//todo 不如下面的方式 &#125; public static int f5(int[] arr,int start,int end) &#123; if (start == end) return arr[start];//递归出口 int mid = (start + end)/2; return f5(arr,start,mid) + f5(arr,mid+1,end);//注意分为mid mid+1 &#125; public static void main(String[] args) &#123;// int[] arr = new int[]&#123;1,2,3,4,5,6,7,8,9,10&#125;;// System.out.println(f1(arr));// System.out.println(f2(arr,0));// System.out.println(f3_2(arr,arr.length - 1));// System.out.println(f4(arr,0,arr.length - 1));// System.out.println(f5(arr,0,arr.length - 1)); &#125;&#125; 打印数组123456789101112131415161718192021222324252627282930package bluebridgecup;public class A02_PrintArray &#123; private static void f1(int n) &#123; if (n &gt; 0) f1(n - 1); System.out.println(n); &#125; private static void f2(int start, int end) &#123; if (start &gt; end)return; System.out.println(start); f2(start+1,end); &#125; public static void f_arr(int[] arr,int start)&#123; if (start &gt; arr.length - 1)return; System.out.println(arr[start]); f_arr(arr,start+1); &#125; public static void main(String[] args)&#123;// f1(10);// f2(0,10);// int[] arr = new int[]&#123;1,2,3,4,5,6,7,8,9,10&#125;;// fa(arr,0); &#125;&#125; 字符串是否相同1234567891011121314151617181920212223242526package bluebridgecup;public class A03_IsSameString &#123; private static boolean isSameString2(String s1, String s2) &#123; if (s1.length() != s2.length()) return false;//******边界******** if (s1.length() == 0) return true;//******边界********// if (s1.charAt(0) != s2.charAt(0)) return false; if (s1.charAt(0) == s2.charAt(0)) return true; return isSameString2(s1.substring(1),s2.substring(1)); &#125; public static boolean isSameString1(String s1,String s2)&#123; return s1.equals(s2); &#125; public static void main(String[] args) &#123;// String s1 = &quot;abc&quot;;// String s2 = &quot;abcd&quot;;// System.out.println(isSameString1(s1,s2));// System.out.println(isSameString2(s1,s2)); &#125;&#125; 求阶乘123456789101112package bluebridgecup;public class A04_FactorialOfN &#123; public static void main(String[] args) &#123; System.out.println(helper(3)); &#125; private static int helper(int n) &#123; if (n &gt; 0)return n * helper(n - 1); return 1; &#125;&#125; 求n个不同元素的全排列*12345678910111213141516171819202122232425262728293031323334package bluebridgecup;/** * 求n个不同元素的全排列 * * 回溯问题，用于八皇后和迷宫问题 * 如果包含重复元素怎么办? * 罗列出每一种可能用什么方法?动态规划怎么实现 * * * R的全排列可归纳定义如下： * 当n=1时，perm(R)=(r)，其中r是集合R中唯一的元素； * 当n&gt;1时，perm(R)由(r1)perm(R1)，(r2)perm(R2)，…，(rn)perm(Rn)构成。 * 实现思想：将整组数中的所有的数分别与第一个数交换，这样就总是在处理后n-1个数的全排列。 */public class A05_FullPermutation &#123; public static void main(String[] args) &#123; char[] chars = &quot;ABC&quot;.toCharArray(); helper(chars,0); &#125; //k:当前的交换位置(关注点),与其后的元素交换 private static void helper(char[] chars,int k) &#123; if (k == chars.length)&#123; for (char c:chars) System.out.print(c +&quot; &quot;); System.out.println(); &#125; for (int i = k; i &lt; chars.length; i++) &#123; &#123;char t = chars[k];chars[k] = chars[i];chars[i] = t;&#125;//试探 helper(chars,k + 1); &#123;char t = chars[k];chars[k] = chars[i];chars[i] = t;&#125;//回溯 &#125; &#125;&#125; 在n个求中，任意取出m个，求有多少种不同取法12345678910111213141516171819package bluebridgecup;/** * 在n个求中，任意取出m个，求有多少种不同取法 */public class A06_GraspBall &#123; //从n个取m个 public static void main(String[] args) &#123; int k = helper(5,3); System.out.println(k); &#125; private static int helper(int n, int m) &#123; if (n &lt; m) return 0; if (n == m) return 1; if (m == 0) return 1; return helper(n - 1,m - 1) + helper(n - 1,m);//n个里有个特殊球x，取法划分，包不包含x &#125;&#125; 求最长公共子序列的长度12345678910111213141516171819package bluebridgecup;public class A07_MaxSequence &#123; public static void main(String[] args) &#123; String s1 = &quot;abc&quot;; String s2 = &quot;xbacd&quot;; int x = helper(s1,s2); System.out.println(x); &#125; private static int helper(String s1, String s2) &#123; if (s1.length() == 0 || s2.length() == 0)return 0; if (s1.charAt(0) == s2.charAt(0))&#123; return 1 + helper(s1.substring(1),s2.substring(1)); &#125; else &#123; return Math.max(helper(s1,s2.substring(1)),helper(s1.substring(1),s2)); &#125; &#125;&#125; 翻转字符串123456789101112package bluebridgecup;public class A08_Reverse &#123; public static void main(String[] args) &#123; System.out.println(helper_str(&quot;12345&quot;));; &#125; private static String helper_str(String s) &#123; if (s.length() == 0)return &quot;&quot;; return helper_str(s.substring(1))+s.charAt(0); &#125;&#125; 杨辉三角1234567891011121314151617181920212223242526272829package bluebridgecup;/** * 1 * 1 1 * 1 2 1 * 1 3 3 1 * 1 4 6 4 1 * 1 5 10 10 5 1 * * 计算第m层，第n个系数，m和n都从0开始 */public class A09_YangTriangle &#123; public static void main(String[] args) &#123; int level = 5; for (int i = 0; i &lt;= level; i++) &#123; System.out.print(helper(level,i) +&quot; &quot;); &#125; System.out.println(); &#125; //m层第n个元素 private static int helper(int m,int n)&#123; if (n == 0)return 1; if (m == n)return 1; return helper(m - 1,n) + helper(m - 1,n - 1); &#125;&#125; m个a,n个b 可以有多少种组合123456789101112package bluebridgecup;// m个a,n个b 可以有多少种组合public class A10_MN_sort &#123; public static void main(String[] args) &#123; System.out.println(helper(3,2)); &#125; private static int helper(int m, int n) &#123; if (m == 0 || n == 0)return 1; return helper(m - 1,n) + helper(m,n - 1); &#125;&#125; split 612345678910111213141516171819202122232425262728package bluebridgecup;public class A11_Split_6 &#123; public static void main(String[] args) &#123; int[] a = new int[100];//做缓冲 helper(3,a,0); &#125; //对n进行加法划分 // a:缓冲 // k:当前的位置 private static void helper(int n,int[] a,int k) &#123; if (n &lt;= 0)&#123; for(int i=0;i &lt; k;i++)System.out.printf(a[i] + &quot; &quot;); System.out.println(); return; &#125; //6 //5...f(1) //4...f(2) for (int i = n; i &gt; 0; i--) &#123; if (k &gt; 0 &amp;&amp; i &gt; a[k - 1])continue;//a[k - 1]表示前一项 ; 后一项 &gt; 前一项 a[k] = i; helper(n - i,a,k + 1); &#125; &#125;&#125; ErrorSum12345678910111213141516171819202122232425262728293031package bluebridgecup;public class A12_ErrorSum &#123; public static void main(String[] args) &#123; int sum = 6; int[] a = &#123;3,2,4,3,1&#125;; boolean[] b = new boolean[a.length];//表示a对应项是否选取 helper(sum,a,0,0,b); &#125; //error_sum:有错误的和 //a：明细 //k:当前处理的位置 //cur_sum:前边元素的累加和 //b:记录取舍 private static void helper(int error_sum, int[] a, int k, int cur_sum, boolean[] b) &#123; if (cur_sum &gt; error_sum)return; if (error_sum == cur_sum)&#123; for(int i = 0;i &lt; b.length;i++) if (b[i] == true)System.out.printf(a[i]+&quot; &quot;); System.out.println(); return; &#125; if (k &gt;= a.length)return; b[k] = false; helper(error_sum,a,k + 1,cur_sum,b); b[k] = true; cur_sum += a[k]; helper(error_sum,a,k + 1,cur_sum,b); b[k] = false;//回溯！！！！！ &#125;&#125; 打印二叉树从根节点到叶子节点的所有路径1234567891011121314151617181920212223242526272829303132333435363738394041424344package bluebridgecup;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;//打印二叉树从根节点到叶子节点的所有路径public class A13_BSTTreePath &#123; List&lt;String&gt; res = new ArrayList&lt;&gt;(); public List&lt;String&gt; binaryTreePaths(TreeNode root) &#123; if (root == null)return new ArrayList&lt;&gt;(); LinkedList&lt;Integer&gt; list = new LinkedList&lt;&gt;(); helper(root,list); return res; &#125; private void helper(TreeNode node,LinkedList&lt;Integer&gt; list) &#123; if (node == null) return; list.add(node.val); if (node.left == null &amp;&amp; node.right == null)&#123; String s = &quot;&quot;; for (Integer n:list) &#123; if (s.equals(&quot;&quot;))&#123; s += n+&quot;&quot;; &#125; else &#123; s = s + &quot;-&gt;&quot;+ n; &#125; &#125; res.add(s); list.removeLast();//TODO 最后一个节点是叶子节点,继续下一条路线,所以要剔除最后一个 return;//TODO 该次结束，返回到上一层 &#125; helper(node.left,list); helper(node.right,list); list.removeLast();//TODO 返回时一定要清除 最后一个节点是最后一个叉的根节点,一定是要排除的,因为这个节点的左右方向都走完了 &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125; 汉诺塔1234567891011121314151617package bluebridgecup;public class A14_Hanoi &#123; public static void main(String[] args) &#123; hanoi(2,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;); &#125; public static void hanoi(int n,String start,String middle,String end)&#123; if (n &lt;= 1)&#123; System.out.println(start+&quot;--&gt;&quot;+end); &#125; else &#123; hanoi(n -1,start,end,middle); System.out.println(start+&quot;--&gt;&quot;+end); hanoi(n - 1,middle,start,end); &#125; &#125;&#125; 遍历二叉树1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package bluebridgecup;import java.util.ArrayList;public class A15_TreeRec &#123; /** * 中序遍历递归解法 * （1）如果二叉树为空，空操作。 * （2）如果二叉树不为空，中序遍历左子树，访问根节点，中序遍历右子树 */ public void inOrderRec(TreeNode root)&#123; if (root == null)return; inOrderRec(root.left); System.out.println(root.val); inOrderRec(root.right); &#125; /** * 分层遍历二叉树（递归） * 很少有人会用递归去做level traversal * 基本思想是用一个大的ArrayList，里面包含了每一层的ArrayList。 * 大的ArrayList的size和level有关系 * */ public static void levelTraversalRec(TreeNode root) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; ret = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); dfs(root, 0, ret); System.out.println(ret); &#125; private static void dfs(TreeNode root, int level, ArrayList&lt;ArrayList&lt;Integer&gt;&gt; ret)&#123; if(root == null)&#123; return; &#125; // 添加一个新的ArrayList表示新的一层 if(level &gt;= ret.size())&#123; ret.add(new ArrayList&lt;Integer&gt;()); &#125; ret.get(level).add(root.val); // 把节点添加到表示那一层的ArrayList里 dfs(root.left, level+1, ret); // 递归处理下一层的左子树和右子树 dfs(root.right, level+1, ret); &#125; /** * 求二叉树中的节点个数递归解法： O(n) * （1）如果二叉树为空，节点个数为0 * （2）如果二叉树不为空，二叉树节点个数 = 左子树节点个数 + 右子树节点个数 + 1 */ public static int getNodeNumRec(TreeNode root) &#123; if (root == null) &#123; return 0; &#125; else &#123; return getNodeNumRec(root.left) + getNodeNumRec(root.right) + 1; //左 右节点加上主节点1为总数 &#125; &#125; /** * 求二叉树的深度（高度） 递归解法： O(n) * （1）如果二叉树为空，二叉树的深度为0 * （2）如果二叉树不为空，二叉树的深度 = max(左子树深度， 右子树深度) + 1 * * * maxDepth() 1. 如果树为空，那么返回0 2. 否则 (a) 递归得到左子树的最大高度 例如，调用maxDepth( tree-&gt; left-subtree ) (b) 递归得到右子树的最大高度 例如，调用maxDepth( tree-&gt; right-subtree ) (c) 对于当前节点，取左右子树高度的最大值并加1。 max_depth = max(左子树的最大高度, 右子树的最大高度) + 1 (d) 返回max_depth */ public static int getDepthRec(TreeNode root) &#123; if (root == null) &#123; return 0; &#125; int leftDepth = getDepthRec(root.left); int rightDepth = getDepthRec(root.right); return Math.max(leftDepth, rightDepth) + 1; // +1是因为根节点已经是一层了,否则root==null直接是0了 &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125; 翻转链表1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.lifeibigdata.offer;import java.util.Stack;/** * 从尾到头打印链表 * Created by lifei on 16/11/13. */public class PrintListReverse &#123; private static class Node &#123; int val; Node next; public Node(int val) &#123; this.val = val; &#125; &#125; public static void main(String[] args) &#123; Node n1 = new Node(1); Node n2 = new Node(2);Node n3 = new Node(3); Node n4 = new Node(4);Node n5 = new Node(5); n1.next = n2;n2.next = n3;n3.next = n4;n4.next = n5; printListReverse2(n1); &#125; static void printListReverse1(Node head)&#123; Stack&lt;Node&gt; stack = new Stack(); Node tmpNode = head; while (tmpNode != null)&#123; stack.push(tmpNode); tmpNode = tmpNode.next; &#125; while (!stack.isEmpty())&#123; System.out.println(stack.pop().val); &#125; &#125; /** * 栈的本质是递归,要实现反过来输出链表,我们访问到一个节点的时候,先递归输出后面的节点,再输入该节点本身,这样链表的输出结果就反过来了 * @param head */ static void printListReverse2(Node head)&#123; if (head != null)&#123; if (head.next != null)&#123; printListReverse2(head.next); &#125; System.out.println(head.val); &#125; &#125;&#125;]]></content>
      <categories>
        <category>recursion</category>
      </categories>
      <tags>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql千万级分页]]></title>
    <url>%2F2016%2F02%2F17%2Fmysql%E5%8D%83%E4%B8%87%E7%BA%A7%E5%88%86%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[Mysql数据库最简单，是利用mysql的LIMIT函数,LIMIT [offset,] rows从数据库表中M条记录(不包含m条)开始检索N条记录的语句为：1SELECT * FROM 表名称 LIMIT M,N 其中limit为可选项，例如我们有个student表，我们选取前5条记录可以使用下面的sql语句1select * from student limit 5; 问题：千万级的表查询分页，怎么优化查询12select * from user limit 10000000,10select * from user where name=&quot;a&quot; limit 10000000,10 为了能更加优化查询，建立联合索引是一个好的方法，where 的条件 和主键id 作为索引 serach(name,id) 第一次建立索引时候 是id 在前 name在后，这样确实也解决这样的问题，都变成0.5s左右 ，但是还不是我想要的 ，最后上网搜索答案，发现建立联合索引顺序不同，性能也就大大提高，于是把建立索引顺序变成是id 在后 name在前。结果打出意料 ，速度变成了0.05秒 。性能大幅度提升. 开始的select id from collect order by id limit 90000,10;这么快就是因为走了索引，可是如果加了where 就不走索引了。抱着试试看的想法加了 search(vtype,id) 这样的索引。然后测试select id from collect where vtype=1 limit 90000,10; 非常快！0.04秒完成！再测试: select id ,title from collect where vtype=1 limit 90000,10; 非常遗憾，8-9秒，没走search索引！再测试：search(id,vtype)，还是select id 这个语句，也非常遗憾，0.5秒。综上：如果对于有where 条件，又想走索引用limit的，必须设计一个索引，将where放第一位，limit用到的主键放第2位，而且只能select 主键！即search(vtype,id) 这样的索引,如下查询select id from collect where vtype=1 limit 90000,10可以快速返回id，然后根据id in (xxxx)的方式去实现 参考:https://blog.csdn.net/m0_37922390/article/details/81976014https://blog.csdn.net/li772030428/article/details/52839987/]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[题目]]></title>
    <url>%2F2016%2F02%2F14%2F%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[简答说一下hadoop的map-reduce编程模型首先map task会从本地文件系统读取数据，转换成key-value形式的键值对集合 使用的是hadoop内置的数据类型，比如longwritable、text等 将键值对集合输入mapper进行业务处理过程，将其转换成需要的key-value在输出 之后会进行一个partition分区操作，默认使用的是hashpartitioner，可以通过重写hashpartitioner的getpartition方法来自定义分区规则 之后会对key进行进行sort排序，grouping分组操作将相同key的value合并分组输出，在这里可以使用自定义的数据类型，重写WritableComparator的Comparator方法来自定义排序规则，重写RawComparator的compara方法来自定义分组规则 之后进行一个combiner归约操作，其实就是一个本地段的reduce预处理，以减小后面shufle和reducer的工作量 reduce task会通过网络将各个数据收集进行reduce处理，最后将数据保存或者显示，结束整个job hadoop的TextInputFormat作用是什么，如何自定义实现InputFormat会在map操作之前对数据进行两方面的预处理1是getSplits，返回的是InputSplit数组，对数据进行split分片，每片交给map操作一次2是getRecordReader，返回的是RecordReader对象，对每个split分片进行转换为key-value键值对格式传递给map 常用的InputFormat是TextInputFormat，使用的是LineRecordReader对每个分片进行键值对的转换，以行偏移量作为键，行内容作为值 自定义类继承InputFormat接口，重写createRecordReader和isSplitable方法在createRecordReader中可以自定义分隔符 hadoop和spark的都是并行计算，那么他们有什么相同和区别两者都是用mr模型来进行并行计算，hadoop的一个作业称为job，job里面分为map task和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束 spark用户提交的任务成为application，一个application对应一个sparkcontext，一个application中存在多个job，每触发一次action操作就会产生一个job,这些job可以并行或串行执行，每个job中有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行，executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算 hadoop的job只有map和reduce操作，表达能力比较欠缺而且在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系 spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作如join，groupby等，而且通过DAG图可以实现良好的容错 为什么要用flume导入hdfs，hdfs的构架是怎样的flume可以实时的导入数据到hdfs中，当hdfs上的文件达到一个指定大小的时候会形成一个文件，或者超过指定时间的话也形成一个文件 文件都是存储在datanode上面的，namenode记录着datanode的元数据信息，而namenode的元数据信息是存在内存中的，所以当文件切片很小或者很多的时候会卡死 简单说一下hadoop和spark的shuffle过程hadoop：map端保存分片数据，通过网络收集到reduce端spark：spark的shuffle是在DAGSchedular划分Stage的时候产生的，TaskSchedule要分发Stage到各个worker的executor 减少shuffle可以提高性能 Hive中存放是什么？表。存的是和hdfs的映射关系，hive是逻辑上的数据仓库，实际操作的都是hdfs上的文件，HQL就是用sql语法来写的mr程序。 Flume工作机制是什么？核心概念是agent，里面包括source、chanel和sink三个组件。source运行在日志收集节点进行日志采集，之后临时存储在chanel中，sink负责将chanel中的数据发送到目的地。只有成功发送之后chanel中的数据才会被删除。首先书写flume配置文件，定义agent、source、chanel和sink然后将其组装，执行flume-ng命令。 Sqoop工作原理是什么？hadoop生态圈上的数据传输工具。可以将关系型数据库的数据导入非结构化的hdfs、hive或者bbase中，也可以将hdfs中的数据导出到关系型数据库或者文本文件中。使用的是mr程序来执行任务，使用jdbc和关系型数据库进行交互。import原理：通过指定的分隔符进行数据切分，将分片传入各个map中，在map任务中在每行数据进行写入处理没有reduce。export原理：根据要操作的表名生成一个java类，并读取其元数据信息和分隔符对非结构化的数据进行匹配，多个map作业同时执行写入关系型数据库 Hbase行健列族的概念，物理模型，表的设计原则？行健：是hbase表自带的，每个行健对应一条数据。列族：是创建表时指定的，为列的集合，每个列族作为一个文件单独存储，存储的数据都是字节数组，其中的数据可以有很多，通过时间戳来区分。物理模型：整个hbase表会拆分为多个region，每个region记录着行健的起始点保存在不同的节点上，查询时就是对各个节点的并行查询，当region很大时使用.META表存储各个region的起始点，-ROOT又可以存储.META的起始点。rowkey的设计原则：各个列簇数据平衡，长度原则、相邻原则，创建表的时候设置表放入regionserver缓存中，避免自动增长和时间，使用字节数组代替string，最大长度64kb，最好16字节以内，按天分表，两个字节散列，四个字节存储时分毫秒。列族的设计原则：尽可能少（按照列族进行存储，按照region进行读取，不必要的io操作），经常和不经常使用的两类数据放入不同列族中，列族名字尽可能短。 Spark Streaming和Storm有何区别？一个实时毫秒一个准实时亚秒，不过storm的吞吐率比较低 mllib支持的算法？大体分为四大类，分类、聚类、回归、协同过滤。 Hadoop平台集群配置、环境变量设置？zookeeper：修改zoo.cfg文件，配置dataDir，和各个zk节点的server地址端口，tickTime心跳时间默认是2000ms，其他超时的时间都是以这个为基础的整数倍，之后再dataDir对应目录下写入myid文件和zoo.cfg中的server相对应。 hadoop：修改hadoop-env.sh配置java环境变量core-site.xml配置zk地址，临时目录等hdfs-site.xml配置nn信息，rpc和http通信地址，nn自动切换、zk连接超时时间等yarn-site.xml配置resourcemanager地址mapred-site.xml配置使用yarnslaves配置节点信息格式化nn和zk。 hbase：修改hbase-env.sh配置java环境变量和是否使用自带的zkhbase-site.xml配置hdfs上数据存放路径，zk地址和通讯超时时间、master节点regionservers配置各个region节点zoo.cfg拷贝到conf目录下 spark：安装Scala修改spark-env.sh配置环境变量和master和worker节点配置信息 环境变量的设置：直接在/etc/profile中配置安装的路径即可，或者在当前用户的宿主目录下，配置在.bashrc文件中，该文件不用source重新打开shell窗口即可，配置在.bash_profile的话只对当前用户有效。 Hadoop性能调优？调优可以通过系统配置、程序编写和作业调度算法来进行。hdfs的block.size可以调到128/256（网络很好的情况下，默认为64）调优的大头：mapred.map.tasks、mapred.reduce.tasks设置mr任务数（默认都是1）mapred.tasktracker.map.tasks.maximum每台机器上的最大map任务数mapred.tasktracker.reduce.tasks.maximum每台机器上的最大reduce任务数mapred.reduce.slowstart.completed.maps配置reduce任务在map任务完成到百分之几的时候开始进入这个几个参数要看实际节点的情况进行配置，reduce任务是在33%的时候完成copy，要在这之前完成map任务，（map可以提前完成）mapred.compress.map.output,mapred.output.compress配置压缩项，消耗cpu提升网络和磁盘io合理利用combiner注意重用writable对象 spark有哪些组件？（1）master：管理集群和节点，不参与计算。（2）worker：计算节点，进程本身不参与计算，和master汇报。（3）Driver：运行程序的main方法，创建spark context对象。（4）spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。（5）client：用户提交程序的入口。 spark工作机制？用户在client端提交作业后，会由Driver运行main方法并创建spark context上下文。执行add算子，形成dag图输入dagscheduler，按照add之间的依赖关系划分stage输入task scheduler。task scheduler会将stage划分为task set分发到各个节点的executor中执行。 spark的优化怎么做？通过spark-env文件、程序中sparkconf和set property设置。（1）计算量大，形成的lineage过大应该给已经缓存了的rdd添加checkpoint，以减少容错带来的开销。（2）小分区合并，过小的分区造成过多的切换任务开销，使用repartition。 kafka工作原理？producer向broker发送事件，consumer从broker消费事件。事件由topic区分开，每个consumer都会属于一个group。相同group中的consumer不能重复消费事件，而同一事件将会发送给每个不同group的consumer。 转：https://www.cnblogs.com/jchubby/p/5449379.html]]></content>
      <categories>
        <category>面试题目</category>
      </categories>
      <tags>
        <tag>面试题目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapreduce模型]]></title>
    <url>%2F2016%2F01%2F26%2Fmapreduce%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[graph TD A[Map] -->|partition,kvBuffer,sort,spill与combiner,merge| B{Shuffle} B -->|copy,cache_sort_merge,spill,merge,file,GroupingComparator| C[reduce] 从JVM的角度看Map和Reduce Map阶段包括：第一读数据：从HDFS读取数据1、问题:读取数据产生多少个Mapper？？ Mapper数据过大的话，会产生大量的小文件，由于Mapper是基于虚拟机的，过多的Mapper创建和初始化及关闭虚拟机都会消耗大量的硬件资源； Mapper数太小，并发度过小，Job执行时间过长，无法充分利用分布式硬件资源； 2、Mapper数量由什么决定？？ （1）输入文件数目 （2）输入文件的大小 （3）配置参数 所以说map数是可以控制的这三个因素决定的。 涉及参数：mapreduce.input.fileinputformat.split.minsize //启动map最小的split size大小，默认0 mapreduce.input.fileinputformat.split.maxsize //启动map最大的split size大小，默认256Mdfs.block.size//block块大小，默认64M计算公式：splitSize = Math.max(minSize, Math.min(maxSize, blockSize)); 例如默认情况下：例如一个文件800M，Block大小是128M，那么Mapper数目就是7个。6个Mapper处理的数据是128M，1个Mapper处理的数据是32M；再例如一个目录下有三个文件大小分别为：5M10M 150M 这个时候其实会产生四个Mapper处理的数据分别是5M，10M，128M，22M。 Mapper是基于文件自动产生的，如果想要自己控制Mapper的个数？？？ 就如上面，5M，10M的数据很快处理完了，128M要很长时间；这个就需要通过参数的控制来调节Mapper的个数。 减少Mapper的个数的话，就要合并小文件，这种小文件有可能是直接来自于数据源的小文件，也可能是Reduce产生的小文件。 设置合并器：（set都是在hive脚本，也可以配置Hadoop） 设置合并器本身：set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;set hive.merge.mapFiles=true;set hive.merge.mapredFiles=true;set hive.merge.size.per.task=256000000;//每个Mapper要处理的数据，就把上面的5M10M……合并成为一个 一般还要配合一个参数：set mapred.max.split.size=256000000 // mapred切分的大小set mapred.min.split.size.per.node=128000000//低于128M就算小文件，数据在一个节点会合并，在多个不同的节点会把数据抓过来进行合并。 Hadoop中的参数：可以通过控制文件的数量控制mapper数量mapreduce.input.fileinputformat.split.minsize（default：0），小于这个值会合并mapreduce.input.fileinputformat.split.maxsize 大于这个值会切分 第二处理数据：Partition说明对于map输出的每一个键值对，系统都会给定一个partition，partition值默认是通过计算key的hash值后对Reduce task的数量取模获得。如果一个键值对的partition值为1，意味着这个键值对会交给第一个Reducer处理。 自定义partitioner的情况：1、我们知道每一个Reduce的输出都是有序的，但是将所有Reduce的输出合并到一起却并非是全局有序的，如果要做到全局有序，我们该怎么做呢？最简单的方式，只设置一个Reduce task，但是这样完全发挥不出集群的优势，而且能应对的数据量也很受限。最佳的方式是自己定义一个Partitioner，用输入数据的最大值除以系统Reduce task数量的商作为分割边界，也就是说分割数据的边界为此商的1倍、2倍至numPartitions-1倍，这样就能保证执行partition后的数据是整体有序的。2、解决数据倾斜：另一种需要我们自己定义一个Partitioner的情况是各个Reduce task处理的键值对数量极不平衡。对于某些数据集，由于很多不同的key的hash值都一样，导致这些键值对都被分给同一个Reducer处理，而其他的Reducer处理的键值对很少，从而拖延整个任务的进度。当然，编写自己的Partitioner必须要保证具有相同key值的键值对分发到同一个Reducer。3、自定义的Key包含了好几个字段，比如自定义key是一个对象，包括type1，type2，type3,只需要根据type1去分发数据，其他字段用作二次排序。 环形缓冲区 Map的输出结果是由collector处理的，每个Map任务不断地将键值对输出到在内存中构造的一个环形数据结构中。使用环形数据结构是为了更有效地使用内存空间，在内存中放置尽可能多的数据。 这个数据结构其实就是个字节数组，叫Kvbuffer，名如其义，但是这里面不光放置了数据，还放置了一些索引数据，给放置索引数据的区域起了一个Kvmeta的别名，在Kvbuffer的一块区域上穿了一个IntBuffer（字节序采用的是平台自身的字节序）的马甲。数据区域和索引数据区域在Kvbuffer中是相邻不重叠的两个区域，用一个分界点来划分两者，分界点不是亘古不变的，而是每次Spill之后都会更新一次。初始的分界点是0，数据的存储方向是向上增长，索引数据的存储方向是向下增长Kvbuffer的存放指针bufindex是一直闷着头地向上增长，比如bufindex初始值为0，一个Int型的key写完之后，bufindex增长为4，一个Int型的value写完之后，bufindex增长为8。 索引是对在kvbuffer中的键值对的索引，是个四元组，包括：value的起始位置、key的起始位置、partition值、value的长度，占用四个Int长度，Kvmeta的存放指针Kvindex每次都是向下跳四个“格子”，然后再向上一个格子一个格子地填充四元组的数据。比如Kvindex初始位置是-4，当第一个键值对写完之后，(Kvindex+0)的位置存放value的起始位置、(Kvindex+1)的位置存放key的起始位置、(Kvindex+2)的位置存放partition的值、(Kvindex+3)的位置存放value的长度，然后Kvindex跳到-8位置，等第二个键值对和索引写完之后，Kvindex跳到-12位置。 第三写数据到磁盘Mapper中的Kvbuffer的大小默认100M，可以通过mapreduce.task.io.sort.mb（default：100）参数来调整。可以根据不同的硬件尤其是内存的大小来调整，调大的话，会减少磁盘spill的次数此时如果内存足够的话，一般都会显著提升性能。spill一般会在Buffer空间大小的80%开始进行spill（因为spill的时候还有可能别的线程在往里写数据，因为还预留空间，有可能有正在写到Buffer中的数据），可以通过mapreduce.map.sort.spill.percent（default：0.80）进行调整，Map Task在计算的时候会不断产生很多spill文件，在Map Task结束前会对这些spill文件进行合并，这个过程就是merge的过程。 mapreduce.task.io.sort.factor（default：10），代表进行merge的时候最多能同时merge多少spill，如果有100个spill个文件，此时就无法一次完成整个merge的过程，这个时候需要调大mapreduce.task.io.sort.factor（default：10）来减少merge的次数，从而减少磁盘的操作； Spill这个重要的过程是由Spill线程承担，Spill线程从Map任务接到“命令”之后就开始正式干活，干的活叫SortAndSpill，原来不仅仅是Spill，在Spill之前还有个颇具争议性的Sort。 Combiner存在的时候，此时会根据Combiner定义的函数对map的结果进行合并，什么时候进行Combiner操作呢？？？和Map在一个JVM中，是由min.num.spill.for.combine的参数决定的，默认是3，也就是说spill的文件数在默认情况下由三个的时候就要进行combine操作，最终减少磁盘数据； 减少磁盘IO和网络IO还可以进行：压缩，对spill，merge文件都可以进行压缩。中间结果非常的大，IO成为瓶颈的时候压缩就非常有用，可以通过mapreduce.map.output.compress（default：false）设置为true进行压缩，数据会被压缩写入磁盘，读数据读的是压缩数据需要解压，在实际经验中Hive在Hadoop的运行的瓶颈一般都是IO而不是CPU，压缩一般可以10倍的减少IO操作，压缩的方式Gzip，Lzo,BZip2,Lzma等，其中Lzo是一种比较平衡选择，mapreduce.map.output.compress.codec（default：org.apache.hadoop.io.compress.DefaultCodec）参数设置。但这个过程会消耗CPU，适合IO瓶颈比较大。 Shuffle和Reduce阶段包括：一、Copy1、由于job的每一个map都会根据reduce(n)数将数据分成map 输出结果分成n个partition，所以map的中间结果中是有可能包含每一个reduce需要处理的部分数据的。所以，为了优化reduce的执行时间，hadoop中是等job的第一个map结束后，所有的reduce就开始尝试从完成的map中下载该reduce对应的partition部分数据，因此map和reduce是交叉进行的，其实就是shuffle。Reduce任务通过HTTP向各个Map任务拖取（下载）它所需要的数据（网络传输），Reducer是如何知道要去哪些机器取数据呢？一旦map任务完成之后，就会通过常规心跳通知应用程序的Application Master。reduce的一个线程会周期性地向master询问，直到提取完所有数据（如何知道提取完？）数据被reduce提走之后，map机器不会立刻删除数据，这是为了预防reduce任务失败需要重做。因此map输出数据是在整个作业完成之后才被删除掉的。2、reduce进程启动数据copy线程(Fetcher)，通过HTTP方式请求maptask所在的TaskTracker获取maptask的输出文件。由于map通常有许多个，所以对一个reduce来说，下载也可以是并行的从多个map下载，那到底同时到多少个Mapper下载数据？？这个并行度是可以通过mapreduce.reduce.shuffle.parallelcopies(default5）调整。默认情况下，每个Reducer只会有5个map端并行的下载线程在从map下数据，如果一个时间段内job完成的map有100个或者更多，那么reduce也最多只能同时下载5个map的数据，所以这个参数比较适合map很多并且完成的比较快的job的情况下调大，有利于reduce更快的获取属于自己部分的数据。 在Reducer内存和网络都比较好的情况下，可以调大该参数；3、reduce的每一个下载线程在下载某个map数据的时候，有可能因为那个map中间结果所在机器发生错误，或者中间结果的文件丢失，或者网络瞬断等等情况，这样reduce的下载就有可能失败，所以reduce的下载线程并不会无休止的等待下去，当一定时间后下载仍然失败，那么下载线程就会放弃这次下载，并在随后尝试从另外的地方下载（因为这段时间map可能重跑）。reduce下载线程的这个最大的下载时间段是可以通过mapreduce.reduce.shuffle.read.timeout（default180000秒）调整的。如果集群环境的网络本身是瓶颈，那么用户可以通过调大这个参数来避免reduce下载线程被误判为失败的情况。一般情况下都会调大这个参数，这是企业级最佳实战。 二、MergeSort1、这里的merge和map端的merge动作类似，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，然后当使用内存达到一定量的时候才spill磁盘。这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置。这个内存大小的控制就不像map一样可以通过io.sort.mb来设定了，而是通过另外一个参数 mapreduce.reduce.shuffle.input.buffer.percent（default 0.7f 源码里面写死了） 来设置，这个参数其实是一个百分比，意思是说，shuffile在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of reduce task。JVM的heapsize的70%。内存到磁盘merge的启动门限可以通过mapreduce.reduce.shuffle.merge.percent（default0.66）配置。也就是说，如果该reduce task的最大heap使用量（通常通过mapreduce.admin.reduce.child.java.opts来设置，比如设置为-Xmx1024m）的一定比例用来缓存数据。默认情况下，reduce会使用其heapsize的70%来在内存中缓存数据。假设 mapreduce.reduce.shuffle.input.buffer.percent 为0.7，reducetask的max heapsize为1G，那么用来做下载数据缓存的内存就为大概700MB左右。这700M的内存，跟map端一样，也不是要等到全部写满才会往磁盘刷的，而是当这700M中被使用到了一定的限度（通常是一个百分比），就会开始往磁盘刷（刷磁盘前会先做sortMerge）。这个限度阈值也是可以通过参数 mapreduce.reduce.shuffle.merge.percent（default0.66）来设定。与map 端类似，这也是溢写的过程，这个过程中如果你设置有Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。这种merge方式一直在运行，直到没有map端的数据时才结束，然后启动磁盘到磁盘的merge方式生成最终的那个文件。 这里需要强调的是，merge有三种形式：1)内存到内存（memToMemMerger）2）内存中Merge（inMemoryMerger）3）磁盘上的Merge（onDiskMerger）具体包括两个：（一）Copy过程中磁盘合并（二）磁盘到磁盘。（1）内存到内存Merge（memToMemMerger） Hadoop定义了一种MemToMem合并，这种合并将内存中的map输出合并，然后再写入内存。这种合并默认关闭，可以通过mapreduce.reduce.merge.memtomem.enabled(default:false) 打开，当map输出文件达到mapreduce.reduce.merge.memtomem.threshold时，触发这种合并。（2）内存中Merge（inMemoryMerger）：当缓冲中数据达到配置的阈值时，这些数据在内存中被合并、写入机器磁盘。阈值有2种配置方式： 配置内存比例：前面提到reduceJVM堆内存的一部分用于存放来自map任务的输入，在这基础之上配置一个开始合并数据的比例。假设用于存放map输出的内存为500M，mapreduce.reduce.shuffle.merge.percent配置为0.66，则当内存中的数据达到330M的时候，会触发合并写入。配置map输出数量： 通过mapreduce.reduce.merge.inmem.threshold配置。在合并的过程中，会对被合并的文件做全局的排序。如果作业配置了Combiner，则会运行combine函数，减少写入磁盘的数据量。（3）磁盘上的Merge（onDiskMerger）： （3.1）Copy过程中磁盘Merge：在copy过来的数据不断写入磁盘的过程中，一个后台线程会把这些文件合并为更大的、有序的文件。如果map的输出结果进行了压缩，则在合并过程中，需要在内存中解压后才能给进行合并。这里的合并只是为了减少最终合并的工作量，也就是在map输出还在拷贝时，就开始进行一部分合并工作。合并的过程一样会进行全局排序。（3.2）最终磁盘中Merge：当所有map输出都拷贝完毕之后，所有数据被最后合并成一个整体有序的文件，作为reduce任务的输入。这个合并过程是一轮一轮进行的，最后一轮的合并结果直接推送给reduce作为输入，节省了磁盘操作的一个来回。最后（所以map输出都拷贝到reduce之后）进行合并的map输出可能来自合并后写入磁盘的文件，也可能来及内存缓冲，在最后写入内存的map输出可能没有达到阈值触发合并，所以还留在内存中。 每一轮合并不一定合并平均数量的文件数，指导原则是使用整个合并过程中写入磁盘的数据量最小，为了达到这个目的，则需要最终的一轮合并中合并尽可能多的数据，因为最后一轮的数据直接作为reduce的输入，无需写入磁盘再读出。因此我们让最终的一轮合并的文件数达到最大，即合并因子的值，通过mapreduce.task.io.sort.factor（default：10）来配置。 如上图：Reduce阶段中一个Reduce过程 可能的合并方式为：假设现在有20个map输出文件，合并因子配置为5，则需要4轮的合并。最终的一轮确保合并5个文件，其中包括2个来自前2轮的合并结果，因此原始的20个中，再留出3个给最终一轮。 三、Reduce函数调用（用户自定义业务逻辑）1、当reduce将所有的map上对应自己partition的数据下载完成后，就会开始真正的reduce计算阶段。reducetask真正进入reduce函数的计算阶段，由于reduce计算时肯定也是需要消耗内存的，而在读取reduce需要的数据时，同样是需要内存作为buffer，这个参数是控制，reducer需要多少的内存百分比来作为reduce读已经sort好的数据的buffer大小？？默认用多大内存呢？？默认情况下为0，也就是说，默认情况下，reduce是全部从磁盘开始读处理数据。可以用mapreduce.reduce.input.buffer.percent（default 0.0）(源代码MergeManagerImpl.java：674行)来设置reduce的缓存。如果这个参数大于0，那么就会有一定量的数据被缓存在内存并输送给reduce，当reduce计算逻辑消耗内存很小时，可以分一部分内存用来缓存数据，可以提升计算的速度。所以默认情况下都是从磁盘读取数据，如果内存足够大的话，务必设置该参数让reduce直接从缓存读数据，这样做就有点Spark Cache的感觉；2、Reduce在这个阶段，框架为已分组的输入数据中的每个 &lt;key, (list of values)&gt;对调用一次 reduce(WritableComparable,Iterator, OutputCollector, Reporter)方法。Reduce任务的输出通常是通过调用 OutputCollector.collect(WritableComparable,Writable)写入文件系统的。Reducer的输出是没有排序的,按照reduce的输出顺序，所以用户可以在reduce中自己实现排序。 性能调优如果能够根据情况对shuffle过程进行调优，对于提供MapReduce性能很有帮助。相关的参数配置列在后面的表格中。 一个通用的原则是给shuffle过程分配尽可能大的内存，当然你需要确保map和reduce有足够的内存来运行业务逻辑。因此在实现Mapper和Reducer时，应该尽量减少内存的使用，例如避免在Map中不断地叠加。 运行map和reduce任务的JVM，内存通过mapred.child.java.opts属性来设置，尽可能设大内存。容器的内存大小通过mapreduce.map.memory.mb和mapreduce.reduce.memory.mb来设置，默认都是1024M。 map优化 在map端，避免写入多个spill文件可能达到最好的性能，一个spill文件是最好的。通过估计map的输出大小，设置合理的mapreduce.task.io.sort.*属性，使得spill文件数量最小。例如尽可能调大mapreduce.task.io.sort.mb。 map端相关的属性如下表： reduce优化 在reduce端，如果能够让所有数据都保存在内存中，可以达到最佳的性能。通常情况下，内存都保留给reduce函数，但是如果reduce函数对内存需求不是很高，将mapreduce.reduce.merge.inmem.threshold（触发合并的map输出文件数）设为0，mapreduce.reduce.input.buffer.percent（用于保存map输出文件的堆内存比例）设为1.0，可以达到很好的性能提升。在2008年的TB级别数据排序性能测试中，Hadoop就是通过将reduce的中间数据都保存在内存中胜利的。 reduce端相关属性： 通用优化Hadoop默认使用4KB作为缓冲，这个算是很小的，可以通过io.file.buffer.size来调高缓冲池大小。]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L897_IncreasingBST]]></title>
    <url>%2F2016%2F01%2F26%2FL897-IncreasingBST%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package LeetCode.topic.tree;/** * 给定一个树，按中序遍历重新排列树，使树中最左边的结点现在是树的根，并且每个结点没有左子结点，只有一个右子结点。 * * * * 示例 ： * * 输入：[5,3,6,2,4,null,8,1,null,null,null,7,9] * * 5 * / \ * 3 6 * / \ \ * 2 4 8 * / / \ * 1 7 9 * * 输出：[1,null,2,null,3,null,4,null,5,null,6,null,7,null,8,null,9] * * 1 * \ * 2 * \ * 3 * \ * 4 * \ * 5 * \ * 6 * \ * 7 * \ * 8 * \ * 9 * * * 提示： * * 给定树中的结点数介于 1 和 100 之间。 * 每个结点都有一个从 0 到 1000 范围内的唯一整数值。 */public class L897_IncreasingBST &#123; TreeNode dummy = new TreeNode(-1); TreeNode res = dummy; public TreeNode increasingBST(TreeNode root) &#123; helper(root); return res.right; &#125; private void helper(TreeNode root) &#123;// if (root == null) return; if (root.left != null) helper(root.left); dummy.right = new TreeNode(root.val); dummy = dummy.right; if (root.right != null) helper(root.right); &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L965_IsUnivalTree]]></title>
    <url>%2F2016%2F01%2F26%2FL965-IsUnivalTree%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425package LeetCode.topic.tree;public class L965_IsUnivalTree &#123; public boolean isUnivalTree(TreeNode root) &#123; if(root == null)&#123; return true; &#125; return helper(root,root.val); &#125; public boolean helper(TreeNode node,int val)&#123; if(node == null)return true; if(node.val != val)return false; if(node.left != null &amp;&amp; node.left.val != val) return false; if(node.right != null &amp;&amp; node.right.val != val) return false; return helper(node.left,val) &amp;&amp; helper(node.right,val); &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L700_SearchBST]]></title>
    <url>%2F2016%2F01%2F26%2FL700-SearchBST%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425package LeetCode.topic.tree;public class L700_SearchBST &#123; public TreeNode searchBST(TreeNode root, int val) &#123; if(root == null) return null; if(root.val == val) return root; TreeNode left = searchBST(root.left,val); TreeNode right = searchBST(root.right,val); if(left != null)&#123; return left; &#125; if(right != null)&#123; return right; &#125; return null; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L687_LongestUnivaluePath]]></title>
    <url>%2F2016%2F01%2F26%2FL687-LongestUnivaluePath%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041package LeetCode.topic.tree;/** * * 解题思路类似于124题, 对于任意一个节点, 如果最长同值路径包含该节点, 那么只可能是两种情况: * 1. 其左右子树中加上该节点后所构成的同值路径中较长的那个继续向父节点回溯构成最长同值路径 * 2. 左右子树加上该节点都在最长同值路径中, 构成了最终的最长同值路径 * 需要注意因为要求同值, 所以在判断左右子树能构成的同值路径时要加入当前节点的值作为判断依据 */public class L687_LongestUnivaluePath &#123; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125; private int max = 0; public int longestUnivaluePath(TreeNode root) &#123; if (root == null)return 0; helper(root,root.val); return max; &#125; private int helper(TreeNode node, int val) &#123; if (node == null)return 0; int left = helper(node.left,node.val); int right = helper(node.right,node.val); max = Math.max(max,left + right);// 路径长度为节点数减1所以此处不加1 if (node.val == val) return Math.max(left,right) + 1; return 0; &#125;// 下面是自己作物的思路// if (node.val == val)&#123;// return Math.max(helper(node.left,node.val,count+1),helper(node.right,node.val,count+1));// &#125; else &#123;// return Math.max(helper(node.left,node.val,count),helper(node.right,node.val,count));// &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L671_BSTFindSecondMinimumValue]]></title>
    <url>%2F2016%2F01%2F26%2FL671-BSTFindSecondMinimumValue%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package LeetCode.topic.tree;/** * 给定一个非空特殊的二叉树，每个节点都是正数，并且每个节点的子节点数量只能为 2 或 0。如果一个节点有两个子节点的话，那么这个节点的值不大于它的子节点的值。 * * 给出这样的一个二叉树，你需要输出所有节点中的第二小的值。如果第二小的值不存在的话，输出 -1 。 * * 示例 1: * * 输入: * 2 * / \ * 2 5 * / \ * 5 7 * * 输出: 5 * 说明: 最小的值是 2 ，第二小的值是 5 。 * 示例 2: * * 输入: * 2 * / \ * 2 2 * * 输出: -1 * 说明: 最小的值是 2, 但是不存在第二小的值。 */public class L671_BSTFindSecondMinimumValue &#123; int second = -1; public int findSecondMinimumValue(TreeNode root) &#123; if (root == null) return -1; helper(root,root.val); return second; &#125; private void helper(TreeNode root, int min) &#123; if (root == null)return; if (root.val == min)&#123; helper(root.left,min); helper(root.right,min); return; &#125; if (second == -1 || root.val &lt; second)&#123; second = root.val; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L654_ConstructMaximumBinaryTree]]></title>
    <url>%2F2016%2F01%2F26%2FL654-ConstructMaximumBinaryTree%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package LeetCode.topic.tree;/** * 给定一个不含重复元素的整数数组。一个以此数组构建的最大二叉树定义如下： * * 二叉树的根是数组中的最大元素。 * 左子树是通过数组中最大值左边部分构造出的最大二叉树。 * 右子树是通过数组中最大值右边部分构造出的最大二叉树。 * 通过给定的数组构建最大二叉树，并且输出这个树的根节点。 * * Example 1: * * 输入: [3,2,1,6,0,5] * 输入: 返回下面这棵树的根节点： * * 6 * / \ * 3 5 * \ / * 2 0 * \ * 1 * 注意: * * 给定的数组的大小在 [1, 1000] 之间。 */public class L654_ConstructMaximumBinaryTree &#123; public TreeNode constructMaximumBinaryTree(int[] nums) &#123; return buildTree(nums,0,nums.length -1); &#125; public TreeNode buildTree(int[] nums, int start, int end)&#123; if (start &gt; end)&#123; return null; &#125; int maxIndex = maxIndex(nums, start, end); TreeNode root = new TreeNode(nums[maxIndex]); root.left = buildTree(nums,start,maxIndex - 1); root.right = buildTree(nums,maxIndex + 1,end); return root; &#125; public int maxIndex(int[] arr,int start,int end)&#123; int maxIndex = start; for(int i = start ;i &lt;= end; i++)&#123; if (arr[i] &gt; arr[maxIndex])&#123; maxIndex = i; &#125; &#125; return maxIndex; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L572_IsSubtree]]></title>
    <url>%2F2016%2F01%2F26%2FL572-IsSubtree%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package LeetCode.topic.tree;/** * 给定两个非空二叉树 s 和 t，检验 s 中是否包含和 t 具有相同结构和节点值的子树。s 的一个子树包括 s 的一个节点和这个节点的所有子孙。s 也可以看做它自身的一棵子树。 * * 示例 1: * 给定的树 s: * * 3 * / \ * 4 5 * / \ * 1 2 * 给定的树 t： * * 4 * / \ * 1 2 * 返回 true，因为 t 与 s 的一个子树拥有相同的结构和节点值。 * * 示例 2: * 给定的树 s： * * 3 * / \ * 4 5 * / \ * 1 2 * / * 0 * 给定的树 t： * * 4 * / \ * 1 2 * 返回 false。 */public class L572_IsSubtree &#123; public boolean isSubtree(TreeNode s, TreeNode t) &#123; if (s == null &amp;&amp; t != null)return false; return isSame(s,t) || isSubtree(s.left,t) || isSubtree(s.right,t); &#125; private boolean isSame(TreeNode s, TreeNode t) &#123; if (s!= null &amp;&amp; t != null)&#123; return s.val == t.val &amp;&amp; isSame(s.left,t.left) &amp;&amp; isSame(s.right,t.right); &#125; else if (s == null &amp;&amp; t == null)&#123; return true; &#125; else &#123; return false; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L617_MergeTrees]]></title>
    <url>%2F2016%2F01%2F26%2FL617-MergeTrees%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243package LeetCode.topic.tree;/** * 给定两个二叉树，想象当你将它们中的一个覆盖到另一个上时，两个二叉树的一些节点便会重叠。 * * 你需要将他们合并为一个新的二叉树。合并的规则是如果两个节点重叠，那么将他们的值相加作为节点合并后的新值，否则不为 NULL 的节点将直接作为新二叉树的节点。 * * 示例 1: * * 输入: * Tree 1 Tree 2 * 1 2 * / \ / \ * 3 2 1 3 * / \ \ * 5 4 7 * 输出: * 合并后的树: * 3 * / \ * 4 5 * / \ \ * 5 4 7 * 注意: 合并必须从两个树的根节点开始。 */public class L617_MergeTrees &#123; public TreeNode mergeTrees(TreeNode t1, TreeNode t2) &#123; if (t1 == null) return t2; if (t2 == null) return t1; TreeNode cur = new TreeNode(t1.val + t2.val); cur.left = mergeTrees(t1.left,t2.left); cur.right = mergeTrees(t1.right,t2.right); return cur; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L543_DiameterOfBinaryTree]]></title>
    <url>%2F2016%2F01%2F26%2FL543-DiameterOfBinaryTree%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041package LeetCode.topic.tree;/** * 给定一棵二叉树，你需要计算它的直径长度。一棵二叉树的直径长度是任意两个结点路径长度中的最大值。这条路径可能穿过根结点。 * * 示例 : * 给定二叉树 * * 1 * / \ * 2 3 * / \ * 4 5 * 返回 3, 它的长度是路径 [4,2,1,3] 或者 [5,2,1,3]。 * * 注意：两结点之间的路径长度是以它们之间边的数目表示。 */public class L543_DiameterOfBinaryTree &#123; private int rst = 0; public int diameterOfBinaryTree(TreeNode root) &#123; if (root == null) return 0; helper(root); return rst; &#125; private int helper(TreeNode root) &#123; if (root == null) return 0; int left = helper(root.left); int right = helper(root.right); if (rst &lt; left + right) rst = left + right; return Math.max(left,right) + 1; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L530_BSTgetMinimumDifference]]></title>
    <url>%2F2016%2F01%2F26%2FL530-BSTgetMinimumDifference%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637package LeetCode.topic.tree;public class L530_BSTgetMinimumDifference &#123; /** * //二叉查找树中，中间节点的值一定是其左右节点值的中间数，因此最小差别一定是在中间节点与左右节点之间 * //中序遍历二叉查找树，每次比较当前节点与前一节点差值的绝对值与目前result中保存的最小值的大小，将较小的保存在result中 * * @param root * @return */ private int result = Integer.MAX_VALUE; private TreeNode preNode = null; public int getMinimumDifference(TreeNode root) &#123; getMin(root); return result; &#125; private void getMin(TreeNode root) &#123; if(root == null)&#123; return; &#125; getMin(root.left); if(preNode != null)//todo &#123; result = Math.min(Math.abs(root.val - preNode.val), result); &#125; preNode = root; getMin(root.right); &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L501_BSTFindMode]]></title>
    <url>%2F2016%2F01%2F26%2FL501-BSTFindMode%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package LeetCode.topic.tree;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;/** * 给定一个有相同值的二叉搜索树（BST），找出 BST 中的所有众数（出现频率最高的元素）。 * * 假定 BST 有如下定义： * * 结点左子树中所含结点的值小于等于当前结点的值 * 结点右子树中所含结点的值大于等于当前结点的值 * 左子树和右子树都是二叉搜索树 * 例如： * 给定 BST [1,null,2,2], * * 1 * \ * 2 * / * 2 * 返回[2]. * * 提示：如果众数超过1个，不需考虑输出顺序 * * 进阶：你可以不使用额外的空间吗？（假设由递归产生的隐式调用栈的开销不被计算在内） * * 使用中序遍历 * https://blog.csdn.net/qq_38959715/article/details/82682383 * */public class L501_BSTFindMode &#123; TreeNode pre = null;//todo must全局 Integer curTimes = 1,maxTimes = 0;//todo must全局 public int[] findMode(TreeNode root) &#123; if (root == null) return new int[]&#123;&#125;; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); inOrder(root,list); int[] result = new int[list.size()]; for(int i = 0;i&lt;list.size();i++)&#123; result[i] = list.get(i); &#125; return result; &#125; private void inOrder(TreeNode root, List&lt;Integer&gt; list) &#123; if (root == null) return; inOrder(root.left,list); if (pre != null)&#123; curTimes = (root.val == pre.val) ? curTimes + 1 : 1; &#125; if (curTimes == maxTimes)&#123; list.add(root.val); &#125; else if (curTimes &gt; maxTimes)&#123; list.clear(); list.add(root.val); maxTimes = curTimes; &#125; pre = root; inOrder(root.right,list); &#125; public static void main(String[] args) &#123; TreeNode r = new TreeNode(1); TreeNode t1 = new TreeNode(2); TreeNode t2 = new TreeNode(2); r.right = t1; t1.left = t2; L501_BSTFindMode t = new L501_BSTFindMode(); int[] mode = t.findMode(r); for (int x:mode) &#123; System.out.println(x); &#125; &#125; public static class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L437_TreePathSum]]></title>
    <url>%2F2016%2F01%2F26%2FL437-TreePathSum%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package LeetCode.topic.tree;import java.util.Stack;/** * 给定一个二叉树，它的每个结点都存放着一个整数值。 * * 找出路径和等于给定数值的路径总数。 * * 路径不需要从根节点开始，也不需要在叶子节点结束，但是路径方向必须是向下的（只能从父节点到子节点）。 * * 二叉树不超过1000个节点，且节点数值范围是 [-1000000,1000000] 的整数。 * * 示例： * * root = [10,5,-3,3,2,null,11,3,-2,null,1], sum = 8 * * 10 * / \ * 5 -3 * / \ \ * 3 2 11 * / \ \ * 3 -2 1 * * 返回 3。和等于 8 的路径有: * * 1. 5 -&gt; 3 * 2. 5 -&gt; 2 -&gt; 1 * 3. -3 -&gt; 11 */public class L437_TreePathSum &#123; public int pathSum(TreeNode root, int sum) &#123; int count = 0; if(root == null) return 0; Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;(); stack.add(root); while(!stack.isEmpty())&#123; TreeNode node = stack.pop(); count += helper(node, sum); if(node.right != null) stack.push(node.right); if(node.left != null) stack.push(node.left); &#125; return count; &#125; public int helper(TreeNode root, int sum) &#123; int count = 0; if(root == null) return 0; if(sum == root.val)&#123; count++; &#125; count += helper(root.left, sum-root.val); count += helper(root.right, sum-root.val); return count; &#125;// public int pathSum(TreeNode root, int sum) &#123;// if(root == null) return 0;// int res = helper(root,sum);// res += pathSum(root.left,sum);// res += pathSum(root.right,sum);// return res;// &#125;// public int helper(TreeNode node,int num)&#123;// if(node == null)&#123;// return 0;// &#125;// int res = 0;// if(node.val == num)&#123;// res++;// &#125;// res += helper(node.left,num - node.val);// res += helper(node.right,num - node.val);// return res;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L429_TreeLevelOrder]]></title>
    <url>%2F2016%2F01%2F26%2FL429-TreeLevelOrder%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142package LeetCode.topic.tree;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;public class L429_TreeLevelOrder &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; levelOrder(Node root) &#123; if(root == null)return new ArrayList&lt;&gt;(); LinkedList&lt;Node&gt; queue = new LinkedList&lt;Node&gt;(); queue.add(root); while(!queue.isEmpty())&#123; int count = queue.size(); List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); while(count &gt; 0)&#123; Node cur = queue.pop(); list.add(cur.val); if(cur.children != null)&#123; for(Node n : cur.children)&#123; queue.add(n); &#125; &#125; count--; &#125; res.add(list); &#125; return res; &#125; class Node &#123; public int val; public List&lt;Node&gt; children; public Node() &#123;&#125; public Node(int _val,List&lt;Node&gt; _children) &#123; val = _val; children = _children; &#125; &#125;;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L404_TreeSumOfLeftLeaves]]></title>
    <url>%2F2016%2F01%2F26%2FL404-TreeSumOfLeftLeaves%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940package LeetCode.topic.tree;public class L404_TreeSumOfLeftLeaves &#123; /** * 本质还是二叉树的遍历 * * 1. 求所有左叶子节点的值的和，那就是求当前节点的左节点和右节点的所有左叶子节点的和 * * 2. 不是左叶子节点返回值为0，是左叶子节点放回叶子的值 * * 3. 递归求和即可 * * @param root * @return */ public int sumOfLeftLeaves(TreeNode root) &#123; return dfs(root, false); &#125; public int dfs(TreeNode node, Boolean isLeft) &#123; if (node == null) return 0; if (node.left == null &amp;&amp; node.right == null) &#123; if (isLeft == true) return node.val; else return 0; &#125; int leftVal = dfs(node.left, true); int rightVal = dfs(node.right, false); return leftVal + rightVal; &#125; class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L297_DeserializeSerializeTree]]></title>
    <url>%2F2016%2F01%2F26%2FL297-DeserializeSerializeTree%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package LeetCode.topic.tree;import java.util.ArrayDeque;import java.util.Arrays;import java.util.Queue;/** * 序列化是将一个数据结构或者对象转换为连续的比特位的操作，进而可以将转换后的数据存储在一个文件或者内存中，同时也可以通过网络传输到另一个计算机环境，采取相反方式重构得到原数据。 * * 请设计一个算法来实现二叉树的序列化与反序列化。这里不限定你的序列 / 反序列化算法执行逻辑，你只需要保证一个二叉树可以被序列化为一个字符串并且将这个字符串反序列化为原始的树结构。 * * 示例: * * 你可以将以下二叉树： * * 1 * / \ * 2 3 * / \ * 4 5 * * 序列化为 &quot;[1,2,3,null,null,4,5]&quot; * 提示: 这与 LeetCode 目前使用的方式一致，详情请参阅 LeetCode 序列化二叉树的格式。你并非必须采取这种方式，你也可以采用其他的方法解决这个问题。 * * 说明: 不要使用类的成员 / 全局 / 静态变量来存储状态，你的序列化和反序列化算法应该是无状态的。 */public class L297_DeserializeSerializeTree &#123; // Encodes a tree to a single string. public String serialize(TreeNode root) &#123; if (root == null) &#123; return &quot;$,&quot;; &#125; return root.val + &quot;,&quot; + serialize(root.left) + serialize(root.right); &#125; // Decodes your encoded data to tree. public TreeNode deserialize(String data) &#123; String[] strings = data.split(&quot;,&quot;); Queue&lt;String&gt; queue = new ArrayDeque&lt;&gt;(Arrays.asList(strings)); return func(queue); &#125; private TreeNode func(Queue&lt;String&gt; strings) &#123; String string = strings.remove(); if (&quot;$&quot;.equals(string)) &#123; return null; &#125; TreeNode newNode = new TreeNode(Integer.parseInt(string)); newNode.left = func(strings); newNode.right = func(strings); return newNode; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L257_BinaryTreePaths]]></title>
    <url>%2F2016%2F01%2F26%2FL257-BinaryTreePaths%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;/** * 给定一个二叉树，返回所有从根节点到叶子节点的路径。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例: * * 输入: * * 1 * / \ * 2 3 * \ * 5 * * 输出: [&quot;1-&gt;2-&gt;5&quot;, &quot;1-&gt;3&quot;] * * 解释: 所有根节点到叶子节点的路径为: 1-&gt;2-&gt;5, 1-&gt;3 */public class L257_BinaryTreePaths &#123; List&lt;String&gt; res = new ArrayList&lt;&gt;(); public List&lt;String&gt; binaryTreePaths(TreeNode root) &#123; if (root == null)return new ArrayList&lt;&gt;(); ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); helper(root,list); return res; &#125; private void helper(TreeNode root, ArrayList&lt;Integer&gt; list) &#123; if (root == null) return; if (root.left == null &amp;&amp; root.right == null)&#123; list.add(root.val); String s = &quot;&quot;; for (Integer node:list) &#123; if (s.equals(&quot;&quot;))&#123; s += node+&quot;&quot;; &#125; else &#123; s = s + &quot;-&gt;&quot;+ node; &#125; &#125; res.add(s); list.remove(list.size() - 1); return; &#125; list.add(root.val); helper(root.right,list); helper(root.left,list); list.remove(list.size() - 1); &#125;// List&lt;String&gt; res = new ArrayList&lt;&gt;();// public List&lt;String&gt; binaryTreePaths(TreeNode root) &#123;// if (root == null)return new ArrayList&lt;&gt;();// ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;();// list.add(root.val);// helper(root,list);// return res;// &#125;//// private void helper(TreeNode root, ArrayList&lt;Integer&gt; list) &#123;// if (root.left == null &amp;&amp; root.right == null)&#123;// String s = &quot;&quot;;// for (Integer node:list) &#123;// if (s.equals(&quot;&quot;))&#123;// s += node+&quot;&quot;;// &#125; else &#123;// s = s + &quot;-&gt;&quot;+ node;// &#125;// &#125;// res.add(s);// &#125; else if (root.left == null)&#123;// list.add(root.right.val);// helper(root.right,list);// &#125; else if (root.right == null)&#123;// list.add(root.left.val);// helper(root.left,list);// &#125; else &#123;// ArrayList&lt;Integer&gt; mycopy=new ArrayList&lt;Integer&gt;();// mycopy=(ArrayList&lt;Integer&gt;) list.clone();// list.add(root.right.val);// helper(root.right,list);// mycopy.add(root.left.val);// helper(root.left,mycopy);// &#125;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L236_TreeLowestCommonAncestor]]></title>
    <url>%2F2016%2F01%2F26%2FL236-TreeLowestCommonAncestor%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package LeetCode.topic.tree;/** * 给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。 * * 百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。” * * 例如，给定如下二叉树: root = [3,5,1,6,2,0,8,null,null,7,4] * * * * * * 示例 1: * * 输入: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 1 * 输出: 3 * 解释: 节点 5 和节点 1 的最近公共祖先是节点 3。 * 示例 2: * * 输入: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 4 * 输出: 5 * 解释: 节点 5 和节点 4 的最近公共祖先是节点 5。因为根据定义最近公共祖先节点可以为节点本身。 * * * 说明: * * 所有节点的值都是唯一的。 * p、q 为不同节点且均存在于给定的二叉树中。 * * LCA 问题，查阅相关资料 */public class L236_TreeLowestCommonAncestor &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if (root == null) return null; if (root == q || root == p)&#123; return root;//找到待查询的值，一层一层向外传递 &#125; TreeNode left = lowestCommonAncestor(root.left,p,q); TreeNode right = lowestCommonAncestor(root.right,p,q); if (left != null &amp;&amp; right != null)&#123; return root; &#125; else if (left != null)&#123; return left; &#125; else if (right != null)&#123; return right; &#125; return null; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L235_BSTLowestCommonAncestor]]></title>
    <url>%2F2016%2F01%2F26%2FL235-BSTLowestCommonAncestor%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package LeetCode.topic.tree;/** * 给定一个二叉搜索树, 找到该树中两个指定节点的最近公共祖先。 * * 百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。” * * 例如，给定如下二叉搜索树: root = [6,2,8,0,4,7,9,null,null,3,5] * * * * * * 示例 1: * * 输入: root = [6,2,8,0,4,7,9,null,null,3,5], p = 2, q = 8 * 输出: 6 * 解释: 节点 2 和节点 8 的最近公共祖先是 6。 * 示例 2: * * 输入: root = [6,2,8,0,4,7,9,null,null,3,5], p = 2, q = 4 * 输出: 2 * 解释: 节点 2 和节点 4 的最近公共祖先是 2, 因为根据定义最近公共祖先节点可以为节点本身。 * * * 说明: * * 所有节点的值都是唯一的。 * p、q 为不同节点且均存在于给定的二叉搜索树中。 * * * //todo 可以利用二叉搜索树的特性 */public class L235_BSTLowestCommonAncestor &#123; TreeNode res = null; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if (root == null) return null; helper(root,p,q); return res; &#125; private void helper(TreeNode root, TreeNode p, TreeNode q) &#123; if(root == null)return; if ((p.val - root.val)*(q.val - root.val) &lt;= 0)&#123;//一定在最近公共祖先的两侧 res = root; &#125; else if (p.val &gt; root.val &amp;&amp; q.val &gt; root.val)&#123; helper(root.right,q,p); &#125; else &#123; helper(root.left,p,q); &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L230_TreeKthSmallest]]></title>
    <url>%2F2016%2F01%2F26%2FL230-TreeKthSmallest%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package LeetCode.topic.tree;import java.util.LinkedList;import java.util.Stack;/** * 给定一个二叉搜索树，编写一个函数 kthSmallest 来查找其中第 k 个最小的元素。 * * 说明： * 你可以假设 k 总是有效的，1 ≤ k ≤ 二叉搜索树元素个数。 * * 示例 1: * * 输入: root = [3,1,4,null,2], k = 1 * 3 * / \ * 1 4 * \ * 2 * 输出: 1 * 示例 2: * * 输入: root = [5,3,6,2,4,null,null,1], k = 3 * 5 * / \ * 3 6 * / \ * 2 4 * / * 1 * 输出: 3 * * 中序遍历，是升序的 */public class L230_TreeKthSmallest &#123; public int kthSmallest(TreeNode root, int k) &#123; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); int i = 0; TreeNode cur = root; while (true)&#123;//重要 while (cur != null)&#123; stack.push(cur); cur = cur.left; &#125; if (stack.isEmpty())&#123; break; &#125; TreeNode node = stack.pop(); if (i == k - 1)&#123; return node.val; &#125; i++; cur = node.right; &#125; return -1; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L226_InvertTree]]></title>
    <url>%2F2016%2F01%2F26%2FL226-InvertTree%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package LeetCode.topic.tree;/** * 翻转一棵二叉树。 * * 示例： * * 输入： * * 4 * / \ * 2 7 * / \ / \ * 1 3 6 9 * 输出： * * 4 * / \ * 7 2 * / \ / \ * 9 6 3 1 * 备注: * 这个问题是受到 Max Howell 的 原问题 启发的 ： * * 谷歌：我们90％的工程师使用您编写的软件(Homebrew)，但是您却无法在面试时在白板上写出翻转二叉树这道题，这太糟糕了。 */public class L226_InvertTree &#123; public TreeNode invertTree(TreeNode root) &#123; if (root == null)return null; helper(root); return root; &#125; private void helper(TreeNode root) &#123; if (root == null)return; // todo 必须在首位 helper(root.left); helper(root.right); //下面的代码，可在最前，也可以在最后 TreeNode tmp = root.left; root.left = root.right; root.right = tmp; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L222_TreeCountNodes]]></title>
    <url>%2F2016%2F01%2F26%2FL222-TreeCountNodes%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package LeetCode.topic.tree;/** * 给出一个完全二叉树，求出该树的节点个数。 * * 说明： * * 完全二叉树的定义如下：在完全二叉树中，除了最底层节点可能没填满外，其余每层节点数都达到最大值，并且最下面一层的节点都集中在该层最左边的若干位置。若最底层为第 h 层，则该层包含 1~ 2h 个节点。 * * 示例: * * 输入: * 1 * / \ * 2 3 * / \ / * 4 5 6 * * 输出: 6 */public class L222_TreeCountNodes &#123; public int countNodes(TreeNode root) &#123; if(root == null) return 0; return countNodes(root.left) + countNodes(root.right) + 1; &#125;// public int countNodes(TreeNode root) &#123;// /**// 完全二叉树的高度可以直接通过不断地访问左子树就可以获取// 判断左右子树的高度:// 如果相等说明左子树是满二叉树, 然后进一步判断右子树的节点数(最后一层最后出现的节点必然在右子树中)// 如果不等说明右子树是深度小于左子树的满二叉树, 然后进一步判断左子树的节点数(最后一层最后出现的节点必然在左子树中)// **/// if (root==null) return 0;// int ld = getDepth(root.left);// int rd = getDepth(root.right);// if(ld == rd)// return (1 &lt;&lt; ld) + countNodes(root.right); // 1(根节点) + (1 &lt;&lt; ld)-1(左完全左子树节点数) + 右子树节点数量// else// return (1 &lt;&lt; rd) + countNodes(root.left); // 1(根节点) + (1 &lt;&lt; rd)-1(右完全右子树节点数) + 左子树节点数量//// &#125;//// private int getDepth(TreeNode r) &#123;// int depth = 0;// while(r != null) &#123;// depth++;// r = r.left;// &#125;// return depth;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L199_TreeRightSideView]]></title>
    <url>%2F2016%2F01%2F26%2FL199-TreeRightSideView%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package LeetCode.topic.tree;import java.util.ArrayDeque;import java.util.ArrayList;import java.util.List;/** * 给定一棵二叉树，想象自己站在它的右侧，按照从顶部到底部的顺序，返回从右侧所能看到的节点值。 * * 示例: * * 输入: [1,2,3,null,5,null,4] * 输出: [1, 3, 4] * 解释: * * 1 &lt;--- * / \ * 2 3 &lt;--- * \ \ * 5 4 &lt;--- */public class L199_TreeRightSideView &#123; public List&lt;Integer&gt; rightSideView(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;Integer&gt;(); if(root == null) return res; ArrayDeque&lt;TreeNode&gt; queue = new ArrayDeque&lt;TreeNode&gt;(); queue.add(root); while(!queue.isEmpty())&#123; int cont = queue.size(); TreeNode node = queue.peek();//todo 队列是先进先出的，因为下面优先遍历的是右子树 res.add(node.val); while(cont &gt; 0)&#123; node = queue.poll(); if(node.right != null) queue.add(node.right);//todo here if(node.left != null) queue.add(node.left); cont--; &#125; &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L129_TreeSumNumbers]]></title>
    <url>%2F2016%2F01%2F26%2FL129-TreeSumNumbers%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109package LeetCode.topic.tree;import java.util.ArrayList;import java.util.Arrays;import java.util.List;/** * 给定一个二叉树，它的每个结点都存放一个 0-9 的数字，每条从根到叶子节点的路径都代表一个数字。 * * 例如，从根到叶子节点路径 1-&gt;2-&gt;3 代表数字 123。 * * 计算从根到叶子节点生成的所有数字之和。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例 1: * * 输入: [1,2,3] * 1 * / \ * 2 3 * 输出: 25 * 解释: * 从根到叶子节点路径 1-&gt;2 代表数字 12. * 从根到叶子节点路径 1-&gt;3 代表数字 13. * 因此，数字总和 = 12 + 13 = 25. * 示例 2: * * 输入: [4,9,0,5,1] * 4 * / \ * 9 0 * / \ * 5 1 * 输出: 1026 * 解释: * 从根到叶子节点路径 4-&gt;9-&gt;5 代表数字 495. * 从根到叶子节点路径 4-&gt;9-&gt;1 代表数字 491. * 从根到叶子节点路径 4-&gt;0 代表数字 40. * 因此，数字总和 = 495 + 491 + 40 = 1026. */public class L129_TreeSumNumbers &#123; private int total; public int sumNumbers(TreeNode root) &#123; if (root == null) return 0; ArrayList&lt;Integer&gt; sum = new ArrayList&lt;&gt;(); helper(root,sum); return total; &#125; private void helper(TreeNode cur,ArrayList&lt;Integer&gt; sum) &#123; if (cur == null) return; if (cur.left == null &amp;&amp; cur.right == null)&#123; sum.add(0,cur.val);//todo int s = 0; for (int i = sum.size() -1 ;i&gt;=0;i--)&#123; s += (int) (Math.pow(10,Double.parseDouble(i+&quot;&quot;))*sum.get(i).intValue()); &#125; total += s; sum.remove(0);//todo return; &#125; sum.add(0,cur.val); helper(cur.left,sum); helper(cur.right,sum); sum.remove(0);//todo &#125;// private int total;// public int sumNumbers(TreeNode root) &#123;// if (root == null) return 0;// ArrayList&lt;Integer&gt; sum = new ArrayList&lt;&gt;();// sum.add(root.val);// helper(root,sum);// return total;// &#125;//// private void helper(TreeNode cur,ArrayList&lt;Integer&gt; sum) &#123;// if (cur.left == null &amp;&amp; cur.right == null)&#123;// int s = 0;// for (int i = sum.size() -1 ;i&gt;=0;i--)&#123;// s += (int) (Math.pow(10,Double.parseDouble(i+&quot;&quot;))*sum.get(i).intValue());// &#125;// total += s;// &#125; else if (cur.left == null)&#123;// sum.add(0,cur.right.val);// helper(cur.right,sum);// &#125; else if (cur.right == null)&#123;// sum.add(0,cur.left.val);// helper(cur.left,sum);// &#125; else &#123;// ArrayList&lt;Integer&gt; mycopy=new ArrayList&lt;Integer&gt;();// mycopy=(ArrayList&lt;Integer&gt;) sum.clone();// sum.add(0,cur.right.val);// helper(cur.right,sum);// mycopy.add(0,cur.left.val);// helper(cur.left,mycopy);// &#125;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L144_PreorderTraversal]]></title>
    <url>%2F2016%2F01%2F26%2FL144-PreorderTraversal%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;import java.util.Stack;/** * 给定一个二叉树，返回它的 前序 遍历。 * * 示例: * * 输入: [1,null,2,3] * 1 * \ * 2 * / * 3 * * 输出: [1,2,3] * 进阶: 递归算法很简单，你可以通过迭代算法完成吗？ */public class L144_PreorderTraversal &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root); while (!stack.isEmpty())&#123; TreeNode cur = stack.pop(); res.add(cur.val); if (cur.right != null)&#123; stack.push(cur.right); &#125; if (cur.left != null)&#123; stack.push(cur.left); &#125; &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L124_TreeMaxPathSum]]></title>
    <url>%2F2016%2F01%2F26%2FL124-TreeMaxPathSum%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package LeetCode.topic.tree;/** * 给定一个非空二叉树，返回其最大路径和。 * * 本题中，路径被定义为一条从树中任意节点出发，达到任意节点的序列。该路径至少包含一个节点，且不一定经过根节点。 * * 示例 1: * * 输入: [1,2,3] * * 1 * / \ * 2 3 * * 输出: 6 * 示例 2: * * 输入: [-10,9,20,null,null,15,7] * * -10 * / \ * 9 20 * / \ * 15 7 * * 输出: 42 */public class L124_TreeMaxPathSum &#123; private int ret = Integer.MIN_VALUE; public int maxPathSum(TreeNode root) &#123; /** 对于任意一个节点, 如果最大和路径包含该节点, 那么只可能是两种情况: 1. 其左右子树中所构成的和路径值较大的那个加上该节点的值后向父节点回溯构成最大路径 2. 左右子树都在最大路径中, 加上该节点的值构成了最终的最大路径 **/ getMax(root); return ret; &#125; private int getMax(TreeNode r) &#123; if(r == null) return 0; int left = Math.max(0, getMax(r.left)); // 如果子树路径和为负则应当置0表示最大路径不包含子树 int right = Math.max(0, getMax(r.right)); ret = Math.max(ret, r.val + left + right); // 判断在该节点包含左右子树的路径和是否大于当前最大路径和 return Math.max(left, right) + r.val; &#125;// private int res = 0;// public int maxPathSum(TreeNode root) &#123;// res = root.val;// helper(root);// return res;// &#125;//// private int helper(TreeNode cur) &#123;// int sum;// if (cur.left != null &amp;&amp; cur.right != null)&#123;// sum = cur.val;// &#125; else if (cur.left == null)&#123;// int right = helper(cur.right);// sum = right &gt;0 ? right + cur.val : cur.val;// &#125; else if (cur.right == null)&#123;// int left = helper(cur.left);// sum = left &gt; 0 ? left + cur.val : cur.val;// &#125; else &#123;// int left = helper(cur.left);// int right = helper(cur.right);// res = Math.max(res,left + right + cur.val);//走当前节点//// int max = Math.max(left,right);//不走当前节点// sum = max &gt; 0 ? max + cur.val : cur.val;// &#125;// res = Math.max(res,sum);// return res;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L116_TreeConnectOne]]></title>
    <url>%2F2016%2F01%2F26%2FL116-TreeConnectOne%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package LeetCode.topic.tree;import java.util.LinkedList;/** * 给定一个二叉树 * * struct TreeLinkNode &#123; * TreeLinkNode *left; * TreeLinkNode *right; * TreeLinkNode *next; * &#125; * 填充它的每个 next 指针，让这个指针指向其下一个右侧节点。如果找不到下一个右侧节点，则将 next 指针设置为 NULL。 * * 初始状态下，所有 next 指针都被设置为 NULL。 * * 说明: * * 你只能使用额外常数空间。 * 使用递归解题也符合要求，本题中递归程序占用的栈空间不算做额外的空间复杂度。 * 你可以假设它是一个完美二叉树（即所有叶子节点都在同一层，每个父节点都有两个子节点）。 * 示例: * * 给定完美二叉树， * * 1 * / \ * 2 3 * / \ / \ * 4 5 6 7 * 调用你的函数后，该完美二叉树变为： * * 1 -&gt; NULL * / \ * 2 -&gt; 3 -&gt; NULL * / \ / \ * 4-&gt;5-&gt;6-&gt;7 -&gt; NULL */public class L116_TreeConnectOne &#123; public class TreeLinkNode &#123; int val; TreeLinkNode left, right, next; TreeLinkNode(int x) &#123; val = x; &#125; &#125; public void connect(TreeLinkNode root) &#123; if (root == null) return; LinkedList&lt;TreeLinkNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while (!queue.isEmpty())&#123; int count = queue.size(); TreeLinkNode last = null; while (count &gt; 0)&#123; TreeLinkNode cur = queue.pop(); if (cur.left != null)&#123; queue.add(cur.left); if (last == null)&#123; last = cur.left; &#125; else &#123; last.next = cur.left; last = last.next;//传递 &#125; &#125; if (cur.right != null)&#123; queue.add(cur.right); if (last == null)&#123; last = cur.right; &#125; else &#123; last.next = cur.right; last = last.next;//传递 &#125; &#125; count--; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L114_BstFlattenList]]></title>
    <url>%2F2016%2F01%2F26%2FL114-BstFlattenList%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package LeetCode.topic.tree;/** * 给定一个二叉树，**原地**将它展开为链表。 * * 例如，给定二叉树 * * 1 * / \ * 2 5 * / \ \ * 3 4 6 * 将其展开为： * * 1 * \ * 2 * \ * 3 * \ * 4 * \ * 5 * \ * 6 * 用了递归的思路，把左子树作为右子树，并把 原右子树(temp) 拼接在 现右子树 的最右端 * * * * 1 * / \ * 2 5 * \ \ * (3) 6 * \ * 4 * * * * 1 * \ * (2,3,4) * \ * 5 * \ * 6 */public class L114_BstFlattenList &#123; public void flatten(TreeNode root) &#123; if (root == null) return; flatten(root.left); flatten(root.right); if (root.left != null)&#123;//注意，一定要判断左子树是否为空 TreeNode right = root.right;//记录右节点 root.right = root.left; root.left = null;//将左节点置空 TreeNode cur = root.right;//到左节点的最后一个节点 while (cur.right != null)&#123;//把左子树作为右子树 cur = cur.right; &#125; cur.right = right; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L115_PostorderTraversal]]></title>
    <url>%2F2016%2F01%2F26%2FL115-PostorderTraversal%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;import java.util.Stack;/** * 给定一个二叉树，返回它的 后序 遍历。 * * 示例: * * 输入: [1,null,2,3] * 1 * \ * 2 * / * 3 * * 输出: [3,2,1] * 进阶: 递归算法很简单，你可以通过迭代算法完成吗？ */public class L115_PostorderTraversal &#123; public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); Stack&lt;TreeNode&gt; outputStack = new Stack&lt;&gt;(); TreeNode cur = root; stack.push(root); while (!stack.isEmpty())&#123; cur = stack.pop(); outputStack.push(cur); if (cur.left != null)&#123; stack.push(cur.left); &#125; if (cur.right != null)&#123; stack.push(cur.right); &#125; &#125; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); while (!outputStack.isEmpty())&#123; res.add(outputStack.pop().val); &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L113_PathSumTree]]></title>
    <url>%2F2016%2F01%2F26%2FL113-PathSumTree%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package LeetCode.topic.tree;import java.util.ArrayList;import java.util.Arrays;import java.util.List;/** * 给定一个二叉树和一个目标和，找到所有从根节点到叶子节点路径总和等于给定目标和的路径。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例: * 给定如下二叉树，以及目标和 sum = 22， * * 5 * / \ * 4 8 * / / \ * 11 13 4 * / \ / \ * 7 2 5 1 * 返回: * * [ * [5,4,11,2], * [5,8,4,5] * ] */public class L113_PathSumTree &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; pathSum(TreeNode root, int sum) &#123; if (root == null) return new ArrayList&lt;&gt;(); helper(root,sum,new ArrayList&lt;Integer&gt;()); return res; &#125; private void helper(TreeNode root, int sum, ArrayList&lt;Integer&gt; list) &#123; if (root == null) return; if (root.left == null &amp;&amp; root.right == null &amp;&amp; sum - root.val == 0)&#123; list.add(root.val);//todo 一定要添加 res.add(new ArrayList&lt;&gt;(list));//todo 重点，此处一定要copy一个list list.remove(list.size() - 1);//todo return; &#125; list.add(root.val); helper(root.left,sum - root.val,list); helper(root.right,sum - root.val,list); list.remove(list.size() - 1);//todo &#125;// List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;();// public List&lt;List&lt;Integer&gt;&gt; pathSum(TreeNode root, int sum) &#123;// if (root == null) return new ArrayList&lt;&gt;();// helper(root,sum,0,new ArrayList&lt;Integer&gt;());// return res;// &#125;//// private boolean helper(TreeNode root, int sum, int tempSum, ArrayList&lt;Integer&gt; list) &#123;// if (root.left == null &amp;&amp; root.right == null)&#123;// if (tempSum + root.val == sum)&#123;// list.add(root.val);// res.add(list);// return true;// &#125;// return false;// &#125; else &#123;// tempSum += root.val;// list.add(root.val);// if (root.left == null)&#123;// return helper(root.right,sum,tempSum,list);// &#125; else if (root.right == null)&#123;// return helper(root.left,sum,tempSum,list);// &#125; else &#123;// ArrayList&lt;Integer&gt; mycopy=new ArrayList&lt;Integer&gt;();// mycopy=(ArrayList&lt;Integer&gt;) list.clone();// helper(root.left,sum,tempSum,list);// helper(root.right,sum,tempSum,mycopy);// &#125;// &#125;// return false;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L112_HasPathSumTree]]></title>
    <url>%2F2016%2F01%2F26%2FL112-HasPathSumTree%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package LeetCode.topic.tree;/** * 给定一个二叉树和一个目标和，判断该树中是否存在根节点到叶子节点的路径，这条路径上所有节点值相加等于目标和。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例: * 给定如下二叉树，以及目标和 sum = 22， * * 5 * / \ * 4 8 * / / \ * 11 13 4 * / \ \ * 7 2 1 * 返回 true, 因为存在目标和为 22 的根节点到叶子节点的路径 5-&gt;4-&gt;11-&gt;2。 */public class L112_HasPathSumTree &#123; public boolean hasPathSum(TreeNode root, int sum) &#123; if (root == null) return false; return helper(root,sum,0); &#125; private boolean helper(TreeNode node, int sum, int tempSum) &#123; if (node.left == null &amp;&amp; node.right == null)&#123; return tempSum + node.val == sum; &#125; else &#123; tempSum += node.val; if (node.left == null)&#123; return helper(node.right,sum,tempSum); &#125; else if (node.right == null)&#123; return helper(node.left,sum,tempSum); &#125; else &#123; return helper(node.right,sum,tempSum) || helper(node.left,sum,tempSum); &#125; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L110_IsBalancedTree]]></title>
    <url>%2F2016%2F01%2F26%2FL110-IsBalancedTree%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package LeetCode.topic.tree;/** * 给定一个二叉树，判断它是否是高度平衡的二叉树。 * * 本题中，一棵高度平衡二叉树定义为： * * 一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过1。 * * 示例 1: * * 给定二叉树 [3,9,20,null,null,15,7] * * 3 * / \ * 9 20 * / \ * 15 7 * 返回 true 。 * * 示例 2: * * 给定二叉树 [1,2,2,3,3,null,null,4,4] * * 1 * / \ * 2 2 * / \ * 3 3 * / \ * 4 4 * 返回 false 。 */public class L110_IsBalancedTree &#123; public boolean isBalanced(TreeNode root) &#123; if (root == null)return true; int left = getDepth(root.left); int right = getDepth(root.right); if (Math.abs(left - right) &gt; 1)&#123; return false; &#125; return isBalanced(root.left) &amp;&amp; isBalanced(root.right); &#125; private int getDepth(TreeNode node) &#123; if (node == null)return 0; return Math.max(1 + getDepth(node.right),1 + getDepth(node.left)); &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L111_MinDepthTree]]></title>
    <url>%2F2016%2F01%2F26%2FL111-MinDepthTree%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738package LeetCode.topic.tree;/** * 给定一个二叉树，找出其最小深度。 * * 最小深度是从根节点到最近叶子节点的最短路径上的节点数量。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例: * * 给定二叉树 [3,9,20,null,null,15,7], * * 3 * / \ * 9 20 * / \ * 15 7 * 返回它的最小深度 2. */public class L111_MinDepthTree &#123;// 错误的写法；如果根节点的一侧为空，另一侧不为空；此时求的是不为空的一侧的最小深度 public int minDepth(TreeNode root) &#123; if (root == null) return 0; if (root.left == null) return 1 + minDepth(root.right); if (root.right == null) return 1 + minDepth(root.left); return Math.min(minDepth(root.left), minDepth(root.right)) + 1; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L108_SortedArrayToBST]]></title>
    <url>%2F2016%2F01%2F26%2FL108-SortedArrayToBST%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445package LeetCode.topic.tree;/** * 将一个按照升序排列的有序数组，转换为一棵高度平衡二叉搜索树。 * * 本题中，一个高度平衡二叉树是指一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过 1。 * * 示例: * * 给定有序数组: [-10,-3,0,5,9], * * 一个可能的答案是：[0,-3,9,-10,null,5]，它可以表示下面这个高度平衡二叉搜索树： * * 0 * / \ * -3 9 * / / * -10 5 * * 左右等分建立左右子树，中间节点作为子树根节点，递归该过程 */public class L108_SortedArrayToBST &#123; public TreeNode sortedArrayToBST(int[] nums) &#123; if (nums == null || nums.length == 0)return null; return helper(nums,0,nums.length - 1); &#125; private TreeNode helper(int[] nums, int low, int high) &#123; if (low &gt; high) return null; if (low == high) return new TreeNode(nums[low]);//low是序号，nums[low]才是其值 int mid = (low + high)&gt;&gt;1; TreeNode cur = new TreeNode(nums[mid]);//mid是序号，nums[low]才是其值 cur.left = helper(nums,low,mid-1); cur.right = helper(nums,mid+1,high); return cur; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L107_levelOrderBottomTraversal]]></title>
    <url>%2F2016%2F01%2F26%2FL107-levelOrderBottomTraversal%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package LeetCode.topic.tree;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;/** * 给定一个二叉树，返回其节点值自底向上的层次遍历。 （即按从叶子节点所在层到根节点所在的层，逐层从左向右遍历） * * 例如： * 给定二叉树 [3,9,20,null,null,15,7], * * 3 * / \ * 9 20 * / \ * 15 7 * 返回其自底向上的层次遍历为： * * [ * [15,7], * [9,20], * [3] * ] */public class L107_levelOrderBottomTraversal &#123; public List&lt;List&lt;Integer&gt;&gt; levelOrderBottom(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); LinkedList&lt;TreeNode&gt; quene = new LinkedList&lt;&gt;(); quene.add(root); while (!quene.isEmpty())&#123; int count = quene.size(); List&lt;Integer&gt; innerList = new ArrayList&lt;&gt;(); while (count &gt; 0)&#123; TreeNode cur = quene.pop(); innerList.add(cur.val); if (cur.left != null)&#123; quene.add(cur.left); &#125; if (cur.right != null)&#123; quene.add(cur.right); &#125; count--; &#125; res.add(0,innerList); &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L106_BuildTreeByMiddlePost]]></title>
    <url>%2F2016%2F01%2F26%2FL106-BuildTreeByMiddlePost%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;/** * 根据一棵树的中序遍历与后序遍历构造二叉树。 * * 注意: * 你可以假设树中没有重复的元素。 * * 例如，给出 * * 中序遍历 inorder = [9,3,15,20,7] * 后序遍历 postorder = [9,15,7,20,3] * 返回如下的二叉树： * * 3 * / \ * 9 20 * / \ * 15 7 * * * 后序遍历最后一个是根 */public class L106_BuildTreeByMiddlePost &#123; public TreeNode buildTree(int[] inorder, int[] postorder) &#123; TreeNode root = null; if (inorder.length != 0 &amp;&amp; postorder.length != 0)&#123; List&lt;Integer&gt; inorderList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; inorder.length; i++) &#123; inorderList.add(inorder[i]); &#125; List&lt;Integer&gt; postorderList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; postorder.length; i++) &#123; postorderList.add(postorder[i]); &#125; return helper(inorderList,postorderList); &#125; return root; &#125; private TreeNode helper(List&lt;Integer&gt; inorderList, List&lt;Integer&gt; postorderList) &#123; TreeNode root = null; List&lt;Integer&gt; leftInorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; rightInorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; leftPostorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; rightPostorderList = new ArrayList&lt;&gt;(); if (inorderList.size() != 0 &amp;&amp; postorderList.size() != 0)&#123; root = new TreeNode(postorderList.get(postorderList.size() - 1)); int rootInorderPos = inorderList.indexOf(root.val); leftInorderList = inorderList.subList(0,rootInorderPos); rightInorderList = inorderList.subList(rootInorderPos+1,inorderList.size()); int leftInorderSize =leftInorderList.size(); leftPostorderList = postorderList.subList(0,leftInorderSize); rightPostorderList = postorderList.subList(leftInorderSize,postorderList.size() - 1); root.left = helper(leftInorderList,leftPostorderList); root.right = helper(rightInorderList,rightPostorderList); &#125; return root; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L105_BuildTreeByPreMiddle]]></title>
    <url>%2F2016%2F01%2F26%2FL105-BuildTreeByPreMiddle%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package LeetCode.topic.tree;import java.util.ArrayList;import java.util.Arrays;import java.util.List;/** * 根据一棵树的前序遍历与中序遍历构造二叉树。 * * 注意: * 你可以假设树中没有重复的元素。 * * 例如，给出 * * 前序遍历 preorder = [3,9,20,15,7] * 中序遍历 inorder = [9,3,15,20,7] * 返回如下的二叉树： * * 3 * / \ * 9 20 * / \ * 15 7 */public class L105_BuildTreeByPreMiddle &#123; public TreeNode buildTree(int[] preorder, int[] inorder) &#123; TreeNode root = null; if (preorder.length != 0 &amp;&amp; inorder.length != 0)&#123; //todo 先转化为list处理 List&lt;Integer&gt; preorderList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; preorder.length; i++) &#123; preorderList.add(preorder[i]); &#125; List&lt;Integer&gt; inorderList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; inorder.length; i++) &#123; inorderList.add(inorder[i]); &#125; return helper(preorderList,inorderList); &#125; return root; &#125; private TreeNode helper(List&lt;Integer&gt; preorderList, List&lt;Integer&gt; inorderList) &#123; TreeNode root = null; List&lt;Integer&gt; leftPreorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; rightPreorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; leftInorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; rightInorderList = new ArrayList&lt;&gt;(); if (preorderList.size() != 0 &amp;&amp; inorderList.size() != 0)&#123; root = new TreeNode(preorderList.get(0)); int inOrderpos = inorderList.indexOf(root.val);//根节点 leftInorderList = inorderList.subList(0,inOrderpos); rightInorderList = inorderList.subList(inOrderpos+1,inorderList.size()); int leftInorderSize = leftInorderList.size(); leftPreorderList = preorderList.subList(1,leftInorderSize+1); rightPreorderList = preorderList.subList(leftInorderSize+1,preorderList.size()); //todo 重点 root.left = helper(leftPreorderList,leftInorderList); root.right = helper(rightPreorderList,rightInorderList); &#125; return root; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L104_TreeMaxDepth]]></title>
    <url>%2F2016%2F01%2F26%2FL104-TreeMaxDepth%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132package LeetCode.topic.tree;/** * 给定一个二叉树，找出其最大深度。 * * 二叉树的深度为根节点到最远叶子节点的最长路径上的节点数。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例： * 给定二叉树 [3,9,20,null,null,15,7]， * * 3 * / \ * 9 20 * / \ * 15 7 * 返回它的最大深度 3 。 */public class L104_TreeMaxDepth &#123; public int maxDepth(TreeNode root) &#123; if (root == null) return 0; return 1 + Math.max(maxDepth(root.left),maxDepth(root.right)); &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L103_ZigzagLevelOrder]]></title>
    <url>%2F2016%2F01%2F26%2FL103-ZigzagLevelOrder%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package LeetCode.topic.tree;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;/** * 给定一个二叉树，返回其节点值的锯齿形层次遍历。（即先从左往右，再从右往左进行下一层遍历，以此类推，层与层之间交替进行）。 * * 例如： * 给定二叉树 [3,9,20,null,null,15,7], * * 3 * / \ * 9 20 * / \ * 15 7 * 返回锯齿形层次遍历如下： * * [ * [3], * [20,9], * [15,7] * ] */public class L103_ZigzagLevelOrder &#123; public List&lt;List&lt;Integer&gt;&gt; zigzagLevelOrder(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); LinkedList&lt;TreeNode&gt; quene = new LinkedList&lt;&gt;(); quene.add(root); int depth = 0; while (!quene.isEmpty())&#123; int count = quene.size(); List&lt;Integer&gt; innerList = new ArrayList&lt;&gt;(); while (count &gt; 0)&#123; TreeNode cur = quene.poll(); if (depth % 2 == 1)&#123;//todo 判断奇偶层，从0开始计数 innerList.add(0,cur.val);//todo 如果是奇数层，就倒排 &#125; else &#123; innerList.add(cur.val); &#125; if (cur.left != null)&#123; quene.add(cur.left); &#125; if (cur.right != null)&#123; quene.add(cur.right);//.add 和 .push方法不同 &#125; count--; &#125; depth++; res.add(innerList); &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L102_LevelorderTraversal]]></title>
    <url>%2F2016%2F01%2F26%2FL102-LevelorderTraversal%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package LeetCode.topic.tree;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;import java.util.Queue;/** * 给定一个二叉树，返回其按层次遍历的节点值。 （即逐层地，从左到右访问所有节点）。 * * 例如: * 给定二叉树: [3,9,20,null,null,15,7], * * 3 * / \ * 9 20 * / \ * 15 7 * 返回其层次遍历结果： * * [ * [3], * [9,20], * [15,7] * ] */public class L102_LevelorderTraversal &#123; public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;();// if (root == null) return new ArrayList&lt;&gt;(); LinkedList&lt;TreeNode&gt; quene = new LinkedList&lt;&gt;(); quene.add(root); while (!quene.isEmpty())&#123; int count = quene.size();// List&lt;Integer&gt; innerList = new ArrayList&lt;&gt;();// while (count &gt; 0)&#123;// TreeNode cur = quene.pop(); innerList.add(cur.val);// System.out.println(cur.val); if (cur.left != null)&#123; quene.add(cur.left); &#125; if (cur.right != null)&#123; quene.add(cur.right); &#125; count--; &#125; res.add(innerList); &#125; return res; &#125;// public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123;//// LinkedList&lt;TreeNode&gt; quene = new LinkedList&lt;&gt;();// quene.add(root);//// while (!quene.isEmpty())&#123;// TreeNode cur = quene.pop();// System.out.println(cur.val);// if (cur.left != null)&#123;// quene.add(cur.left);// &#125;// if (cur.right != null)&#123;// quene.add(cur.right);// &#125;// &#125;// return null;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L101_IsSymmetric]]></title>
    <url>%2F2016%2F01%2F26%2FL101-IsSymmetric%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445package LeetCode.topic.tree;/** * 给定一个二叉树，检查它是否是镜像对称的。 * * 例如，二叉树 [1,2,2,3,4,4,3] 是对称的。 * * 1 * / \ * 2 2 * / \ / \ * 3 4 4 3 * 但是下面这个 [1,2,2,null,3,null,3] 则不是镜像对称的: * * 1 * / \ * 2 2 * \ \ * 3 3 * 说明: * * 如果你可以运用递归和迭代两种方法解决这个问题，会很加分。 */public class L101_IsSymmetric &#123; public boolean isSymmetric(TreeNode root) &#123; if (root == null)return true; return helper(root.left,root.right); &#125; private boolean helper(TreeNode leftNode, TreeNode rightNode) &#123; if (leftNode == null &amp;&amp; rightNode == null) return true; if ((leftNode != null &amp;&amp; rightNode != null &amp;&amp; leftNode.val == rightNode.val))&#123; return helper(leftNode.left,rightNode.right) &amp;&amp; helper(leftNode.right,rightNode.left); &#125; else &#123; return false; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L100_IsSameTree]]></title>
    <url>%2F2016%2F01%2F26%2FL100-IsSameTree%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package LeetCode.topic.tree;/** * 给定两个二叉树，编写一个函数来检验它们是否相同。 * * 如果两个树在结构上相同，并且节点具有相同的值，则认为它们是相同的。 * * 示例 1: * * 输入: 1 1 * / \ / \ * 2 3 2 3 * * [1,2,3], [1,2,3] * * 输出: true * 示例 2: * * 输入: 1 1 * / \ * 2 2 * * [1,2], [1,null,2] * * 输出: false * 示例 3: * * 输入: 1 1 * / \ / \ * 2 1 1 2 * * [1,2,1], [1,1,2] * * 输出: false */public class L100_IsSameTree &#123; public boolean isSameTree(TreeNode p, TreeNode q) &#123; if (p == null &amp;&amp; q == null)&#123; return true; &#125; else &#123; if (p == null || q == null)&#123; return false; &#125; else &#123; if (p.val == q.val)&#123; return isSameTree(p.left,q.left) &amp;&amp; isSameTree(p.right,q.right); &#125; else &#123; return false; &#125; &#125; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L99_RecoverTree]]></title>
    <url>%2F2016%2F01%2F26%2FL99-RecoverTree%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112package LeetCode.topic.tree;import java.util.Stack;/** * 二叉搜索树中的两个节点被错误地交换。 * * 请在不改变其结构的情况下，恢复这棵树。 * * 示例 1: * * 输入: [1,3,null,null,2] * * 1 * / * 3 * \ * 2 * * 输出: [3,1,null,null,2] * * 3 * / * 1 * \ * 2 * 示例 2: * * 输入: [3,1,4,null,null,2] * * 3 * / \ * 1 4 * / * 2 * * 输出: [2,1,4,null,null,3] * * 2 * / \ * 1 4 * / * 3 * 进阶: * * 使用 O(n) 空间复杂度的解法很容易实现。 * 你能想出一个只使用常数空间的解决方案吗？ * * 个人思路： * 中序遍历，是递增的，找到不一致的，然后进行交换 */public class L99_RecoverTree &#123; TreeNode first = null; TreeNode second = null; TreeNode prev = null; public void recoverTree(TreeNode root) &#123; if (root == null) return; helper(root); int tmp = first.val; first.val = second.val; second.val = tmp; &#125; private void helper(TreeNode root) &#123; if (root == null) return; helper(root.left); if (prev != null &amp;&amp; prev.val &gt;= root.val)&#123; if (first == null) first = prev; second = root;// &#125; prev =root;//如果正常继续轮转 helper(root.right); &#125;// public void recoverTree(TreeNode root) &#123;// if (root == null) return;// TreeNode first = null;// TreeNode second = null;// TreeNode prev = null;//// TreeNode cur = root;// Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();// while (!stack.isEmpty() || cur != null)&#123;// if (cur != null)&#123;// stack.push(cur);// cur = cur.left;// &#125; else &#123;// cur = stack.pop();// if (prev != null &amp;&amp; prev.val &gt;= cur.val)&#123;// if (first == null) first = prev;// second = cur;//// &#125;// prev = cur;// cur = cur.right;// &#125;// &#125;// int tmp = first.val;// first.val = second.val;// second.val = tmp;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L98_IsValidBST]]></title>
    <url>%2F2016%2F01%2F26%2FL98-IsValidBST%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package LeetCode.topic.tree;/** * 给定一个二叉树，判断其是否是一个有效的二叉搜索树。 * * 假设一个二叉搜索树具有如下特征： * * 节点的左子树只包含小于当前节点的数。 * 节点的右子树只包含大于当前节点的数。 * 所有左子树和右子树自身必须也是二叉搜索树。 * 示例 1: * * 输入: * 2 * / \ * 1 3 * 输出: true * 示例 2: * * 输入: * 5 * / \ * 1 4 * / \ * 3 6 * 输出: false * 解释: 输入为: [5,1,4,null,null,3,6]。 * 根节点的值为 5 ，但是其右子节点值为 4 。 * */public class L98_IsValidBST &#123; public boolean isValidBST(TreeNode root) &#123; if (root == null) return true; return helper(root,null,null); &#125; private boolean helper(TreeNode root, Integer min, Integer max) &#123; if (root == null) return true; if (min != null &amp;&amp; root.val &lt;= min) return false; if (max != null &amp;&amp; root.val &gt;= max) return false; return helper(root.left,min,root.val) &amp;&amp; helper(root.right,root.val,max); &#125;// double min = -Double.MAX_VALUE;// public boolean isValidBST(TreeNode root) &#123;// if (root == null) return true;// if (isValidBST(root.left))&#123;// if (root.val &gt; min)&#123;// min = root.val;// return isValidBST(root.right);// &#125;// &#125;// return false;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L96_NumTrees*]]></title>
    <url>%2F2016%2F01%2F26%2FL96-NumTrees%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839package LeetCode.topic.tree;/** * 给定一个整数 n，求以 1 ... n 为节点组成的二叉搜索树有多少种？ * * 示例: * * 输入: 3 * 输出: 5 * 解释: * 给定 n = 3, 一共有 5 种不同结构的二叉搜索树: * * 1 3 3 2 1 * \ / / / \ \ * 3 2 1 1 3 2 * / / \ \ * 2 1 2 3 * * n = 3 * root: 1 left:0 right:2 f(0)*f(2) * root: 2 left:1 right:1 f(1)*f(1) * root: 3 left:2 right:0 f(2)*f(0) * * f(n) = f(0)*f(n-1)+f(1)*(n-2)+...+ f(n-2)*f(1) + f(n-1)*f(0) * time:O(n) * space:O(n) */public class L96_NumTrees &#123; public int numTrees(int n) &#123; int[] res = new int[n + 1]; res[0] = 1; for (int i = 1; i &lt;= n; i++) &#123;//从1开始 for (int j = 0; j &lt; i; j++) &#123;//从0开始 res[i] += res[j] * res[i -j - 1];//res[j]是左子树 i是总个数，1是根节点，所以i - 1 -j是右子树 &#125; &#125; return res[n]; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L95_GenerateTrees*]]></title>
    <url>%2F2016%2F01%2F26%2FL95-GenerateTrees%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;/** * 给定一个整数 n，生成所有由 1 ... n 为节点所组成的二叉搜索树。 * * 示例: * * 输入: 3 * 输出: * [ * [1,null,3,2], * [3,2,null,1], * [3,1,null,null,2], * [2,1,3], * [1,null,2,null,3] * ] * 解释: * 以上的输出对应以下 5 种不同结构的二叉搜索树： * * 1 3 3 2 1 * \ / / / \ \ * 3 2 1 1 3 2 * / / \ \ * 2 1 2 3 */public class L95_GenerateTrees &#123; public List&lt;TreeNode&gt; generateTrees(int n) &#123; if (n ==0) return new ArrayList&lt;&gt;(); return generateTrees(1,n); &#125; private List&lt;TreeNode&gt; generateTrees(int start, int end) &#123; List&lt;TreeNode&gt; res = new ArrayList&lt;&gt;(); if (start &gt; end)&#123; res.add(null); return res; &#125; // 每一个i作为根 // start～～i-1为左子树 // i+1～～end为右子树 for (int i = start;i &lt;= end;i++)&#123; List&lt;TreeNode&gt; subLeftTree = generateTrees(start,i - 1);//todo i-1 List&lt;TreeNode&gt; subRightTree = generateTrees(i + 1,end);//todo i+1 for (TreeNode left : subLeftTree)&#123; for (TreeNode right: subRightTree) &#123; TreeNode node = new TreeNode(i);//todo i node.left = left; node.right = right; res.add(node); &#125; &#125; &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L94_InorderTraversal]]></title>
    <url>%2F2016%2F01%2F26%2FL94-InorderTraversal%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;import java.util.Stack;/** * 给定一个二叉树，返回它的中序 遍历。 * * 示例: * * 输入: [1,null,2,3] * 1 * \ * 2 * / * 3 * * 输出: [1,3,2] */public class L94_InorderTraversal &#123; public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode cur = root; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); while (cur != null || !stack.isEmpty())&#123;//变形 while (cur != null)&#123; stack.push(cur); cur = cur.left; &#125;// if (stack.isEmpty())&#123;// break;// &#125; cur = stack.pop(); list.add(cur.val); cur = cur.right; &#125; return list; &#125;// public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123;// if (root == null) return new ArrayList&lt;&gt;();// Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();// TreeNode cur = root;// List&lt;Integer&gt; list = new ArrayList&lt;&gt;();// while (true)&#123;// while (cur != null)&#123;// stack.push(cur);// cur = cur.left;// &#125;// if (stack.isEmpty())&#123;// break;// &#125;// cur = stack.pop();// list.add(cur.val);// cur = cur.right;// &#125;// return list;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GroupComparator原理]]></title>
    <url>%2F2016%2F01%2F25%2Fmr-GroupComparator%2F</url>
    <content type="text"><![CDATA[分析最近看dadoop中关于辅助排序（SecondarySort）的实现，说到了三个东西要设置：1. partioner；2. Key Comparator；3. Group Comparator。前两个都比较容易理解，但是关于group的概念我一直理解不了：一，有了partioner，所有的key已经放到一个分区了，每个分区对应一个reducer，而且key也可以排序了，那么不是实现了整个数据集的全排序了吗？第二，mapper产生的中间结果经过shuffle和sort后，每个key整合成一个记录(集合)，每次reduce方法调用处理这个记录(集合)，但是group的目的是让一次reduce调用处理多条记录(将该集合进行内部分组)，这不是矛盾吗，找了好久一直都没找到这个问题的清晰解释。 后来找到一本书，《Pro Hadoop》，里面有一部分内容详细解释了这个问题，看后终于明白了，和大家分享一下。reduce方法每次是读一条记录(集合)，读到相应的key，但是处理value集合时，处理完当前记录的value后，还会判断下一条记录是不是和当前的key是不是同一个组，如果是的话，会继续读取这些记录的值，而这个记录也会被认为已经处理了，直到记录不是当前组，这次reduce调用才结束，这样一次reduce调用就会处理掉一个组中的所有记录，而不仅仅是一条完整的记录(集合)了。 这个有什么用呢？如果不用分组，那么同一组的记录就要在多次reduce方法中独立处理(所有的数据都在同一组中)，那么有些状态数据就要传递了，就会增加复杂度，在一次调用中处理的话，这些状态只要用方法内的变量就可以的。比如查找最大值，只要读第一个值就可以了。 参考：https://blog.csdn.net/qq_20641565/article/details/52770872 源码分析目标：弄明白，我们配置的GroupComparator是如何对进入reduce函数中的key Iterable 进行影响。如下是一个配置了GroupComparator 的reduce 函数。具体影响是我们可以在自定义的GroupComparator 中确定哪儿些value组成一组，进入一个reduce函数 123456789101112131415161718192021public static class DividendGrowthReducer extends Reducer&lt;Stock, DoubleWritable, NullWritable, DividendChange&gt; &#123; private NullWritable outputKey = NullWritable.get(); private DividendChange outputValue = new DividendChange(); @Override protected void reduce(Stock key, Iterable&lt;DoubleWritable&gt; values, Context context) throws IOException, InterruptedException &#123; double previousDividend = 0.0; for(DoubleWritable dividend : values) &#123; double currentDividend = dividend.get(); double growth = currentDividend - previousDividend; if(Math.abs(growth) &gt; 0.000001) &#123; outputValue.setSymbol(key.getSymbol()); outputValue.setDate(key.getDate()); outputValue.setChange(growth); context.write(outputKey, outputValue); previousDividend = currentDividend; &#125; &#125; &#125; &#125; 着先我们找到向上找，是谁调用了我们写的这个reduce函数。 Reducer类的run 方法。通过如下代码，可以看到是在run方法中，对于每个key，调用一次reduce函数。此处传入reduce函数的都是对象引用。1234567891011121314/** * Advanced application writers can use the * &#123;@link #run(org.apache.hadoop.mapreduce.Reducer.Context)&#125; method to * control how the reduce task works. */ public void run(Context context) throws IOException, InterruptedException &#123; ..... while (context.nextKey()) &#123; reduce(context.getCurrentKey(), context.getValues(), context); ..... &#125; ..... &#125;&#125; 结合我们写的reduce函数，key是在遍历value的时候会对应变化。那我们继续跟踪context.getValues 得到的迭代器的next方法。context 此处是ReduceContext.java （接口）. 对应的实现类为ReduceContextImpl.java123456789101112131415161718protected class ValueIterable implements Iterable&lt;VALUEIN&gt; &#123; private ValueIterator iterator = new ValueIterator(); @Override public Iterator&lt;VALUEIN&gt; iterator() &#123; return iterator; &#125; &#125; /** * Iterate through the values for the current key, reusing the same value * object, which is stored in the context. * @return the series of values associated with the current key. All of the * objects returned directly and indirectly from this method are reused. */ public Iterable&lt;VALUEIN&gt; getValues() throws IOException, InterruptedException &#123; return iterable; &#125; 直接返回了一个iterable。继续跟踪ValueIterable 类型的iterable。那明白了，在reduce 函数中进行Iterable的遍历，其实调用的是ValueIterable的next方法。下面看一下next的实现。 1234567@Override public VALUEIN next() &#123; ……………… nextKeyValue(); return value; ……………… &#125; 再继续跟踪nextKeyValue()方法。终于找了一个comparator。 这个就是我们配置的GroupingComparator.1234567891011121314151617@Overridepublic boolean nextKeyValue() throws IOException, InterruptedException &#123; …………………………………… if (hasMore) &#123; nextKey = input.getKey(); nextKeyIsSame = comparator.compare(currentRawKey.getBytes(), 0, currentRawKey.getLength(), nextKey.getData(), nextKey.getPosition(), nextKey.getLength() - nextKey.getPosition() ) == 0; &#125; else &#123; nextKeyIsSame = false; &#125; inputValueCounter.increment(1); return true;&#125; 为了证明这个就是我们配置的GroupingComparator。 跟踪ReduceContextImpl的构造调用者。 ReduceTask的run方法。12345678@Override @SuppressWarnings(&quot;unchecked&quot;) public void run(JobConf job, final TaskUmbilicalProtocol umbilical)&#123; ……………………………… RawComparator comparator = job.getOutputValueGroupingComparator(); runNewReducer(job, umbilical, reporter, rIter, comparator, keyClass, valueClass); &#125; 下面把runNewReducer 的代码也贴出来。1234567891011121314151617void runNewReducer(JobConf job, final TaskUmbilicalProtocol umbilical, final TaskReporter reporter, RawKeyValueIterator rIter, RawComparator&lt;INKEY&gt; comparator, Class&lt;INKEY&gt; keyClass, Class&lt;INVALUE&gt; valueClass ) &#123; org.apache.hadoop.mapreduce.Reducer.Context reducerContext = createReduceContext(reducer, job, getTaskID(), rIter, reduceInputKeyCounter, reduceInputValueCounter, trackedRW, committer, reporter, comparator, keyClass, valueClass); 好吧，关于自定义GroupingComparator如何起做用的代码分析，就到此吧。参考：http://blog.itpub.net/30066956/viewspace-2095520/]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L1_twosum]]></title>
    <url>%2F2016%2F01%2F23%2Ftwosum%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829/** * 给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。 * * 你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。 * * 示例: * * 给定 nums = [2, 7, 11, 15], target = 9 * * 因为 nums[0] + nums[1] = 2 + 7 = 9 * 所以返回 [0, 1] */public class L1_twoSum &#123; public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); int[] rst = new int[2]; for (int i = 0; i &lt; nums.length; i++) &#123; int diff = target - nums[i]; if (map.containsKey(diff))&#123; rst[0] = map.get(diff); rst[1] = i; break; &#125; map.put(nums[i],i); &#125; return rst; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[equal和hashcode]]></title>
    <url>%2F2015%2F02%2F15%2Fequal%E5%92%8Chashcode%2F</url>
    <content type="text"><![CDATA[1、 为什么要重载equal方法？答案：因为Object的equal方法默认是两个对象的引用的比较，意思就是指向同一内存地址则相等，否则不相等；如果你现在需要利用对象里面的值来判断是否相等，则重载equal方法。 2、 为什么重载hashCode方法？答案：一般的地方不需要重载hashCode，只有当类需要放在HashTable、HashMap、HashSet等等hash结构的集合时才会 重载hashCode，那么为什么要重载hashCode呢？就HashMap来说，好比HashMap就是一个大内存块，里面有很多小内存块，小内存块 里面是一系列的对象，可以利用hashCode来查找小内存块hashCode%size(小内存块数量-即容量)，所以当equal相等时，hashCode必须相等，而且如果是object对象，必须重载hashCode和equal方法。 3、 为什么equals()相等，hashCode就一定要相等，而hashCode相等，却不要求equals相等?答案：1、因为是按照hashCode来访问小内存块，所以hashCode必须相等。2、HashMap获取一个对象是比较key的hashCode相等和equal为true。之所以hashCode相等，却可以equal不等，就比如ObjectA和ObjectB他们都有属性name，那么hashCode都以name计算，所以hashCode一样，但是两个对象属于不同类型，所以equal为false。 4、 为什么需要hashCode?1、 通过hashCode可以很快的查到小内存块。2、 通过hashCode比较比equal方法快，当get时先比较hashCode，如果hashCode不同，直接返回false。 ==比较符只会比较地址,如果地址不同就返回falsejava中任何类都可以重写equals()方法来实现自己的比较方式,String类重写了equals()方法. String类的equals()方法不仅会比较两个对象的地址,还会比较他们的字符串的内容.如果被比较的两个引用指向不同的地址,但是两个地址中的字符串的内容是相同的String的equals()方法仍然会返回true. 哈希码(HashCode)哈希码产生的依据：哈希码并不是完全唯一的，它是一种算法，让同一个类的对象按照自己不同的特征尽量的有不同的哈希码，但不表示不同的对象哈希码完全不同。也有相同的情况，看程序员如何写哈希码的算法。 什么是哈希码(HashCode)在Java中，哈希码代表对象的特征。例如对象String str1 = “aa”, str1.hashCode= 3104String str2 = “bb”, str2.hashCode= 3106String str3 = “aa”, str3.hashCode= 3104根据HashCode由此可得出str1!=str2,str1==str3下面给出几个常用的哈希码的算法。1：Object类的hashCode.返回对象的内存地址经过处理后的结构，由于每个对象的内存地址都不一样，所以哈希码也不一样。2：String类的hashCode.根据String类包含的字符串的内容，根据一种特殊算法返回哈希码，只要字符串所在的堆空间相同，返回的哈希码也相同。3：Integer类，返回的哈希码就是Integer对象里所包含的那个整数的数值，例如Integer i1=new Integer(100),i1.hashCode的值就是100 。由此可见，2个一样大小的Integer对象，返回的哈希码也一样。 案例分析https://www.cnblogs.com/keyi/p/7119825.html 总结1、equals方法用于比较对象的内容是否相等（覆盖以后）2、hashcode方法只有在集合中用到3、当覆盖了equals方法时，比较对象是否相等将通过覆盖后的equals方法进行比较（判断对象的内容是否相等）。4、将对象放入到集合中时，首先判断要放入对象的hashcode值与集合中的任意一个元素的hashcode值是否相等，如果不相等直接将该对象放入集合中。如果hashcode值相等，然后再通过equals方法判断要放入对象与集合中的任意一个对象是否相等，如果equals判断不相等，直接将该元素放入到集合中，否则不放入。5、将元素放入集合的流程图：]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二进制与16进制]]></title>
    <url>%2F2015%2F02%2F15%2F%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%8E16%E8%BF%9B%E5%88%B6%2F</url>
    <content type="text"><![CDATA[首先呢，先要看看十六位数的表示方法 再来掌握二进制数与十六进制数之间的对应关系表 二进制转换成十六进制的方法是，取四合一法，即从二进制的小数点为分界点，向左（或向右）每四位取成一位举例:组分好以后，对照二进制与十六进制数的对应表（如图2中所示），将四位二进制按权相加，得到的数就是一位十六进制数，然后按顺序排列，小数点的位置不变哦，最后得到的就是十六进制数哦]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hashmap]]></title>
    <url>%2F2015%2F02%2F14%2FHashmap%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[哈希(hash)Hash，一般翻译做“散列”，也有直接音译为“哈希”的，就是把任意长度的输入，通过散列算法，变换成固定长度的输出，该输出就是散列值。这种转换是一种压缩映射，也就是，散列值的空间通常远小于输入的空间，不同的输入可能会散列成相同的输出，所以不可能从散列值来唯一的确定输入值。简单的说就是一种将任意长度的消息压缩到某一固定长度的消息摘要的函数。所有散列函数都有如下一个基本特性：根据同一散列函数计算出的散列值如果不同，那么输入值肯定也不同。但是，根据同一散列函数计算出的散列值如果相同，输入值不一定相同。两个不同的输入值，根据同一散列函数计算出的散列值相同的现象叫做碰撞。常见的Hash函数有以下几个：1234567891011121314151617直接定址法：直接以关键字k或者k加上某个常数（k+c）作为哈希地址。数字分析法：提取关键字中取值比较均匀的数字作为哈希地址。除留余数法：用关键字k除以某个不大于哈希表长度m的数p，将所得余数作为哈希表地址。分段叠加法：按照哈希表地址位数将关键字分成位数相等的几部分，其中最后一部分可以比较短。然后将这几部分相加，舍弃最高进位后的结果就是该关键字的哈希地址。平方取中法：如果关键字各个部分分布都不均匀的话，可以先求出它的平方值，然后按照需求取中间的几位作为哈希地址。伪随机数法：采用一个伪随机数当作哈希函数。 上面介绍过碰撞。衡量一个哈希函数的好坏的重要指标就是发生碰撞的概率以及发生碰撞的解决方案。任何哈希函数基本都无法彻底避免碰撞，常见的解决碰撞的方法有以下几种：1234567891011开放定址法：开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。链地址法将哈希表的每个单元作为链表的头结点，所有哈希地址为i的元素构成一个同义词链表。即发生冲突时就把该关键字链在以该单元为头结点的链表的尾部。再哈希法当哈希地址发生冲突用其他的函数计算另一个哈希函数地址，直到冲突不再产生为止。建立公共溢出区将哈希表分为基本表和溢出表两部分，发生冲突的元素都放入溢出表中。 HashMapHashMap是Array和Linkedlist的结合体 我们可以从上图看到，左边很明显是个数组，数组的每个成员是一个链表。该数据结构所容纳的所有元素均包含一个指针，用于元素间的链接。我们根据元素的自身特征把元素分配到不同的链表中去，反过来我们也正是通过这些特征找到正确的链表，再从链表中找出正确的元素。其中，根据元素特征计算元素数组下标的方法就是哈希算法，即本文的主角hash()函数（当然，还包括indexOf()函数）。 源码解析首先，在同一个版本的Jdk中，HashMap、HashTable以及ConcurrentHashMap里面的hash方法的实现是不同的。再不同的版本的JDK中（Java7 和 Java8）中也是有区别的。我会尽量全部介绍到。相信，看文这篇文章，你会彻底理解hash方法。 在上代码之前，我们先来做个简单分析。我们知道，hash方法的功能是根据Key来定位这个K-V在链表数组中的位置的。也就是hash方法的输入应该是个Object类型的Key，输出应该是个int类型的数组下标。如果让你设计这个方法，你会怎么做？ 其实简单，我们只要调用Object对象的hashCode()方法，该方法会返回一个整数，然后用这个数对HashMap或者HashTable的容量进行取模就行了。没错，其实基本原理就是这个，只不过，在具体实现上，由两个方法int hash(Object k)和int indexFor(int h, int length)来实现。但是考虑到效率等问题，HashMap的实现会稍微复杂一点。 12hash ：该方法主要是将Object转换成一个整型。indexFor ：该方法主要是将hash生成的整型转换成链表数组中的下标。 HashMap In Java 71234567891011121314final int hash(Object k) &#123; int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; indexFor方法前面我说过，indexFor方法其实主要是将hash生成的整型转换成链表数组中的下标。那么return h &amp; (length-1);是什么意思呢？其实，他就是取模。Java之所有使用位运算(&amp;)来代替取模运算(%)，最主要的考虑就是效率。位运算(&amp;)效率要比代替取模运算(%)高很多，主要原因是位运算直接对内存数据进行操作，不需要转成十进制，因此处理速度非常快。 那么，为什么可以使用位运算(&amp;)来实现取模运算(%)呢？这实现的原理如下：123456789X % 2^n = X &amp; (2^n – 1)2^n表示2的n次方，也就是说，一个数对2^n取模 == 一个数和(2^n – 1)做按位与运算 。假设n为3，则2^3 = 8，表示成2进制就是1000。2^3 -1 = 7 ，即0111。此时X &amp; (2^3 – 1) 就相当于取X的2进制的最后三位数。从2进制角度来看，X / 8相当于 X &gt;&gt; 3，即把X右移3位，此时得到了X / 8的商，而被移掉的部分(后三位)，则是X % 8，也就是余数。 上面的解释不知道你有没有看懂，没看懂的话其实也没关系，你只需要记住这个技巧就可以了。或者你可以找几个例子试一下。126 % 8 = 6 ，6 &amp; 7 = 610 &amp; 8 = 2 ，10 &amp; 7 = 2 所以，return h &amp; (length-1);只要保证length的长度是2^n的话，就可以实现取模运算了。而HashMap中的length也确实是2的倍数，初始值是16，之后每次扩充为原来的2倍。扩容因子是0.75。 为了推断HashMap的默认长度为什么是1612长度16或者其他2的幂,length - 1的值是所有二进制位全为1,这种情况下,index的结果等同于hashcode后几位的值只要输入的hashcode本身分布均匀,hash算法的结果就是均匀的 所以,HashMap的默认长度为16,是为了降低hash碰撞的几率https://blog.csdn.net/zjcjava/article/details/78495416 分析完indexFor方法后，我们接下来准备分析hash方法的具体原理和实现。在深入分析之前，至此，先做个总结。 HashMap的数据是存储在链表数组里面的。在对HashMap进行插入/删除等操作时，都需要根据K-V对的键值定位到他应该保存在数组的哪个下标中。而这个通过键值求取下标的操作就叫做哈希。HashMap的数组是有长度的，Java中规定这个长度只能是2的倍数，初始值为16。简单的做法是先求取出键值的hashcode，然后在将hashcode得到的int值对数组长度进行取模。为了考虑性能，Java总采用按位与操作实现取模操作。 hash方法接下来我们会发现，无论是用取模运算还是位运算都无法直接解决冲突较大的问题。比如：CA11 0000和0001 0000在对0000 1111进行按位与运算后的值是相等的。 两个不同的键值，在对数组长度进行按位与运算后得到的结果相同，这不就发生了冲突吗。那么如何解决这种冲突呢，来看下Java是如何做的。 其中的主要代码部分如下：1234h ^= k.hashCode();h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12);return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); 这段代码是为了对key的hashCode进行扰动计算，防止不同hashCode的高位不同但低位相同导致的hash冲突。简单点说，就是为了把高位的特征和低位的特征组合起来，降低哈希冲突的概率，也就是说，尽量做到任何一位的变化都能对最终得到的结果产生影响。 举个例子来说，我们现在想向一个HashMap中put一个K-V对，Key的值为“hollischuang”，经过简单的获取hashcode后，得到的值为“1011000110101110011111010011011”，如果当前HashTable的大小为16，即在不进行扰动计算的情况下，他最终得到的index结果值为11。由于15的二进制扩展到32位为“00000000000000000000000000001111”，所以，一个数字在和他进行按位与操作的时候，前28位无论是什么，计算结果都一样（因为0和任何数做与，结果都为0）。如下图所示。 可以看到，后面的两个hashcode经过位运算之后得到的值也是11 ，虽然我们不知道哪个key的hashcode是上面例子中的那两个，但是肯定存在这样的key，这就产生了冲突。 那么，接下来，我看看一下经过扰动的算法最终的计算结果会如何。 从上面图中可以看到，之前会产生冲突的两个hashcode，经过扰动计算之后，最终得到的index的值不一样了，这就很好的避免了冲突。 其实，使用位运算代替取模运算，除了性能之外，还有一个好处就是可以很好的解决负数的问题。1因为我们知道，hashcode的结果是int类型，而int的取值范围是-2^31 ~ 2^31 – 1，即[ -2147483648, 2147483647]；这里面是包含负数的，我们知道，对于一个负数取模还是有些麻烦的。如果使用二进制的位运算的话就可以很好的避免这个问题。首先，不管hashcode的值是正数还是负数。length-1这个值一定是个正数。那么，他的二进制的第一位一定是0（有符号数用最高位作为符号位，“0”代表“+”，“1”代表“-”），这样里两个数做按位与运算之后，第一位一定是个0，也就是，得到的结果一定是个正数。 HashTable In Java 7上面是Java 7中HashMap的hash方法以及indexOf方法的实现，那么接下来我们要看下，线程安全的HashTable是如何实现的，和HashMap有何不同，并试着分析下不同的原因。以下是Java 7中HashTable的hash方法的实现。1234private int hash(Object k) &#123; // hashSeed will be zero if alternative hashing is disabled. return hashSeed ^ k.hashCode();&#125; 我们可以发现，很简单，(1)相当于只是对k做了个简单的hash，取了一下其hashCode。(1)而HashTable中也没有indexOf方法，取而代之的是这段代码：int index = (hash &amp; 0x7FFFFFFF) % tab.length;。也就是说，HashMap和HashTable对于计算数组下标这件事，采用了两种方法。HashMap采用的是位运算，而HashTable采用的是直接取模。12为啥要把hash值和0x7FFFFFFF做一次按位与操作呢，主要是为了保证得到的index的第一位为0，也就是为了得到一个正数。因为有符号数第一位0代表正数，1代表负数。 0x7FFFFFFF 是long int的最大值 我们前面说过，HashMap之所以不用取模的原因是为了提高效率。有人认为，因为HashTable是个线程安全的类，本来就慢，所以Java并没有考虑效率问题，就直接使用取模算法了呢？但是其实并不完全是，Java这样设计还是有一定的考虑在的，虽然这样效率确实是会比HashMap慢一些。 其实，HashTable采用简单的取模是有一定的考虑在的。这就要涉及到HashTable的构造函数和扩容函数了。由于篇幅有限，这里就不贴代码了，直接给出结论：12345HashTable默认的初始大小为11，之后每次扩充为原来的2n+1。也就是说，HashTable的链表数组的默认大小是一个素数、奇数。之后的每次扩充结果也都是奇数。由于HashTable会尽量使用素数、奇数作为容量的大小。当哈希表的大小为素数时，简单的取模哈希的结果会更加均匀。（这个是可以证明出来的，由于不是本文重点，暂不详细介绍，可参考：http://zhaox.github.io/algorithm/2015/06/29/hash） 至此，我们看完了Java 7中HashMap和HashTable中对于hash的实现，我们来做个简单的总结。12345HashMap默认的初始化大小为16，之后每次扩充为原来的2倍。HashTable默认的初始大小为11，之后每次扩充为原来的2n+1。当哈希表的大小为素数时，简单的取模哈希的结果会更加均匀，所以单从这一点上看，HashTable的哈希表大小选择，似乎更高明些。因为hash结果越分散效果越好。在取模计算时，如果模数是2的幂，那么我们可以直接使用位运算来得到结果，效率要大大高于做除法。所以从hash计算的效率上，又是HashMap更胜一筹。但是，HashMap为了提高效率使用位运算代替哈希，这又引入了哈希分布不均匀的问题，所以HashMap为解决这问题，又对hash算法做了一些改进，进行了扰动计算。 ConcurrentHashMap In Java 71234567891011121314151617181920private int hash(Object k) &#123; int h = hashSeed; if ((0 != h) &amp;&amp; (k instanceof String)) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); // Spread bits to regularize both segment and index locations, // using variant of single-word Wang/Jenkins hash. h += (h &lt;&lt; 15) ^ 0xffffcd7d; h ^= (h &gt;&gt;&gt; 10); h += (h &lt;&lt; 3); h ^= (h &gt;&gt;&gt; 6); h += (h &lt;&lt; 2) + (h &lt;&lt; 14); return h ^ (h &gt;&gt;&gt; 16);&#125; int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; 上面这段关于ConcurrentHashMap的hash实现其实和HashMap如出一辙。都是通过位运算代替取模，然后再对hashcode进行扰动。区别在于，ConcurrentHashMap 使用了一种变种的Wang/Jenkins 哈希算法，其主要母的也是为了把高位和低位组合在一起，避免发生冲突。至于为啥不和HashMap采用同样的算法进行扰动，我猜这只是程序员自由意志的选择吧。至少我目前没有办法证明哪个更优。 HashMap In Java 8在Java 8 之前，HashMap和其他基于map的类都是通过链地址法解决冲突，它们使用单向链表来存储相同索引值的元素。在最坏的情况下，这种方式会将HashMap的get方法的性能从O(1)降低到O(n)。为了解决在频繁冲突时hashmap性能降低的问题，Java 8中使用平衡树来替代链表存储冲突的元素。这意味着我们可以将最坏情况下的性能从O(n)提高到O(logn)。关于HashMap在Java 8中的优化，我后面会有文章继续深入介绍。 如果恶意程序知道我们用的是Hash算法，则在纯链表情况下，它能够发送大量请求导致哈希碰撞，然后不停访问这些key导致HashMap忙于进行线性查找，最终陷入瘫痪，即形成了拒绝服务攻击（DoS）。 关于Java 8中的hash函数，原理和Java 7中基本类似。Java 8中这一步做了优化，只做一次16位右位移异或混合，而不是四次，但原理是不变的。1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的。以上方法得到的int的hash值，然后再通过h &amp; (table.length -1)来得到该对象在数据中保存的位置。 HashTable In Java 8在Java 8的HashTable中，已经不在有hash方法了。但是哈希的操作还是在的，比如在put方法中就有如下实现：12int hash = key.hashCode();int index = (hash &amp; 0x7FFFFFFF) % tab.length; 这其实和Java 7中的实现几乎无差别，就不做过多的介绍了 ConcurrentHashMap In Java 8Java 8 里面的求hash的方法从hash改为了spread。实现方式如下：123static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; Java 8的ConcurrentHashMap同样是通过Key的哈希值与数组长度取模确定该Key在数组中的索引。同样为了避免不太好的Key的hashCode设计，它通过如下方法计算得到Key的最终哈希值。不同的是，Java 8的ConcurrentHashMap作者认为引入红黑树后，即使哈希冲突比较严重，寻址效率也足够高，所以作者并未在哈希值的计算上做过多设计，只是将Key的hashCode值与其高16位作异或并保证最高位为0（从而保证最终结果为正整数）。 总结至此，我们已经分析完了HashMap、HashTable以及ConcurrentHashMap分别在Jdk 1.7 和 Jdk 1.8中的实现。我们可以发现，为了保证哈希的结果可以分散、为了提高哈希的效率，JDK在一个小小的hash方法上就有很多考虑，做了很多事情。当然，我希望我们不仅可以深入了解背后的原理，还要学会这种对代码精益求精的态度。 非线程安全问题解决方案HashMap为什么线程不安全1、在两个线程同时尝试扩容HashMap时，可能将一个链表形成环形的链表，所有的next都不为空，进入死循环2、在两个线程同时进行put时可能造成一个线程数据的丢失 我们可以使用CocurrentHashMap来代替HashTable吗？这是另外一个很热门的面试题，因为ConcurrentHashMap越来越多人用了。我们知道HashTable是synchronized的，但是ConcurrentHashMap同步性能更好，因为它仅仅根据同步级别对map的一部分进行上锁。ConcurrentHashMap当然可以代替HashTable，但是HashTable提供更强的线程安全性。 先来些简单的问题你用过HashMap吗？什么是HashMap？你为什么用到它？ 几乎每个人都会回答“是的”，然后回答HashMap的一些特性，譬如HashMap可以接受null键值和值，而HashTable则不能；HashMap是非synchronized;HashMap很快；以及HashMap储存的是键值对等等。这显示出你已经用过HashMap，而且对它相当的熟悉。但是面试官来个急转直下，从此刻开始问出一些刁钻的问题，关于HashMap的更多基础的细节。面试官可能会问出下面的问题： 你知道HashMap的工作原理吗？你知道HashMap的get()方法的工作原理吗？ 你也许会回答“我没有详查标准的Java API，你可以看看Java源代码或者Open JDK。”“我可以用Google找到答案。” 但一些面试者可能可以给出答案，“HashMap是基于hashing的原理，我们使用put(key, value)存储对象到HashMap中，使用get(key)从HashMap中获取对象。当我们给put()方法传递键和值时，我们先对键调用hashCode()方法，返回的hashCode用于找到bucket位置来储存Entry对象。”这里关键点在于指出，HashMap是在bucket中储存键对象和值对象，作为Map.Entry。这一点有助于理解获取对象的逻辑。如果你没有意识到这一点，或者错误的认为仅仅只在bucket中存储值的话，你将不会回答如何从HashMap中获取对象的逻辑。这个答案相当的正确，也显示出面试者确实知道hashing以及HashMap的工作原理。但是这仅仅是故事的开始，当面试官加入一些Java程序员每天要碰到的实际场景的时候，错误的答案频现。下个问题可能是关于HashMap中的碰撞探测(collision detection)以及碰撞的解决方法： 当两个对象的hashcode相同会发生什么？hashcode相同只会说明存在相同的bucket下。从这里开始，真正的困惑开始了，一些面试者会回答因为hashcode相同，所以两个对象是相等的，HashMap将会抛出异常，或者不会存储它们。然后面试官可能会提醒他们有equals()和hashCode()两个方法，并告诉他们两个对象就算hashcode相同，但是它们可能并不相等。一些面试者可能就此放弃，而另外一些还能继续挺进，他们回答“因为hashcode相同，所以它们的bucket位置相同，‘碰撞’会发生。因为HashMap使用LinkedList存储对象，这个Entry(包含有键值对的Map.Entry对象)会存储在LinkedList中 。”这个答案非常的合理，虽然有很多种处理碰撞的方法，这种方法是最简单的，也正是HashMap的处理方法。但故事还没有完结，面试官会继续问： 如果两个键的hashcode相同，你如何获取值对象？ 面试者会回答：当我们调用get()方法，HashMap会使用键对象的hashcode找到bucket位置，然后获取值对象。面试官提醒他如果有两个值对象储存在同一个bucket，他给出答案:将会遍历LinkedList直到找到值对象。面试官会问因为你并没有值对象去比较，你是如何确定确定找到值对象的？除非面试者直到HashMap在LinkedList中存储的是键值对，否则他们不可能回答出这一题 。 其中一些记得这个重要知识点的面试者会说，找到bucket位置之后，会调用keys.equals()方法去找到LinkedList中正确的节点，最终找到要找的值对象。完美的答案！ 许多情况下，面试者会在这个环节中出错，因为他们混淆了hashCode()和equals()方法。因为在此之前hashCode()屡屡出现，而equals()方法仅仅在获取值对象的时候才出现。一些优秀的开发者会指出使用不可变的、声明作final的对象，并且采用合适的equals()和hashCode()方法的话，将会减少碰撞的发生，提高效率。不可变性使得能够缓存不同键的hashcode，这将提高整个获取对象的速度，使用String，Interger这样的wrapper类作为键是非常好的选择。 如果你认为到这里已经完结了，那么听到下面这个问题的时候，你会大吃一惊。 如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？除非你真正知道HashMap的工作原理，否则你将回答不出这道题。默认的负载因子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其它集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程叫作rehashing，因为它调用hash方法找到新的bucket位置。 如果你能够回答这道问题，下面的问题来了： 你了解重新调整HashMap大小存在什么问题吗？你可能回答不上来，这时面试官会提醒你当多线程的情况下，可能产生条件竞争(race condition) 。 当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在LinkedList中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在LinkedList的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。这个时候，你可以质问面试官，为什么这么奇怪，要在多线程的环境下使用HashMap呢？ 热心的读者贡献了更多的关于HashMap的问题： 为什么String, Interger这样的wrapper类适合作为键？String, Interger这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的，也是final的，而且已经重写了equals()和hashCode()方法了。其他的wrapper类也有这个特点。不可变性是必要的，因为为了要计算hashCode()，就要防止键值改变，如果键值在放入时和获取时返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性还有其他的优点如线程安全。如果你可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法是非常重要的。如果两个不相等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能。 我们可以使用自定义的对象作为键吗？这是前一个问题的延伸。当然你可能使用任何对象作为键，只要它遵守了equals()和hashCode()方法的定义规则，并且当对象插入到Map中之后将不会再改变了。如果这个自定义对象时不可变的，那么它已经满足了作为键的条件，因为当它创建之后就已经不能改变了。我们可以使用CocurrentHashMap来代替HashTable吗？这是另外一个很热门的面试题，因为ConcurrentHashMap越来越多人用了。我们知道HashTable是synchronized的，但是ConcurrentHashMap同步性能更好，因为它仅仅根据同步级别对map的一部分进行上锁。ConcurrentHashMap当然可以代替HashTable，但是HashTable提供更强的线程安全性。 总结 HashMap的工作原理 HashMap基于hashing原理，我们通过put()和get()方法储存和获取对象。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，然后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用LinkedList来解决碰撞问题，当发生碰撞了，对象将会储存在LinkedList的下一个节点中。 HashMap在每个LinkedList节点中储存键值对对象。 当两个不同的键对象的hashcode相同时会发生什么？ 它们会储存在同一个bucket位置的LinkedList中。键对象的equals()方法用来找到键值对。 Jdk的源代码，每一行都很有意思，都值得花时间去钻研、推敲。参考:https://blog.csdn.net/skiof007/article/details/80253587]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
</search>
