<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[hive-set设置总结]]></title>
    <url>%2F2019%2F02%2F23%2Fhive-set%E8%AE%BE%E7%BD%AE%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[直接set命令可以看到所有变量值。set单个参数，可以看见这个参数的值。 问题 hive输出格式的配置项是哪个？ hive被各种语言调用如何配置？ hive提交作业是在hive中还是hadoop中？ 一个查询的最后一个map/reduce任务输出是否被压缩的标志，通过哪个配置项？ 当用户自定义了UDF或者SerDe，这些插件的jar都要放到这个目录下，通过那个配置项？ 每个reducer的大小，默认是1G，输入文件如果是10G，那么就会起10个reducer；通过那个配置项可以配置？ group by操作是否允许数据倾斜，通过那个配置项配置？ 本地模式时，map/reduce的内存使用量该如何配置？ 在做表join时缓存在内存中的行数，默认25000；通过那个配置项可以修改？ 是否开启数据倾斜的join优化，通过那个配置项可以优化？ 并行运算开启时，允许多少作业同时计算，默认是8；该如何修改这个配置项？ 常用hiveconfHive相关的配置属性总结1234set hive.cli.print.current.db=true; 在cli hive提示符后显示当前数据库。 set hive.cli.print.header=true; 显示表头。select时会显示对应字段。 set hive.mapred.mode=strict; 防止笛卡儿积的执行;如果对分区表查询，且没有在where中对分区字段进行限制，报错FAILED:SemanticException [Error 10041]: No partition predicate found for Alias &quot;test_part&quot; Table &quot;test_part&quot;；对应还有nonstrict模式（默认模式）。 hive.support.sql11.reserved.keywords该选项的目的是：是否启用对SQL2011保留关键字的支持。 启用后，将支持部分SQL2011保留关键字。 设置作业优先级mapred.job.priority123# 手动指定队列```set mapreduce.job.queuename=hive; 手动指定job namemapreduce.job.name1# 动态分区 set hive.exec.dynamic.partition=true;set hive.exec.dynamic.partition.mode=nonstrict;set hive.exec.max.dynamic.partitions=100;set hive.exec.max.dynamic.partitions.pernode=100;hive.exec.dynamic.partition 开启动态分区hive.exec.dynamic.partition.mode 设置可以动态分区；因为严格模式下，不允许所有的分区都被动态指定。（详细使用看上面“导出数据到表”章节）hive.exec.max.dynamic.partitions 默认是1000；在所有执行的MR节点上，一共可以创建最大动态分区数hive.exec.max.dynamic.partitions.pernode (上面参数也要加上)默认是100；在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。123动态分区参考：http://lxw1234.com/archives/2015/06/286.htm# 并发优化 set hive.exec.parallel=true;set hive.exec.parallel.thread.number=8;123456第一个参数：开启任务并行执行；第二个参数：同一个sql允许并行任务的最大线程数job之间没有前后依赖的都可以并行执行。# join/group by倾斜优化 set hive.map.aggr=true;set hive.groupby.skewindata=true;set hive.groupby.mapaggr.checkinterval=100000;set hive.optimize.skewjoin=true;set hive.skewjoin.key=100000;1234参数解释：hive数据倾斜# 解决小文件问题详细hive小文件合并 set hive.merge.mapfiles=true;set hive.merge.mapredfiles=true;set hive.merge.size.per.task=25610001000;set hive.merge.smallfiles.avgsize=16000000;1上面参数在 文件输出时合并。但是它们 和 压缩 并存时会失效，并对orc格式的表（orc本身就已经压缩）不起作用。 set hive.hadoop.supports.splittable.combineinputformat=true;set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;set mapred.max.split.size=2048000000;set mapred.min.split.size.per.node=2048000000;set mapred.min.split.size.per.rack=2048000000;12上面参数在 文件进入时合并文件，减少map个数。下面参数是减少mr最终输出文件个数 set hive.exec.reducers.bytes.per.reducer=5120000000;insert overwrite table test partition(dt)select * from iteblog_tmpDISTRIBUTE BY rand();DISTRIBUTE BY rand() 强制产生reduce，set hive.exec.reducers.bytes.per.reducer控制reduce个数（reduce处理数据数量），两者一起使用控制小文件输出。1# 内存 set mapreduce.map.memory.mb=2048;set mapred.child.map.java.opts=’-Xmx2048M’;set mapreduce.map.java.opts=’-Xmx2048M’;set mapreduce.reduce.memory.mb=2048;set mapred.child.reduce.java.opts=’-Xmx2048m’;set mapreduce.reduce.java.opts=’-Xmx2048M’;set yarn.app.mapreduce.am.resource.mb=3000;set yarn.app.mapreduce.am.command-opts=’-Xmx2048m’;set mapreduce.map.memory.mb container的内存 运行mapper的容器的物理内存，1024M = 1Gset mapreduce.map.java.opts jvm堆内存set yarn.app.mapreduce.am.resource.mb app内存。am指 Yarn中AppMaster，针对MapReduce计算框架就是MR AppMaster，通过配置这两个选项，可以设定MR AppMaster使用的内存。 一般看hadoop日志时可以看到map/reduce，但是当没有map/reduce时就开始报beyond memory limit类似的错时，说明是am的内存不够。123456789在yarn container这种模式下，map/reduce task是运行在Container之中的，所以上面提到的mapreduce.map(reduce).memory.mb大小都大于mapreduce.map(reduce).java.opts值的大小。mapreduce.&#123;map|reduce&#125;.java.opts能够通过Xmx设置JVM最大的heap的使用，一般设置为0.75倍的memory.mb，因为需要为java code等预留些空间。来源于网络：虚拟内存的计算由 物理内存 和 yarn-site.xml中的yarn.nodemanager.vmem-pmem-ratio制定。 yarn.nodemanager.vmem-pmem-ratio是 一个比例，默认是2.1 虚拟内存 = 物理内存 × 这个比例 yarn.nodemanager.vmem-pmem-ratio 的比率，默认是2.1.这个比率的控制影响着虚拟内存的使用，当yarn计算出来的虚拟内存，比在mapred-site.xml里的mapreduce.map.memory.mb或mapreduce.reduce.memory.mb的2.1倍还要多时，会被kill掉。参考：https://blog.csdn.net/yisun123456/article/details/81327372# 压缩 set hive.exec.compress.output=true;set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;set mapred.output.compression.type=BLOCK; set hive.exec.compress.intermediate=true;set mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;12345前三个参数是输出压缩； 最后两个参数是map输出压缩。 详细hive压缩 # reducer个数相关 set mapreduce.job.reduces=15; 指定reducer个数set hive.exec.reducers.bytes.per.reducer 每个reduce任务处理的数据量，默认为1000^3=1G1# 推测式执行配置项 set mapred.map.tasks.speculative.execution=true;set mapred.reduce.tasks.speculative.execution=true;123456这是两个推测式执行的配置项,默认是true 所谓的推测执行，就是当所有task都开始运行之后，Job Tracker会统计所有任务的平均进度，如果某个task所在的task node机器配 置比较低或者CPU load很高（原因很多），导致任务执行比总体任务的平均执行要慢，此时Job Tracker会启动一个新的任务 （duplicate task），原有任务和新任务哪个先执行完就把另外一个kill掉，这也是我们经常在Job Tracker页面看到任务执行成功，但是总有些任务被kill，就是这个原因。 # 关闭mapjoin SET hive.auto.convert.join=false;set hive.ignore.mapjoin.hint=false;` mapreduce.task.io.sort.mbmapreduce.task.io.sort.mb map shuffle时的内存 溢出 hive.ddl.output.formathive的ddl语句的输出格式，默认是text，纯文本，还有json格式，这个是0.90以后才出的新配置； hive.exec.script.wrapperhive调用脚本时的包装器，默认是null，如果设置为python的话，那么在做脚本调用操作时语句会变为python script command，null的话就是直接执行script command； hive.exec.planhive执行计划的文件路径，默认是null，会在运行时自动设置，形如hdfs://xxxx/xxx/xx； hive.exec.scratchdirhive用来存储不同阶段的map/reduce的执行计划的目录，同时也存储中间输出结果，默认是/tmp/&lt;user.name&gt;/hive，我们实际一般会按组区分，然后组内自建一个tmp目录存储； hive.exec.submitviachild在非local模式下，决定hive是否要在独立的jvm中执行map/reduce；默认是false，也就是说默认map/reduce的作业是在hive的jvm上去提交的； hive.exec.script.maxerrsize：当用户调用transform或者map或者reduce执行脚本时，最大的序列化错误数，默认100000，一般也不用修改； hive.exec.compress.output：一个查询的最后一个map/reduce任务输出是否被压缩的标志，默认为false，但是一般会开启为true，好处的话，节省空间不说，在不考虑cpu压力的时候会提高io； hive.exec.compress.intermediate：类似上个，在一个查询的中间的map/reduce任务输出是否要被压缩，默认false， hive.jar.path：当使用独立的jvm提交作业时，hive_cli.jar所在的位置，无默认值； hive.aux.jars.path：当用户自定义了UDF或者SerDe，这些插件的jar都要放到这个目录下，无默认值； hive.partition.pruning：在编译器发现一个query语句中使用分区表然而未提供任何分区谓词做查询时，抛出一个错误从而保护分区表，默认是nonstrict；（待读源码后细化，网上资料极少） hive.map.aggr：map端聚合是否开启，默认开启； hive.join.emit.interval：在发出join结果之前对join最右操作缓存多少行的设定，默认1000；hive jira里有个对该值设置太小的bugfix； hive.map.aggr.hash.percentmemory：map端聚合时hash表所占用的内存比例，默认0.5，这个在map端聚合开启后使用， hive.default.fileformat：CREATE TABLE语句的默认文件格式，默认TextFile，其他可选的有SequenceFile、RCFile还有Orc； hive.merge.mapfiles：在只有map的作业结束时合并小文件，默认开启true； hive.merge.mapredfiles：在一个map/reduce作业结束后合并小文件，默认不开启false； hive.merge.size.per.task：作业结束时合并文件的大小，默认256MB； hive.merge.smallfiles.avgsize：在作业输出文件小于该值时，起一个额外的map/reduce作业将小文件合并为大文件，小文件的基本阈值，设置大点可以减少小文件个数，需要mapfiles和mapredfiles为true，默认值是16MB； mapred.reduce.tasks：每个作业的reduce任务数，默认是hadoop client的配置1个； hive.exec.reducers.bytes.per.reducer：每个reducer的大小，默认是1G，输入文件如果是10G，那么就会起10个reducer； hive.exec.reducers.max：reducer的最大个数，如果在mapred.reduce.tasks设置为负值，那么hive将取该值作为reducers的最大可能值。当然还要依赖（输入文件大小/hive.exec.reducers.bytes.per.reducer）所得出的大小，取其小值作为reducer的个数，hive默认是999； hive.fileformat.check：加载数据文件时是否校验文件格式，默认是true； hive.groupby.skewindata：group by操作是否允许数据倾斜，默认是false，当设置为true时，执行计划会生成两个map/reduce作业，第一个MR中会将map的结果随机分布到reduce中，达到负载均衡的目的来解决数据倾斜， hive.groupby.mapaggr.checkinterval：map端做聚合时，group by 的key所允许的数据行数，超过该值则进行分拆，默认是100000； hive.mapred.local.mem：本地模式时，map/reduce的内存使用量，默认是0，就是无限制； hive.mapjoin.followby.map.aggr.hash.percentmemory：map端聚合时hash表的内存占比，该设置约束group by在map join后进行，否则使用hive.map.aggr.hash.percentmemory来确认内存占比，默认值0.3； hive.map.aggr.hash.force.flush.memeory.threshold：map端聚合时hash表的最大可用内存，如果超过该值则进行flush数据，默认是0.9； hive.map.aggr.hash.min.reduction：如果hash表的容量与输入行数之比超过这个数，那么map端的hash聚合将被关闭，默认是0.5，设置为1可以保证hash聚合永不被关闭； hive.optimize.groupby：在做分区和表查询时是否做分桶group by，默认开启true； hive.multigroupby.singlemr：将多个group by产出为一个单一map/reduce任务计划，当然约束前提是group by有相同的key，默认是false； hive.optimize.cp：列裁剪，默认开启true，在做查询时只读取用到的列，这个是个有用的优化； hive.optimize.index.filter：自动使用索引，默认不开启false； hive.optimize.index.groupby：是否使用聚集索引优化group-by查询，默认关闭false； hive.optimize.ppd：是否支持谓词下推，默认开启；所谓谓词下推，将外层查询块的 WHERE 子句中的谓词移入所包含的较低层查询块（例如视图），从而能够提早进行数据过滤以及有可能更好地利用索引。 hive.optimize.ppd.storage：谓词下推开启时，谓词是否下推到存储handler，默认开启，在谓词下推关闭时不起作用； hive.ppd.recognizetransivity：在等值join条件下是否产地重复的谓词过滤器，默认开启； hive.join.cache.size：在做表join时缓存在内存中的行数，默认25000； hive.mapjoin.bucket.cache.size：mapjoin时内存cache的每个key要存储多少个value，默认100； hive.optimize.skewjoin：是否开启数据倾斜的join优化，默认不开启false； hive.skewjoin.key：判断数据倾斜的阈值，如果在join中发现同样的key超过该值则认为是该key是倾斜的join key，默认是100000； hive.skewjoin.mapjoin.map.tasks：在数据倾斜join时map join的map数控制，默认是10000； hive.skewjoin.mapjoin.min.split：数据倾斜join时map join的map任务的最小split大小，默认是33554432，该参数要结合上面的参数共同使用来进行细粒度的控制； hive.mapred.mode：hive操作执行时的模式，默认是nonstrict非严格模式，如果是strict模式，很多有风险的查询会被禁止运行，比如笛卡尔积的join和动态分区； hive.exec.script.maxerrsize：一个map/reduce任务允许打印到标准错误里的最大字节数，为了防止脚本把分区日志填满，默认是100000； hive.exec.script.allow.partial.consumption：hive是否允许脚本不从标准输入中读取任何内容就成功退出，默认关闭false； hive.script.operator.id.env.var：在用户使用transform函数做自定义map/reduce时，存储唯一的脚本标识的环境变量的名字，默认HIVE_SCRIPT_OPERATOR_ID； hive.exec.compress.output：控制hive的查询结果输出是否进行压缩，压缩方式在hadoop的mapred.output.compress中配置，默认不压缩false； hive.exec.compress.intermediate：控制hive的查询中间结果是否进行压缩，同上条配置，默认不压缩false； hive.exec.parallel：hive的执行job是否并行执行，默认不开启false，在很多操作如join时，子查询之间并无关联可独立运行，这种情况下开启并行运算可以大大加速； hvie.exec.parallel.thread.number：并行运算开启时，允许多少作业同时计算，默认是8； hive.exec.rowoffset：是否提供行偏移量的虚拟列，默认是false不提供，Hive有两个虚拟列:一个是INPUT__FILE__NAME,表示输入文件的路径，另外一个是BLOCK__OFFSET__INSIDE__FILE，表示记录在文件中的块偏移量，这对排查出现不符合预期或者null结果的查询是很有帮助的； hive.task.progress：控制hive是否在执行过程中周期性的更新任务进度计数器，开启这个配置可以帮助job tracker更好的监控任务的执行情况，但是会带来一定的性能损耗，当动态分区标志hive.exec.dynamic.partition开启时，本配置自动开启； hive.exec.pre.hooks：执行前置条件，一个用逗号分隔开的实现了org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext接口的java class列表，配置了该配置后，每个hive任务执行前都要执行这个执行前钩子，默认是空； hive.exec.post.hooks：同上，执行后钩子，默认是空； hive.exec.failure.hooks：同上，异常时钩子，在程序发生异常时执行，默认是空； hive.mergejob.maponly：试图生成一个只有map的任务去做merge，前提是支持CombineHiveInputFormat，默认开启true； hive.mapjoin.smalltable.filesize：输入表文件的mapjoin阈值，如果输入文件的大小小于该值，则试图将普通join转化为mapjoin，默认25MB； hive.mapjoin.localtask.max.memory.usage：mapjoin本地任务执行时hash表容纳key/value的最大量，超过这个值的话本地任务会自动退出，默认是0.9； hive.mapjoin.followby.gby.localtask.max.memory.usage：类似上面，只不过是如果mapjoin后有一个group by的话，该配置控制类似这样的query的本地内存容量上限，默认是0.55； hive.mapjoin.check.memory.rows：在运算了多少行后执行内存使用量检查，默认100000； hive.heartbeat.interval：发送心跳的时间间隔，在mapjoin和filter操作中使用，默认1000； hive.auto.convert.join：根据输入文件的大小决定是否将普通join转换为mapjoin的一种优化，默认不开启false； hive.script.auto.progress：hive的transform/map/reduce脚本执行时是否自动的将进度信息发送给TaskTracker来避免任务没有响应被误杀，本来是当脚本输出到标准错误时，发送进度信息，但是开启该项后，输出到标准错误也不会导致信息发送，因此有可能会造成脚本有死循环产生，但是TaskTracker却没有检查到从而一直循环下去； hive.script.serde：用户脚本转换输入到输出时的SerDe约束，默认是org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe； hive.script.recordreader：从脚本读数据的时候的默认reader，默认是org.apache.hadoop.hive.ql.exec.TextRecordReader； hive.script.recordwriter：写数据到脚本时的默认writer，默认org.apache.hadoop.hive.ql.exec.TextRecordWriter； hive.input.format：输入格式，默认是org.apache.hadoop.hive.ql.io.CombineHiveInputFormat，如果出现问题，可以改用org.apache.hadoop.hive.ql.io.HiveInputFormat； hive.udtf.auto.progress：UDTF执行时hive是否发送进度信息到TaskTracker，默认是false； hive.mapred.reduce.tasks.speculative.execution：reduce任务推测执行是否开启，默认是true； hive.exec.counters.pull.interval：运行中job轮询JobTracker的时间间隔，设置小会影响JobTracker的load，设置大可能看不出运行任务的信息，要去平衡，默认是1000； hive.enforce.bucketing：数据分桶是否被强制执行，默认false，如果开启，则写入table数据时会启动分桶， hive.enforce.sorting：开启强制排序时，插数据到表中会进行强制排序，默认false； hive.optimize.reducededuplication：如果数据已经根据相同的key做好聚合，那么去除掉多余的map/reduce作业，此配置是文档的推荐配置，建议打开，默认是true； hive.exec.dynamic.partition：在DML/DDL中是否支持动态分区，默认false； hive.exec.dynamic.partition.mode：默认strict，在strict模式下，动态分区的使用必须在一个静态分区确认的情况下，其他分区可以是动态； hive.exec.max.dynamic.partitions：动态分区的上限，默认1000； hive.exec.max.dynamic.partitions.pernode：每个mapper/reducer节点可以创建的最大动态分区数，默认100； hive.exec.max.created.files：一个mapreduce作业能创建的HDFS文件最大数，默认是100000； hive.exec.default.partition.name：当动态分区启用时，如果数据列里包含null或者空字符串的话，数据会被插入到这个分区，默认名字是HIVE_DEFAULT_PARTITION； hive.fetch.output.serde：FetchTask序列化fetch输出时需要的SerDe，默认是org.apache.hadoop.hive.serde2.DelimitedJSONSerDe; hive.exec.mode.local.auto：是否由hive决定自动在local模式下运行，默认是false， hive.exec.drop.ignorenoneexistent：在drop表或者视图时如果发现表或视图不存在，是否报错，默认是true； hive.exec.show.job.failure.debug.info：在作业失败时是否提供一个任务debug信息，默认true； hive.auto.progress.timeout：运行自动progressor的时间间隔，默认是0等价于forever； hive.table.parameters.default：新建表的属性字段默认值，默认是empty空； hive.variable.substitute：是否支持变量替换，如果开启的话，支持语法如${var} ${system:var}和${env.var}，默认是true； hive.error.on.empty.partition：在遇到结果为空的动态分区时是否报错，默认是false； hive.exim.uri.scheme.whitelist：在导入导出数据时提供的一个白名单列表，列表项之间由逗号分隔，默认hdfs,pfile； hive.limit.row.max.size：字面意思理解就是在使用limit做数据的子集查询时保证的最小行数据量，默认是100000； hive.limit.optimize.limit.file：使用简单limit查询数据子集时，可抽样的最大文件数，默认是10； hive.limit.optimize.enable：使用简单limit抽样数据时是否开启优化选项，默认是false，关于limit的优化问题，在hive programming书中解释的是这个feature有drawback，对于抽样的不确定性给出了风险提示； hive.limit.optimize.fetch.max：使用简单limit抽样数据允许的最大行数，默认50000，查询query受限，insert不受影响； hive.rework.mapredwork：是否重做mapreduce，默认是false； hive.sample.seednumber：用来区分抽样的数字，默认是0； hive.io.exception.handlers：io异常处理handler类列表，默认是空，当record reader发生io异常时，由这些handler来处理异常； hive.autogen.columnalias.prefix.label：当在执行中自动产生列别名的前缀，当类似count这样的聚合函数起作用时，如果不明确指出count(a) as xxx的话，那么默认会从列的位置的数字开始算起添加，比如第一个count的结果会冠以列名_c0，接下来依次类推，默认值是_c，数据开发过程中应该很多人都看到过这个别名； hive.autogen.columnalias.prefix.includefuncname：在自动生成列别名时是否带函数的名字，默认是false； hive.exec.perf.logger：负责记录客户端性能指标的日志类名，必须是org.apache.hadoop.hive.ql.log.PerfLogger的子类，默认是org.apache.hadoop.hive.ql.log.PerfLogger； hive.start.cleanup.scratchdir：当启动hive服务时是否清空hive的scratch目录，默认是false； hive.output.file.extension：输出文件扩展名，默认是空； hive.insert.into.multilevel.dirs：是否插入到多级目录，默认是false； hive.files.umask.value：hive创建文件夹时的dfs.umask值，默认是0002； hive.metastore.local：控制hive是否连接一个远程metastore服务器还是开启一个本地客户端jvm，默认是true，Hive0.10已经取消了该配置项； javax.jdo.option.ConnectionURL：JDBC连接字符串，默认jdbc:derby:;databaseName=metastore_db;create=true； javax.jdo.option.ConnectionDriverName：JDBC的driver，默认org.apache.derby.jdbc.EmbeddedDriver； javax.jdo.PersisteneManagerFactoryClass：实现JDO PersistenceManagerFactory的类名，默认org.datanucleus.jdo.JDOPersistenceManagerFactory； javax.jdo.option.DetachAllOnCommit：事务提交后detach所有提交的对象，默认是true； javax.jdo.option.NonTransactionalRead：是否允许非事务的读，默认是true； javax.jdo.option.ConnectionUserName：username，默认APP； javax.jdo.option.ConnectionPassword：password，默认mine； javax.jdo.option.Multithreaded：是否支持并发访问metastore，默认是true； datanucleus.connectionPoolingType：使用连接池来访问JDBC metastore，默认是DBCP； datanucleus.validateTables：检查是否存在表的schema，默认是false； datanucleus.validateColumns：检查是否存在列的schema，默认false； datanucleus.validateConstraints：检查是否存在constraint的schema，默认false； datanucleus.stroeManagerType：元数据存储类型，默认rdbms； datanucleus.autoCreateSchema：在不存在时是否自动创建必要的schema，默认是true； datanucleus.aotuStartMechanismMode：如果元数据表不正确，抛出异常，默认是checked； datanucleus.transactionIsolation：默认的事务隔离级别，默认是read-committed； datanucleus.cache.level2：使用二级缓存，默认是false； datanucleus.cache.level2.type：二级缓存的类型，有两种，SOFT:软引用，WEAK:弱引用，默认是SOFT； datanucleus.identifierFactory：id工厂生产表和列名的名字，默认是datanucleus； datanucleus.plugin.pluginRegistryBundleCheck：当plugin被发现并且重复时的行为，默认是LOG； hive.metastroe.warehouse.dir：dw的位置，默认是/user/hive/warehouse； hive.metastore.execute.setugi：非安全模式，设置为true会令metastore以客户端的用户和组权限执行DFS操作，默认是false，这个属性需要服务端和客户端同时设置； hive.metastore.event.listeners：metastore的事件监听器列表，逗号隔开，默认是空； hive.metastore.partition.inherit.table.properties：当新建分区时自动继承的key列表，默认是空； hive.metastore.end.function.listeners：metastore函数执行结束时的监听器列表，默认是空； hive.metastore.event.expiry.duration：事件表中事件的过期时间，默认是0； hive.metastore.event.clean.freq：metastore中清理过期事件的定时器的运行周期，默认是0； hive.metastore.connect.retries：创建metastore连接时的重试次数，默认是5； hive.metastore.client.connect.retry.delay：客户端在连续的重试连接等待的时间，默认1； hive.metastore.client.socket.timeout：客户端socket超时时间，默认20秒； hive.metastore.rawstore.impl：原始metastore的存储实现类，默认是org.apache.hadoop.hive.metastore.ObjectStore； hive.metastore.batch.retrieve.max：在一个batch获取中，能从metastore里取出的最大记录数，默认是300； hive.metastore.ds.connection.url.hook：查找JDO连接url时hook的名字，默认是javax.jdo.option.ConnectionURL； hive.metastore.ds.retry.attempts：当出现连接错误时重试连接的次数，默认是1次； hive.metastore.ds.retry.interval：metastore重试连接的间隔时间，默认1000毫秒； hive.metastore.server.min.threads：在thrift服务池中最小的工作线程数，默认是200； hive.metastore.server.max.threads：最大线程数，默认是100000； hive.metastore.server.tcp.keepalive：metastore的server是否开启长连接，长连可以预防半连接的积累，默认是true； hive.metastore.sasl.enabled：metastore thrift接口的安全策略，开启则用SASL加密接口，客户端必须要用Kerberos机制鉴权，默认是不开启false； hive.metastore.kerberos.keytab.file：在开启sasl后kerberos的keytab文件存放路径，默认是空； hive.metastore.kerberos.principal：kerberos的principal，_HOST部分会动态替换，默认是hive-metastore/_HOST@EXAMPLE.COM； hive.metastore.cache.pinobjtypes：在cache中支持的metastore的对象类型，由逗号分隔，默认是Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order； hive.metastore.authorization.storage.checks：在做类似drop partition操作时，metastore是否要认证权限，默认是false； hive.metastore.schema.verification：强制metastore的schema一致性，开启的话会校验在metastore中存储的信息的版本和hive的jar包中的版本一致性，并且关闭自动schema迁移，用户必须手动的升级hive并且迁移schema，关闭的话只会在版本不一致时给出警告，默认是false不开启； hive.index.compact.file.ignore.hdfs：在索引文件中存储的hdfs地址将在运行时被忽略，如果开启的话；如果数据被迁移，那么索引文件依然可用，默认是false； hive.optimize.index.filter.compact.minsize：压缩索引自动应用的最小输入大小，默认是5368709120； hive.optimize.index.filter.compact.maxsize：同上，相反含义，如果是负值代表正无穷，默认是-1； hive.index.compact.query.max.size：一个使用压缩索引做的查询能取到的最大数据量，默认是10737418240 个byte；负值代表无穷大； hive.index.compact.query.max.entries：使用压缩索引查询时能读到的最大索引项数，默认是10000000；负值代表无穷大； hive.index.compact.binary.search：在索引表中是否开启二分搜索进行索引项查询，默认是true； hive.exec.concatenate.check.index：如果设置为true，那么在做ALTER TABLE tbl_name CONCATENATE on a table/partition（有索引） 操作时，抛出错误；可以帮助用户避免index的删除和重建； hive.stats.dbclass：存储hive临时统计信息的数据库，默认是jdbc:derby； hive.stats.autogather：在insert overwrite命令时自动收集统计信息，默认开启true； hive.stats.jdbcdriver：数据库临时存储hive统计信息的jdbc驱动； hive.stats.dbconnectionstring：临时统计信息数据库连接串，默认jdbc:derby:databaseName=TempStatsStore;create=true； hive.stats.defaults.publisher：如果dbclass不是jdbc或者hbase，那么使用这个作为默认发布，必须实现StatsPublisher接口，默认是空； hive.stats.defaults.aggregator：如果dbclass不是jdbc或者hbase，那么使用该类做聚集，要求实现StatsAggregator接口，默认是空； hive.stats.jdbc.timeout：jdbc连接超时配置，默认30秒； hive.stats.retries.max：当统计发布合聚集在更新数据库时出现异常时最大的重试次数，默认是0，不重试； hive.stats.retries.wait：重试次数之间的等待窗口，默认是3000毫秒； hive.client.stats.publishers：做count的job的统计发布类列表，由逗号隔开，默认是空；必须实现org.apache.hadoop.hive.ql.stats.ClientStatsPublisher接口； hive.client.stats.counters：没什么用~~~ hive.security.authorization.enabled：hive客户端是否认证，默认是false； hive.security.authorization.manager：hive客户端认证的管理类，默认是org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider；用户定义的要实现org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider； hive.security.authenticator.manager：hive客户端授权的管理类，默认是org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator；用户定义的需要实现org.apache.hadoop.hive.ql.security.HiveAuthenticatorProvider； hive.security.authorization.createtable.user.grants：当表创建时自动授权给用户，默认是空； hive.security.authorization.createtable.group.grants：同上，自动授权给组，默认是空； hive.security.authorization.createtable.role.grants：同上，自动授权给角色，默认是空； hive.security.authorization.createtable.owner.grants：同上，自动授权给owner，默认是空； hive.security.metastore.authorization.manager：metastore的认证管理类，默认是org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider；用户定义的必须实现org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider接口；接口参数要包含org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider接口；使用HDFS的权限控制认证而不是hive的基于grant的方式； hive.security.metastore.authenticator.manager：metastore端的授权管理类，默认是org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator，自定义的必须实现org.apache.hadoop.hive.ql.security.HiveAuthenticatorProvider接口； hive.metastore.pre.event.listeners：在metastore做数据库任何操作前执行的事件监听类列表； fs.har.impl：访问Hadoop Archives的实现类，低于hadoop 0.20版本的都不兼容，默认是org.apache.hadoop.hive.shims.HiveHarFileSystem； hive.archive.enabled：是否允许归档操作，默认是false； hive.archive.har.parentdir.settable：在创建HAR文件时必须要有父目录，需要手动设置，在新的hadoop版本会支持，默认是false； hive.support.concurrency：hive是否支持并发，默认是false，支持读写锁的话，必须要起zookeeper； hive.lock.mapred.only.operation：控制是否在查询时加锁，默认是false； hive.lock.numretries：获取锁时尝试的重试次数，默认是100； hive.lock.sleep.between.retries：在重试间隔的睡眠时间，默认60秒； hive.zookeeper.quorum：zk地址列表，默认是空； hive.zookeeper.client.port：zk服务器的连接端口，默认是2181； hive.zookeeper.session.timeout：zk客户端的session超时时间，默认是600000； hive.zookeeper.namespace：在所有zk节点创建后的父节点，默认是hive_zookeeper_namespace； hive.zookeeper.clean.extra.nodes：在session结束时清除所有额外node； hive.cluster.delegation.token.store.class：代理token的存储实现类，默认是org.apache.hadoop.hive.thrift.MemoryTokenStore，可以设置为org.apache.hadoop.hive.thrift.ZooKeeperTokenStore来做负载均衡集群； hive.cluster.delegation.token.store.zookeeper.connectString：zk的token存储连接串，默认是localhost:2181； hive.cluster.delegation.token.store.zookeeper.znode：token存储的节点跟路径，默认是/hive/cluster/delegation； hive.cluster.delegation.token.store.zookeeper.acl：token存储的ACL，默认是sasl:hive/host1@example.com:cdrwa,sasl:hive/host2@example.com:cdrwa； hive.use.input.primary.region：从一张input表创建表时，创建这个表到input表的主region，默认是true； hive.default.region.name：默认region的名字，默认是default； hive.region.properties：region的默认的文件系统和jobtracker，默认是空； hive.cli.print.header：查询输出时是否打印名字和列，默认是false； hive.cli.print.current.db：hive的提示里是否包含当前的db，默认是false； hive.hbase.wal.enabled：写入hbase时是否强制写wal日志，默认是true； hive.hwi.war.file：hive在web接口是的war文件的路径，默认是lib/hive-hwi-xxxx(version).war； hive.hwi.listen.host：hwi监听的host地址，默认是0.0.0.0； hive.hwi.listen.port：hwi监听的端口，默认是9999； hive.test.mode：hive是否运行在测试模式，默认是false； hive.test.mode.prefix：在测试模式运行时，表的前缀字符串，默认是test_； hive.test.mode.samplefreq：如果hive在测试模式运行，并且表未分桶，抽样频率是多少，默认是32； hive.test.mode.nosamplelist：在测试模式运行时不进行抽样的表列表，默认是空； 参考https://blog.csdn.net/yycdaizi/article/details/43341239https://meihuakaile.github.io/2018/10/19/hive-set%E8%AE%BE%E7%BD%AE/http://www.blogjava.net/changedi/archive/2013/11/13/406295.htmlhttps://www.cnblogs.com/hark0623/p/5650075.html]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBASE中，region server挂了之后，如何把这台server上的region转移到另外的region server上呢]]></title>
    <url>%2F2019%2F02%2F22%2FHBASE%E4%B8%AD%EF%BC%8Cregion-server%E6%8C%82%E4%BA%86%E4%B9%8B%E5%90%8E%EF%BC%8C%E5%A6%82%E4%BD%95%E6%8A%8A%E8%BF%99%E5%8F%B0server%E4%B8%8A%E7%9A%84region%E8%BD%AC%E7%A7%BB%E5%88%B0%E5%8F%A6%E5%A4%96%E7%9A%84region-server%E4%B8%8A%E5%91%A2%2F</url>
    <content type="text"><![CDATA[无论哪种情况，region server（实指RegionServer进程挂掉，下文同）都无法继续为它的region提供服务了，此时master会删除server目录下代表这台region server的文件，并将这台region server的region分配给其它还活着的同志。我想问，region server既然挂了，那每个region的详细信息也就丢了，又没多个备份，怎么能重新分配给别的region server呢？HBase基于Hadoop，Region Server中的数据保存在HDFS上，默认情况下HDFS保存3份，会尽量放在3台机器上。所以当一台机器挂掉的情况下，HDFS层面会检查到这部分数据的复制份数不够，再复制一份出来到其它机器上去的。你应该先看hbase是如何找到regionserver的。你可以看到hbase找到regionserver是向zookeeper请求-ROOT-表，然后找到-META-表，然后在-META-表里找到regionserver信息。如果regionserver挂了，它把这个元数据，转移到其它活着的regionserver上，然后把wal日志分到其它regionserver上去重新加载。记住这里是元数据，数据本身是在hdfs上的，所以数据本身不会挂。元数据可以理解成对文件的索引，是放在内存中的。这个分配region -&gt; regionserver的过程叫assign刚开始也迷惑这个问题，后来想了下，可能是这样的，HBase的HRegion文件和HLog文件都存储在HDFS中的（一般有多份），RegionServer挂了，并不是数据文件就丢失了。RegionServer挂了后，HRegion和HLog都还在，Master重新分配Region，并读取Hlog进行恢复。 HBase的运行时有三个集群： Zookeeper集群，负责HBase（当然还有HDFS）集群的协调服务和配置服务。其中的节点是ZooKeeper服务器 HDFS集群，是一个分布式文件系统，有单一的名字空间。其中的节点有NameNode和DataNode，前者负责存储文件系统的元数据（包括目录名，文件名，等），后则负责存储实际文件的块。 HBase集群，其中的节点有Master和Region Server HDFS存储在HBase中，关于数据的元数据也是存储在HBase的表中的，而元数据和数据都通过HDFS文件系统存储。在HDFS中，HBase的文件如下所示： /hbase，hbase的根目录 /hbase/weblogs，一个名为weblogs的表的目录 /hbase/weblogs/.tableinfo.0000000001，表的元数据信息 /hbase/weblogs/e048f132e3c5ac596e4fbcc486c0ef6e，表的一个Region目录 /hbase/weblogs/e048f132e3c5ac596e4fbcc486c0ef6e/.regioninfo，Region的元数据信息 /hbase/weblogs/e048f132e3c5ac596e4fbcc486c0ef6e/pageviews，Region的一个ColumnFamily目录 /hbase/weblogs/e048f132e3c5ac596e4fbcc486c0ef6e/pageviews/837a8a7bbf044ed3849b77881a3089a7，Region的HFile Zookeeper存储不但在HDFS中存储信息，HBase还在Zookeeper中存储信息，其中的znode如下所示： /hbase/root-region-server ，Root region的位置 /hbase/table/-ROOT-，根元数据信息 /hbase/table/.META.，元数据信息 /hbase/master，当选的Mater /hbase/backup-masters，备选的Master /hbase/rs ，Region Server的信息 /hbase/unassigned，未分配的Region也就是说，由Zookeeper保持了集群的状态信息 Region Server在来看Region Server，如图所示： Region Server存储了三部分信息： HFile，数据文件，存储在HDFS上 Write-Ahead Log（WAL日志就是Hlog,所有的region共享一个hlog,hlog是滚动日志，有很多小文件，会被删除），重做日志，类似于Oracle的Redo Log和MySQL的Binlog，也存在HDFS上。 Memstore，内存中的数据缓存（没有flush的部分可能会丢失），类似Oracle的Buffer Cache。 Region Server fail时重新分配Region其中HFile和WAL都存储在HDFS上，当Region Server fail的时候，数据是不会丢失的，丢失的只是Memstore中尚没有写入HFile的部分。Region Server宕机后，该服务器所负责的Region就会变成不可用，HMaster就会根据集群的负载状况，将这些Region分配给其他Region Server。接手的Region Server从Zookeeper上，以及ROOT表和META表上可以找到充分的元数据信息，数据又是存在HDFS上，由于HDFS会保持多个副本，数据也不会丢，那么Region Server只需要根据这些信息组织起内存结构，并获取WAL（在hdfs上不会丢失）文件进行重放，就可以最大程度的恢复到原来那台Region Server宕机前的状态。当然由于RegionServer大多和DataNode在一台服务器上，在写入时，也会调用本地的DFSClient写入，数据块会在本机有一个副本，在另外的机架上有一个副本，还有另外一个随机分配的副本，这样在读写时可以保证Data Locality。如果Region Server接手了一个Region，那么大多数数据块不在本地的Name Node上，读写性能会有下降，但会在后续的Compact过程中逐渐将文件写入本地的Data Node，从而恢复DataLocality。当然由于HDFS少了一个Data Node，改Data Node上的数据块都少了一个副本，HDFS也会在其他的Data Node上重新建立这些副本，以保证可靠性。http://www.kejik.com/article/270124.htmlHBase之WAL机制详细讲解http://blog.sina.com.cn/s/blog_15e0e0a700102w2v4.htmlHLog类 实现了WAL的类叫做HLog,当hregion被实例化时，HLog实例会被当做一个参数传到HRegion的构造器中，当一个Region接收到一个更新操作时，它可以直接把数据保存到一个共享的WAL实例中去 HLogKey类 1、当前的WAL使用的是hadoop的sequencefile格式，其key是HLogKey实例。HLogKey中记录了写入数据的归属信息，，除了table和region名字外，同时还包括sequence number和timestamp，timestamp是“写入时间“，sequence number的起始值为0，或者是最近一次存入文件系统中sequence number 2、HLog sequence File的value是HBase的KeyValue对象，即对应HFile中的KeyValue WALEdit类 1、客户端发送的每个修改都会封装成WALEdit类，一个WALEdit类包含了多个更新操作，可以说一个WALEdit就是一个原子操作，包含若干个操作的集合 LogSyncer类 1、Table在创建的时候，有一个参数可以设置，是否每次写Log日志都需要往集群的其他机器同步一次，默认是每次都同步，同步的开销是比较大的，但不及时同步又可能因为机器宕而丢日志。同步的操作现在是通过pipeline的方式来实现的，pipeline是指datanode接收数据后，再传给另外一台datanode，是一种串行的方式，n-Way writes是指多datanode同时接收数据，最慢的一台结束就是整个结束，差别在于一个延迟大，一个开发高，hdfs现在正在开发中，以便可以选择是按pipeline还是n-way writes来实现写操作 2、Table如果设置每次不同步，则写操作会被RegionServer缓存，并启动一个LogSyncer线程来定时同步日志，定时时间默认是一秒也可由hbase.regionserver.optionallogflushinterval设置 LogRoller类 1、日志写入的大小是有限制的，LogRoller类会作为一个后台线程运行，在特定的时间间隔内滚动日志，通过hbase.regionserver.logroll.period属性控制，默认1小时]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark中的checkpoint作用与用法]]></title>
    <url>%2F2019%2F02%2F22%2FSpark%E4%B8%AD%E7%9A%84checkpoint%E4%BD%9C%E7%94%A8%E4%B8%8E%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[checkpoint的意思就是建立检查点,类似于快照,例如在spark计算里面 计算流程DAG特别长,服务器需要将整个DAG计算完成得出结果,但是如果在这很长的计算流程中突然中间算出的数据丢失了,spark又会根据RDD的依赖关系从头到尾计算一遍,这样子就很费性能,当然我们可以将中间的计算结果通过cache或者persist放到内存或者磁盘中,但是这样也不能保证数据完全不会丢失,存储的这个内存出问题了或者磁盘坏了,也会导致spark从头再根据RDD计算一遍,所以就有了checkpoint,其中checkpoint的作用就是将DAG中比较重要的中间数据做一个检查点将结果存储到一个高可用的地方(通常这个地方就是HDFS里面)一般我们先进行cache然后做checkpoint就会只走一次流程,checkpoint的时候就会从刚cache到内存中取数据写入hdfs中其中作者也说明了,在checkpoint的时候强烈建议先进行cache,并且当你checkpoint执行成功了,那么前面所有的RDD依赖都会被销毁]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据环境下的平台架构乱弹]]></title>
    <url>%2F2019%2F02%2F22%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84%E4%B9%B1%E5%BC%B9%2F</url>
    <content type="text"><![CDATA[逻辑上，一般都有数据采集层、数据存储与分析层、数据共享层、数据应用层。可能叫法有所不同，本质上的角色都大同小异。借图发挥： 信息采集 数据采集层的任务就是把数据从各种数据源中采集和存储到数据存储上，期间有可能会做一些简单的清洗。 网站日志 业务数据库 来自于Ftp/Http的数据源 其他数据源 监控报警：系统信息collectd除运维外其他很少会用到。各平台信息(hadoop、spark、stom)的指标可以通过自身的监控系统。主要是应用程序的监控报警、任务打点的监控报警。 数据存储与分析 Hive、MR、Spark、SparkSQL分析和计算的结果，还是在HDFS上 实时计算实时计算使用Spark Streaming以及部分Java程序消费Kafka中收集的日志数据，实时计算结果大多保存在Redis中 数据共享 这里的数据共享，其实指的是前面数据分析与计算后的结果存放的地方，其实就是关系型数据库和NOSQL数据库； 数据应用 业务产品(业务产品所使用的数据，已经在数据共享层，他们直接访问数据共享层即可) 报表数据可视化，es,kylin 即席查询 OLAP(phenix，zeeplin，presto;这时候，需要做相应的开发，从HDFS或者HBase中获取数据，完成OLAP的功能；比如：根据用户在界面上选择的不定的维度和指标，通过开发接口，从HBase中获取数据来展示。) 其他数据接口(这种接口有通用的，有定制的。比如：一个从Redis中获取用户属性的接口是通用的，所有的业务都可以调用这个接口来获取用户属性。) 任务调度与监控 元数据管理字典系统、etl系统 总结架构，并不是技术越多越新越好，而是在可以满足需求的情况下，简单、稳定。目前在我们的数据平台中，开发更多的是关注业务，而不是技术，他们把业务和需求搞清楚了，基本上只需要做简单的SQL开发，然后配置到调度系统就可以了，如果任务异常，会收到告警。这样，可以使更多的资源专注于业务之上。 参考：http://lxw1234.com/archives/2015/08/471.htm]]></content>
      <categories>
        <category>dw</category>
      </categories>
      <tags>
        <tag>dw</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反作弊]]></title>
    <url>%2F2019%2F02%2F22%2F%E5%8F%8D%E4%BD%9C%E5%BC%8A%2F</url>
    <content type="text"><![CDATA[1）所有的作弊都有动机和目的，从目的出发，更容易发现作弊者。2）真实的用户行为具有一定的规律，不符合规律的就有可能是作弊。3）反作弊的最终目的是无限压缩作弊行为在正常商业行为中的比例，而非绝对根除，因为绝对根除成本太大。4）最好的实现方法在于让作弊者成本剧增，无利可图时，作弊团队自然也烟消云散。 区分属于哪一种：广告投放带来收益,比如拉新，注册，安装 https://my.oschina.net/datacube/blog/788435用户在网站刷排名 需要了解常见的作弊手段，才能知己知彼：没有怎么接触过PC端的作弊以及反作弊，从移动端说说常用的作弊方法以及应对策略吧。了解过一般需要ip,cookie,屏幕鼠标捕捉等行为，以及其他 假量 也就是下游的一些网盟，Affiliate利用平台的漏洞，制造一些根本不存在的用户，这种量的特征是：量大了之后IP段，手机机型，User-agent等数据会有集中的倾向，用户的n日留存为零；原因就是造假者很难弄到很多当地的IP（PS 我是做海外市场的，国内不太了解）。应对措施：监测下游的点击IP，UA等重复率；商务测需要经常和上游核对数据；上面两种情况都可能出现 “以次充好”的量（含刷量），严格来说这种流量算是处于中间态，是真量，但是在用户留存，用户购买，用户活跃等数据上面来说真的是特别特别烂；怎么做到的：使用一些激励性质的流量平台去买量，或者使用一些激励手段去刷榜；应对措施：使用了激励流量的渠道一般而言转化率（CVR）是比较高的，高于正常渠道的转化率；当然他们可以在正常的高质量流量中掺一部分”质次“的量用来维持利润率，这就需要看能否拿到跳转Landing Page的前一跳了；还要随机对于下游渠道的广告投放平台进行审查。 ”抢归因“的量 当然除了二跳率这些指标外，异常表现还包括广告来源异常；曝光、点击频次异常； 曝光、点击IP/地域集中； 用户平均曝光量、点击量过大；曝光、点击的UA分布异常；数据时段分布异常；到达率、转化率异常等等。 ip,mac,gid,adid,idfa,imei,mode,os,双卡双待机型等信息 用户行为 只搜不点：看意图（刷sug、刷排名） 大量搜索（冷门query、商业query、医疗query、新闻query等）行为，且无点击视为异常。 只点不搜： 只有click行为，且无搜索行为视为异常。 搜点作弊： cookie下的行为很规律，一搜一点或者几搜几点，点击url相同。 集中点击: 同一时间大量点击不同url或相同url，或者相隔很短时间（2s内）的大量点击。 若重复点击、同一秒行为是阿拉丁点击行为，则为正常。 转码点击即在转码页的点击，会出现点击同一url的情况，判为正常。 Query语义不相关 cookie出现连续搜索、连续主动修改query，搜索query上下文不相关，且query语义上多为商业query和新闻query，刷热词。 时间序列的模型，关注流量方面 Big_cookie一小时日志行为数大于300的cookie 策略计算:判断line_cnt是否大于300；即该cookie的行为数（包括点击和展现）是否过多；判定为spam_cookie.以cookie为最终判断维度即若某个小时内一个cookie被判定为spam，则其在此小时内所有行为都将被判为spam。判断spam cookie的过程分为两个阶段。第一阶段根据若干规则识别spam query/url/IP/UA/from，第二阶段由这些spam query/url/IP/UA/From株连到spam cookie。对于行为量过大的cookie，由专门的大cookie策略召回。此外，反作弊系统还有若干判断无效行为的策略。主要有预读、回翻、双击、query长度异常、referrer异常、内网IP流量、爬虫等。这些策略短期将继续放在UDW执行，后续会有重构计划。 参考http://www.iamniu.com/2012/08/15/apps-anti-cheating/]]></content>
      <categories>
        <category>反作弊</category>
      </categories>
      <tags>
        <tag>反作弊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Thread类中的join]]></title>
    <url>%2F2019%2F02%2F22%2FThread%E7%B1%BB%E4%B8%AD%E7%9A%84join%2F</url>
    <content type="text"><![CDATA[Thread类中的join方法的主要作用就是同步，它可以使得线程之间的并行执行变为串行执行12345678910111213141516171819202122232425public class JoinTest &#123; public static void main(String [] args) throws InterruptedException &#123; ThreadJoinTest t1 = new ThreadJoinTest(&quot;小明&quot;); ThreadJoinTest t2 = new ThreadJoinTest(&quot;小东&quot;); t1.start(); /**join的意思是使得放弃当前线程的执行，并返回对应的线程，例如下面代码的意思就是： 程序在main线程中调用t1线程的join方法，则main线程放弃cpu控制权，并返回t1线程继续执行,直到线程t1执行完毕,main线程才继续 所以结果是t1线程执行完后，才到主线程执行，相当于在main线程中同步t1线程，t1执行完了，main线程才有执行的机会 */ t1.join(); t2.start(); &#125;&#125;class ThreadJoinTest extends Thread&#123; public ThreadJoinTest(String name)&#123; super(name); &#125; @Override public void run()&#123; for(int i=0;i&lt;1000;i++)&#123; System.out.println(this.getName() + &quot;:&quot; + i); &#125; &#125;&#125; 上面程序结果是先打印完小明线程，在打印小东线程join方法必须写在start()之前才能实现同步]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jmx tomcat]]></title>
    <url>%2F2019%2F02%2F22%2Fjmx-tomcat%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637#!/bin/shexport TOMCAT_USER=&quot;tomcat&quot;export JAVA_OPTS=&quot;-Xms4096m -Xmx4096m -XX:+UseG1GC -XX:MaxGCPauseMillis=300 -server -XX:+DisableExplicitGC -Dqnr.logs=$CATALINA_BASE/logs -Dqnr.cache=$CATALINA_BASE/cache -verbose:gc -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:$CATALINA_BASE/logs/gc.log-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/q/www/mobiledcs-hbase-server/temp/oom -Djava.rmi.server.hostname=10.90.14.87 -Dcom.sun.management.jmxremote.port=1098 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=true -Dcom.sun.management.jmxremote.password.file=/home/q/java/default/jre/lib/management/jmxremote.password -Dcom.sun.management.jmxremote.access.file=/home/q/java/default/jre/lib/management/jmxremote.access&quot;# export JAVA_OPTS=&quot;-Xms4096m -Xmx4096m -XX:NewSize=512m -XX:PermSize=512m -server -XX:+DisableExplicitGC -Dqnr.logs=$CATALINA_BASE/logs -Dqnr.cache=$CATALINA_BASE/cache -verbose:gc -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:$CATALINA_BASE/logs/gc.log# -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/q/www/mobiledcs-hbase-server/temp/oom # -Djava.rmi.server.hostname=10.90.14.87 # -Dcom.sun.management.jmxremote.port=1098 # -Dcom.sun.management.jmxremote.ssl=false # -Dcom.sun.management.jmxremote.authenticate=true # -Dcom.sun.management.jmxremote.password.file=/home/q/java/default/jre/lib/management/jmxremote.password # -Dcom.sun.management.jmxremote.access.file=/home/q/java/default/jre/lib/management/jmxremote.access&quot;chown -R tomcat:tomcat $CATALINA_BASE/logschown -R tomcat:tomcat $CATALINA_BASE/cachechown -R tomcat:tomcat $CATALINA_BASE/confchown -R tomcat:tomcat $CATALINA_BASE/workchown -R tomcat:tomcat $CATALINA_BASE/temp/home/q/java/default/jre/lib/management/jmxremote.passwordmonitorRole 1qaz@WSXcontrolRole 1qaz@WSX/home/q/java/default/jre/lib/management/jmxremote.accessmonitorRole readonlycontrolRole readwrite \ create javax.management.monitor.*,javax.management.timer.* \ unregister]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查看调用接口的哪个具体实现类]]></title>
    <url>%2F2019%2F02%2F22%2F%E6%9F%A5%E7%9C%8B%E8%B0%83%E7%94%A8%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%93%AA%E4%B8%AA%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[1bucket.getAggregations().getClass().getCanonicalName() http://sunyimaying0925-gmail-com.iteye.com/blog/768789]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程通信机制：共享内存 VS 消息传递]]></title>
    <url>%2F2019%2F02%2F22%2F%E7%BA%BF%E7%A8%8B%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6%EF%BC%9A%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98-VS-%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92%2F</url>
    <content type="text"><![CDATA[在并发编程中，我们必须考虑的问题时如何在两个线程间进行通讯。这里的通讯指的是不同的线程之间如何交换信息。目前有两种方式： 共享内存 消息传递（actor 模型） #共享内存 共享内存这种方式比较常见，我们经常会设置一个共享变量。然后多个线程去操作同一个共享变量。从而达到线程通讯的目的。例如，我们使用多个线程去执行页面抓取任务，我们可以使用一个共享变量count来记录任务完成的数量。每当一个线程完成抓取任务，会在原来的count上执行加1操作。这样每个线程都可以通过获取这个count变量来获得当前任务的完成情况。当然必须要考虑的是共享变量的同步问题，这也共享内存容易出错的原因所在。 这种通讯模型中，不同的线程之间是没有直接联系的。都是通过共享变量这个“中间人”来进行交互。而这个“中间人”必要情况下还需被保护在临界区内（加锁或同步）。由此可见，一旦共享变量变得多起来，并且涉及到多种不同线程对象的交互，这种管理会变得非常复杂，极容易出现死锁等问题。给出案例 #消息传递 消息传递方式采取的是线程之间的直接通信，不同的线程之间通过显式的发送消息来达到交互目的。消息传递最有名的方式应该是actor模型了。在这种模型下，一切都是actor，所有的actor之间的通信都必须通过传递消息才能达到。每个actor都有一个收件箱（消息队列）用来保存收到其他actor传递来的消息。actor自己也可以给自己发送消息。这才是面向对象的精髓啊！ 这种模型看起来比共享内存模型要复杂。但是一旦碰到复杂业务的话，actor模型的优势就体现出来了。我们还是以刚才多线程抓取网站为例子看一下在这种模型下如何去解决。 首先我们定义一个统计actor用来统计任务完成量。然后把多个网址（消息方式）发给多个抓取actor，抓取actor处理完任务后发送消息通知统计actor任务完成，统计actor对自己保存的变量count（这个只有统计actor才能看到）加一。最后让我们来总结一下这两种通讯模式： 并发模型 通信机制 同步机制 共享内存 线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信 同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。 消息传递（actor) 线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信. 由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM内存]]></title>
    <url>%2F2019%2F02%2F22%2FJVM%E5%86%85%E5%AD%98%2F</url>
    <content type="text"><![CDATA[JVM运行时数据区由程序计数器、堆、虚拟机栈(栈)、本地方法栈、方法区部分组成，结构图如下所示。1）程序计数器 几乎不占有内存。用于取下一条执行的指令。2）堆 所有通过new创建的对象的内存都在堆中分配，其大小可以通过-Xmx和-Xms来控制。堆被划分为新生代和老年代，新生代又被进一步划分为Eden和Survivor区，最后Survivor由FromSpace和ToSpace组成，结构图如下所示： 新生代。新建的对象都是用新生代分配内存，Eden空间不足的时候，会把存活的对象转移到Survivor中，新生代大小可以由-Xmn来控制，也可以用-XX:SurvivorRatio来控制Eden和Survivor的比例，老生代。用于存放新生代中经过多次垃圾回收仍然存活的对象 3)栈 每个线程执行每个方法的时候都会在栈中申请一个栈帧，每个栈帧包括局部变量区和操作数栈，用于存放此次方法调用过程中的临时变量、参数和中间结果。4）本地方法栈 用于支持native方法的执行，存储了每个native方法调用的状态5）方法区 存放了要加载的类信息、静态变量、final类型的常量、属性和方法信息。JVM用永久代（PermanetGeneration）来存放方法区，（在JDK的HotSpot虚拟机中，可以认为方法区就是永久代，但是在其他类型的虚拟机中，没有永久代的概念，有关信息可以看周志明的书）可通过-XX:PermSize和-XX:MaxPermSize来指定最小值和最大值。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Callable、Future和FutureTask]]></title>
    <url>%2F2019%2F02%2F22%2FCallable%E3%80%81Future%E5%92%8CFutureTask%2F</url>
    <content type="text"><![CDATA[创建线程的2种方式，一种是直接继承Thread，另外一种就是实现Runnable接口。这2种方式都有一个缺陷就是：在执行完任务之后无法获取执行结果。 自从Java 1.5开始，就提供了Callable和Future，通过它们可以在任务执行完毕之后得到任务执行结果。 Callable先说一下java.lang.Runnable吧，它是一个接口，在它里面只声明了一个run()方法：123public interface Runnable &#123; public abstract void run();&#125; 由于run()方法返回值为void类型，所以在执行完任务之后无法返回任何结果。Callable位于java.util.concurrent包下，它也是一个接口，在它里面也只声明了一个方法，只不过这个方法叫做call()：123456789public interface Callable&lt;V&gt; &#123; /** * Computes a result, or throws an exception if unable to do so. * * @return computed result * @throws Exception if unable to compute a result */ V call() throws Exception;&#125; 可以看到，这是一个泛型接口，call()函数返回的类型就是传递进来的V类型那么怎么使用Callable呢？一般情况下是配合ExecutorService来使用的，在ExecutorService接口中声明了若干个submit方法的重载版本123&lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task);&lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result);Future&lt;?&gt; submit(Runnable task); 第一个submit方法里面的参数类型就是Callable。 暂时只需要知道Callable一般是和ExecutorService配合来使用的，具体的使用方法讲在后面讲述。 一般情况下我们使用第一个submit方法和第三个submit方法，第二个submit方法很少使用。 Future Future就是对于具体的Runnable或者Callable任务的执行结果进行取消、查询是否完成、获取结果。必要时可以通过get方法获取执行结果，该方法会阻塞直到任务返回结果。 Future类位于java.util.concurrent包下，它是一个接口：12345678public interface Future&lt;V&gt; &#123; boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; 在Future接口中声明了5个方法，下面依次解释每个方法的作用： cancel方法用来取消任务，如果取消任务成功则返回true，如果取消任务失败则返回false。参数mayInterruptIfRunning表示是否允许取消正在执行却没有执行完毕的任务，如果设置true，则表示可以取消正在执行过程中的任务。如果任务已经完成，则无论mayInterruptIfRunning为true还是false，此方法肯定返回false，即如果取消已经完成的任务会返回false；如果任务正在执行，若mayInterruptIfRunning设置为true，则返回true，若mayInterruptIfRunning设置为false，则返回false；如果任务还没有执行，则无论mayInterruptIfRunning为true还是false，肯定返回true。 isCancelled方法表示任务是否被取消成功，如果在任务正常完成前被取消成功，则返回 true。 isDone方法表示任务是否已经完成，若任务完成，则返回true； get()方法用来获取执行结果，这个方法会产生阻塞，会一直等到任务执行完毕才返回； get(long timeout, TimeUnit unit)用来获取执行结果，如果在指定时间内，还没获取到结果，就直接返回null。因为Future只是一个接口，所以是无法直接用来创建对象使用的，因此就有了下面的FutureTask。 FutureTask1234public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt;&#123;&#125;public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; &#123; void run();&#125; 可以看出RunnableFuture继承了Runnable接口和Future接口，而FutureTask实现了RunnableFuture接口。所以它既可以作为Runnable被线程执行，又可以作为Future得到Callable的返回值。FutureTask提供了2个构造器： 12public FutureTask(Callable&lt;V&gt; callable) &#123;&#125;public FutureTask(Runnable runnable, V result) &#123;&#125; 事实上，FutureTask是Future接口的一个唯一实现类。 使用示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class Test &#123; public static void main(String[] args) &#123; ExecutorService executor = Executors.newCachedThreadPool(); Task task = new Task(); Future&lt;Integer&gt; result = executor.submit(task); executor.shutdown(); //第2种方式 //ExecutorService executor = Executors.newCachedThreadPool(); //Task task = new Task(); //FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;Integer&gt;(task); //executor.submit(futureTask); //executor.shutdown(); //第3种方式，注意这种方式和第2种方式效果是类似的，只不过一个使用的是ExecutorService，一个使用的是Thread //Task task = new Task(); //FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;Integer&gt;(task); //Thread thread = new Thread(futureTask); //thread.start(); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; System.out.println("主线程在执行任务"); try &#123; System.out.println("task运行结果"+result.get()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; System.out.println("所有任务执行完毕"); &#125;&#125;class Task implements Callable&lt;Integer&gt;&#123; @Override public Integer call() throws Exception &#123; System.out.println("子线程在进行计算"); Thread.sleep(3000); int sum = 0; for(int i=0;i&lt;100;i++) sum += i; return sum; &#125;&#125; 运行结果：子线程在进行计算主线程在执行任务task运行结果4950所有任务执行完毕 Thread、Runnable、Callable，其中Runnable实现的是void run()方法，Callable实现的是 V call()方法，并且可以返回执行结果，其中Runnable可以提交给Thread来包装下，直接启动一个线程来执行，而Callable则一般都是提交给ExecuteService来执行。简单来说，Executor就是Runnable和Callable的调度容器，Future就是对于具体的调度任务的执行结果进行查看，最为关键的是Future可以检查对应的任务是否已经完成，也可以阻塞在get方法上一直等待任务返回结果。Runnable和Callable的差别就是Runnable是没有结果可以返回的，就算是通过Future也看不到任务调度的结果的。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * 通过简单的测试程序来试验Runnable、Callable通过Executor来调度的时候与Future的关系 */ package com.hadoop.thread; import java.util.concurrent.Callable; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.Future; public class RunnableAndCallable2Future &#123; public static void main(String[] args) &#123; // 创建一个执行任务的服务 ExecutorService executor = Executors.newFixedThreadPool(3); try &#123; //1.Runnable通过Future返回结果为空 //创建一个Runnable，来调度，等待任务执行完毕，取得返回结果 Future&lt;?&gt; runnable1 = executor.submit(new Runnable() &#123; @Override public void run() &#123; System.out.println("runnable1 running."); &#125; &#125;); System.out.println("Runnable1:" + runnable1.get()); // 2.Callable通过Future能返回结果 //提交并执行任务，任务启动时返回了一个 Future对象， // 如果想得到任务执行的结果或者是异常可对这个Future对象进行操作 Future&lt;String&gt; future1 = executor.submit(new Callable&lt;String&gt;() &#123; @Override public String call() throws Exception &#123; // TODO Auto-generated method stub return "result=task1"; &#125; &#125;); // 获得任务的结果，如果调用get方法，当前线程会等待任务执行完毕后才往下执行 System.out.println("task1: " + future1.get()); //3. 对Callable调用cancel可以对对该任务进行中断 //提交并执行任务，任务启动时返回了一个 Future对象， // 如果想得到任务执行的结果或者是异常可对这个Future对象进行操作 Future&lt;String&gt; future2 = executor.submit(new Callable&lt;String&gt;() &#123; @Override public String call() throws Exception &#123; try &#123; while (true) &#123; System.out.println("task2 running."); Thread.sleep(50); &#125; &#125; catch (InterruptedException e) &#123; System.out.println("Interrupted task2."); &#125; return "task2=false"; &#125; &#125;); // 等待5秒后，再停止第二个任务。因为第二个任务进行的是无限循环 Thread.sleep(10); System.out.println("task2 cancel: " + future2.cancel(true)); // 4.用Callable时抛出异常则Future什么也取不到了 // 获取第三个任务的输出，因为执行第三个任务会引起异常 // 所以下面的语句将引起异常的抛出 Future&lt;String&gt; future3 = executor.submit(new Callable&lt;String&gt;() &#123; @Override public String call() throws Exception &#123; throw new Exception("task3 throw exception!"); &#125; &#125;); System.out.println("task3: " + future3.get()); &#125; catch (Exception e) &#123; System.out.println(e.toString()); &#125; // 停止任务执行服务 executor.shutdownNow(); &#125; &#125; 执行结果如下：runnable1 running.Runnable1:nulltask1: result=task1task2 running.task2 cancel: trueInterrupted task2.java.util.concurrent.ExecutionException: java.lang.Exception:Bad flag value!FutureTask则是一个RunnableFuture，即实现了Runnbale又实现了Futrue这两个接口，另外它还可以包装Runnable和Callable，所以一般来讲是一个符合体了，它可以通过Thread包装来直接执行，也可以提交给ExecuteService来执行，并且还可以通过v get()返回执行结果，在线程体没有执行完成的时候，主线程一直阻塞等待，执行完则直接返回结果. public class FutureTaskTest { /** * @param args */ public static void main(String[] args) { Callable&lt;String&gt; task = new Callable&lt;String&gt;() { public String call() { System.out.println("Sleep start."); try { Thread.sleep(1000 * 10); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } System.out.println("Sleep end."); return "time=" + System.currentTimeMillis(); } }; //直接使用Thread的方式执行 FutureTask&lt;String&gt; ft = new FutureTask&lt;String&gt;(task); Thread t = new Thread(ft); t.start(); try { System.out.println("waiting execute result"); System.out.println("result = " + ft.get()); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (ExecutionException e) { // TODO Auto-generated catch block e.printStackTrace(); } //使用Executors来执行 System.out.println("========="); FutureTask&lt;String&gt; ft2 = new FutureTask&lt;String&gt;(task); Executors.newSingleThreadExecutor().submit(ft2); try { System.out.println("waiting execute result"); System.out.println("result = " + ft2.get()); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (ExecutionException e) { // TODO Auto-generated catch block e.printStackTrace(); } } } 执行结果如下：waiting execute resultSleep start.Sleep end.result = time=1370844662537 ========= waiting execute resultSleep start.Sleep end.result = time=1370844672542]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个servlet引发的血案]]></title>
    <url>%2F2019%2F02%2F22%2F%E4%B8%80%E4%B8%AAservlet%E5%BC%95%E5%8F%91%E7%9A%84%E8%A1%80%E6%A1%88%2F</url>
    <content type="text"><![CDATA[无状态servlet:1234567891011121314151617181920212223242526272829303132333435package net.jcip.examples;import java.math.BigInteger;import javax.servlet.*;import net.jcip.annotations.*;/** * StatelessFactorizer * * A stateless servlet * * @author Brian Goetz and Tim Peierls */@ThreadSafepublic class StatelessFactorizer extends GenericServlet implements Servlet &#123; public void service(ServletRequest req, ServletResponse resp) &#123; BigInteger i = extractFromRequest(req); BigInteger[] factors = factor(i); encodeIntoResponse(resp, factors); &#125; void encodeIntoResponse(ServletResponse resp, BigInteger[] factors) &#123; &#125; BigInteger extractFromRequest(ServletRequest req) &#123; return new BigInteger("7"); &#125; BigInteger[] factor(BigInteger i) &#123; // Doesn't really factor return new BigInteger[] &#123; i &#125;; &#125;&#125; 线程不安全：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127package net.jcip.examples;import java.math.BigInteger;import javax.servlet.*;import net.jcip.annotations.*;/** * UnsafeCountingFactorizer * * Servlet that counts requests without the necessary synchronization * * @author Brian Goetz and Tim Peierls */@NotThreadSafepublic class UnsafeCountingFactorizer extends GenericServlet implements Servlet &#123; private long count = 0; public long getCount() &#123; return count; &#125; public void service(ServletRequest req, ServletResponse resp) &#123; BigInteger i = extractFromRequest(req); BigInteger[] factors = factor(i); ++count; encodeIntoResponse(resp, factors); &#125; void encodeIntoResponse(ServletResponse res, BigInteger[] factors) &#123; &#125; BigInteger extractFromRequest(ServletRequest req) &#123; return new BigInteger("7"); &#125; BigInteger[] factor(BigInteger i) &#123; // Doesn't really factor return new BigInteger[] &#123; i &#125;; &#125;&#125;``` # 线程安全： 这里使用到现有的线程安全类```javapackage net.jcip.examples;import java.math.BigInteger;import java.util.concurrent.atomic.*;import javax.servlet.*;import net.jcip.annotations.*;/** * CountingFactorizer * * Servlet that counts requests using AtomicLong * * @author Brian Goetz and Tim Peierls */@ThreadSafepublic class CountingFactorizer extends GenericServlet implements Servlet &#123; private final AtomicLong count = new AtomicLong(0); public long getCount() &#123; return count.get(); &#125; public void service(ServletRequest req, ServletResponse resp) &#123; BigInteger i = extractFromRequest(req); BigInteger[] factors = factor(i); count.incrementAndGet(); encodeIntoResponse(resp, factors); &#125; void encodeIntoResponse(ServletResponse res, BigInteger[] factors) &#123;&#125; BigInteger extractFromRequest(ServletRequest req) &#123;return null; &#125; BigInteger[] factor(BigInteger i) &#123; return null; &#125;&#125;``` # 线程不安全：```javapackage net.jcip.examples;import java.math.BigInteger;import java.util.concurrent.atomic.*;import javax.servlet.*;import net.jcip.annotations.*;/** * UnsafeCachingFactorizer * * Servlet that attempts to cache its last result without adequate atomicity * * @author Brian Goetz and Tim Peierls */@NotThreadSafepublic class UnsafeCachingFactorizer extends GenericServlet implements Servlet &#123; private final AtomicReference&lt;BigInteger&gt; lastNumber = new AtomicReference&lt;BigInteger&gt;(); private final AtomicReference&lt;BigInteger[]&gt; lastFactors = new AtomicReference&lt;BigInteger[]&gt;(); public void service(ServletRequest req, ServletResponse resp) &#123; BigInteger i = extractFromRequest(req); if (i.equals(lastNumber.get())) encodeIntoResponse(resp, lastFactors.get()); else &#123; BigInteger[] factors = factor(i); lastNumber.set(i); lastFactors.set(factors); encodeIntoResponse(resp, factors); &#125; &#125; void encodeIntoResponse(ServletResponse resp, BigInteger[] factors) &#123; &#125; BigInteger extractFromRequest(ServletRequest req) &#123; return new BigInteger("7"); &#125; BigInteger[] factor(BigInteger i) &#123; // Doesn't really factor return new BigInteger[]&#123;i&#125;; &#125;&#125; 很有问题的写法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176package net.jcip.examples;import java.math.BigInteger;import javax.servlet.*;import net.jcip.annotations.*;/** * SynchronizedFactorizer * * Servlet that caches last result, but with unnacceptably poor concurrency * * @author Brian Goetz and Tim Peierls */@ThreadSafepublic class SynchronizedFactorizer extends GenericServlet implements Servlet &#123; @GuardedBy("this") private BigInteger lastNumber; @GuardedBy("this") private BigInteger[] lastFactors; public synchronized void service(ServletRequest req, ServletResponse resp) &#123;//TODO BigInteger i = extractFromRequest(req); if (i.equals(lastNumber)) encodeIntoResponse(resp, lastFactors); else &#123; BigInteger[] factors = factor(i); lastNumber = i; lastFactors = factors; encodeIntoResponse(resp, factors); &#125; &#125; void encodeIntoResponse(ServletResponse resp, BigInteger[] factors) &#123; &#125; BigInteger extractFromRequest(ServletRequest req) &#123; return new BigInteger("7"); &#125; BigInteger[] factor(BigInteger i) &#123; // Doesn't really factor return new BigInteger[] &#123; i &#125;; &#125;&#125;``` # 推荐的做法：```javapackage net.jcip.examples;import java.math.BigInteger;import javax.servlet.*;import net.jcip.annotations.*;/** * CachedFactorizer * &lt;p/&gt; * Servlet that caches its last request and result * * @author Brian Goetz and Tim Peierls */@ThreadSafepublic class CachedFactorizer extends GenericServlet implements Servlet &#123; @GuardedBy("this") private BigInteger lastNumber; @GuardedBy("this") private BigInteger[] lastFactors; @GuardedBy("this") private long hits; @GuardedBy("this") private long cacheHits; public synchronized long getHits() &#123; return hits; &#125; public synchronized double getCacheHitRatio() &#123; return (double) cacheHits / (double) hits; &#125; public void service(ServletRequest req, ServletResponse resp) &#123; BigInteger i = extractFromRequest(req); BigInteger[] factors = null; synchronized (this) &#123; ++hits; if (i.equals(lastNumber)) &#123; ++cacheHits; factors = lastFactors.clone(); &#125; &#125; if (factors == null) &#123; factors = factor(i); synchronized (this) &#123; lastNumber = i; lastFactors = factors.clone(); &#125; &#125; encodeIntoResponse(resp, factors); &#125; void encodeIntoResponse(ServletResponse resp, BigInteger[] factors) &#123; &#125; BigInteger extractFromRequest(ServletRequest req) &#123; return new BigInteger("7"); &#125; BigInteger[] factor(BigInteger i) &#123; // Doesn't really factor return new BigInteger[]&#123;i&#125;; &#125;&#125;``` 使用volatile发布不可变对象： ```javapackage net.jcip.examples;import java.math.BigInteger;import java.util.*;import net.jcip.annotations.*;@Immutablepublic class OneValueCache &#123; private final BigInteger lastNumber; private final BigInteger[] lastFactors; public OneValueCache(BigInteger i, BigInteger[] factors) &#123; lastNumber = i; lastFactors = Arrays.copyOf(factors, factors.length); &#125; public BigInteger[] getFactors(BigInteger i) &#123; if (lastNumber == null || !lastNumber.equals(i)) return null; else return Arrays.copyOf(lastFactors, lastFactors.length); &#125;&#125;//===============package net.jcip.examples;import java.math.BigInteger;import javax.servlet.*;import net.jcip.annotations.*;@ThreadSafepublic class VolatileCachedFactorizer extends GenericServlet implements Servlet &#123; private volatile OneValueCache cache = new OneValueCache(null, null); public void service(ServletRequest req, ServletResponse resp) &#123; BigInteger i = extractFromRequest(req); BigInteger[] factors = cache.getFactors(i); if (factors == null) &#123; factors = factor(i); cache = new OneValueCache(i, factors); &#125; encodeIntoResponse(resp, factors); &#125; void encodeIntoResponse(ServletResponse resp, BigInteger[] factors) &#123; &#125; BigInteger extractFromRequest(ServletRequest req) &#123; return new BigInteger("7"); &#125; BigInteger[] factor(BigInteger i) &#123; // Doesn't really factor return new BigInteger[]&#123;i&#125;; &#125;&#125;]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[共享对象]]></title>
    <url>%2F2019%2F02%2F22%2F%E5%85%B1%E4%BA%AB%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[可见性在没有同步的情况下，共享变量(错误的做法)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package net.jcip.examples;public class NoVisibility &#123; private static boolean ready; private static int number; private static class ReaderThread extends Thread &#123; public void run() &#123; while (!ready) Thread.yield(); System.out.println(number); &#125; &#125; public static void main(String[] args) &#123; new ReaderThread().start(); number = 42; ready = true; &#125;&#125;``` * 重排序 非线程安全可变整数的访问器： ```javapackage net.jcip.examples;import net.jcip.annotations.*;@NotThreadSafepublic class MutableInteger &#123; private int value; public int get() &#123; return value; &#125; public void set(int value) &#123; this.value = value; &#125;&#125;``` 线程安全可变整数的访问器： ```javapackage net.jcip.examples;import net.jcip.annotations.*;@ThreadSafepublic class SynchronizedInteger &#123; @GuardedBy("this") private int value; public synchronized int get() &#123;//TODO 不仅仅要有setter还要有getter return value; &#125; public synchronized void set(int value) &#123; this.value = value; &#125;&#125; volatile同步的弱形式，仅仅可见，而且对它的操作不会与其他的内存操作一起被重排序，volatile变量不会缓存中寄存器或缓存在其他处理器隐藏的地方，所以读取一个volatile变量，总是会返回有某一个线程所写入的最新值，但是volatile不会阻塞。一个很好的例子，数羊🐑： 1234567891011121314151617181920212223242526272829303132package net.jcip.examples;public class CountingSheep &#123; volatile boolean asleep; void tryToSleep() &#123; while (!asleep) countSomeSheep(); &#125; void countSomeSheep() &#123; // One, two, three... &#125;&#125;``` 2. 发布和溢出 发布对象： ```javapackage net.jcip.examples;class Secrets &#123; public static Set&lt;Secret&gt; knownSecrets; public void initialize() &#123; knownSecrets = new HashSet&lt;Secret&gt;(); &#125;&#125;class Secret &#123;&#125; 允许内部可变的数据溢出(不要这样做) ： 1234567891011package net.jcip.examples;class UnsafeStates &#123; private String[] states = new String[]&#123; "AK", "AL" /*...*/ &#125;; public String[] getStates() &#123; return states; &#125;&#125; 隐式的运行this溢出(不要这样做)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package net.jcip.examples;public class ThisEscape &#123; public ThisEscape(EventSource source) &#123; source.registerListener(new EventListener() &#123; public void onEvent(Event e) &#123; doSomething(e); &#125; &#125;); &#125; void doSomething(Event e) &#123; &#125; interface EventSource &#123; void registerListener(EventListener e); &#125; interface EventListener &#123; void onEvent(Event e); &#125; interface Event &#123; &#125;&#125;``` 使用工厂方法防止this引用在构造期间溢出： ```javapackage net.jcip.examples;public class SafeListener &#123; private final EventListener listener; private SafeListener() &#123; listener = new EventListener() &#123; public void onEvent(Event e) &#123; doSomething(e); &#125; &#125;; &#125; public static SafeListener newInstance(EventSource source) &#123; SafeListener safe = new SafeListener(); source.registerListener(safe.listener); return safe; &#125; void doSomething(Event e) &#123; &#125; interface EventSource &#123; void registerListener(EventListener e); &#125; interface EventListener &#123; void onEvent(Event e); &#125; interface Event &#123; &#125;&#125; 栈限制： package net.jcip.examples; import java.util.*; public class Animals { Ark ark; Species species; Gender gender; public int loadTheArk(Collection&lt;Animal&gt; candidates) { SortedSet&lt;Animal&gt; animals; int numPairs = 0; Animal candidate = null; // animals confined to method, don't let them escape! animals = new TreeSet&lt;Animal&gt;(new SpeciesGenderComparator()); animals.addAll(candidates); for (Animal a : animals) { if (candidate == null || !candidate.isPotentialMate(a)) candidate = a; else { ark.load(new AnimalPair(candidate, a)); ++numPairs; candidate = null; } } return numPairs; } class Animal { Species species; Gender gender; public boolean isPotentialMate(Animal other) { return species == other.species &amp;&amp; gender != other.gender; } } enum Species { AARDVARK, BENGAL_TIGER, CARIBOU, DINGO, ELEPHANT, FROG, GNU, HYENA, IGUANA, JAGUAR, KIWI, LEOPARD, MASTADON, NEWT, OCTOPUS, PIRANHA, QUETZAL, RHINOCEROS, SALAMANDER, THREE_TOED_SLOTH, UNICORN, VIPER, WEREWOLF, XANTHUS_HUMMINBIRD, YAK, ZEBRA } enum Gender { MALE, FEMALE } class AnimalPair { private final Animal one, two; public AnimalPair(Animal one, Animal two) { this.one = one; this.two = two; } } class SpeciesGenderComparator implements Comparator&lt;Animal&gt; { public int compare(Animal one, Animal two) { int speciesCompare = one.species.compareTo(two.species); return (speciesCompare != 0) ? speciesCompare : one.gender.compareTo(two.gender); } } class Ark { private final Set&lt;AnimalPair&gt; loadedAnimals = new HashSet&lt;AnimalPair&gt;(); public void load(AnimalPair pair) { loadedAnimals.add(pair); } } } ThreadLocal: package net.jcip.examples; import java.sql.Connection; import java.sql.DriverManager; import java.sql.SQLException; /** * ConnectionDispenser * &lt;p/&gt; * Using ThreadLocal to ensure thread confinement * * @author Brian Goetz and Tim Peierls */ public class ConnectionDispenser { static String DB_URL = "jdbc:mysql://localhost/mydatabase"; private ThreadLocal&lt;Connection&gt; connectionHolder = new ThreadLocal&lt;Connection&gt;() { public Connection initialValue() { try { return DriverManager.getConnection(DB_URL); } catch (SQLException e) { throw new RuntimeException("Unable to acquire Connection, e"); } }; }; public Connection getConnection() { return connectionHolder.get(); } } 不可变性 package net.jcip.examples; import java.util.*; import net.jcip.annotations.*; @Immutable public final class ThreeStooges { private final Set&lt;String&gt; stooges = new HashSet&lt;String&gt;(); public ThreeStooges() { stooges.add("Moe"); stooges.add("Larry"); stooges.add("Curly"); } public boolean isStooge(String name) { return stooges.contains(name); } public String getStoogeNames() { List&lt;String&gt; stooges = new Vector&lt;String&gt;(); stooges.add("Moe"); stooges.add("Larry"); stooges.add("Curly"); return stooges.toString(); } } 安全的发布对象：1、 通过静态初始化器初始化对象的引用。2、将它的引用存储到volatile域或AtomicReference。3、将它的引用存储到正确创建的对象的final域中(创建期间没有发生 this 引用的溢出)。4、将它的引用存储到由锁正确保护的域中。线程安全容器内的同步，意味着将对象放入这些容器，就保证了安全。]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[同步集合类的应用]]></title>
    <url>%2F2019%2F02%2F22%2F%E5%90%8C%E6%AD%A5%E9%9B%86%E5%90%88%E7%B1%BB%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[使用Concurrentmap代替HashmapCollections.synchronizedMap(null);1234567891011121314151617181920212223242526package cn.itcast.heima2;import java.util.ArrayList;import java.util.Collection;import java.util.Iterator;import java.util.concurrent.CopyOnWriteArrayList;public class CollectionModifyExceptionTest &#123; public static void main(String[] args) &#123; Collection users = new CopyOnWriteArrayList();//如果使用ArrayList,在遍历过程中不能使用remove //new ArrayList(); users.add(new User(&quot;张三&quot;,28)); users.add(new User(&quot;李四&quot;,25)); users.add(new User(&quot;王五&quot;,31)); Iterator itrUsers = users.iterator(); while(itrUsers.hasNext())&#123; System.out.println(&quot;aaaa&quot;); User user = (User)itrUsers.next(); if(&quot;张三&quot;.equals(user.getName()))&#123; users.remove(user); //itrUsers.remove(); &#125; else &#123; System.out.println(user); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阻塞队列]]></title>
    <url>%2F2019%2F02%2F22%2F%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[阻塞队列的应用12345678910111213141516171819202122232425262728293031323334353637383940414243444546package cn.itcast.heima2;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;public class BlockingQueueTest &#123; public static void main(String[] args) &#123; final BlockingQueue queue = new ArrayBlockingQueue(3); for(int i=0;i&lt;2;i++)&#123; new Thread()&#123; public void run()&#123; while(true)&#123; try &#123; Thread.sleep((long)(Math.random()*1000)); System.out.println(Thread.currentThread().getName() + &quot;准备放数据&quot;); queue.put(1); System.out.println(Thread.currentThread().getName() + &quot;已经放了数据,&quot; + &quot;队列目前有&quot; + queue.size() + &quot;个数据&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;.start(); &#125; new Thread()&#123; public void run()&#123; while(true)&#123; try &#123; //此处改为1000和100分别观察结果 Thread.sleep(1000); System.out.println(Thread.currentThread().getName() + &quot;准备取数据&quot;); queue.take(); System.out.println(Thread.currentThread().getName() + &quot;已经取走数据,&quot; + &quot;目前队列有&quot; + queue.size() + &quot;个数据&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;.start(); &#125;&#125; 两个阻塞队列实现同步通知的功能123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package cn.itcast.heima2;import java.util.Collections;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.atomic.AtomicInteger;public class BlockingQueueCommunication &#123; /** * 两个阻塞队列实现同步通知的功能 * @param args */ public static void main(String[] args) &#123; final Business business = new Business(); new Thread( new Runnable() &#123; @Override public void run() &#123; for(int i=1;i&lt;=50;i++)&#123; business.sub(i); &#125; &#125; &#125; ).start(); for(int i=1;i&lt;=50;i++)&#123; business.main(i); &#125; &#125; static class Business &#123; BlockingQueue&lt;Integer&gt; queue1 = new ArrayBlockingQueue&lt;Integer&gt;(1); BlockingQueue&lt;Integer&gt; queue2 = new ArrayBlockingQueue&lt;Integer&gt;(1); &#123; try &#123; System.out.println(&quot;xxxxxdfsdsafdsa&quot;); queue2.put(1); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; public void sub(int i)&#123; try &#123; queue1.put(1); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; for(int j=1;j&lt;=10;j++)&#123; System.out.println(&quot;sub thread sequece of &quot; + j + &quot;,loop of &quot; + i); &#125; try &#123; queue2.take(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; public void main(int i)&#123; try &#123; queue2.put(1); &#125; catch (InterruptedException e1) &#123; // TODO Auto-generated catch block e1.printStackTrace(); &#125; for(int j=1;j&lt;=100;j++)&#123; System.out.println(&quot;main thread sequece of &quot; + j + &quot;,loop of &quot; + i); &#125; try &#123; queue1.take(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程同步工具]]></title>
    <url>%2F2019%2F02%2F22%2F%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[Semaphere同步工具1234567891011121314151617181920212223242526272829303132333435363738package cn.itcast.heima2;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Semaphore;public class SemaphoreTest &#123; public static void main(String[] args) &#123; ExecutorService service = Executors.newCachedThreadPool(); final Semaphore sp = new Semaphore(3); for(int i=0;i&lt;10;i++)&#123; Runnable runnable = new Runnable()&#123; public void run()&#123; try &#123; sp.acquire(); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;进入,当前已有&quot; + (3-sp.availablePermits()) + &quot;进入&quot;); try &#123; Thread.sleep((long)(Math.random()*10000)); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;即将离开&quot;); sp.release(); //todo System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;已离开,当前已有&quot; + (3-sp.availablePermits()) + &quot;进入&quot;); &#125; &#125;; service.execute(runnable); &#125; &#125;&#125; Semaphore保证了,我管理的那部分代码同一时刻最多可以有n个线程访问Semaphore可以维护当前访问自身的线程个数，并提 供了同步机制。使用Semaphore可以控制同时访问资源的线程个数，例如，实现一个文件允许的并发访问数。Semaphore实现的功能就类似厕所有5个坑，假如有十个人要上厕所，那么同时能有多少个人去上厕所呢？同时只能有5 个人能够占用，当5个人中的任何一个人让开后，其中在等待的另外5个人中又有一个可以占用了。 另外等待的5个人中可以是随机获得优先机会，也可以是按照先来后到的顺序获得机会，这取决于构造Semaphore对象时 传入的参数选项。单个信号量的Semaphore对象可以实现互斥锁的功能， 并且可以是由一个线程获得了“锁”，再由另一个线程释放“锁”，这可应用于死锁恢复的一些场合。 CyclicBarrier同步工具12345678910111213141516171819202122232425262728293031323334353637package cn.itcast.heima2;import java.util.concurrent.CyclicBarrier;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class CyclicBarrierTest &#123; public static void main(String[] args) &#123; ExecutorService service = Executors.newCachedThreadPool(); final CyclicBarrier cb = new CyclicBarrier(3);//todo for(int i=0;i&lt;3;i++)&#123; Runnable runnable = new Runnable()&#123; public void run()&#123; try &#123; Thread.sleep((long)(Math.random()*10000)); System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;即将到达集合地点1,当前已有&quot; + (cb.getNumberWaiting()+1) + &quot;已经到达,&quot; + (cb.getNumberWaiting()==2?&quot;都到齐了,继续走啊&quot;:&quot;正在等候&quot;)); cb.await();//todo 每次都等待,如果其他也全部到达,此时cb为3,则所有线程进行下一步 Thread.sleep((long)(Math.random()*10000)); System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;即将到达集合地点2,当前已有&quot; + (cb.getNumberWaiting()+1) + &quot;已经到达,&quot; + (cb.getNumberWaiting()==2?&quot;都到齐了,继续走啊&quot;:&quot;正在等候&quot;)); cb.await(); Thread.sleep((long)(Math.random()*10000)); System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;即将到达集合地点3,当前已有&quot; + (cb.getNumberWaiting()+1) + &quot;已经到达,&quot; + (cb.getNumberWaiting()==2?&quot;都到齐了,继续走啊&quot;:&quot;正在等候&quot;)); cb.await(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;; service.execute(runnable); &#125; service.shutdown(); &#125;&#125; 字面意思回环栅栏，通过它可以实现让一组线程等待至某个状态之后再全部同时执行。叫做回环是因为当所有等待线程都被释放以后，CyclicBarrier可以被重用。我们暂且把这个状态就叫做barrier，当调用await()方法之后，线程就处于barrier了。 CountDownLatch同步工具1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package cn.itcast.heima2;import java.util.concurrent.CountDownLatch;import java.util.concurrent.CyclicBarrier;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class CountdownLatchTest &#123; public static void main(String[] args) &#123; ExecutorService service = Executors.newCachedThreadPool(); final CountDownLatch cdOrder = new CountDownLatch(1); final CountDownLatch cdAnswer = new CountDownLatch(3); for(int i=0;i&lt;3;i++)&#123; Runnable runnable = new Runnable()&#123; public void run()&#123; try &#123; System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;正准备接受命令&quot;); cdOrder.await(); System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;已接受命令&quot;); Thread.sleep((long)(Math.random()*10000)); System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;回应命令处理结果&quot;); cdAnswer.countDown(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;; service.execute(runnable); &#125; try &#123; Thread.sleep((long)(Math.random()*10000)); System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;即将发布命令&quot;); cdOrder.countDown(); System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;已发布命令,正在等待结果&quot;); cdAnswer.await(); System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;已收到所有响应结果&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; service.shutdown(); &#125;&#125; 利用它可以实现类似计数器的功能。比如有一个任务A，它要等待其他4个任务执行完毕之后才能执行，此时就可以利用CountDownLatch来实现这种功能了。 Exchanger同步工具1234567891011121314151617181920212223242526272829303132333435363738394041424344package cn.itcast.heima2;import java.util.concurrent.Exchanger;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class ExchangerTest &#123; public static void main(String[] args) &#123; ExecutorService service = Executors.newCachedThreadPool(); final Exchanger exchanger = new Exchanger(); service.execute(new Runnable()&#123; public void run() &#123; try &#123; String data1 = &quot;zxx&quot;; System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;正在准备把&quot; + data1 +&quot;换出去&quot;); Thread.sleep((long)(Math.random()*10000)); String data2 = (String)exchanger.exchange(data1); System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;换回的数据为&quot; + data2); &#125;catch(Exception e)&#123; &#125; &#125; &#125;); service.execute(new Runnable()&#123; public void run() &#123; try &#123; String data1 = &quot;lhm&quot;; System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;正在准备把&quot; + data1 +&quot;换出去&quot;); Thread.sleep((long)(Math.random()*10000)); String data2 = (String)exchanger.exchange(data1); System.out.println(&quot;线程&quot; + Thread.currentThread().getName() + &quot;换回的数据为&quot; + data2); &#125;catch(Exception e)&#123; &#125; &#125; &#125;); &#125;&#125; 参考：https://www.cnblogs.com/itermis/p/9025148.html]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[condition]]></title>
    <url>%2F2019%2F02%2F22%2Fcondition%2F</url>
    <content type="text"><![CDATA[锁只能互斥，但是不能通信。condition就是解决该问题的，类似于wait()和notify()。 一个condition1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package cn.itcast.heima2;import java.util.concurrent.atomic.AtomicInteger;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class ConditionCommunication &#123; /** * @param args */ public static void main(String[] args) &#123; final Business business = new Business(); new Thread( new Runnable() &#123; @Override public void run() &#123; for(int i=1;i&lt;=50;i++)&#123; business.sub(i); &#125; &#125; &#125; ).start(); for(int i=1;i&lt;=50;i++)&#123; business.main(i); &#125; &#125; static class Business &#123; Lock lock = new ReentrantLock(); Condition condition = lock.newCondition();//todo private boolean bShouldSub = true; public void sub(int i)&#123; lock.lock(); try&#123; while(!bShouldSub)&#123; try &#123; condition.await();//todo &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; for(int j=1;j&lt;=10;j++)&#123; System.out.println(&quot;sub thread sequence of &quot; + j + &quot;,loop of &quot; + i); &#125; bShouldSub = false; condition.signal();//todo &#125;finally&#123; lock.unlock(); &#125; &#125; public void main(int i)&#123; lock.lock(); try&#123; while(bShouldSub)&#123;//todo while避免假醒 try &#123; condition.await();//todo &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; for(int j=1;j&lt;=100;j++)&#123; System.out.println(&quot;main thread sequence of &quot; + j + &quot;,loop of &quot; + i); &#125; bShouldSub = true; condition.signal();//todo &#125;finally&#123; lock.unlock(); &#125; &#125; &#125;&#125; 两个condition12345678910111213141516171819202122232425262728293031323334353637383940class BoundedBuffer &#123; final Lock lock = new ReentrantLock(); final Condition notEmpty = lock.newCondition(); final Condition notFull = lock.newCondition(); final int MAX_SIZE = 5; List&lt;Object&gt; list = new ArrayList&lt;Object&gt;(); int count = 0; public void put(Object obj) throws InterruptedException &#123; lock.lock(); try &#123; while (count == MAX_SIZE) &#123; notFull.await(); &#125; list.add(obj); count++; System.out.println(&quot;call: notEmpty.signal()&quot;); notEmpty.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; public Object take() throws InterruptedException &#123; lock.lock(); try &#123; while (count == 0) &#123; notEmpty.await(); &#125; Object obj = list.remove(0); count --; System.out.println(&quot;call: notFull.signal()&quot;); notFull.signal(); return obj; &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; 三个condition123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120package cn.itcast.heima2;import java.util.concurrent.atomic.AtomicInteger;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class ThreeConditionCommunication &#123; /** * @param args */ public static void main(String[] args) &#123; final Business business = new Business(); new Thread( new Runnable() &#123; @Override public void run() &#123; for(int i=1;i&lt;=50;i++)&#123; business.sub2(i); &#125; &#125; &#125; ).start(); new Thread( new Runnable() &#123; @Override public void run() &#123; for(int i=1;i&lt;=50;i++)&#123; business.sub3(i); &#125; &#125; &#125; ).start(); for(int i=1;i&lt;=50;i++)&#123; business.main(i); &#125; &#125; static class Business &#123; Lock lock = new ReentrantLock(); Condition condition1 = lock.newCondition(); Condition condition2 = lock.newCondition(); Condition condition3 = lock.newCondition(); private int shouldSub = 1; public void sub2(int i)&#123; lock.lock(); try&#123; while(shouldSub != 2)&#123; try &#123; condition2.await(); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; for(int j=1;j&lt;=10;j++)&#123; System.out.println(&quot;sub2 thread sequence of &quot; + j + &quot;,loop of &quot; + i); &#125; shouldSub = 3; condition3.signal(); &#125;finally&#123; lock.unlock(); &#125; &#125; public void sub3(int i)&#123; lock.lock(); try&#123; while(shouldSub != 3)&#123; try &#123; condition3.await(); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; for(int j=1;j&lt;=20;j++)&#123; System.out.println(&quot;sub3 thread sequence of &quot; + j + &quot;,loop of &quot; + i); &#125; shouldSub = 1; condition1.signal(); &#125;finally&#123; lock.unlock(); &#125; &#125; public void main(int i)&#123; lock.lock(); try&#123; while(shouldSub != 1)&#123; try &#123; condition1.await(); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; for(int j=1;j&lt;=100;j++)&#123; System.out.println(&quot;main thread sequence of &quot; + j + &quot;,loop of &quot; + i); &#125; shouldSub = 2; condition2.signal(); &#125;finally&#123; lock.unlock(); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lock]]></title>
    <url>%2F2019%2F02%2F22%2FLock%2F</url>
    <content type="text"><![CDATA[Lock1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package cn.itcast.heima2;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;public class LockTest &#123; /** * @param args */ public static void main(String[] args) &#123; new LockTest().init(); &#125; private void init()&#123; final Outputer outputer = new Outputer(); new Thread(new Runnable()&#123; @Override public void run() &#123; while(true)&#123; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; outputer.output(&quot;zhangxiaoxiang&quot;); &#125; &#125; &#125;).start(); new Thread(new Runnable()&#123; @Override public void run() &#123; while(true)&#123; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; outputer.output(&quot;lihuoming&quot;); &#125; &#125; &#125;).start(); &#125; static class Outputer&#123; Lock lock = new ReentrantLock();//TODO 锁 public void output(String name)&#123; int len = name.length(); lock.lock(); try&#123; for(int i=0;i&lt;len;i++)&#123; System.out.print(name.charAt(i)); &#125; System.out.println(); &#125;finally&#123; //todo 保证正常或者非正常退出,都会释放锁 lock.unlock(); &#125; &#125; &#125;&#125; 读写锁Lock比Synchronized方式更加面向对象，而且锁本身也是一个对象，两个线程要实现同步互斥的效果，他们必须使用同一个lock对象读写锁：多个读锁不互斥，读锁和写锁互斥，写锁和写锁互斥。这是由jvm控制的，你只要上相应的锁即可。如果你的代码只读数据，可以很多人读，但不能同时写那就上读锁；如果你的代码修改数据，只能有一个人在写，且不能同时读取，那就上写锁。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package cn.itcast.heima2;import java.util.Random;import java.util.concurrent.locks.ReadWriteLock;import java.util.concurrent.locks.ReentrantReadWriteLock;public class ReadWriteLockTest &#123; public static void main(String[] args) &#123; final Queue3 q3 = new Queue3(); for(int i=0;i&lt;3;i++) &#123; new Thread()&#123; public void run()&#123; while(true)&#123; q3.get(); &#125; &#125; &#125;.start(); new Thread()&#123; public void run()&#123; while(true)&#123; q3.put(new Random().nextInt(10000)); &#125; &#125; &#125;.start(); &#125; &#125;&#125;class Queue3&#123; private Object data = null;//共享数据,只有一个线程能写该数据,但是多个线程能同时读 ReadWriteLock rwl = new ReentrantReadWriteLock(); //todo 不能上普通的锁,否则读和写都会互斥 public void get()&#123; rwl.readLock().lock();//todo try &#123; System.out.println(Thread.currentThread().getName() + &quot; be ready to read data!&quot;); Thread.sleep((long)(Math.random()*1000)); System.out.println(Thread.currentThread().getName() + &quot;have read data :&quot; + data); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally&#123; rwl.readLock().unlock(); //todo &#125; &#125; public void put(Object data)&#123; rwl.writeLock().lock(); //todo try &#123; System.out.println(Thread.currentThread().getName() + &quot; be ready to write data!&quot;); Thread.sleep((long)(Math.random()*1000)); this.data = data; System.out.println(Thread.currentThread().getName() + &quot; have write data: &quot; + data); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally&#123; rwl.writeLock().unlock(); //todo &#125; &#125;&#125; 读写锁缓存demo123456789101112131415161718192021222324252627282930313233343536373839package cn.itcast.heima2;import java.util.HashMap;import java.util.Map;import java.util.concurrent.locks.ReadWriteLock;import java.util.concurrent.locks.ReentrantReadWriteLock;public class CacheDemo &#123; private Map&lt;String, Object&gt; cache = new HashMap&lt;String, Object&gt;(); public static void main(String[] args) &#123; // TODO Auto-generated method stub &#125; private ReadWriteLock rwl = new ReentrantReadWriteLock(); public Object getData(String key)&#123; //可以用synchronized但是不推荐 rwl.readLock().lock(); Object value = null; try&#123; value = cache.get(key); if(value == null)&#123; rwl.readLock().unlock(); rwl.writeLock().lock(); try&#123; if(value==null)&#123; value = &quot;aaaa&quot;;//实际去queryDB &#125; &#125;finally&#123; rwl.writeLock().unlock(); &#125; rwl.readLock().lock(); &#125; &#125;finally&#123; rwl.readLock().unlock(); &#125; return value; &#125;&#125;]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Callable与Future的应用]]></title>
    <url>%2F2019%2F02%2F22%2FCallable%E4%B8%8EFuture%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package cn.itcast.heima2;import java.util.Random;import java.util.concurrent.Callable;import java.util.concurrent.CompletionService;import java.util.concurrent.ExecutionException;import java.util.concurrent.ExecutorCompletionService;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Future;import java.util.concurrent.TimeUnit;public class CallableAndFuture &#123; /** * @param args */ public static void main(String[] args) &#123; ExecutorService threadPool = Executors.newSingleThreadExecutor(); Future&lt;String&gt; future = threadPool.submit( new Callable&lt;String&gt;() &#123;//todo public String call() throws Exception &#123; Thread.sleep(2000); return &quot;hello&quot;; &#125;; &#125; ); System.out.println(&quot;等待结果:&quot;); try &#123; System.out.println(&quot;拿到结果:&quot; + future.get());//阻塞 &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; ExecutorService threadPool2 = Executors.newFixedThreadPool(10); CompletionService&lt;Integer&gt; completionService = new ExecutorCompletionService&lt;Integer&gt;(threadPool2); for(int i=1;i&lt;=10;i++)&#123; final int seq = i; completionService.submit(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; Thread.sleep(new Random().nextInt(5000)); return seq; &#125; &#125;); &#125; for(int i=0;i&lt;10;i++)&#123; try &#123; System.out.println(completionService.take().get());//非阻塞 &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (ExecutionException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; &#125;]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池]]></title>
    <url>%2F2019%2F02%2F22%2F%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package cn.itcast.heima2;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;public class ThreadPoolTest &#123; /** * @param args */ public static void main(String[] args) &#123;// ExecutorService threadPool = Executors.newFixedThreadPool(3); ExecutorService threadPool = Executors.newCachedThreadPool();// ExecutorService threadPool = Executors.newSingleThreadExecutor();//todo 线程死掉以后怎么重新启动,只能找一个替补方案 for(int i=1;i&lt;=10;i++)&#123; final int task = i; threadPool.execute(new Runnable()&#123; @Override public void run() &#123; for(int j=1;j&lt;=10;j++)&#123; try &#123; Thread.sleep(20); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot; is looping of &quot; + j + &quot; for task of &quot; + task); &#125; &#125; &#125;); &#125; System.out.println(&quot;all of 10 tasks have committed! &quot;); //threadPool.shutdownNow(); Executors.newScheduledThreadPool(3).scheduleAtFixedRate( new Runnable()&#123; @Override public void run() &#123; System.out.println(&quot;bombing!&quot;); &#125;&#125;, 6, 2, TimeUnit.SECONDS);//6s后炸,以后每2s炸 &#125;&#125;]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多个线程共享数据]]></title>
    <url>%2F2019%2F02%2F22%2F%E5%A4%9A%E4%B8%AA%E7%BA%BF%E7%A8%8B%E5%85%B1%E4%BA%AB%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package cn.itcast.heima2;public class MultiThreadShareData &#123; /** * 多个线程共享数据 */ private static ShareData1 data1 = new ShareData1(); public static void main(String[] args) &#123; ShareData1 data2 = new ShareData1(); new Thread(new MyRunnable1(data2)).start(); new Thread(new MyRunnable2(data2)).start(); final ShareData1 data1 = new ShareData1(); new Thread(new Runnable()&#123; @Override public void run() &#123; data1.decrement(); &#125; &#125;).start(); new Thread(new Runnable()&#123; @Override public void run() &#123; data1.increment(); &#125; &#125;).start(); &#125;&#125; class MyRunnable1 implements Runnable&#123; private ShareData1 data1; public MyRunnable1(ShareData1 data1)&#123; this.data1 = data1; &#125; public void run() &#123; data1.decrement(); &#125; &#125; class MyRunnable2 implements Runnable&#123; private ShareData1 data1; public MyRunnable2(ShareData1 data1)&#123; this.data1 = data1; &#125; public void run() &#123; data1.increment(); &#125; &#125; class ShareData1 /*implements Runnable*/&#123;/* private int count = 100; @Override public void run() &#123; // TODO Auto-generated method stub while(true)&#123; count--; &#125; &#125;*/ private int j = 0; public synchronized void increment()&#123; j++; &#125; public synchronized void decrement()&#123; j--; &#125; &#125; 总结一个ThreadLocal只能定义一个变量，多个变量需要多少ThreadLocal,也可将多个变量定义为一个实体，然后定义一个存放该实体的ThreadLocal；将ThreadLocal封装到一个类中(构建一个线程范围内的对象)，这样的代码比较优雅。；每个线程结束的时候ThreadLocal会自动收集，除非被其他地方引用。 参考：http://ifeve.com/java-theadlocal/]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThreadLocal]]></title>
    <url>%2F2019%2F02%2F22%2FThreadLocal%2F</url>
    <content type="text"><![CDATA[ThreadLocal前身1234567891011121314151617181920212223242526272829303132333435363738394041package cn.itcast.heima2;import java.util.HashMap;import java.util.Map;import java.util.Random;public class ThreadScopeShareData &#123; private static Map&lt;Thread, Integer&gt; threadData = new HashMap&lt;Thread, Integer&gt;();//todo 共享全局实例 public static void main(String[] args) &#123; for(int i=0;i&lt;2;i++)&#123; new Thread(new Runnable()&#123; @Override public void run() &#123; int data = new Random().nextInt(); System.out.println(Thread.currentThread().getName() + &quot; has put data :&quot; + data); threadData.put(Thread.currentThread(), data); new A().get(); new B().get(); &#125; &#125;).start(); &#125; &#125; static class A&#123; public void get()&#123; int data = threadData.get(Thread.currentThread()); System.out.println(&quot;A from &quot; + Thread.currentThread().getName() + &quot; get data :&quot; + data); &#125; &#125; static class B&#123; public void get()&#123; int data = threadData.get(Thread.currentThread()); System.out.println(&quot;B from &quot; + Thread.currentThread().getName() + &quot; get data :&quot; + data); &#125; &#125;&#125; ThreadLocal正确用法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package cn.itcast.heima2;import java.util.HashMap;import java.util.Map;import java.util.Random;public class ThreadLocalTest &#123; private static ThreadLocal&lt;Integer&gt; x = new ThreadLocal&lt;Integer&gt;();//todo 共享的 private static ThreadLocal&lt;MyThreadScopeData&gt; myThreadScopeData = new ThreadLocal&lt;MyThreadScopeData&gt;(); public static void main(String[] args) &#123; for(int i=0;i&lt;2;i++)&#123; new Thread(new Runnable()&#123; @Override public void run() &#123; int data = new Random().nextInt(); System.out.println(Thread.currentThread().getName() + &quot; has put data :&quot; + data); x.set(data);/* MyThreadScopeData myData = new MyThreadScopeData(); myData.setName(&quot;name&quot; + data); myData.setAge(data); myThreadScopeData.set(myData);*/ MyThreadScopeData.getThreadInstance().setName(&quot;name&quot; + data); MyThreadScopeData.getThreadInstance().setAge(data); new A().get(); new B().get(); &#125; &#125;).start(); &#125; &#125; static class A&#123; public void get()&#123; int data = x.get(); System.out.println(&quot;A from &quot; + Thread.currentThread().getName() + &quot; get data :&quot; + data);/* MyThreadScopeData myData = myThreadScopeData.get();; System.out.println(&quot;A from &quot; + Thread.currentThread().getName() + &quot; getMyData: &quot; + myData.getName() + &quot;,&quot; + myData.getAge());*/ MyThreadScopeData myData = MyThreadScopeData.getThreadInstance(); System.out.println(&quot;A from &quot; + Thread.currentThread().getName() + &quot; getMyData: &quot; + myData.getName() + &quot;,&quot; + myData.getAge()); &#125; &#125; static class B&#123; public void get()&#123; int data = x.get(); System.out.println(&quot;B from &quot; + Thread.currentThread().getName() + &quot; get data :&quot; + data); MyThreadScopeData myData = MyThreadScopeData.getThreadInstance(); System.out.println(&quot;B from &quot; + Thread.currentThread().getName() + &quot; getMyData: &quot; + myData.getName() + &quot;,&quot; + myData.getAge()); &#125; &#125;&#125;class MyThreadScopeData&#123; private MyThreadScopeData()&#123;&#125; //private static MyThreadScopeData instance = null;//new MyThreadScopeData(); private static ThreadLocal&lt;MyThreadScopeData&gt; map = new ThreadLocal&lt;MyThreadScopeData&gt;(); public static /*synchronized*/ MyThreadScopeData getThreadInstance()&#123; MyThreadScopeData instance = map.get(); if(instance == null)&#123;// synchronized(MyThreadScopeData.class)&#123; //因为map已经是饿汉模式，所以此处不需要再双重检索// if (instance == null)&#123;// instance = new MyThreadScopeData();// map.set(instance);// &#125;// &#125; instance = new MyThreadScopeData(); map.set(instance); &#125; return instance; &#125; private String name; private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125;]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[传统wait()和notify()通信技术]]></title>
    <url>%2F2019%2F02%2F22%2F%E4%BC%A0%E7%BB%9Fwait-%E5%92%8Cnotify-%E9%80%9A%E4%BF%A1%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package cn.itcast.heima2;public class TraditionalThreadCommunication &#123; /** * 子线程10次,主线程100次,交替执行50次 * @param args */ public static void main(String[] args) &#123; final Business business = new Business();//todo 共享实例 new Thread( new Runnable() &#123; @Override public void run() &#123; for(int i=1;i&lt;=50;i++)&#123; business.sub(i); &#125; &#125; &#125; ).start(); for(int i=1;i&lt;=50;i++)&#123; business.main(i); &#125; &#125;&#125; class Business &#123; private boolean bShouldSub = true;//todo 共享变量 public synchronized void sub(int i)&#123;//synchronized while(!bShouldSub)&#123;//非 try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; for(int j=1;j&lt;=10;j++)&#123; System.out.println(&quot;sub thread sequence of &quot; + j + &quot;,loop of &quot; + i); &#125; bShouldSub = false; this.notify(); &#125; public synchronized void main(int i)&#123;//synchronized while(bShouldSub)&#123;//是 try &#123; this.wait(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; for(int j=1;j&lt;=100;j++)&#123; System.out.println(&quot;main thread sequence of &quot; + j + &quot;,loop of &quot; + i); &#125; bShouldSub = true; this.notify(); &#125; &#125;]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[传统synchronized互斥技术]]></title>
    <url>%2F2019%2F02%2F22%2F%E4%BC%A0%E7%BB%9Fsynchronized%E4%BA%92%E6%96%A5%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package cn.itcast.heima2;public class TraditionalThreadSynchronized &#123; /** * @param args */ public static void main(String[] args) &#123; new TraditionalThreadSynchronized().init(); &#125; private void init()&#123;//todo 如果在不使用outputer时,是线程不安全的,打印时会出现串行的情况 final Outputer outputer = new Outputer(); new Thread(new Runnable()&#123; @Override public void run() &#123; while(true)&#123; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; outputer.output(&quot;zhangxiaoxiang&quot;);//todo 与下面的比较 &#125; &#125; &#125;).start(); new Thread(new Runnable()&#123; @Override public void run() &#123; while(true)&#123; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; outputer.output2(&quot;lihuoming&quot;);//todo 与上面的比较 &#125; &#125; &#125;).start(); &#125; static class Outputer&#123; public void output(String name)&#123; int len = name.length(); synchronized (this) //todo 代码块的锁闩为Outputer.class时可以和方法3互斥,在this的情况下只能和方法2互斥 &#123; for(int i=0;i&lt;len;i++)&#123; System.out.print(name.charAt(i)); &#125; System.out.println(); &#125; &#125; public synchronized void output2(String name)&#123; //todo 方法 int len = name.length(); for(int i=0;i&lt;len;i++)&#123; System.out.print(name.charAt(i)); &#125; System.out.println(); &#125; public static synchronized void output3(String name)&#123; //todo 静态方法 int len = name.length(); for(int i=0;i&lt;len;i++)&#123; System.out.print(name.charAt(i)); &#125; System.out.println(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[传统定时器]]></title>
    <url>%2F2019%2F02%2F22%2F%E4%BC%A0%E7%BB%9F%E5%AE%9A%E6%97%B6%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package cn.itcast.heima2;import java.util.Date;import java.util.Timer;import java.util.TimerTask;public class TraditionalTimerTest &#123; private static int count = 0; public static void main(String[] args) &#123;/* new Timer().schedule(new TimerTask() &#123;//todo 第一次炸是10s后,以后每隔3s炸一次 @Override public void run() &#123; System.out.println(&quot;bombing!&quot;); &#125; &#125;, 10000,3000);*/ class MyTimerTask extends TimerTask&#123; @Override public void run() &#123; count = (count+1)%2; System.out.println(&quot;bombing!&quot;); new Timer().schedule(/*new TimerTask() &#123; @Override public void run() &#123; System.out.println(&quot;bombing!&quot;); &#125; &#125;*/new MyTimerTask(),2000+2000*count); &#125; &#125; new Timer().schedule(new MyTimerTask(), 2000);//间隔2s和4s炸 while(true)&#123; System.out.println(new Date().getSeconds()); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[两种线程创建方式的比较]]></title>
    <url>%2F2019%2F02%2F22%2F%E4%B8%A4%E7%A7%8D%E7%BA%BF%E7%A8%8B%E5%88%9B%E5%BB%BA%E6%96%B9%E5%BC%8F%E7%9A%84%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package cn.itcast.heima2;public class TraditionalThread &#123; /** * @param args */ public static void main(String[] args) &#123; Thread thread = new Thread()&#123;//todo Thread子类 @Override public void run() &#123; while(true)&#123; try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;1:&quot; + Thread.currentThread().getName()); System.out.println(&quot;2:&quot; + this.getName()); &#125; &#125; &#125;; thread.start(); Thread thread2 = new Thread(new Runnable()&#123;//todo 传入Runnable @Override public void run() &#123; while(true)&#123; try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;1:&quot; + Thread.currentThread().getName()); &#125; &#125; &#125;); thread2.start(); new Thread(//todo 会使用Thread子类的方式执行 new Runnable()&#123; public void run() &#123; while(true)&#123; try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;runnable :&quot; + Thread.currentThread().getName());//不会被执行 &#125; &#125; &#125; )&#123; public void run() &#123; while(true)&#123; try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;thread :&quot; + Thread.currentThread().getName()); &#125; &#125; &#125;.start(); &#125;&#125; 多线程一定会提高程序效率吗？https://blog.csdn.net/tigerjin/article/details/74142304]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内存数据库]]></title>
    <url>%2F2019%2F02%2F22%2F%E5%86%85%E5%AD%98%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[derby123456789101112131415161718192021222324252627282930313233343536import java.sql.Connection;import java.sql.DriverManager;import java.sql.ResultSet;import java.sql.Statement;/** * Created by lifei on 16/7/14. */public class Derby &#123; public static void main(String[] args) &#123; try &#123; Class.forName("org.apache.derby.jdbc.EmbeddedDriver").newInstance(); Connection conn = DriverManager.getConnection("jdbc:derby:memory:myDB;create=true"); Statement st = conn.createStatement(); st.execute("create table test2 (id int, name varchar(20))"); st.execute("insert into test2 values(1,'sinboy')"); st.execute("insert into test2 values(2,'Tom')"); ResultSet rs = st.executeQuery("select * from test2");// ResultSet rs = st.executeQuery("SELECT * from test2 where name REGEXP '^T'");//不支持 while (rs.next()) &#123; System.out.println("id:" + rs.getInt(1) + " name:" + rs.getString(2)); &#125; rs.close(); st.close(); conn.commit(); conn.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; sqllite1234567891011121314151617181920212223242526272829303132333435363738394041package com.qnr.parsesql;import java.sql.*;/** * Created by lifei on 16/7/14. */public class Sqllite &#123; public static void main(String[] args) throws Exception&#123; Class.forName("org.sqlite.JDBC"); Connection conn = DriverManager.getConnection("jdbc:sqlite::memory:"); Statement stat = conn.createStatement(); stat.executeUpdate("drop table if exists people;"); stat.executeUpdate("create table people (name, occupation);"); PreparedStatement prep = conn.prepareStatement( "insert into people values (?, ?);"); prep.setString(1, "Gandhi"); prep.setString(2, "politics"); prep.addBatch(); prep.setString(1, "Turing"); prep.setString(2, "computers"); prep.addBatch(); prep.setString(1, "Wittgenstein"); prep.setString(2, "smartypants"); prep.addBatch(); conn.setAutoCommit(false); prep.executeBatch(); conn.setAutoCommit(true);// ResultSet rs = stat.executeQuery("select * from people;"); ResultSet rs = stat.executeQuery("select * from people where name REGEXP '^G'"); //不支持 while (rs.next()) &#123; System.out.println("name = " + rs.getString("name")); System.out.println("job = " + rs.getString("occupation")); &#125; rs.close(); conn.close(); &#125;&#125; h2123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import java.sql.Connection;import java.sql.DriverManager;import java.sql.ResultSet;import java.sql.Statement;import java.util.UUID;/** * &lt;p&gt;ClassName: H2ConnTest1&lt;p&gt; * &lt;p&gt;Description: Java通过JDBC方式连接H2数据库&lt;p&gt; * [[@author](http://my.oschina.net/arthor)](http://my.oschina.net/arthor) xudp * [[@version](http://my.oschina.net/u/931210)](http://my.oschina.net/u/931210) 1.0 V * @createTime 2014-12-18 上午11:22:12 */public class H2ConnTest1 &#123; //数据库连接URL，当前连接的是E:/H2目录下的gacl数据库 private static final String JDBC_URL = "jdbc:h2:mem:mydb"; //连接数据库时使用的用户名 private static final String USER = ""; //连接数据库时使用的密码 private static final String PASSWORD = ""; //连接H2数据库时使用的驱动类，org.h2.Driver这个类是由H2数据库自己提供的，在H2数据库的jar包中可以找到 private static final String DRIVER_CLASS="org.h2.Driver"; public static void main(String[] args) throws Exception &#123; // 加载H2数据库驱动 Class.forName(DRIVER_CLASS); // 根据连接URL，用户名，密码获取数据库连接 Connection conn = DriverManager.getConnection(JDBC_URL, USER, PASSWORD); Statement stmt = conn.createStatement(); //如果存在USER_INFO表就先删除USER_INFO表 stmt.execute("DROP TABLE IF EXISTS USER_INFO"); //创建USER_INFO表 stmt.execute("CREATE TABLE USER_INFO(id VARCHAR(36) PRIMARY KEY,name VARCHAR(100),sex VARCHAR(4))"); //新增 stmt.executeUpdate("INSERT INTO USER_INFO VALUES('" + UUID.randomUUID()+ "','大日如来','男')"); stmt.executeUpdate("INSERT INTO USER_INFO VALUES('" + UUID.randomUUID()+ "','青龙','男')"); stmt.executeUpdate("INSERT INTO USER_INFO VALUES('" + UUID.randomUUID()+ "','白虎','男')"); stmt.executeUpdate("INSERT INTO USER_INFO VALUES('" + UUID.randomUUID()+ "','朱雀','女')"); stmt.executeUpdate("INSERT INTO USER_INFO VALUES('" + UUID.randomUUID()+ "','玄武','男')"); stmt.executeUpdate("INSERT INTO USER_INFO VALUES('" + UUID.randomUUID()+ "','苍狼','男')"); //删除 stmt.executeUpdate("DELETE FROM USER_INFO WHERE name='大日如来'"); //修改 stmt.executeUpdate("UPDATE USER_INFO SET name='孤傲苍狼' WHERE name='苍狼'"); //查询// ResultSet rs = stmt.executeQuery("SELECT * FROM USER_INFO"); ResultSet rs = stmt.executeQuery("SELECT * FROM USER_INFO where name REGEXP '^大|白'"); //遍历结果集 while (rs.next()) &#123; System.out.println(rs.getString("id") + "," + rs.getString("name")+ "," + rs.getString("sex")); &#125; //释放资源 stmt.close(); //关闭连接 conn.close(); &#125;&#125; maven1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.xerial&lt;/groupId&gt; &lt;artifactId&gt;sqlite-jdbc&lt;/artifactId&gt; &lt;version&gt;3.8.11.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.derby&lt;/groupId&gt; &lt;artifactId&gt;derby&lt;/artifactId&gt; &lt;version&gt;10.8.3.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.h2database&lt;/groupId&gt; &lt;artifactId&gt;h2&lt;/artifactId&gt; &lt;version&gt;1.4.192&lt;/version&gt; &lt;/dependency&gt;]]></content>
      <categories>
        <category>db</category>
      </categories>
      <tags>
        <tag>db</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[乐观锁和悲观锁]]></title>
    <url>%2F2019%2F02%2F22%2F%E4%B9%90%E8%A7%82%E9%94%81%E5%92%8C%E6%82%B2%E8%A7%82%E9%94%81%2F</url>
    <content type="text"><![CDATA[1、悲观锁，前提是，一定会有并发抢占资源，强行独占资源，在整个数据处理过程中，将数据处于锁定状态。2、乐观锁，前提是，不会发生并发抢占资源，只有在提交操作的时候检查是否违反数据完整性。只能防止脏读后数据的提交，不能解决脏读。 乐观锁（ Optimistic Locking ） 相对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则让返回用户错误的信息，让用户决定如何去做。那么我们如何实现乐观锁呢，一般来说有以下2种方式： 乐观锁-version的使用https://www.cnblogs.com/DengGao/p/6479824.html悲观锁https://blog.csdn.net/z69183787/article/details/46779335]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图遍历]]></title>
    <url>%2F2019%2F02%2F22%2F%E5%9B%BE%E9%81%8D%E5%8E%86%2F</url>
    <content type="text"><![CDATA[图的遍历，所谓遍历，即是对结点的访问。一个图有那么多个结点，如何遍历这些结点，需要特定策略，一般有两种访问策略： 深度优先遍历 广度优先遍历 深度优先深度优先遍历，从初始访问结点出发，我们知道初始访问结点可能有多个邻接结点，深度优先遍历的策略就是首先访问第一个邻接结点，然后再以这个被访问的邻接结点作为初始结点，访问它的第一个邻接结点。总结起来可以这样说：每次都在访问完当前结点后首先访问当前结点的第一个邻接结点。我们从这里可以看到，这样的访问策略是优先往纵向挖掘深入，而不是对一个结点的所有邻接结点进行横向访问。具体算法表述如下： 访问初始结点v，并标记结点v为已访问。 查找结点v的第一个邻接结点w。 若w存在，则继续执行4，否则算法结束。 若w未被访问，对w进行深度优先遍历递归（即把w当做另一个v，然后进行步骤123）。查找结点v的w邻接结点的下一个邻接结点，转到步骤3。例如下图，其深度优先遍历顺序为 1-&gt;2-&gt;4-&gt;8-&gt;5-&gt;3-&gt;6-&gt;7 广度优先类似于一个分层搜索的过程，广度优先遍历需要使用一个队列以保持访问过的结点的顺序，以便按这个顺序来访问这些结点的邻接结点。具体算法表述如下： 访问初始结点v并标记结点v为已访问。 结点v入队列 当队列非空时，继续执行，否则算法结束。 出队列，取得队头结点u。 查找结点u的第一个邻接结点w。 若结点u的邻接结点w不存在，则转到步骤3；否则循环执行以下三个步骤：1). 若结点w尚未被访问，则访问结点w并标记为已访问。2). 结点w入队列3). 查找结点u的继w邻接结点后的下一个邻接结点w，转到步骤6。如下图，其广度优先算法的遍历顺序为：1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;6-&gt;7-&gt;8 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798package com.lifeibigdata.algorithms.graph.liantongfenliang;import java.util.LinkedList;import java.util.Queue;/** * Created by leofei.li on 2016/5/21. */public class BFS &#123; void addEdge(int i,int j)&#123; //添加节点关系 if(i == j)return; edge[i][j]=1; edge[j][i]=1; &#125; void dfsTraverse()&#123; visited = new boolean[verNum]; //图中的节点是否访问过 for(int i = 0; i&lt; verNum; i ++)&#123; //遍历行 if(visited[i] == false)&#123; //如果该行没有被遍历过 dfs(i); &#125; &#125; &#125; void dfs(int i)&#123; visited[i] = true; //标记遍历该行 System.out.print(ver[i] + " "); for(int j = 0; j &lt; verNum; j++)&#123; //遍历列 if(visited[j] == false &amp;&amp; edge[i][j] == 1)&#123; dfs(j); //列转行 &#125; &#125; &#125; void bfsTraverse()&#123; visited = new boolean[verNum]; Queue&lt;Integer&gt; quene = new LinkedList&lt;Integer&gt;(); for (int i = 0; i &lt; verNum; i ++)&#123; if(visited[i] == false)&#123; visited[i] = true; System.out.print(ver[i]+" "); quene.add(i); while (!quene.isEmpty())&#123; int j = quene.poll(); for (int k = 0; k &lt; verNum; k++)&#123; if(edge[j][k] == 1 &amp;&amp; visited[k] == false)&#123; visited[k] = true; System.out.print(ver[k]+" "); quene.add(k); &#125; &#125; &#125; &#125; &#125; &#125; void con()&#123; int count = 0; visited = new boolean[verNum]; for(int i = 0; i &lt; verNum; i ++)&#123; //遍历 if(!visited[i])&#123; count++; dfsTraverse(); &#125; &#125; System.out.println("共有"+count+"个连通分量!"); &#125; static int verNum;//节点数 static boolean []visited;//定义辅助数组 static String []ver=&#123;"A","B","C","D","E"&#125;; //定义节点 static int [][]edge;//邻接矩阵 public static void main(String[] args) &#123; verNum = ver.length; edge = new int[verNum][verNum]; for(int i=0;i&lt;verNum;i++)&#123; for (int j=0;j&lt;verNum;j++)&#123; edge[i][j]=0; &#125; &#125; BFS b = new BFS(); b.addEdge(0, 3); b.addEdge(0, 4); b.addEdge(1, 2); b.addEdge(2, 4); b.addEdge(2, 3); System.out.println("图的深度遍历操作:"); b.dfsTraverse(); System.out.println(); System.out.println("图的广度遍历操作："); b.bfsTraverse(); System.out.println(); System.out.println("连通分量:"); b.con(); &#125;&#125; 不管深搜还是广搜都需要申请总节点个数长度的数组用来标记节点是否访问过(访问数组)；深搜是遍历访问数组，从某个未被访问的节点辐射出去，每次辐射结束后，判断访问数组中是否还有未被访问的节点，如果有，再次辐射，如果没有，深搜结束；广搜是同样是遍历数组，判断是否有为被访问的节点，如果有放到对列中，然后弹出队列，将弹出节点所有未被访问的邻接节点放到队列中，继续进行访问队列，直到队列为空时继续判断访问数组 原文链接：https://segmentfault.com/a/1190000002685939#articleHeader0]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AIOdemo源码]]></title>
    <url>%2F2019%2F02%2F21%2FAIOdemo%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[server:12345678910111213141516171819202122232425262728293031323334353637383940414243package com.test.nio;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.AsynchronousServerSocketChannel;import java.nio.channels.AsynchronousSocketChannel;import java.nio.channels.CompletionHandler;import java.util.concurrent.ExecutionException;public class AIOServer &#123; public AIOServer(int port) throws IOException&#123; final AsynchronousServerSocketChannel listener=AsynchronousServerSocketChannel.open().bind(new InetSocketAddress(port)); listener.accept(null,new CompletionHandler&lt;AsynchronousSocketChannel,Void&gt;()&#123; @Override public void completed(AsynchronousSocketChannel ch, Void attachment) &#123; listener.accept(null, this);//接受下一个链接 try &#123; handler(ch); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; @Override public void failed(Throwable exc, Void attachment) &#123; System.out.println("AIo失败！"); &#125; &#125;); &#125; //真正逻辑处理 public void handler(AsynchronousSocketChannel ch) throws Exception, ExecutionException&#123; ByteBuffer bytebuffer=ByteBuffer.allocate(32); ch.read(bytebuffer).get(); bytebuffer.flip(); System.out.println("服务端接受数据"+new String(bytebuffer.array())); &#125; public static void main(String[] args) throws Exception &#123; AIOServer server =new AIOServer(7080); System.out.println("监听端口7080"); Thread.sleep(10000); &#125;&#125; client:123456789101112131415161718192021222324252627package com.test.nio;/** * Created by lifei on 16/11/9. */import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.AsynchronousSocketChannel;import java.util.concurrent.Future;public class AIOClient &#123; private AsynchronousSocketChannel client=null; public AIOClient(String host,int port) throws Exception&#123; client =AsynchronousSocketChannel.open(); Future&lt;?&gt; future=client.connect(new InetSocketAddress(host,port)); System.out.println("返回结果:"+future.get()); &#125; public void write(byte[] b)&#123; ByteBuffer bytebuffer=ByteBuffer.allocate(32); bytebuffer.put(b); bytebuffer.flip(); client.write(bytebuffer); &#125; public static void main(String[] args) throws Exception &#123; AIOClient client=new AIOClient("localhost",7080); client.write("hello ...".getBytes()); &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[NIO经典demo源码]]></title>
    <url>%2F2019%2F02%2F21%2FNIO%E7%BB%8F%E5%85%B8demo%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[server:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194package com.test.nio;import java.io.IOException;import java.net.InetSocketAddress;import java.net.ServerSocket;import java.nio.ByteBuffer;import java.nio.channels.*;import java.util.Iterator;import java.util.Set;/** * Created by lifei on 16/11/8. */public class NIOServer &#123; private int flag = 0;/*标识数字*/ private int blockSize = 4096;/*缓冲区大小*/ private ByteBuffer sendBuffer = ByteBuffer.allocate(blockSize);/*接受数据缓冲区*/ private ByteBuffer receiveBuffer = ByteBuffer.allocate(blockSize);/*发送数据缓冲区*/ private Selector selector; public NIOServer(int port) &#123; try &#123; ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();// 打开服务器套接字通道 serverSocketChannel.configureBlocking(false);//// 服务器配置为非阻塞 ServerSocket serverSocket = serverSocketChannel.socket(); // 检索与此通道关联的服务器套接字 serverSocket.bind(new InetSocketAddress(port));//绑定ip和端口 selector = Selector.open();//打开选择器 serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);// 注册到selector，等待连接 System.out.println("Server start -&gt; " + port); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; //监听 public void listen()&#123; while (true)&#123; try &#123; selector.select();//如果查询不到,会阻塞 选择一组键，并且相应的通道已经打开 Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys();// 返回此选择器的已选择键集。 Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator(); while (iterator.hasNext())&#123; SelectionKey selectionKey = iterator.next(); iterator.remove(); //业务逻辑 handleKey(selectionKey); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; // 处理请求 public void handleKey(SelectionKey selectionKey)&#123; try &#123; // 接受请求 ServerSocketChannel server; SocketChannel client; String receiveText; String sendText; int count; if (selectionKey.isAcceptable())&#123;// 测试此键的通道是否已准备好接受新的套接字连接。 server = (ServerSocketChannel) selectionKey.channel();// 返回为之创建此键的通道。 client = server.accept();// 接受到此通道套接字的连接。 此方法返回的套接字通道（如果有）将处于阻塞模式。 client.configureBlocking(false);// 配置为非阻塞 client.register(selector,SelectionKey.OP_READ);// 注册到selector，等待连接 &#125; else if (selectionKey.isReadable())&#123; client = (SocketChannel) selectionKey.channel();// 返回为之创建此键的通道。 receiveBuffer.clear();//todo 将缓冲区清空以备下次读取 count = client.read(receiveBuffer);//读取服务器发送来的数据到缓冲区中 if (count &gt; 0)&#123; receiveText = new String(receiveBuffer.array(),0,count); System.out.println("服务端接收到客户端的信息:"+receiveText); client.register(selector,SelectionKey.OP_WRITE); &#125; &#125; else if (selectionKey.isWritable())&#123; sendBuffer.clear();//将缓冲区清空以备下次写入 client = (SocketChannel) selectionKey.channel();// 返回为之创建此键的通道。 sendText = "msg send to clent ..."+(flag++);//发送的数据 sendBuffer.put(sendText.getBytes());//向缓冲区中输入数据 sendBuffer.flip();//将缓冲区各标志复位,因为向里面put了数据标志被改变要想从中读取数据发向服务器,就要复位 client.write(sendBuffer);//输出到通道 System.out.println("服务端发送数据给客户端:"+sendText); client.register(selector,SelectionKey.OP_READ); &#125; &#125; catch (Exception e)&#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; int port = 7080; NIOServer server = new NIOServer(port); server.listen(); &#125;&#125;``` # client: ```javapackage com.test.nio;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.SocketChannel;import java.util.Iterator;import java.util.Set;/** * Created by lifei on 16/11/8. * http://developer.51cto.com/art/201112/307685.htm */public class NIOClient &#123; private static int flag = 0;/*标识数字*/ private static int blockSize = 4096;/*缓冲区大小*/ private static ByteBuffer sendBuffer = ByteBuffer.allocate(blockSize);/*接受数据缓冲区*/ private static ByteBuffer receiveBuffer = ByteBuffer.allocate(blockSize); /*服务器端地址*/ private final static InetSocketAddress serverAddress = new InetSocketAddress("127.0.0.1",7080); public static void main(String[] args) &#123; try &#123; SocketChannel socketChannel = SocketChannel.open();// 打开socket通道 socketChannel.configureBlocking(false);// 设置为非阻塞方式 Selector selector = Selector.open();//打开选择器 socketChannel.register(selector, SelectionKey.OP_CONNECT);// 注册连接服务端socket动作 socketChannel.connect(serverAddress);// 连接 // 分配缓冲区大小内存 Set&lt;SelectionKey&gt; selectionKeys; Iterator&lt;SelectionKey&gt; iterator; SelectionKey selectionKey; SocketChannel client; String receiveText; String sendText; int count; while (true)&#123; try &#123; selector.select();//todo 选择一组键，其相应的通道已为 I/O 操作准备就绪。 此方法执行处于阻塞模式的选择操作。 selectionKeys = selector.selectedKeys();//返回此选择器的已选择键集。 iterator = selectionKeys.iterator(); while (iterator.hasNext())&#123; selectionKey = iterator.next(); if (selectionKey.isConnectable())&#123; System.out.println("client connet"); client = (SocketChannel) selectionKey.channel(); if (client.isConnectionPending())&#123;// 判断此通道上是否正在进行连接操作。 完成套接字通道的连接过程。 client.finishConnect(); System.out.println("客户端完成连接操作"); sendBuffer.clear(); sendText = "Hello server..."; sendBuffer.put(sendText.getBytes()); sendBuffer.flip(); client.write(sendBuffer); &#125; client.register(selector,SelectionKey.OP_READ); &#125; else if (selectionKey.isReadable())&#123; client = (SocketChannel) selectionKey.channel(); receiveBuffer.clear();//将缓冲区清空以备下次读取 count = client.read(receiveBuffer);//读取服务器发送来的数据到缓冲区中 if (count &gt; 0)&#123; receiveText = new String(receiveBuffer.array(),0,count); System.out.println("客户端接收到服务端数据:"+receiveText); client.register(selector,SelectionKey.OP_WRITE); &#125; &#125; else if (selectionKey.isWritable())&#123; sendBuffer.clear(); client = (SocketChannel) selectionKey.channel(); sendText = "Msg to server ..."+(flag++); sendBuffer.put(sendText.getBytes()); sendBuffer.flip();//将缓冲区各标志复位,因为向里面put了数据标志被改变要想从中读取数据发向服务器,就要复位 client.write(sendBuffer); System.out.println("客户端发送数据给服务端"+sendText); client.register(selector,SelectionKey.OP_READ); &#125; &#125; selectionKeys.clear(); &#125; catch (Exception e)&#123; &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[mac实战mesos]]></title>
    <url>%2F2019%2F02%2F21%2Fmac%E5%AE%9E%E6%88%98mesos%2F</url>
    <content type="text"><![CDATA[如果程序一直deploy说明一定有问题mac安装12brew install mesosbrew upgrade mesos 启动zookeeper启动master1234HOST_IP=100.80.128.98sudo /usr/local/Cellar/mesos/1.4.1/sbin/mesos-master --ip=$&#123;HOST_IP&#125; \ --log_dir=/Users/lifei/dockerproject/mesos/master/log --work_dir=/Users/lifei/dockerproject/mesos/master/work \ --ZK=zk://$&#123;HOST_IP&#125;:2181/mesos --quorum=1 启动slave(注意端口，如果marathon中用到80等端口，需要将ports范围足够大)12345678910/usr/local/Cellar/mesos/1.4.1/sbin/mesos-slave --help 查看帮助启动第一个slavesudo /usr/local/Cellar/mesos/1.4.1/sbin/mesos-slave --master=$&#123;HOST_IP&#125;:5050 \--log_dir=/Users/lifei/dockerproject/mesos/slave/log --work_dir=/Users/lifei/dockerproject/mesos/slave/work \--containerizers=docker,mesos --no-hostname_lookup --ip=$&#123;HOST_IP&#125; --resources=&apos;ports:[1-32000];&apos; 启动第二个slavesudo /usr/local/Cellar/mesos/1.4.1/sbin/mesos-slave --master=$&#123;HOST_IP&#125;:5050 \--log_dir=/Users/lifei/dockerproject/mesos/slave/log2 --work_dir=/Users/lifei/dockerproject/mesos/slave/work2 \--containerizers=docker,mesos --no-hostname_lookup --ip=$&#123;HOST_IP&#125; --resources=&apos;ports:[1-32000];&apos; 测试task1/usr/local/Cellar/mesos/1.4.1/bin/mesos-execute --master=localhost:5050 --name=hellomesos --command=&quot;echo &apos;hello,mesos&apos;&quot; 启动marathon1sudo ./bin/start --http_port 8088 --master $&#123;HOST_IP&#125;:5050 --zk zk://$&#123;HOST_IP&#125;:2181/marathon -h $&#123;HOST_IP&#125; 测试marathon123Commandwhile [ true ] ; do echo &apos;Hello Marathon&apos; ; sleep 5 ; done 从marathon中使用docker启动nginx1234567891011121314151617181920212223&#123; &quot;type&quot;: &quot;DOCKER&quot;, &quot;volumes&quot;: [], &quot;docker&quot;: &#123; &quot;image&quot;: &quot;library/nginx&quot;, &quot;network&quot;: &quot;BRIDGE&quot;, &quot;portMappings&quot;: [ &#123; &quot;containerPort&quot;: 80, &quot;hostPort&quot;: 0, &quot;servicePort&quot;: 2000, &quot;protocol&quot;: &quot;tcp&quot;, &quot;labels&quot;: &#123;&#125; &#125; ], &quot;privileged&quot;: false, &quot;parameters&quot;: [], &quot;forcePullImage&quot;: false &#125;&#125;使用docker inspect containerid，查看动态分配的hostportps:在container里边， 这个web服务运行的端口是8080（containerPort的值）。在container外，Marathon会分配一个随机端口（hostPort设置是0） marathon-lb安装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172采用bridge方式，host方式失败，不明白问题在哪儿**********&#123; &quot;id&quot;: &quot;/marathon-lb&quot;, &quot;cmd&quot;: null, &quot;cpus&quot;: 1, &quot;mem&quot;: 128, &quot;disk&quot;: 0, &quot;instances&quot;: 1, &quot;constraints&quot;: [ [ &quot;hostname&quot;, &quot;UNIQUE&quot; ] ], &quot;container&quot;: &#123; &quot;type&quot;: &quot;DOCKER&quot;, &quot;volumes&quot;: [ &#123; &quot;containerPath&quot;: &quot;/var&quot;, &quot;hostPath&quot;: &quot;/Users/lifei/dockerproject/marathon/marathon-lb-var&quot;, &quot;mode&quot;: &quot;RW&quot; &#125;, &#123; &quot;containerPath&quot;: &quot;/tmp&quot;, &quot;hostPath&quot;: &quot;/Users/lifei/dockerproject/marathon/marathon-lb-tmp&quot;, &quot;mode&quot;: &quot;RW&quot; &#125; ], &quot;docker&quot;: &#123; &quot;image&quot;: &quot;docker.io/mesosphere/marathon-lb&quot;, &quot;network&quot;: &quot;BRIDGE&quot;, &quot;portMappings&quot;: [ &#123; &quot;containerPort&quot;: 80, &quot;hostPort&quot;: 80, &quot;servicePort&quot;: 10001, &quot;protocol&quot;: &quot;tcp&quot;, &quot;labels&quot;: &#123;&#125; &#125;, &#123; &quot;containerPort&quot;: 9090, &quot;hostPort&quot;: 9090, &quot;servicePort&quot;: 10002, &quot;protocol&quot;: &quot;tcp&quot;, &quot;labels&quot;: &#123;&#125; &#125; ], &quot;privileged&quot;: true, &quot;parameters&quot;: [], &quot;forcePullImage&quot;: false &#125; &#125;, &quot;portDefinitions&quot;: [ &#123; &quot;port&quot;: 10001, &quot;protocol&quot;: &quot;tcp&quot;, &quot;labels&quot;: &#123;&#125; &#125;, &#123; &quot;port&quot;: 10002, &quot;protocol&quot;: &quot;tcp&quot;, &quot;labels&quot;: &#123;&#125; &#125; ], &quot;args&quot;: [ &quot;sse&quot;, &quot;-m&quot;, &quot;http://100.80.128.98:8088&quot;, &quot;--group&quot;, &quot;external&quot; ]&#125; 测试marathon-lb123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&#123; &quot;id&quot;: &quot;/test-lb-nginx&quot;, &quot;cmd&quot;: null, &quot;cpus&quot;: 0.2, &quot;mem&quot;: 20, &quot;disk&quot;: 0, &quot;instances&quot;: 2, &quot;container&quot;: &#123; &quot;type&quot;: &quot;DOCKER&quot;, &quot;volumes&quot;: [], &quot;docker&quot;: &#123; &quot;image&quot;: &quot;docker.io/nginx&quot;, &quot;network&quot;: &quot;BRIDGE&quot;, &quot;portMappings&quot;: [ &#123; &quot;containerPort&quot;: 80, &quot;hostPort&quot;: 0, &quot;servicePort&quot;: 80, &quot;protocol&quot;: &quot;tcp&quot;, &quot;labels&quot;: &#123;&#125; &#125; ], &quot;privileged&quot;: false, &quot;parameters&quot;: [], &quot;forcePullImage&quot;: false &#125; &#125;, &quot;healthChecks&quot;: [ &#123; &quot;path&quot;: &quot;/&quot;, &quot;protocol&quot;: &quot;HTTP&quot;, &quot;portIndex&quot;: 0, &quot;gracePeriodSeconds&quot;: 300, &quot;intervalSeconds&quot;: 60, &quot;timeoutSeconds&quot;: 20, &quot;maxConsecutiveFailures&quot;: 3, &quot;ignoreHttp1xx&quot;: false &#125; ], &quot;labels&quot;: &#123; &quot;HAPROXY_GROUP&quot;: &quot;external&quot;, &quot;HAPROXY_0_VHOST&quot;: &quot;nginx.marathon.mesos&quot; &#125;, &quot;portDefinitions&quot;: [ &#123; &quot;port&quot;: 80, &quot;protocol&quot;: &quot;tcp&quot;, &quot;labels&quot;: &#123;&#125; &#125; ]&#125; 测试url12http://100.80.128.98:9090/haproxy?statshttp://100.80.128.98]]></content>
      <categories>
        <category>mesos</category>
      </categories>
      <tags>
        <tag>mesos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批量查找替换]]></title>
    <url>%2F2019%2F02%2F20%2F%E6%89%B9%E9%87%8F%E6%9F%A5%E6%89%BE%E6%9B%BF%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[批量替换xxx为yyy1find . -name &quot;*.md&quot; | xargs grep &quot;xxx&quot; -l | xargs sed -i &quot;s/xxx/yyy/g&quot;]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用iptables连接hang住问题*]]></title>
    <url>%2F2019%2F02%2F20%2F%E4%BD%BF%E7%94%A8iptables%E8%BF%9E%E6%8E%A5hang%E4%BD%8F%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[问题来自于hbase的namenode高可用测试，测试采用iptables来模拟active namenode机器宕机问题，测试过程中发现，当hadoop的namenode发生了failover之后，hbase需要过很长时间（大约需要15分钟）才能写入。 经过分析主要是linux tcp参数的影响，主要影响参数是tcp_retries2tcp_retries2 ：INTEGER默认值为15在丢弃激活(已建立通讯状况)的TCP连接之前﹐需要进行多少次重试。默认值为15，根据RTO的值来决定，相当于13-30分钟(RFC1122规定，必须大于100秒).线上一般不会有这个问题，所以不建议修改 该参数的修改，需要在与原active namenode机器通信的其他主机上设置echo 2 &gt; /proc/sys/net/ipv4/tcp_retries2]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[检查网卡流量]]></title>
    <url>%2F2019%2F02%2F20%2F%E6%A3%80%E6%9F%A5%E7%BD%91%E5%8D%A1%E6%B5%81%E9%87%8F%2F</url>
    <content type="text"><![CDATA[1、iftop12345678910111213141516171819TX：发送流量RX：接收流量TOTAL：总流量Cumm：运行iftop到目前时间的总流量peak：流量峰值rates：分别表示过去 2s 10s 40s 的平均流量相关参数-i设定监测的网卡，如：# iftop -i eth1-B 以bytes为单位显示流量(默认是bits)，如：# iftop -B-n使host信息默认直接都显示IP，如：# iftop -n-N使端口信息默认直接都显示端口号，如: # iftop -N-F显示特定网段的进出流量，如# iftop -F 10.10.1.0/24或# iftop -F 10.10.1.0/255.255.255.0-h（display this message），帮助，显示参数信息-p使用这个参数后，中间的列表显示的本地主机信息，出现了本机以外的IP信息;-b使流量图形条默认就显示;-f这个暂时还不太会用，过滤计算包用的;-P使host信息及端口信息默认就都显示;-m设置界面最上边的刻度的最大值，刻度分五个大段显示，例：# iftop -m 100M]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卸载datanode节点]]></title>
    <url>%2F2019%2F02%2F20%2F%E5%8D%B8%E8%BD%BDdatanode%E8%8A%82%E7%82%B9%2F</url>
    <content type="text"><![CDATA[datanode可能会被卸载以便把它从集群中安全移除，同时还要保持主机上所有块的复制因子，这个过程可能很漫长，取决于被卸主机上的数据量、集群工作数以及网络速度等因素。 1、在slaves配置文件中去掉下线机器（我们配置的dfs.hosts参数就是slaves配置文件中的所有机器） 2、刷新节点1[hadoop@hostname /home/q/hadoop/hadoop-2.3.0-cdh5.1.0]$ ./bin/hdfs dfsadmin -refreshNodes 3、监控namenode的web界面，确保卸载正在进行并完成，有时更新会滞后几秒 4、停止datanode进程12[hadoop@hostname /home/q/hadoop/hadoop-2.3.0-cdh5.1.0]$ ./sbin/hadoop-daemon.sh stop datanodestopping datanode]]></content>
  </entry>
  <entry>
    <title><![CDATA[hbase和hadoop运维操作]]></title>
    <url>%2F2019%2F02%2F20%2Fhbase%E5%92%8Chadoop%E8%BF%90%E7%BB%B4%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[hbase shell管理操作1、使用flush命令可将表中所有区域的数据都写入磁盘1hbase(main):002:0&gt; flush &apos;usertable&apos; 2、单独将表的某个区域的数据写入磁盘1hbase(main):006:0&gt; flush &apos;usertable,user3078440486144287982,1408008575537.c018c1bb15879c1c3aea12239e8f0c08.&apos; 3、使用compact命令可对指定表的所有区域进行合并1hbase(main):003:0&gt; compact &apos;usertable&apos; 4、使用major_compact命令可对指定表进行主合并1hbase(main):004:0&gt; major_compact &apos;usertable&apos; 5、使用balance_switch命令来启用/关闭负载均衡功能12hbase(main):008:0&gt; balance_switch truehbase(main):008:0&gt; balance_switch false 6、使用balancer命令来对集群进行负载均衡1hbase(main):009:0&gt; balancer 7、执行split命令对表的指定区域进行分割1hbase(main):004:0&gt; split &apos;usertable,,1408008575537.485eb84ae427530a3f50351928b3826c.&apos; 8、计算表行数123./bin/hbase org.apache.hadoop.hbase.mapreduce.RowCounter &apos;usertable&apos;./bin/hbase org.apache.hadoop.hbase.mapreduce.RowCounter -Dfs.defaultFS=file:/// &apos;usertable&apos; 9、使用hbck命令检查集群的健康情况1./bin/hbase hbck 10、 hbase集群管理操作平稳关闭regionserver12./bin/graceful_stop.sh hostnamehbase(main):004:0&gt; balance_switch true 使用平稳关闭regionserver，在真正关闭区域服务器守护程序之前，graceful_stop.sh脚本会首先关闭hbase的负载均衡功能，因此如果关闭后还需要使用负载均衡功能，可以显示地重新启动它 11、 滚动重启在将hbase升级到新版本或想让某些配置修改生效时，可能需要进行滚动重启。以下是实现滚动重启的步骤 1、运行hbck检查集群是否处于一致状态 ./bin/hbase hbck 2、重启master ./bin/hbase-daemon.sh stop master ./bin/hbase-daemon.sh start master 3、关闭region负载均衡功能 hbase(main):001:0&gt; balance_switch false 4、在每个region服务器上运行graceful_stop.sh脚本 开启一个screen，执行以下命令 for i in `cat conf/regionservers`;do echo $i;./bin/graceful_stop.sh --restart --reload --debug $i;done &gt; /tmp/log.txt 5、启动region负载均衡功能 hbase(main):003:0&gt; balance_switch true 6、运行hbck确认集群是否属于一致状态 ./bin/hbase hbck 12、 hbase新增服务器 新增hmaster ./bin/hbase-daemon.sh start master 新增regionserver ./bin/hbase-daemon.sh start regionserver hadoop shell管理操作1、 检查namenode是否处于安全模式12[hadoop@hostname /home/q/hadoop/hadoop-2.3.0-cdh5.1.0]$ ./bin/hdfs dfsadmin -safemode getSafe mode is OFF 2、 离开安全模式1[hadoop@hostname /home/q/hadoop/hadoop-2.3.0-cdh5.1.0]$ ./bin/hdfs dfsadmin -safemode leave 3、 检查hdfs是否正常1[hadoop@hostname /home/q/hadoop/hadoop-2.3.0-cdh5.1.0]$ ./bin/hdfs fsck / 4、 移除 missing block [hadoop@hostname /home/q/hadoop/hadoop-2.3.0-cdh5.1.0]$ ./bin/hdfs fsck -delete 或者 ./bin/hdfs fsck -move]]></content>
  </entry>
  <entry>
    <title><![CDATA[hue]]></title>
    <url>%2F2019%2F02%2F20%2Fhue%2F</url>
    <content type="text"><![CDATA[使用hue服务前需要启动thift服务: ./hbase-daemon.sh restart thrift -threadpool -m 200 -w 500 （指定连接数） hue安装下载安装包https://github.com/cloudera/hue/releaseswget https://github.com/cloudera/hue/archive/release-3.9.0.tar.gz下载解压到/home/q/hue/hue-release-3.9.0依赖包安装123456789yum install -y python-develyum install -y gcc gcc-c++yum install -y libxml2-develyum install -y sqlite-develyum install -y openldap-develyum install -y python-ldapyum install libxslt-devel.x86_64yum install cyrus-sasl-gssapiyum install gmp-devel.x86_64 cd /home/q/hue/hue-release-3.9.0 进入hue目录PREFIX=/usr/share make install执行命令编译安装 (该步骤会下载hue支持功能的所有依赖包，时间较长) 报错：123cd /home/hue/hue-release-3.9.0/desktop/libs/hadoop/java &amp;&amp; mvn clean install -DskipTests/bin/bash: mvn: command not found 配置环境变量，然后生效123export MAVEN_HOME=/home/q/apache-mavenexport PATH=$PATH:$JAVA_HOME/bin:$MAVEN_HOME/bin 报错处理 需要hue用户，否则启动报错123456789101112131415161718192021[root@hostname /home/q/hue]# /usr/share/hue/build/env/bin/supervisor -dTraceback (most recent call last): File &quot;/usr/share/hue/build/env/bin/supervisor&quot;, line 9, in &lt;module&gt; load_entry_point(&apos;desktop==3.9.0&apos;, &apos;console_scripts&apos;, &apos;supervisor&apos;)() File &quot;/usr/share/hue/desktop/core/src/desktop/supervisor.py&quot;, line 319, in main setup_user_info() File &quot;/usr/share/hue/desktop/core/src/desktop/supervisor.py&quot;, line 257, in setup_user_info desktop.lib.daemon_utils.get_uid_gid(SETUID_USER, SETGID_GROUP) File &quot;/usr/share/hue/desktop/core/src/desktop/lib/daemon_utils.py&quot;, line 45, in get_uid_gid raise KeyError(&quot;Couldn&apos;t get user id for user %s&quot; % (username,))KeyError: &quot;Couldn&apos;t get user id for user hue&quot; 解决方法：useradd hue chown -R hue:hue /home/q/hue 再次启动/usr/share/hue/build/env/bin/supervisor -d]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThriftServer 服务假死导致HUE HBASE Broswer异常(无法展示数据)]]></title>
    <url>%2F2019%2F02%2F20%2FThriftServer-%E6%9C%8D%E5%8A%A1%E5%81%87%E6%AD%BB%E5%AF%BC%E8%87%B4HUE-HBASE-Broswer%E5%BC%82%E5%B8%B8-%E6%97%A0%E6%B3%95%E5%B1%95%E7%A4%BA%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[业务反馈Hue无法展示数据，多半是Java GC导致。解决方法是：修改启动参数并重启。具体详情如下： 登陆hue部署的机器，查看thritserver的gc情况123456789101112131415[hadoop@hostname /home/q/hbase/q_hbase/bin]$ jps23125 HRegionServer23542 DataNode3600ThriftServer6954 Jps[hadoop@hostname /home/q/hbase/q_hbase/bin]$ jstat -gcutil 3600 S0 S1 E O P YGC YGCT FGC FGCT GCT 0.00 53.12 28.15 93.10 99.17 455569 3210.991 3470 196.670 3407.661 S0 — Heap上的 Survivor space 0 区已使用空间的百分比S1 — Heap上的 Survivor space 1 区已使用空间的百分比E — Heap上的 Eden space 区已使用空间的百分比O — Heap上的 Old space 区已使用空间的百分比P — Perm space 区已使用空间的百分比YGC — 从应用程序启动到采样时发生 Young GC 的次数YGCT– 从应用程序启动到采样时 Young GC 所用的时间(单位秒)FGC — 从应用程序启动到采样时发生 Full GC 的次数FGCT– 从应用程序启动到采样时 Full GC 所用的时间(单位秒)GCT — 从应用程序启动到采样时用于垃圾回收的总时间(单位秒) 如果gc没有问题，继续查看thriftserver的连接数是否异常,如果连接数打满也会有问题:1netstat -apn|grep 15046|grep 9090|wc -l 异常案例: Old区使用了93.1%基本上进程夯死，内存满了后会导致更加频繁的GC,FGC次数达到了3470次。深入的化就需要看下配置jvm内存策略，CMS默认80%会触发FGC，如果FGC后Old仍大于80%则因为内存不够用会频繁FGC，进程基本假死状态。 问题：hue 数据展示异常，能telnet无服务。解决： 重启thriftserver ./hbase-daemon.sh restart thrift -threadpool -m 200 -w 500 （指定连接数）]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hdfs优化]]></title>
    <url>%2F2019%2F02%2F20%2Fhdfs%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[HDFS作为HBase最终数据存储系统，通常会使用三副本策略存储HBase数据文件以及日志文件。从HDFS的角度望上层看，HBase即是它的客户端，HBase通过调用它的客户端进行数据读写操作，因此HDFS的相关优化也会影响HBase的读写性能。 Short-Circuit Local Read功能是否开启？优化原理：当前HDFS读取数据都需要经过DataNode，客户端会向DataNode发送读取数据的请求，DataNode接受到请求之后从硬盘中将文件读出来，再通过TPC发送给客户端。Short Circuit策略允许客户端绕过DataNode直接读取本地数据。（具体原理参考此处 http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/ ) 优化建议：开启Short Circuit Local Read功能，具体配置戳这里 (https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html ) 详解HDFS Short Circuit Local ReadsHadoop的一大基本原则是移动计算的开销要比移动数据的开销小。因此，Hadoop通常是尽量移动计算到拥有数据的节点上。这就使得Hadoop中读取数据的客户端DFSClient和提供数据的Datanode经常是在一个节点上，也就造成了很多“Local Reads”。 最初设计的时候，这种Local Reads和Remote Reads（DFSClient和Datanode不在同一个节点）的处理方式都是一样的，也就是都是先由Datanode读取数据，然后再通过RPC把数据传给DFSClient。这样处理是比较简单的，但是性能会受到一些影响，因为需要Datanode在中间做一次中转。本文将介绍针对这个问题的一些优化。 既然DFSClient和数据是在一个机器上面，那么很自然的想法，就是让DFSClient绕开Datanode自己去读取数据，在具体实现上有如下两种方案。 HDFS-2246在这个JIRA中，工程师们的想法是既然读取数据DFSClient和数据在同一台机器上，那么Datanode就把数据在文件系统中的路径，从什么地方开始读(offset)和需要读取多少(length)等信息告诉DFSClient，然后DFSClient去打开文件自己读取。想法很好，问题在于配置复杂以及安全问题。 首先是配置问题，因为是让DFSClient自己打开文件读取数据，那么就需要配置一个白名单，定义哪些用户拥有访问Datanode的数据目录权限。如果有新用户加入，那么就得修改白名单。需要注意的是，这里是允许客户端访问Datanode的数据目录，也就意味着，任何用户拥有了这个权限，就可以访问目录下其他数据，从而导致了安全漏洞。因此，这个实现已经不建议使用了。 HDFS-347在Linux中，有个技术叫做Unix Domain Socket。Unix Domain Socket是一种进程间的通讯方式，它使得同一个机器上的两个进程能以Socket的方式通讯。它带来的另一大好处是，利用它两个进程除了可以传递普通数据外，还可以在进程间传递文件描述符。 假设机器上的两个用户A和B，A拥有访问某个文件的权限而B没有，而B又需要访问这个文件。借助Unix Domain Socket，可以让A打开文件得到一个文件描述符，然后把文件描述符传递给B，B就能读取文件里面的内容了即使它没有相应的权限。在HDFS的场景里面，A就是Datanode，B就是DFSClient，需要读取的文件就是Datanode数据目录中的某个文件。 这个方案在安全上就比上一个方案上好一些，至少它只允许DFSClient读取它需要的文件。 如果你想了解更多关于Unix Domain Socket的知识，可以看看：http://www.thomasstover.com/uds.html 和http://troydhanson.github.io/misc/Unix_domain_sockets.html如何配置因为Java不能直接操作Unix Domain Socket，所以需要安装Hadoop的native包libhadoop.so。如果你的集群是用各大Hadoop发行版（比如Pivotal HD，CDH等）来安装的，这些native包通常在安装Hadoop的时候会被安装好的。你可以用如下命令来检查这些native包是否安装好。CDH版本，native包check已安装:123456789[hadoop@hostname /home/q/hadoop/q_hadoop/bin]$ /home/q/hadoop/q_hadoop/bin/hadoop checknativeNative library checking:hadoop: true /home/q/hadoop/hadoop-2.5.0-cdh5.2.0/lib/native/libhadoop.sozlib: true /lib64/libz.so.1snappy: true /home/q/hadoop/hadoop-2.5.0-cdh5.2.0/lib/native/libsnappy.so.1lz4: true revision:99bzip2: falseopenssl: false Cannot load libcrypto.so.1.0.0 (libcrypto.so.1.0.0: cannot open shared object file: No such file or directory)! Short Circuit Local Reads相关的配置项（在hdfs-site.xml中）如下：12345678&lt;property&gt; &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.domain.socket.path&lt;/name&gt; &lt;value&gt;/home/q/hadoop/hadoop-2.5.0-cdh5.2.0/etc/hadoop/dn_socket&lt;/value&gt; &lt;/property&gt; 其中：dfs.client.read.shortcircuit是打开这个功能的开关，dfs.domain.socket.path是Datanode和DFSClient之间沟通的Socket的本地路径。 如何确认配置生效了按照上面的配置，如何确认从HDFS读取数据的时候，Short Circuit Local Reads真的起作用了？通常生成了socket文件即可表示成功，细粒度查看可以通过如下方法： 查看dn_socket文件是否真实存在和Datanode的日志. 12[hadoop@hostname /home/q/hadoop/q_hadoop/etc/hadoop]$ ls /home/q/hadoop/q_hadoop/etc/hadoop/dn_socket/home/q/hadoop/q_hadoop/etc/hadoop/dn_socket 在Datanode的启动日志中，也可以看到如下相关的日志表明Unix Domain Socket被启用了。 123452017-06-14 17:28:09,098 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 52017-06-14 17:28:09,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 5242880 bytes/s2017-06-14 17:28:09,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 52017-06-14 17:28:09,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Listening on UNIX domain socket: /home/q/hadoop/hadoop-2.5.0-cdh5.2.0/etc/hadoop/dn_socket 往hdfs创建一个tmp目录并上传一个a.txt文件,123456789101112131415161718192021222324252627282930313233343536373839404142434445[hadoop@hostname /home/q/hadoop/q_hadoop/bin]$ ./hdfs dfs -mkdir /tmp [hadoop@hostname /home/q/hadoop/q_hadoop/bin]$ ./hdfs dfs -ls /Found 2 itemsdrwxr-xr-x - hadoop supergroup 0 2017-03-06 11:10 /hbasedrwxr-xr-x - hadoop supergroup 0 2017-06-14 17:45 /tmp [hadoop@hostname /home/q/hadoop/q_hadoop/bin]$ touch /tmp/a.txt[hadoop@hostname /home/q/hadoop/q_hadoop/bin]$ echo &apos;Hello HBase!&apos; &gt; /tmp/a.txt[hadoop@hostname /home/q/hadoop/q_hadoop/bin]$ ./hdfs dfs -put /tmp/a.txt /tmp[hadoop@hostname /home/q/hadoop/q_hadoop/bin]$ ./hdfs dfs -ls /tmpFound 1 items-rw-r--r-- 3 hadoop supergroup 13 2017-06-14 17:58 /tmp/a.txt [hadoop@hostname /home/q/hadoop/q_hadoop/bin]$ ./hdfs fsck /tmp/a.txt -files -blocks17/06/14 17:59:25 WARN ssl.FileBasedKeyStoresFactory: The property &apos;ssl.client.truststore.location&apos; has not been set, no TrustStore will be loadedConnecting to namenode via http://hostname:50070FSCK started by hadoop (auth:SIMPLE) from /10.90.18.144 for path /tmp/a.txt at Wed Jun 14 17:59:25 CST 2017/tmp/a.txt 13 bytes, 1 block(s): OK0. BP-1703971618-10.90.18.140-1488769422929:blk_1073784567_43767 len=13 repl=3Status: HEALTHY Total size: 13 B Total dirs: 0 Total files: 1 Total symlinks: 0 Total blocks (validated): 1 (avg. block size 13 B) Minimally replicated blocks: 1 (100.0 %) Over-replicated blocks: 0 (0.0 %) Under-replicated blocks: 0 (0.0 %) Mis-replicated blocks: 0 (0.0 %) Default replication factor: 3 Average block replication: 3.0 Corrupt blocks: 0 Missing replicas: 0 (0.0 %) Number of data-nodes: 5 Number of racks: 1FSCK ended at Wed Jun 14 17:59:25 CST 2017 in 1 millisecondsThe filesystem under path &apos;/tmp/a.txt&apos; is HEALTHYBP-1703971618-10.90.18.140-1488769422929:blk_1073784567_43767 len=13 repl=3 可以看出该文件有一个block，id是：blk_1073784567_43767 将文件拷贝到本地: 12[hadoop@hostname /home/q/hadoop/q_hadoop/bin]$ rm -rf /tmp/a.txt[hadoop@hostname /home/q/hadoop/q_hadoop/bin]$ ./hadoop fs -get /tmp/a.txt /tmp datanode日志中会打印读取block使用了Short Circuit Local Reads读取block。 op: REQUEST_SHORT_CIRCUIT_FDS 1234562017-06-14 18:03:10,513 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: cliID: DFSClient_NONMAPREDUCE_859197200_1, src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_SHM, shmId: 3acf48858a1aca1c047bccc214f8ea8d, srvID: 8af8b978-b80f-4c5b-9ea4-8cd1d8d91200, success: true2017-06-14 18:03:10,536 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: 127.0.0.1, dest: 127.0.0.1, op: REQUEST_SHORT_CIRCUIT_FDS, blockid: 1073784567, srvID: 8af8b978-b80f-4c5b-9ea4-8cd1d8d91200, success: true Hedged Read功能是否开启？优化原理：HBase数据在HDFS中一般都会存储三份，而且优先会通过Short-Circuit Local Read功能尝试本地读。但是在某些特殊情况下，有可能会出现因为磁盘问题或者网络问题引起的短时间本地读取失败，为了应对这类问题，社区开发者提出了补偿重试机制 – Hedged Read。该机制基本工作原理为：客户端发起一个本地读，一旦一段时间之后还没有返回，客户端将会向其他DataNode发送相同数据的请求。哪一个请求先返回，另一个就会被丢弃。123This feature is off by default. To enable this feature, set &lt;code&gt;dfs.client.hedged.read.threadpool.size&lt;/code&gt; to a positive number. The threadpool size is how many threads to dedicate to the running of these &apos;hedged&apos;, concurrent reads in your client.Then set &lt;code&gt;dfs.client.hedged.read.threshold.millis&lt;/code&gt; to the number of milliseconds to wait before starting up a &apos;hedged&apos; read. For example, if you set this property to 10, then if a read has not returned within 10 milliseconds, we will start up a new read against a different block replica. 优化建议：开启Hedged Read功能，具体配置参考这里 (https://issues.apache.org/jira/browse/HDFS-5776) 配置步骤 修改一台rs hbase-site.xml,分发到其它rs并重启整个集群。12345678&lt;property&gt; &lt;name&gt;dfs.client.hedged.read.threadpool.size&lt;/name&gt; &lt;value&gt;20&lt;/value&gt; &lt;!-- 默认20 threads --&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.client.hedged.read.threshold.millis&lt;/name&gt; &lt;value&gt;50&lt;/value&gt; &lt;!-- 默认10 milliseconds --&gt;&lt;/property&gt; 1for i in `cat regionservers`;do &quot;scp hbase-site.xml $i:/home/q/hbase/q_hbase/conf/ &quot;;done 重启整个集群RS1/home/q/hbase/q_hbase/bin/hbase-daemons.sh restart regionserver 12345his feature emits new metrics: + hedgedReadOps + hedgeReadOpsWin -- how many times the hedged read &apos;beat&apos; the original read + hedgedReadOpsInCurThread -- how many times we went to do a hedged read but we had to run it in the current thread because dfs.client.hedged.read.threadpool.size was at a maximum.]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase运维要点]]></title>
    <url>%2F2019%2F02%2F20%2FHBase%E8%BF%90%E7%BB%B4%E8%A6%81%E7%82%B9%2F</url>
    <content type="text"><![CDATA[运维细节1.建表细节: 单列族、预分区、压缩、设置TTL2.flush和compaction操作是针对一个Region。所以当一个列族操作大量数据的时候会引发一个flush。那些不相关的列族也有进行flush操作，尽管他们没有操作多少数据。3.Compaction操作是根据一个列族下的全部文件的数量触发的，而不是根据文件大小触发的。当很多的列族在flush和compaction时,会造成很多没用的I/O负载(要想解决这个问题，需要将flush和compaction操作只针对一个列族)4.清空表数据:保留预分区 truncate操作只保留当前一个region，truncate_preserve 保留原有region, 推荐使用truncate_preserve hbase(main):006:0&gt; truncate_preserve ‘sec_data_hb:raw_data’ 5.Hbase数据写入和读取 默认先写WAL再写MEMSTORE、刷HFILE。 因为存储文件是不会改变的(memstore是写满就刷HFile，移出内存)，所以无法通过删除某个键值来真正的删除，而是做一个删除标记，表明给定行已被删除。 在检索过程中这些删除标记会被过滤后再返回客户端。（Hbase权威指南：删除是一种特殊的更改，当删除标记被存储之后，查找会跳过这些删除过的键。当页被重写时（即大合并）有删除标记的键会被丢弃） 数据读取需要合并两部分的读取数据，一部分是memstore中还没有刷入磁盘的数据，一部分是磁盘上的存储数据。即memstore + HFile。 数据查询先查找内存中的存储，然后再查找磁盘上的文件。数据读取不用WAL，WAL只是用于数据恢复才会应用WAL。 数据持久化 用户向HRegionServer发起HTable.put(Put)请求时会将请求给对应的Hregion实例来处理。第一步就是要决定数据是否需要写到由HLog类实现的预写日志中。 WAL是Hadoop SequenceFile并且存储了HLogKey实例，这些键包括了序列号和实际数据。可以通过Put.setwriteToWAL(boolean)方法关闭该步骤。 一旦数据被写入到WAL中，数据就会被放到MemStore中，同时会检查MemStore是否写满，写满则会被请求刷写到磁盘中去，刷写请求由另外一个HRegionServer的线程处理，它会把数据写成HDFS中的一个新HFILE，同时会保存最后写入的序号，系统可以知道哪些数据已经被持久化。 新版本的是缓存和wal同时写，细节见其他文章。 大小合并大合并、小合并:a)定时任务触发单个大表的大合并b)单线程手动触发单个大表某个region的合并 minor合并负责重写最后生成的几个文件到一个更大的文件中。 major合并负责把个region中一个列族的若干个HFile重写并压缩为一个新HFile。 拆分合并风暴：Hbase 自动处理region拆分，一旦达到阀值region将会拆分成两个。当用户的region大小以恒定的速度保持增长时，region拆分会在同一时间发生，因为同时需要压缩region中的存储文件，这个过程或重写拆分后的region，导致磁盘IO上升。 建议用户关闭Hbase自动管理拆分，手动调用split或major_compact命令，可以设置hbase.hregion.max.filesize或者在列族级别上把表模式中对应参数设置一个非常大的值。为了防止手动拆分无法运行，最好不要设置Long.MAX_VALUE。 手动运行可以控制系统资源负载，避免拆分合并风暴。 重点：大合并是将一个region中一个列族的若干个HFile重写并压缩为一个新HFile，和小合并不同的是：大合并能扫描所有的键值对，顺序重写全部的数据，重写数据的过程会跳过删除标记的数据，对于那些超过版本号限制和生存时间到期的数据，在重写数据时同样不会重写入新HFile。minor不会删除标示为删除的数据和过期的数据，major会删除需删除的数据，major合并之后，一个store只有一个storeFile文件，会对store的所有数据进行重写，有较大的性能消耗。 Compaction与Flush不同之处在于：Flush是针对一个Region整体执行操作，而Compaction操作是针对Region上的一个Store而言. 写性能优化是否需要写WAL？WAL是否需要同步写入？ 优化原理：数据写入流程可以理解为一次顺序写WAL+一次写缓存，通常情况下写缓存延迟很低，因此提升写性能就只能从WAL入手。默认WAL机制开启且使用同步机制写入WAL。首先考虑业务是否需要写WAL，通常情况下大多数业务都会开启WAL机制（默认），但是对于部分业务可能并不特别关心异常情况下部分数据的丢失，而更关心数据写入吞吐量，比如某些推荐业务，这类业务即使丢失一部分用户行为数据可能对推荐结果并不构成很大影响，但是对于写入吞吐量要求很高，不能造成数据队列阻塞。这种场景下可以考虑关闭WAL写入，写入吞吐量可以提升2x~3x。退而求其次，有些业务不能接受不写WAL，但可以接受WAL异步写入，也是可以考虑优化的，通常也会带来1x～2x的性能提升 优化推荐：根据业务关注点在WAL机制与写入吞吐量之间做出选择 其他注意点：对于使用Increment操作的业务，WAL可以设置关闭，也可以设置异步写入，方法同Put类似。相信大多数Increment操作业务对WAL可能都不是那么敏感～ 关闭WAL写入 setWriteToWAL(false): a.Put、Delete在客户端上可以通过setWriteToWAL(false)方法来关闭该操作的日志 可提升写入吞吐量，但存在数据丢失风险 一旦RegionServer宕机， Put/Delete的数据将会无法根据WAL日志进行恢复.WAL异步写入 setDurability(Durability. SYNC_WAL ); WAL持久化等级HBase中可以通过设置WAL的持久化等级决定是否开启WAL机制、以及HLog的落盘方式。WAL的持久化等级分为如下四个等级： SKIP_WAL：只写缓存，不写HLog日志。这种方式因为只写内存，因此可以极大的提升写入性能，但是数据有丢失的风险。在实际应用过程中并不建议设置此等级，除非确认不要求数据的可靠性。 ASYNC_WAL：异步将数据写入HLog日志中。 SYNC_WAL：同步将数据写入日志文件中，需要注意的是数据只是被写入文件系统中，并没有真正落盘。 FSYNC_WAL：同步将数据写入日志文件并强制落盘。最严格的日志写入等级，可以保证数据不会丢失，但是性能相对比较差。 USER_DEFAULT：默认如果用户没有指定持久化等级，HBase使用SYNC_WAL等级持久化数据。用户可以通过客户端设置WAL持久化等级，代码：put.setDurability(Durability. SYNC_WAL ); Put是否可以同步批量提交？优化原理：HBase分别提供了单条put以及批量put的API接口，使用批量put接口可以减少客户端到RegionServer之间的RPC连接数，提高写入性能。另外需要注意的是，批量put请求要么全部成功返回，要么抛出异常 优化建议：使用批量put进行写入请求 Put是否可以异步批量提交？优化原理：业务如果可以接受异常情况下少量数据丢失的话，还可以使用异步批量提交的方式提交请求。提交分为两阶段执行：用户提交写请求之后，数据会写入客户端缓存，并返回用户写入成功；当客户端缓存达到阈值（默认2M）之后批量提交给RegionServer。需要注意的是，在某些情况下客户端异常的情况下缓存数据有可能丢失. Hbase Client写入的数据量缓存达到setWriteBufferSize设置的阀值后才批量提交给RegionServer，可以减少RPC连接次数。需要注意服务端消耗的内存,在减少RPC交互次数和增加服务器端内存之间找到平衡点：hbase.client.write.buffer * hbase.regionserver.handler.count(公司线上设置64)优化建议：在业务可以接受的情况下开启异步批量提交；使用方式：setAutoFlush(false) 达到缓存阀值后批量提交 123HTable htable = new HTable(config, tablename); htable.setWriteBufferSize(6 * 1024 * 1024); htable.setAutoFlush(false); Region是否太少？优化原理：当前集群中表的Region个数如果小于RegionServer个数，即Num(Region of Table) &lt; Num(RegionServer)，可以考虑切分Region并尽可能分布到不同RegionServer来提高系统请求并发度，如果Num(Region of Table) &gt; Num(RegionServer)，再增加Region个数效果并不明显。 优化建议：在Num(Region of Table) &lt; Num(RegionServer)的场景下切分部分请求负载高的Region并迁移到其他RegionServer；建表时预分配Region 123create &apos;namespace:tablename&apos;, &#123;NUMREGIONS =&gt; 5, SPLITALGO =&gt; &apos;HexStringSplit&apos;&#125;, &#123;NAME =&gt; &apos;f&apos;, COMPRESSION=&gt;&apos;SNAPPY&apos;, VERSIONS =&gt; 1&#125; 写入请求是否不均衡？优化原理：另一个需要考虑的问题是写入请求是否均衡，如果不均衡，一方面会导致系统并发度较低，另一方面也有可能造成部分节点负载很高，进而影响其他业务。分布式系统中特别害怕一个节点负载很高的情况，一个节点负载很高可能会拖慢整个集群，这是因为很多业务会使用Mutli批量提交读写请求，一旦其中一部分请求落到该节点无法得到及时响应，就会导致整个批量请求超时。因此不怕节点宕掉，就怕节点奄奄一息！ 优化建议：检查RowKey设计以及预分区策略，保证写入请求均衡。ROWKEY设计尽量均衡分布到各个RegionServer。 Waited 3722ms on a compaction to clean up too many store files”对于数据写入很快的集群，还需要特别关注一个参数：hbase.hstore.blockingStoreFiles，此参数表示如果当前hstore中文件数大于该值，系统将会强制执行compaction操作进行文件合并，合并的过程会阻塞整个hstore的写入。通常情况下该场景发生在数据写入很快的情况下，在日志中可以发现”Waited 3722ms on a compaction to clean up too many store files” 问题检查点： 参数设置是否合理？hbase.hstore.compactionThreshold表示启动compaction的最低阈值，该值不能太大，否则会积累太多文件，一般建议设置为5～8左右。hbase.hstore.blockingStoreFiles默认设置为7，可以适当调大一些。 写入KeyValue数据或单行数据是否太大？KeyValue太大会导致HLog文件写入频繁切换、flush以及compaction频繁触发，写入性能急剧下降。 目前针对这种较大KeyValue写入性能较差的问题还没有直接的解决方案，好在社区已经意识到这个问题，在接下来即将发布的下一个大版本HBase 2.0.0版本会针对该问题进行深入优化，详见HBase MOB，优化后用户使用HBase存储文档、图片等二进制数据都会有极佳的性能体验。 “java.lang.OutOfMemoryError: Requested array size exceeds VM limit “ 原因分析：通过查看源码以及相关文档，确认该异常发生在scan结果数据回传给客户端时由于数据量太大导致申请的array大小超过JVM规定的最大值（ Interge.Max_Value-2）。造成该异常的两种最常见原因分别是： 123表列太宽（几十万列或者上百万列），并且scan返回没有对列数量做任何限制，导致一行数据就可能因为包含大量列而数据超过array大小阈值KeyValue太大，并且scan返回没有对返回结果大小做任何限制，导致返回数据结果大小超过array大小阈值 如果已经对返回结果大小做了限制，在表列太宽的情况下是不是就可以不对列数量做限制呢。这里需要澄清一下，如果不对列数据做限制，数据总是一行一行返回的，即使一行数据大小大于设置的返回结果限制大小，也会返回完整的一行数据。在这种情况下，如果这一行数据已经超过array大小阈值，也会触发OOM异常。 解决方案：目前针对该异常有两种解决方案:其一是升级集群到1.0，问题都解决了。其二是要求客户端访问的时候对返回结果大小做限制(scan.setMaxResultSize(210241024))、并且对列数量做限制(scan.setBatch(100))，当然，0.98.13版本以后也可以对返回结果大小在服务器端进行限制，设置参数 hbase.server.scanner.max.result.size即可。 大字段scan的解决方法： 对返回结果大小限制 (scan.setMaxResultSize(210241024)) 对列数量做限制 (scan.setBatch(100)) 对返回结果大小在服务器端进行限制，设置参数hbase.server.scanner.max.result.size]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[压缩--HBase配置snappy]]></title>
    <url>%2F2019%2F02%2F20%2F%E5%8E%8B%E7%BC%A9-HBase%E9%85%8D%E7%BD%AEsnappy%2F</url>
    <content type="text"><![CDATA[编译安装snappy 12345git clone https://github.com/google/snappy.gityum install -y automake autoconf gcc-c++ cmake libedit./configuremakemake install 编译安装hadoop-snappy 123456git clone https://github.com/louishust/hadoop-snappy.gitln -s /home/q/java/default/jre/lib/amd64/server/libjvm.so /usr/local/lib/mvn packagecp target/hadoop-snappy-0.0.1-SNAPSHOT.jar /home/q/hadoop/hadoop-2.5.0-cdh5.2.0/lib/cp target/hadoop-snappy-0.0.1-SNAPSHOT-tar/hadoop-snappy-0.0.1-SNAPSHOT/lib/native/Linux-amd64-64/* /home/q/hadoop/hadoop-2.5.0-cdh5.2.0/lib/nativecp target/hadoop-snappy-0.0.1-SNAPSHOT-tar/hadoop-snappy-0.0.1-SNAPSHOT/lib/native/Linux-amd64-64/* /home/q/hbase/hbase-0.98.6-cdh5.2.0/lib/native/Linux-amd64-64/ 重新编译libhadoop以支持snappy首先需要protocbuf 2.5.0以上 12mvn clean package -Pdist,native -DskipTests -Drequire.snappycp /home/q/hadoop/hadoop-2.5.0-cdh5.2.0/src/hadoop-dist/target/hadoop-2.5.0-cdh5.2.0/lib/native/* /home/q/hadoop/hadoop-2.5.0-cdh5.2.0/lib/native 修改hadoop配置文件core-site.xml 123&lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;&lt;/property&gt; hadoop-env.sh12export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/q/hadoop/hadoop-2.5.0-cdh5.2.0/lib/native:/usr/local/lib/export JAVA_LIBRARY_PATH=$JAVA_LIBRARY_PATH:/home/q/hadoop/hadoop-2.5.0-cdh5.2.0/lib/native:/home/q/hadoop/hadoop-2.5.0-cdh5.2.0/lib yarn-site.xml12345678&lt;property&gt; &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapred.map.output.compress.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt;&lt;/property&gt; 修改hbase配置文件hbase-env.sh 1234### for snappyexport HBASE_HOME=/home/q/hbase/hbase-0.98.6-cdh5.2.0/export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native/:/usr/local/lib/export HBASE_LIBRARY_PATH=$HBASE_LIBRARY_PATH:$HBASE_HOME/lib/native/Linux-amd64-64/:/usr/local/lib/export CLASSPATH=$CLASSPATH:$HBASE_LIBRARY_PATH 配置libjvm 1sudo ln -s /home/q/java/default/jre/lib/amd64/server/libjvm.so /usr/local/lib/ 重启集群 1234bin/stop-hbase.shsbin/stop-dfs.shsbin/start-dfs.shbin/start-hbase.sh h2. 8. 验证snappy1234touch /tmp/ahbase org.apache.hadoop.hbase.util.CompressionTest /tmp/a snappybin/hbase shellcreate &apos;tsnappy&apos;,&#123; NAME =&gt; &apos;f&apos;, COMPRESSION =&gt; &apos;snappy&apos;&#125;]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[netty]]></title>
    <url>%2F2019%2F02%2F19%2Fnetty%2F</url>
    <content type="text"><![CDATA[Netty提供异步的、事件驱动的网络应用程序框架和工具，用以快速开发高性能、高可靠性的网络服务器和客户端程序 NIO模型同步非阻塞NIO有同步阻塞和同步非阻塞两种模式，一般讲的是同步非阻塞，服务器实现模式为一个请求一个线程，但客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。 AIO模型异步非阻塞服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理，注：AIO又称为NIO2.0，在JDK7才开始支持。 为什么Netty使用NIO而不是AIO？ Netty不看重Windows上的使用，在Linux系统上，AIO的底层实现仍使用EPOLL，没有很好实现AIO，因此在性能上没有明显的优势，而且被JDK封装了一层不容易深度优化 Netty整体架构是reactor模型, 而AIO是proactor模型, 混合在一起会非常混乱,把AIO也改造成reactor模型看起来是把epoll绕个弯又绕回来 AIO还有个缺点是接收数据需要预先分配缓存, 而不是NIO那种需要接收时才需要分配缓存, 所以对连接数量非常大但流量小的情况, 内存浪费很多 Linux上AIO不够成熟，处理回调结果速度跟不到处理需求，比如外卖员太少，顾客太多，供不应求，造成处理速度有瓶颈 NIO是同步的，netty为什么是异步的netty是通过事件驱动实现异步的，所以netty才可以提供异步非阻塞服务，它提供了对TCP、UDP和文件传输的支持。所有的IO操作都是异步的，用户可以通过Future-Listeren机制主动get结果或者等IO线程完成操作之后主动Notify来通知，用户业务线程不需要同步等待。用户能够方便的主动获取或者通过通知机制获得IO操作结果。 io.netty.channel.Channel类的一段注释请参考。123456All I/O operations are asynchronous.All I/O operations in Netty are asynchronous. It means any I/O calls willreturn immediately with no guarantee that the requested I/O operation hasbeen completed at the end of the call. Instead, you will be returned witha &#123;@link ChannelFuture&#125; instance which will notify you when the requested I/Ooperation has succeeded, failed, or canceled. Netty说自己是异步事件驱动的框架，并没有说网络模型用的是异步模型，异步事件驱动框架体现在所有的I/O操作是异步的，所有的IO调用会立即返回，并不保证调用成功与否，但是调用会返回ChannelFuture，netty会通过ChannelFuture通知你调用是成功了还是失败了亦或是取消了。 Netty的宣传：Netty is an asynchronous event-driven network application frameworkfor rapid development of maintainable high performance protocol servers &amp; clients 参考：https://www.jianshu.com/p/df1d6d8c3f9dhttps://www.cnblogs.com/UncleCatMySelf/p/9187220.htmlhttps://segmentfault.com/q/1010000016221286]]></content>
      <categories>
        <category>netty</category>
      </categories>
      <tags>
        <tag>netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本概念]]></title>
    <url>%2F2019%2F02%2F19%2FI-O%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[同步就是如果有多个任务或者事件要发生，这些任务或者事件必须逐个地进行，一个事件或者任务的执行会导致整个流程的暂时等待，这些事件没有办法并发地执行。 异步就是如果有多个任务或者事件发生，这些事件可以并发地执行，一个事件或者任务的执行不会导致整个流程的暂时等待 重点：判断是同步还是异步，就看同时多个任务和事件发生时，一个事件的发生或执行是否会导致整个流程的暂时等待如果导致整个流程的暂时等待，就是同步；如果可以并行执行，那就是异步。 阻塞就是当某个事件或者任务在执行过程中，它发出一个请求操作，但是由于该请求操作需要的条件不满足，那么就会一直在那等待，直至条件满足。 非阻塞就是当某个事件或者任务在执行过程中，它发出一个请求操作，如果该请求操作需要的条件不满足，会立即返回一个标志信息告知条件不满足，不会一直在那等待。 重点:阻塞和非阻塞的区别。也就是说阻塞和非阻塞的区别关键在于当发出请求一个操作时，如果条件不满足，是会一直等待还是返回一个标志信息。 总结同步和异步着重点在于多个任务的执行过程中，一个任务的执行是否会导致整个流程的暂时等待； 而阻塞和非阻塞着重点在于发出一个请求操作时，如果进行操作的条件不满足是否会返会一个标志信息告知条件不满足。]]></content>
      <categories>
        <category>netty</category>
      </categories>
      <tags>
        <tag>netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive的ACID]]></title>
    <url>%2F2019%2F02%2F19%2Fhive%E7%9A%84ACID%2F</url>
    <content type="text"><![CDATA[Hive自0.14版本开始支持update和delete，要执行update和delete的表必须支持ACID。但缺省是不支持的，，需要一些附加的配置。如果一个表要实现update和delete功能，该表就必须支持ACID，而支持ACID，就必须满足以下条件：1、表的存储格式必须是ORC（STORED AS ORC）；2、表必须进行分桶（CLUSTERED BY (col_name, col_name, …) INTO num_buckets BUCKETS）；3、Table property中参数transactional必须设定为True（tblproperties(‘transactional’=’true’)）；4、以下配置项必须被设定：Client端：1234hive.support.concurrency – truehive.enforce.bucketing – truehive.exec.dynamic.partition.mode – nonstrict hive.txn.manager – org.apache.hadoop.hive.ql.lockmgr.DbTxnManager 服务端：123hive.compactor.initiator.on – truehive.compactor.worker.threads – 1hive.txn.manager – org.apache.hadoop.hive.ql.lockmgr.DbTxnManager 说明：不能修改bucket列的值，否则会报以下错误：FAILED: SemanticException [Error 10302]: Updating values of bucketing columns is not supported. Column id.说明：不能update分区键，否则会报以下错误：FAILED: SemanticException [Error 10292]: Updating values of partition columns is not supported 由于HDFS是不支持本地文件更改的，同时在写的时候也不支持读。表或者分区内的数据作为基础数据。事务产生的新数据如Insert/Update/Flume/Storm等会存储在增量文件（Delta Files）中。读取这个文件的时候，通常是Table Scan阶段，会合并更改，使读出的数据一致。 Hive Metastore上面增加了若干个线程，会周期性地合并并合并删除这些增量文件。 具体可以实现参考这个网页。 https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark 算子错误使用]]></title>
    <url>%2F2019%2F02%2F19%2Fspark-%E7%AE%97%E5%AD%90%E9%94%99%E8%AF%AF%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[cache本例中，首先计算出一个baseRDD，然后对其进行cache，后续启动三个子任务基于cache进行后续计算。 对于5分钟小数据量，采用StorageLevel.MEMORY_ONLY，而对于大数据下我们直接采用了StorageLevel.DISK_ONLY。DISK_ONLY_2相较DISK_ONLY具有2备份，cache的稳定性更高，但同时开销更大，cache除了在executor本地进行存储外，还需走网络传输至其他节点。后续我们的优化，会保证executor的稳定性，故没有必要采用DISK_ONLY_2。实时上，如果优化的不好，我们发现executor也会大面积挂掉，这时候即便DISK_ONLY_2，也是然并卵，所以保证executor的稳定性才是保证cache稳定性的关键。 cache是lazy执行的，这点很容易犯错，例如：12345678910val raw = sc.textFile(file)val baseRDD = raw.map(...).filter(...)baseRDD.cache()val threadList = new Array( new Thread(new SubTaskThead1(baseRDD)), new Thread(new SubTaskThead2(baseRDD)), new Thread(new SubTaskThead3(baseRDD)))threadList.map(_.start())threadList.map(_.join()) 这个例子在三个子线程开始并行执行的时候，baseRDD由于lazy执行，还没被cache，这时候三个线程会同时进行baseRDD的计算，cache的功能形同虚设。可以在baseRDD.cache()后增加baseRDD.count()，显式的触发cache，当然count()是一个action，本身会触发一个job。 再举一个错误的例子：12345val raw = sc.textFile(file)val pvLog = raw.filter(isPV(_))val clLog = raw.filter(isCL(_))val baseRDD = pvLog.union(clLog)val baseRDD.count() 由于textFile()也是lazy执行的，故本例会进行两次相同的hdfs文件的读取，效率较差。解决办法，是对pvLog和clLog共同的父RDD进行cache。 Partition一个stage由若干partition并行执行，partition数是一个很重要的优化点。 本例中，一天的日志由6000个小文件组成，加上后续复杂的统计操作，某个stage的parition数达到了100w。parition过多会有很多问题，比如所有task返回给driver的MapStatus都已经很大了，超过spark.driver.maxResultSize（默认1G），导致driver挂掉。虽然spark启动task的速度很快，但是每个task执行的计算量太少，有一半多的时间都在进行task序列化，造成了浪费，另外shuffle过程的网络消耗也会增加。 对于reduceByKey()，如果不加参数，生成的rdd与父rdd的parition数相同，否则与参数相同。还可以使用coalesce()和repartition()降低parition数。例如，本例中由于有6000个小文件，导致baseRDD有6000个parition，可以使用coalesce()降低parition数，这样parition数会减少，每个task会读取多个小文件。123val raw = sc.textFile(file).coalesce(300)val baseRDD = raw.map(...).filter(...)baseRDD.cache() 那么对于每个stage设置多大的partition数合适那？当然不同的程度的复杂度不同，这个数值需要不断进行调试，本例中经测试保证每个parition的输入数据量在1G以内即可，如果parition数过少，每个parition读入的数据量变大，会增加内存的压力。例如，我们的某一个stage的ShuffleRead达到了3T，我设置parition数为6000，平均每个parition读取500M数据。12val bigRDD = ...bigRDD.coalesce(6000).reduceBy(...) 最后，一般我们的原始日志很大，但是计算结果很小，在saveAsTextFile前，可以减少结果rdd的parition数目，这样会计算hdfs上的结果文件数，降低小文件数会降低hdfs namenode的压力，也会减少最后我们收集结果文件的时间。12val resultRDD = ...resultRDD.repartition(1).saveAsTextFile(output) 这里使用repartition()不使用coalesce()，是为了不降低resultRDD计算的并发量，通过再做一次shuffle将结果进行汇总。 repartition和coalesce的区别12repartition(numPartitions:Int):RDD[T] coalesce(numPartitions:Int，shuffle:Boolean=false):RDD[T] 他们两个都是RDD的分区进行重新划分，repartition只是coalesce接口中shuffle为true的简易实现，（假设RDD有N个分区，需要重新划分成M个分区） 1）、N&lt;M。一般情况下N个分区有数据分布不均匀的状况，利用HashPartitioner函数将数据重新分区为M个，这时需要将shuffle设置为true。 2）如果N&gt;M并且N和M相差不多，(假如N是1000，M是100)那么就可以将N个分区中的若干个分区合并成一个新的分区，最终合并为M个分区，这时可以将shuff设置为false，在shuffl为false的情况下，如果M&gt;N时，coalesce为无效的，不进行shuffle过程，父RDD和子RDD之间是窄依赖关系。 3）如果N&gt;M并且两者相差悬殊，这时如果将shuffle设置为false，父子ＲＤＤ是窄依赖关系，他们同处在一个Ｓｔａｇｅ中，就可能造成spark程序的并行度不够，从而影响性能，如果在M为1的时候，为了使coalesce之前的操作有更好的并行度，可以讲shuffle设置为true。 总之：如果shuff为false时，如果传入的参数大于现有的分区数目，RDD的分区数不变，也就是说不经过shuffle，是无法将RDDde分区数变多的。 http://www.cnblogs.com/jiangxiaoxian/p/9539760.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark优化]]></title>
    <url>%2F2019%2F02%2F19%2Fspark-gc%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[设置合适的资源参数spark程序跑在yarn集群上1234--queue：集群队列--num-executors：executor数量，默认2--executor-memory：executor内存，默认512M--executor-cores：每个executor的并发数，默认1 executor的数量可以根据任务的并发量进行估算，例如我有1000个任务，每个任务耗时1分钟，若10个并发则耗时100分钟，100个并发耗时10分钟，根据自己对并发需求进行调整即可。默认每个executor内有一个并发执行任务，一般够用，也可适当增加，当然内存的使用也会有所增加。 对于yarn-client模式，整个application所申请的资源为：12total vores = executor-cores * num-executors + spark.yarn.am.corestotal memory= (executor-memory + spark.yarn.executor.memoryOverhead) * num-executors + (spark.yarn.am.memory + spark.yarn.am.memoryOverhead) 当申请的资源超出所指定的队列的min cores和min memory时，executor就有被yarn kill掉的风险。而spark的每个stage是有状态的，如果被kill掉，对性能影响比较大。例如，本例中的baseRDD被cache，如果某个executor被kill掉，会导致其上的cache的parition失效，需要重新计算，对性能影响极大。 executor-memory这里还有一点需要注意，executor-memory设置的是executor jvm启动的最大堆内存，java内存除了堆内存外，还有栈内存、堆外内存等，所以spark使用spark.yarn.executor.memoryOverhead对非堆内存进行限制，也就是说executor-memory + spark.yarn.executor.memoryOverhead是所能使用的内存的上线，如果超过此上线，就会被yarn kill掉。本次优化，堆外内存的优化起到了至关重要的作用，我们后续会看到。 spark.yarn.executor.memoryOverheadspark.yarn.executor.memoryOverhead默认是executor-memory * 0.1，最小是384M。比如，我们的executor-memory设置为1G，spark.yarn.executor.memoryOverhead是默认的384M，则我们向yarn申请使用的最大内存为1408M，但由于yarn的限制为倍数（不知道是不是只是我们的集群是这样），实际上yarn运行我们运行的最大内存为2G。这样感觉浪费申请的内存，申请的堆内存为1G，实际上却给我们分配了2G，如果对spark.yarn.executor.memoryOverhead要求不高的话，可以对executor-memory再精细化，比如申请executor-memory为640M，加上最小384M的spark.yarn.executor.memoryOverhead，正好一共是1G。 spark.yarn.am.memory除了启动executor外，spark还会启动一个am，可以使用spark.yarn.am.memory设置am的内存大小，默认是512M， spark.yarn.am.memoryOverheadspark.yarn.am.memoryOverhead默认也是最小384M。有时am会出现OOM的情况，可以适当调大spark.yarn.am.memory。 spark.executor.extraJavaOptionsexecutor默认的永久代内存是64K，可以看到永久代使用率长时间为99%，通过设置spark.executor.extraJavaOptions适当增大永久代内存，例如：–conf spark.executor.extraJavaOptions=”-XX:MaxPermSize=64m” driver-memorydriver端在yarn-client模式下运行在本地，也可以对相关参数进行配置，如–driver-memory等。 Direct Memory我们使用的spark版本是1.5.2（更准确的说是1.5.3-shapshot），shuffle过程中block的传输使用netty（spark.shuffle.blockTransferService）。基于netty的shuffle，使用direct memory存进行buffer（spark.shuffle.io.preferDirectBufs），所以在大数据量shuffle时，堆外内存使用较多。当然，也可以使用传统的nio方式处理shuffle，但是此方式在spark 1.5版本设置为deprecated，并将会在1.6版本彻底移除，所以我最终还是采用了netty的shuffle。 jvm关于堆外内存的配置相对较少，通过-XX:MaxDirectMemorySize可以指定最大的direct memory。默认如果不设置，则与最大堆内存相同。 Direct Memory是受GC控制的，例如ByteBuffer bb = ByteBuffer.allocateDirect(1024)，这段代码的执行会在堆外占用1k的内存，Java堆内只会占用一个对象的指针引用的大小，堆外的这1k的空间只有当bb对象被回收时，才会被回收，这里会发现一个明显的不对称现象，就是堆外可能占用了很多，而堆内没占用多少，导致还没触发GC。加上-XX:MaxDirectMemorySize这个大小限制后，那么只要Direct Memory使用到达了这个大小，就会强制触发GC，这个大小如果设置的不够用，那么在日志中会看到java.lang.OutOfMemoryError: Direct buffer memory。 例如，在我们的例子中，发现堆外内存飙升的比较快，很容易被yarn kill掉，所以应适当调小-XX:MaxDirectMemorySize（也不能过小，否则会报Direct buffer memory异常）。当然你也可以调大spark.yarn.executor.memoryOverhead，加大yarn对我们使用内存的宽容度，但是这样比较浪费资源了。 GC优化GC优化前，最好是把gc日志打出来，便于我们进行调试。1--conf spark.executor.extraJavaOptions=&quot;-XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintHeapAtGC -XX:+PrintGCApplicationConcurrentTime -Xloggc:gc.log&quot; 通过看gc日志，我们发现一个case，特定时间段内，堆内存其实很闲，堆内存使用率也就5%左右，长时间不进行父gc，导致Direct Memory一直不进行回收，一直在飙升。所以，我们的目标是让父gc更频繁些，多触发一些Direct Memory回收。 第一，可以减少整个堆内存的大小，当然也不能太小，否则堆内存也会报OOM。这里，我配置了1G的最大堆内存。 第二，可以让年轻代的对象尽快进入年老代，增加年老代的内存。这里我使用了-Xmn100m，将年轻代大小设置为100M。另外，年轻代的对象默认会在young gc 15次后进入年老代，这会造成年轻代使用率比较大，young gc比较多，但是年老代使用率低，full gc比较少，通过配置-XX:MaxTenuringThreshold=1，年轻代的对象经过一次young gc后就进入年老代，加快年老代fullgc的频率。 第三，可以让年老代更频繁的进行fullgc。一般年老代gc策略我们主要有-XX:+UseParallelOldGC和-XX:+UseConcMarkSweepGC这两种，ParallelOldGC吞吐率较大，ConcMarkSweepGC延迟较低。我们希望fullgc频繁些，对吞吐率要求较低，而且ConcMarkSweepGC可以设置-XX:CMSInitiatingOccupancyFraction，即年老代内存使用率达到什么比例时触发CMS。我们决定使用CMS，并设置-XX:CMSInitiatingOccupancyFraction=10，即年老代使用率10%时触发fullgc。 通过对GC策略的配置，我们发现fullgc进行的频率加快了，带来好处就是Direct Memory能够尽快进行回收，当然也有坏处，就是gc时间增加了，cpu使用率也有所增加。 最终我们对executor的配置如下：1--executor-memory 1G --num-executors 160 --executor-cores 1 --conf spark.yarn.executor.memoryOverhead=2048 --conf spark.executor.extraJavaOptions=&quot;-XX:MaxPermSize=64m -XX:+CMSClassUnloadingEnabled -XX:MaxDirectMemorySize=1536m -Xmn100m -XX:MaxTenuringThreshold=1 -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=10 -XX:+UseCompressedOops -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintHeapAtGC -XX:+PrintGCApplicationConcurrentTime -Xloggc:gc.log -XX:+HeapDumpOnOutOfMemoryError&quot;]]></content>
  </entry>
  <entry>
    <title><![CDATA[hbase读流程代码分析]]></title>
    <url>%2F2019%2F02%2F18%2Fhbase%E8%AF%BB%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[流程总览 从zookeeper中获取meta信息，并通过meta信息找到需要查找的table的startkey所在的region信息 和该region所在的regionserver进行rpc交互获取result region server查询memstore（memstore是是一个按key排序的树形结构的缓冲区），如果有该rowkey，则直接返回，若没有进入步骤4 查询blockcache，如果有该rowkey，则直接返回，若没有进入步骤5 查询storefile，不管有没有都直接返回 client代码分析hbase读数据除了直接操作hfile之外有3个入口，get()，batch()和scan()，get()相对而言就比较简单，找到对应的regionserver然后发rpc即可，batch()采用单rpc多action的策略流程和get()类似，下面主要对scan涉及的核心接口进行分析。核心接口有以下几个 Connection：负责和zk建立连接 Table：负责维护相关对象 ResultScanner：负责给使用者遍历纾解 Caller：负责调用Callable Callable：客户端和hbase交互的主要接口 Connection默认的连接器是HConnectionImplementation，可以通过配置hbase.client.connection.impl修改。核心思路是基于zk的watcher，保持长连接，然后获取hbase元数据 Tabletable通过Connection.getTable()实例化，默认的实现是HTable。这个类比较简单，只是维护了针对hbase一张表所用到的对象。主要关注遍历的方法，通过HTable.getScanner()实例化一个新的ResultScanner，使用者通过ResultScanner迭代器遍历获取result数据。 Scannerclient提供了4种scanner，参考HTable.getScanner()，1. ClientScanner，读取result的流程需要3次rpc，openScanner，next和closeScanner；2. 针对小量数据优化的ClientSmallScanner，和ClientScanner的区别在于，将openScanner,next和closeScanner合并到一个rpc执行，官方建议拉取的数据在64KB之内可以考虑用SmallScanner的方式；另外两个是基于reversed配置，也就是倒序遍历region，需要交换startkey和endkey的位置。ClientScanner是我们最常用的Scanner，也是默认的Scanner，下面对其进行分析 在初始化的时候通过nextScanner()方法，实例化一个新的Callable对象，并调用其call()方法 next()方法，当使用者不断的调用next()时，ClientScanner()会先从缓存中找，是否还有result，如果还有那么直接返回，如果缓存中没有result，那么调用loadCache()方法 loadCache()方法，调用Callable.call()，获取result数组。这里的异常处理需要特别关注，如果是UnkonwnScannerException，那么重试rpc直到本次scan超时，如果是OutOfOrderScannerNextException异常，scanner会重试rpc请求重复步骤3，如果已经重试过，那么直接抛出异常。重试的超时时间的配置hbase.client.scanner.timeout.period，默认是60s 拉取到result后，ClientScanner会进行合并，这是由于拉取到的result是部分的，不是完整的，说到底hbase是以Cell为最小单位进行存储或者传输的，要封装成result的话就需要进行合并。合并完之后将result缓存在内存中，缓存策略基于caching和maxResultSize，caching表示hbase client最多可以缓存在内存多少条数据，也就是多少个result；maxResultSize表示hbase client最多可以缓存多少内存大小的result，也就是控制result占用堆的大小 判断是否还需要再拉取result，这里有两种拉取判断，一种是之前的region拉取失败，转而拉取其replica，另一种是调用rpc拉取下一组result。 result达到内存限制或者数量（maxResultSize，caching）则返回 服务器代码ScannerCallableClientScanne对应的Callable是ScannerCallable，也是最典型的Callable，下面对其核心方法进行分析 prepare()方法核心功能是通过RPCRetryingCallerWithReadReplicas.getRegionLocations获取待遍历的table startkey的region，从而定位到region server。 核心call()方法 首次调用call()，client会发送一次开始rpc，高速region server本次scan开始了，此次rpc不获取result，只生成scannerId，之后的rpc不需要再传递scan配置，这形成了一个会话的概念。 通过rpc controller获取CellScanner，再转换成Result数组，这里参考ResponseConverter.getResults。注意，这里由于获取的result是连续的，也就是说region server是有状态的服务，client每次rpc都会带上当前请求的序号，也就是nextCallSeq，这有的类似传统数据库中的分页作用。当出现序号不匹配，region server会抛出异常。 如果需要关闭，那么向region server发送close的rpc 总结hbase-client的scan操作总体上可以看成是两层迭代器，面向使用者的Scanner以及面向region server的Callable。Callable负责从regionserver中获取result，主要解决，Scanner负责整合result提供给使用者。这样做的思路很明显，数据大小是肯定会大于内存的，通过迭代器接口，可以让使用者处理完之前的result再拉取其他result，从而起到分页的效果，这操作对使用者是透明的。如果需要详细的scan日志，可以通过配置hbase.client.log.scanner.activity来打开开关，默认是false。 对于scan操作而言，拿ClientScanner来说，一次“完整rpc”过程包含3次rpc，open，result和close。如果失败了，region不可用或者在split，那么client会重试新的一次“完整rpc”，那么就是6次rpc。其他操作会少一点，例如SmallClientScanner一次“完整rpc”只需要1次rpc，它把open，close集成到了一起。hbase在client还是花了不少心思的。 hbase客户端重要参数由于作为在线服务,需要能够保证在快速失败、失败容错重试等特性。 快速失败能保证系统的低延时,能防止因为等待某个资源，造成服务资源暂用,最后导致服务不可用。 失败容错能够提供服务的稳定性,进行服务失败是重试。 Hbase客户端提供的重试机制，并通过配置合理的参数使得客户端在保证一定容错性的同时还能够保证系统的低延迟特性。 hbase.client.pause失败重试时等待时间，随着重试次数越多,重试等待时间越长，计算方式如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116public static int RETRY_BACKOFF[] = &#123; 1, 2, 3, 5, 10, 20, 40, 100, 100, 100, 100, 200, 200 &#125;; long normalPause = pause * HConstants.RETRY_BACKOFF[ntries];long jitter = (long)(normalPause * RANDOM.nextFloat() * 0.01f); ``` 所以如果重试10次,hbase.client.pause=50ms，则每次重试等待时间为&#123;50，100，150，250，500，1000，2000，5000，5000，5000&#125;。 属性默认值为100ms。 hbase.client.pause属性控制的是让客户端在两次重试之间休眠多久。其默认值是100毫秒（0.1秒），建议设置为20## hbase.client.retries.number 失败时重试次数,默认为31次。可以根据自己应用的需求将该值调整的比较小。 hbase.client.retries.number属性用来指定最大重试次数。其默认值是350，建议设置为11 每两次重试之间的休眠时间可按下面这个公式计算得出 其中，RETRY_BACKOFF是一个重试系数表，其定义如下。 public static int RETRY_BACKOFF[] =(1, 1, 1, 2, 2, 4, 4, 8, 16, 32) 在重试10次以后，HBase就会一直使用最后一个系数（32）来计算休眠时间。 如果将暂停时间设为20毫秒，最大重试次数设为11，每两次连接集群重试之间的暂停时间将依次为：(20, 20, 20, 40, 40, 80, 80, 160, 320, 640, 640) 这就意味着客户端将在2060毫秒内重试11次，然后放弃连接到集群。## hbase.rpc.timeout 该参数表示一次RPC请求的超时时间。如果某次RPC时间超过该值，客户端就会主动关闭socket。 默认该值为1min,应用为在线服务时,可以根据应用的超时时间,设置该值.如果应用总共超时为3s,**则该值也应该为3s或者更小**. ## hbase.client.operation.timeout 该参数表示HBase客户端发起一次数据操作直至得到响应之间总的超时时间，数据操作类型包括get、append、increment、delete、put等。该值与hbase.rpc.timeout的区别为,hbase.rpc.timeout为一次rpc调用的超时时间。而hbase.client.operation.timeout为一次操作总的时间(从开始调用到重试n次之后失败的总时间)。 举个例子说明，比如一次Put请求，客户端首先会将请求封装为一个caller对象，该对象发送RPC请求到服务器，假如此时因为服务器端正好发生了严重的Full GC，导致这次RPC时间超时引起SocketTimeoutException，对应的就是hbase.rpc.timeout。那假如caller对象发送RPC请求之后刚好发生网络抖动，进而抛出网络异常，HBase客户端就会进行重试，重试多次之后如果总操作时间超时引起SocketTimeoutException，对应的就是hbase.client.operation.timeout。 ## hbase.client.scanner.timeout.period 该参数是表示HBase客户端发起一次scan操作的rpc调用至得到响应之间总的超时时间。一次scan操作是指发起一次regionserver rpc调用的操作,hbase会根据scan查询条件的cacheing、batch设置将scan操作会分成多次rpc操作。比如满足scan条件的rowkey数量为10000个，scan查询的cacheing=200，则查询所有的结果需要执行的rpc调用次数为50个。而该值是指50个rpc调用的单个相应时间的最大值。 ## 可以考虑使用HtableInterface带诶Htable ## hbase.ipc.client.tcpnodelay此设置将禁止使用Nagle算法来进行客户端和服务器之间的套接字传输。Nagle算法是一种提高网络效率的手段，它会将若干较小的传出消息存在缓冲区中，然后再将它们一次全都发送出去。Nagle算法默认是启用的。低延迟系统应该将hbase.ipc.client.tcpnodelay设置为true，从而禁用Nagle算法。默认已经是true，不需要修改```java/** * hbase client 单例连接 * @author chun * */public class HbaseClient &#123; private static final Logger LOGGER = LoggerFactory.getLogger(HbaseClient.class); private Configuration conf = null; private Connection conn = null; private static HbaseClient instance = null; private HbaseClient()&#123; init(); &#125; public void init()&#123; try&#123; conf = HBaseConfiguration.create(); conf.set("hbase.zookeeper.quorum", "127.0.0.1"); conf.set("zookeeper.znode.parent", "/hbase"); conf.set("hbase.zookeeper.property.clientPort", "2181"); conf.set("hbase.client.pause", "50"); conf.set("hbase.client.retries.number", "3"); conf.set("hbase.rpc.timeout", "2000"); conf.set("hbase.client.operation.timeout", "3000"); conf.set("hbase.client.scanner.timeout.period", "10000"); conn = ConnectionFactory.createConnection(conf); &#125;catch(Exception e)&#123; LOGGER.error("初始化hbase连接失败"+e.getMessage(),e); &#125; &#125; public static HbaseClient getInstance()&#123; if(instance == null)&#123; synchronized (HbaseClient.class) &#123; if(instance == null)&#123; instance = new HbaseClient(); &#125; &#125; &#125; return instance; &#125; /** * 获取htable操作类 * @param tableName * @return * @throws IOException */ public Table getHtable(String tableName) throws IOException&#123; return conn.getTable(TableName.valueOf(tableName)); &#125; /** * * @param hTableInterface */ public void relaseHtable(Table table)&#123; if(table == null)&#123; return; &#125; try &#123; table.close(); &#125; catch (IOException e) &#123; LOGGER.error(e.getMessage(),e); &#125; &#125; /** * 关闭hbase连接 */ public void destory()&#123; try &#123; conn.close(); instance = null; &#125; catch (IOException e) &#123; LOGGER.error(e.getMessage(),e); &#125; &#125;&#125; 参考:https://www.cnblogs.com/ulysses-you/p/10072883.html#_labelTop]]></content>
  </entry>
  <entry>
    <title><![CDATA[spark参数优化]]></title>
    <url>%2F2019%2F02%2F14%2Fspark%E5%B8%B8%E8%A7%84%E4%BC%98%E5%8C%96%E7%82%B9%2F</url>
    <content type="text"><![CDATA[spark集群优化 数据本地性如何配置Locality呢？可以统一采用spark.locality.wait来设置，默认3s（例如设置5000ms）。当然可以分别设置spark.locality.wait.process、spark.locality.wait.node、spark.locality.wait.rack等；123spark.locality.wait.process 进程内等待时间 3 3spark.locality.wait.node 节点内等待时间 3 8spark.locality.wait.rack 机架内等待时间 3 5 Spark参数优化 计算资源123spark.executor.overhead.memory executor堆外内存 512m 1.5gspark.executor.memory executor堆内存 1g 9gspark.executor.cores executor拥有的core数 1 3 num-executors参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。 executor-memory参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。每个executor进程所使用的内存数。默认值为“1g”。根据集群的硬件情况，应该把这个值设置为“4g”、“8g”、“16g”或者更高。 executor-cores参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。 driver-memory参数说明：该参数用于设置Driver进程的内存。参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。driver进程使用的总内存数。和内核数一样，建议根据你的应用及硬件情况，把这个值设置为“16g”或“32g”。默认”1g”。 spark.driver.cores在集群模式下管理资源时，用于driver程序的CPU内核数量。默认为1。在生产环境的硬件上，这个值可能最少要上调到8或16。 spark.driver.maxResultSize如果应用频繁用此driver程序，建议对这个值的设置高于其默认值“1g”。0表示没有限制。这个值反映了Spark action的全部分区中最大的结果集的大小。 spark.local.dir这个看起来很简单，就是Spark用于写中间数据，如RDD Cache，Shuffle，Spill等数据的位置，那么有什么可以注意的呢。 首先，最基本的当然是我们可以配置多个路径（用逗号分隔）到多个磁盘上增加整体IO带宽，这个大家都知道。 其次，目前的实现中，Spark是通过对文件名采用hash算法分布到多个路径下的目录中去，如果你的存储设备有快有慢，比如SSD+HDD混合使用，那么你可以通过在SSD上配置更多的目录路径来增大它被Spark使用的比例，从而更好地利用SSD的IO带宽能力。当然这只是一种变通的方法，终极解决方案还是应该像目前HDFS的实现方向一样，让Spark能够感知具体的存储设备类型，针对性的使用。 需要注意的是，在Spark 1.0 以后，SPARK_LOCAL_DIRS(Standalone, Mesos) or LOCAL_DIRS (YARN)参数会覆盖这个配置。比如Spark On YARN的时候，Spark Executor的本地路径依赖于Yarn的配置，而不取决于这个参数。 spark.default.parallelism一般为 executor_cores*num_executors 的 1~4 倍，系统默认值 64，不设置的话会导致 task 很多的时候被分批串行执行，或大量 cores 空闲，资源浪费严重。 spark.files.maxPartitionBytes = 128 M（默认）代表着rdd的一个分区能存放数据的最大字节数，如果一个400m的文件，只分了两个区，则在action时会发生错误。当一个spark应用程序执行时，生成spark.context，同时会生成两个参数，由上面得到的spark.default.parallelism推导出这两个参数的值 12sc.defaultParallelism = spark.default.parallelismsc.defaultMinPartitions = min(spark.default.parallelism,2) 当sc.defaultParallelism和sc.defaultMinPartitions最终确认后，就可以推算rdd的分区数了。 spark.serializer问题：序列化时间长、结果大 解决方案： spark默认使用JDK 自带的ObjectOutputStream，这种方式产生的结果大、CPU处理时间长，可以通过设置spark.serializer为org.apache.spark.serializer.KeyoSerializer。 另外如果结果已经很大，那就最好使用广播变量方式了，结果你懂得。 spark.speculation问题: 任务执行速度倾斜 解决方案： 如果数据倾斜，一般是partition key取得不好，可以考虑其他的并行处理方式，并在中间加上aggregation操作；如果是Worker倾斜，例如在某些Worker上的executor执行缓慢，可以通过设置spark.speculation=true 把那些持续慢的节点去掉； 12345678使用场景:一个stage有10个task：task0~task9，分别分配到了worker0~worker9上去执行计算，其中task0~task8都只用了5s就运行成功返回了，而由于worker9本身可能由于CPU资源长期被别的线程占用、磁盘IO缓慢等缘故，造成了task9执行缓慢，迟迟不返回，于是这个stage只能慢慢等待task9的返回。也就是整个stage的运行时间被这个task9给拖后腿了。 而如果调度端如果引入了speculatable策略，那么上述事件的实际情况被改善为： step1：TaskSetManager在task0~task8成功返回后，过了一段时间检测到task9迟迟没有返回，于是认定task9：你他妈的是个speculatableTask； step2：TaskSetManager此时没有task需要调度，而且此时有speculatableTask，所以调度器决定再次调度一下task9，利用和普通task一样的调度策略将task9分发到某台机器上，不过这次不会让task9在worker9上调度了。假设新的task9调度到了worker0。 step3：这时，计算集群上就有了两个同时运行的task9。在worker0上的task9运行了5s成功返回了，这时候TaskSetManager接收到task9的成功状态，由于10个task都运行完了taskSetManager自己标识为运行完成。 PS：而那个在worker9上依然慢慢运行的task9就没什么用了，worker上的Executor会用Failed的形式。 spark.cleaner.ttlSpark记录任何对象的元数据的持续时间（按照秒来计算）。默认值设为“infinite”（无限），对长时间运行的job来说，可能会造成内存泄漏。适当地进行调整，最好先设置为3600，然后监控性能。 spark.executor.cores每个executor的CPU核数。这个默认值基于选择的资源调度器。如果使用YARN或者Standalone集群模式，应该调整这个值 spark.akka.frameSizeSpark集群通信中最大消息的大小。当程序在带有上千个map及reduce任务的大数据集上运行时，应该把这个值从128调为512，或者更高。 spark.akka.threads用于通信的Akka的线程数。对于运行多核的driver应用，推荐将这个属性值从4提高为16、32或更高 spark.cores.max设置应用程序从集群所有节点请求的最大CPU内核数。如果程序在资源有限的环境下运行，应该把这个属性设置最大为集群中spark可用的CPU核数 offheap(executor)1spark.executor.overhead.memory executor堆外内存 512 1.5g 默认executor的0.1 spark.executor.logs.rolling.*有四个属性用于设定及维护spark日志的滚动。当spark应用长周期运行时（超过24小时），应该设置这个属性 spark.python.worker.memory如果使用python开发spark应用，这个属性将为每个python worker进程分配总内存数。默认值512m 优化shuffle过程参数1234spark.rpc.askTimeout rpc超时时间 10 1000 spark.shuffle.sort.bypassMergeThreshold shuffle read task阈值，小于该值则shuffle write过程不进行排序 200 600spark.shuffle.io.retryWait 每次重试拉取数据的等待间隔 5 30spark.shuffle.io.maxRetries 拉取数据重试次数 3 10 1234spark.shuffle.manager 默认sortspark.shuffle.file.buffer 默认32kspark.reducer.maxSizeInFlight 默认48mspark.shuffle.memoryFraction 默认0.2 其中spark.shuffle.file.buffer主要负责shuffle write过程写数据到磁盘过程的buffer，如果内存大的话建议提高该参数；spark.reducer.maxSizeInFlight负责shuffle read过程中reduce端机器从map端机器同时读取数据的大小。 spark.shuffle.manager这是Spark里shuffle数据的实现方法，默认为“sort”。这里提到它是因为，如果应用使用Tungsten，应该把这个属性值设置为“Tungsten-sort”。 调节map端内存缓冲区为什么要调节map端内存缓冲区默认情况下，shuffle的map task,输出的文件到内存缓冲区，当内存缓冲区满了，才会溢写spill操作到磁盘，如果该缓冲区比较小，而map端输出文件又比较大，会频繁的出现溢写到磁盘，影响性能。如何调整12//设置map 端内存缓冲区大小（默认32k）conf.set(&quot;spark.shuffle.file.buffer&quot;, &quot;64k&quot;); 调节reduce端内存占比为什么要调节reduce端内存占比,reduce task 在进行汇聚，聚合等操作时，实际上使用的是自己对应的executor内存，默认情况下executor分配给reduce进行聚合的内存比例是0.2，如果拉取的文件比较大，会频繁溢写到本地磁盘，影响性能。12//设置reduce端内存占比conf.set(&quot;spark.shuffle.memoryFraction&quot;, &quot;0.4&quot;); spark.shuffle.service.enabled在Spark内开启外部的shuffle服务。如果需要调度动态分配，就必须设置这个属性。默认为false spark.reducer.maxSizeInFlight调节reduce端缓冲区大小避免OOM异常 为什么要调节reduce端缓冲区大小 对于map端不断产生的数据，reduce端会不断拉取一部分数据放入到缓冲区，进行聚合处理； 当map端数据特别大时，reduce端的task拉取数据是可能全部的缓冲区都满了，此时进行reduce聚合处理时创建大量的对象，导致OOM异常； 如何调节reduce端缓冲区大小 当由于以上的原型导致OOM异常出现是，可以通过减小reduce端缓冲区大小来避免OOM异常的出现 但是如果在内存充足的情况下，可以适当增大reduce端缓冲区大小，从而减少reduce端拉取数据的次数，提供性能。12//调节reduce端缓存的大小(默认48M)conf.set(&quot;spark.reducer.maxSizeInFlight&quot;, &quot;24&quot;); 解决JVM GC导致的shuffle文件拉取失败问题描述 下一个stage的task去拉取上一个stage的task的输出文件时，如果正好上一个stage正处在full gc的情况下（所有线程后停止运行），它们之间是通过netty进行通信的，就会出现很长时间拉取不到数据，此时就会报shuffle file not found的错误；但是下一个stage又重新提交task就不会出现问题了。 如何解决 调节最大尝试拉取次数：spark.shuffle.io.maxRetries 默认为3次 调节每次拉取最大的等待时长：spark.shuffle.io.retryWait 默认为5秒12345//调节拉取文件的最大尝试次数(默认3次)conf.set(&quot;spark.shuffle.io.maxRetries&quot;, &quot;60&quot;); //调节每次拉取数据时最大等待时长(默认为5s)conf.set(&quot;spark.shuffle.io.retryWait&quot;, &quot;5s&quot;); spark.shuffle.consolidateFiles为true问题：map|reduce数量大，造成shuffle小文件数目多解决方案： 通过设置spark.shuffle.consolidateFiles为true，来合并shuffle中间文件，此时文件数为reduce tasks数目；开启map端输出文件的合并机制为什么要开启map端输出文件的合并机制 默认情况下，map端的每个task会为reduce端的每个task生成一个输出文件，reduce段的每个task拉取map端每个task生成的相应文件 开启后，map端只会在并行执行的task生成reduce端task数目的文件，下一批map端的task执行时，会复用首次生成的文件 如何开启12//开启map端输出文件的合并机制conf.set(&quot;spark.shuffle.consolidateFiles&quot;, &quot;true&quot;); 修改shuffle管理器有哪些shuffle管理器HashShuffleManager：1.2.x版本前的默认选择SortShuffleManager：1.2.x版本之后的默认选择，会对每个task要处理的数据进行排序；同时，可以避免像 HashShuffleManager那么默认去创建多份磁盘文件，而是一个task只会写入一个磁盘文件，不同reduce task需要的的数据使用offset来进行划分。tungsten-sort（钨丝）：1.5.x之后的出现，和SortShuffleManager相似，但是它本事实现了一套内存管理机制，性能有了很大的提高，而且避免了shuffle过程中产生大量的OOM、GC等相关问题。 如何选择如果不需要排序，建议使用HashShuffleManager以提高性能如果需要排序，建议使用SortShuffleManager如果不需要排序，但是希望每个task输出的文件都合并到一个文件中，可以去调节bypassMergeThreshold这个阀值（默认为200），因为在合并文件的时候会进行排序，所以应该让该阀值大于reduce task数量。如果需要排序，而且版本在1.5.x或者更高，可以尝试使用 tungsten-sort 在项目中如何使用 12345//设置spark shuffle manager (hash,sort,tungsten-sort)conf.set(&quot;spark.shuffle.manager&quot;, &quot;tungsten-sort&quot;); //设置文件合并的阀值conf.set(&quot;spark.shuffle.sort.bypassMergeThreshold&quot;, &quot;550&quot;); yarn-cluster模式的JVM内存溢出无法执行的问题 问题描述 有些spark作业，在yarn-client模式下是可以运行的，但在yarn-cluster模式下，会报出JVM的PermGen(永久代)的内存溢出，OOM. 出现以上原因是：yarn-client模式下，driver运行在本地机器上，spark使用的JVM的PermGen的配置，是本地的默认配置128M； 但在yarn-cluster模式下，driver运行在集群的某个节点上，spark使用的JVM的PermGen是没有经过默认配置的，默认是82M，故有时会出现PermGen Out of Memory error log. 如何处理在spark-submit脚本中设置PermGen--conf spark.driver.extraJavaOptions=&quot;-XX:PermSize=128M -XX:MaxPermSize=256M&quot;(最小128M，最大256M) 如果使用spark sql，sql中使用大量的or语句，可能会报出jvm stack overflow,jvm栈内存溢出，此时可以把复杂的sql简化为多个简单的sql进行处理即可。 checkpoint的使用checkpoint的作用 默认持久化的Rdd会保存到内存或磁盘中，下次使用该Rdd时直接冲缓存中获取，不需要重新计算；如果内存或者磁盘中文件丢失，再次使用该Rdd时需要重新进行。 如果将持久化的Rdd进行checkpoint处理，会把内存写入到hdfs文件系统中，此时如果再次使用持久化的Rdd，但文件丢失后，会从hdfs中获取Rdd并重新进行缓存。 如何使用 首先设置checkpoint目录12//设置checkpoint目录javaSparkContext.checkpointFile(&quot;hdfs://hadoop-senior.ibeifeng.com:8020/user/yanglin/spark/checkpoint/UserVisitSessionAnalyzeSpark&quot;); 将缓存后的Rdd进行checkpoint处理12//将缓存后的Rdd进行checkpointsessionRowPairRdd.checkpoint(); 使用KryoSerializer进行序列化使用KryoSerializer序列化的好处默认情况，spark使用的是java的序列化机制，ObjectOutputStream / ObjectInputStream，对象输入输出流机制，来进行序列化。 该序列化的好处是方便使用，但必须实现Serializable接口，缺点是效率低，速度慢，序列化后的占用空间大 KryoSerializer序列化机制，效率高，速度快，占用空间小（只有java序列化的1/10），可以减少网络传输 使用方法1234//配置使用KryoSerializer进行序列化conf.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)//（为了使序列化效果达到最优）注册自定义的类型使KryoSerializer序列化 .registerKryoClasses(new Class[]&#123;ExtractSession.class,FilterCount.class,SessionDetail.class,Task.class,Top10Session.class,Top10.class,VisitAggr.class&#125;); 使用KryoSerializer序列化的场景 算子函数中使用到的外部变量，使用KryoSerializer后，可以优化网络传输效率，优化集群中内存的占用和消耗持久化Rdd,优化内存占用，task过程中创建对象，减少GC次数shuffle过程，优化网络的传输性能1234spark.kryo.classToRegister 当使用Kryo做对象序列化时，需要注册这些类。对于Kryo将序列化的应用所用的所有自定义对象，必须设置这些属性spark.kryo.register 当自定义类需要扩展“KryoRegister”类接口时，用它代替上面的属性。spark.rdd.compress 设置是否应当压缩序列化的RDD，默认false。但是和前面说过的一样，如果硬件充足，应该启用这个功能，因为这时的CPU性能损失可以忽略不计spark.serializer 如果设置这个值，将使用Kryo序列化，而不是使用java的默认序列化方法。强烈推荐配置成Kryo序列化，因为这样可以获得最佳的性能，并改善程序的稳定性 将每个task中都使用的大的外部变量作为广播变量没有使用广播变量的缺点 默认情况，task使用到了外部变量，每个task都会获取一份外部变量的副本，会占用不必要的内存消耗，导致在Rdd持久化时不能写入到内存，只能持久化到磁盘中，增加了IO读写操作。 同时，在task创建对象时，内存不足，进行频繁的GC操作，降低效率 使用广播变量的好处 广播变量不是每个task保存一份，而是每个executor保存一份。 广播变量初始化时，在Driver上生成一份副本，task运行时需要用到广播变量中的数据，首次使用会在本地的Executor对应的BlockManager中尝试获取变量副本；如果本地没有，那么就会从Driver远程拉取变量副本，并保存到本地的BlockManager中；此后这个Executor中的task使用到的数据都从本地的BlockManager中直接获取。 Executor中的BlockManager除了从远程的Driver中拉取变量副本，也可能从其他节点的BlockManager中拉取数据，距离越近越好。 将rdd进行持久化持久化的原则 Rdd的架构重构和优化 尽量复用Rdd，差不多的Rdd进行抽象为一个公共的Rdd，供后面使用 公共Rdd一定要进行持久化 对应对次计算和使用的Rdd，一定要进行持久化 持久化是可以序列化的 首先采用纯内存的持久化方式，如果出现OOM异常，则采用纯内存+序列化的方法，如果依然存在OOM异常，使用内存+磁盘，以及内存+磁盘+序列化的方法 为了数据的高可靠性，而且内存充足时，可以使用双副本机制进行持久化 持久化的代码实现 .persist(StorageLevel.MEMORY_ONLY()) 持久化等级 StorageLevel.MEMORY_ONLY() 纯内存 等效于 .cache() 序列化的：后缀带有_SER 如：StorageLevel.MEMORY_ONLY_SER() 内存+序列化 后缀带有_DISK 表示磁盘，如：MEMORY_AND_DISK() 内存+磁盘 后缀带有_2表示副本数，如：MEMORY_AND_DISK_2() 内存+磁盘且副本数为2 代码优化 collect输出大量结果时速度慢解决方案： collect源码中是把所有的结果以一个Array的方式放在内存中，可以直接输出到分布式的文件系统，然后查看文件系统中的内容； repartition问题: 通过多步骤的RDD操作后有很多空任务或者小任务产生 解决方案： 使用coalesce或者repartition去减少RDD中partition数量； shuffle算子并行度调优 数据倾斜 优化数据结构 推荐使用dataset spark sqlspark.sql.shuffle.partitions1spark.sql.shuffle.partitions 并发度 200 800 对于大SQL，提高该值显著提升执行效率和稳定性 大小表join1spark.sql.autoBroadcastJoinThreshold 小表需要broadcast的大小阈值 10485760 33554432 默认10M 谓词下推1spark.sql.orc.filterPushdown orc格式表是否谓词下推 false true spark.sql.files.openCostInBytes该参数默认4M，表示小于4M的小文件会合并到一个分区中，用于减小小文件，防止太多单个小文件占一个分区情况。 存储格式选择spark.sql.hive.convertCTAS默认为false，如果为true则表示用户创建的表使用parquet格式，下面测试不同数据格式建表的存储大小和查询时间记录orc格式的存储效率最佳，查询时间性能也比较好。考虑开启参数，建表默认使用orc格式12spark.sql.hive.convertCTAS 创建表是否使用默认格式 false truespark.sql.sources.default 默认数据源格式 parquet orc spark.dynamicAllocation.enabled该特性用于join操作，目的是实现无shuffle的join，不是对所有SQL有效，调整为true可开启该特性 spark.streamingspark.streaming.concurrentJobs问题:Spark Streaming吞吐量不高 可以设置spark.streaming.concurrentJobs Spark Streaming 运行速度突然下降了，经常会有任务延迟和阻塞解决方案： 这是因为我们设置job启动interval时间间隔太短了，导致每次job在指定时间无法正常执行完成，换句话说就是创建的windows窗口时间间隔太密集了； GC调优 这里根据调研，有效的参数主要是InitiatingHeapOccupancyPercent，表示ConcGCThreads和G1HeapRegionSize。 推荐35，15，4 executor的stdout、stderr日志在集群本地，当出问题时，可以到相应的节点查询，当然从web ui上也可以直接看到。 executor除了stdout、stderr日志，我们可以把gc日志打印出来，便于我们对jvm的内存和gc进行调试。1--conf spark.executor.extraJavaOptions=&quot;-XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintHeapAtGC -XX:+PrintGCApplicationConcurrentTime -Xloggc:gc.log&quot; 除了executor的日志，nodemanager的日志也会给我们一些帮助，比如因为超出内存上限被kill、资源抢占被kill等原因都能看到。 除此之外，spark am的日志也会给我们一些帮助，从yarn的application页面可以直接看到am所在节点和log链接。123456789./bin/spark-submit \ --master yarn-cluster \ --num-executors 100 \ --executor-memory 6G \ --executor-cores 4 \ --driver-memory 1G \ --conf spark.default.parallelism=1000 \ --conf spark.storage.memoryFraction=0.5 \ --conf spark.shuffle.memoryFraction=0.3 \ 参考：https://blog.csdn.net/stark_summer/article/details/42981201https://www.cnblogs.com/lifeone/p/6428885.htmlhttps://www.cnblogs.com/lifeone/p/6434840.htmlhttps://www.jianshu.com/p/da7d5edfb61f https://blog.csdn.net/xwc35047/article/details/71039830https://www.cnblogs.com/dreamfly2016/p/5720526.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark jvm优化]]></title>
    <url>%2F2019%2F02%2F14%2Fspark-jvm%E4%BC%98%E5%8C%96%E7%82%B9%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/kwu_ganymede/article/details/51299115关于JVM内存的深入知识在这里不赘述，请大家自行对相关知识进行补充。好，说回Spark，运行Spark作业的时候，JVM对会对Spark作业产生什么影响呢？答案很简单，如果数据量过大，一定会导致JVM内存不足。在Spark作业运行时，会创建出来大量的对象，每一次将对象放入JVM时，首先将创建的对象都放入到eden区域和其中一个survivor区域中；当eden区域和一个survivor区域放满了以后，这个时候会触发minor gc，把不再使用的对象全部清除，而剩余的对象放入另外一个servivor区域中。JVM中默认的eden，survivor1，survivor2的内存占比为8:1:1。当存活的对象在一个servivor中放不下的时候，就会将这些对象移动到老年代。如果JVM的内存不够大的话，就会频繁的触发minor gc，这样会导致一些短生命周期的对象进入到老年代，老年代的对象不断的囤积，最终触发full gc。一次full gc会使得所有其他程序暂停很长时间。最终严重影响我们的Spark的性能和运行速度。 内存瓶颈 Spark从1.6.0版本开始，内存管理模块就发生了改变，旧版本的内存管理模块是实现了StaticMemoryManager 类，现在被称为”legacy”。”Legacy”模式默认被置为不可用，这就意味着当你用Spark1.5.x和Spark1.6.x运行相同的代码会有不同的结果，应当多加注意。考虑的兼容性，可以通过设置spark.memory.useLegacyMode为可用，默认是false. 这篇文章介绍自spark1.6.0版本后的新的内存管理模型，它实现的是UnifiedMemoryManager类。 在这张图中你可以看到三个主要内存区域。 Reserved Memory.这部分内存是被系统预留的，它的大小也是被硬编码的。在Spark1.6.0版本，它的大小是300MB，这就意味着这部分内存不能计入Spark内存计算，除非重新编译源码或设置spark.testing.reservedMemory，它的大小是不可改变的，因为park.testing.reservedMemory只是一个测试参数所以在生产中不推荐使用。注意，这部分内存只是被称为“Reserved”,实际上它不会被spark用来干任何事情 ，但是它限制了你在spark中可分配的内存大小。即使你想将全部JVM堆内存用于spark缓存数据，也不能使用这部分空闲内存（不是真的就浪费了，其实它存储了Spark的一些内部对象）。供参考，如果你不能为executor至少1.5 * Reserved Memory = 450MB的堆内存，任务将会失败并提示”please use larger heap size“的错误信息。 User Memory.这部分内存是分配Spark Memory内存之后的部分，而且这部分用来干什么完全取决于你。你可以用来存储RDD transformations过程使用的数据结构。例如，你可以通过mapPartitions transformation 重写Spark aggregation，mapPartitions transformations 保存hash表保证aggregation运行。这部分数据就保存在User Memory。再次强调，这是User Memory它完全由你决定存什么、如何使用，Spark完全不会管你拿这块区域用来做什么，怎么用，也不会考虑你的代码在这块区域是否会导致内存溢出。 Spark Memory.这部分内存就是由Spark管理了。这部分内存大小的计算：(“Java Heap” – “Reserved Memory”) * spark.memory.fraction，而且在spark1.6.0版本默认大小为： (“Java Heap” – 300MB) 0.75。例如：如果堆内存大小有4G，将有2847MB的Spark Memory,Spark Memory=(4\1024MB-300)*0.75=2847MB。这部分内存会被分成两部分：Storage Memory和Execution Memory，而且这两部分的边界由spark.memory.storageFraction参数设定，默认是0.5即50%。新的内存管理模型中的优点是，这个边界不是固定的，在内存压力下这个边界是可以移动的。如一个区域内存不够用时可以从另一区域借用内存。 下边来讨论如何移动及使用的： 1231. Storage Memory.这部分内存即可以用来缓存spark数据也可以用来做unroll序列化数据的临时空间。广播变量以block的形式也存储在这里。你奇怪的是unroll,因为你可能会说，并不需要那么多空间去unroll block使其可用——在没有足够内存去unroll bolock的情况下，如果得到持久化级别的允许，将直接在这部分内存unroll block。至于广播变量，当它的持久化级别为MEMORY_AND_DISK时，就会缓存到此。2. Execution Memory.这部分内存用于存储执行task过程中的一些对象。例如，它可以用来shuflle map端的中间缓存，也可以用来存储hash aggregation过程的hash table.在没有足够内存的时候，这部分内存支持溢室到磁盘，但是这部分内存的blocks不会被其它线程的task挤出去。 下边我们来说一下Storage Memory 和Execution Memory之间的边界移动。从Execution Memory的本质来看，你不能将这部分内存空间的数据挤出去，因为这部分内存的数据是用来计算的中间结果，如果计算过程找不到原来存到这的block数据任务就会失败。但是对于Storage Memory内存就不会这样，它只是用来缓存内存中数据，如果将里边的block数据驱逐出去，就会更新block 元数据映射信息使用到时告知该block被移除了，要想再拿到这些数据从HDD中读取即可（或者如果缓存级别没有溢写就重新计算）。 所以，我们只能Execution Memory可以向Storage Memory挤用空间，反之不可。那么当什么时候会发生Execution Memory 向Storage Memory挤用空间呢？有两种可能： 只要Storage Memory有可用空间，就可以增大Execution Memory 大小，减少Storage Memory 大小。Storage Memory的空间大小已经超出了初始设定的大小，并且将这部分空间全部占用，在这种情况下就可以强制将从Storage Memory中移出Blocks,减少它的空间到初始大小。 反过来，在只有当Execution Memory空间有空余时，Storage Memory才可以向Execution Memory借用空间，也就是说Execution Memory只要不够用了就可以向Storage Memory挤占空间不管Storage Memory有没有空余，而Storage Memory只能当Execution Memory有空余时才要以借用不能抢占。 初始Storage Memory 大小：“Spark Memory” spark.memory.storageFraction = (“Java Heap” – “Reserved Memory”) spark.memory.fraction spark.memory.storageFraction。根据默认值，即(“Java Heap” – 300MB) 0.75 0.5 = (“Java Heap” – 300MB) 0.375. 如果Java Heap=4G，那么就有1423.5MB大小的Storage Memory空间。 这就意味着当我们使用Spark cacheu并加载全部数据到executor中时，至少要将Storage Memory大小等于默认初始值大小。因为当Storage Memory区域还没满时，Execution Memory区域已经膨胀大于其初始设定大小时，我们不能强制将Execution Memory抢占的空间数据驱逐，所以最终Storage Memory会变小。 希望这篇文章可以帮你更好的理解spark新的内存管理机制，并以此来应用。 https://www.cnblogs.com/dreamfly2016/p/5720526.html 降低cache操作的内存占比spark中，堆内存又被划分成了两块儿，一块儿是专门用来给RDD的cache、persist操作进行RDD数据缓存用的；另外一块儿，就是我们刚才所说的，用来给spark算子函数的运行使用的，存放函数中自己创建的对象。默认情况下，给RDD cache操作的内存占比是0.6，即60%的内存都给了cache操作了。但是问题是，如果某些情况下cache占用的内存并不需要占用那么大，这个时候可以将其内存占比适当降低。怎么判断在什么时候调整RDD cache的内存占用比呢？其实通过Spark监控平台就可以看到Spark作业的运行情况了，如果发现task频繁的gc，就可以去调整cache的内存占用比了。通过SparkConf.set(&quot;spark.storage.memoryFraction&quot;,&quot;0.3&quot;)来设定。0.6 -&gt; 0.5 -&gt; 0.4 -&gt; 0.2大家可以自己去调，然后观察spark作业的运行统计针对该情况，大家可以 在 spark webui观察。yarn 去运行的话，那么就通过yarn的界面，去查看你的spark作业的 运行统计。很简单， 大家一层一层点击进去就好。 可以看到每个stage 的运行情况。 包括每个task的运行时间统计。gc时间。 如果发现gc太贫乏。时间太长，那么就可以适当调整这个比例。 降低cache操作的内存占比，大不了用persist 操作。选择一部分缓存的rdd数据写入磁盘，或者序列化的的方式。配合kryo序列化类。减少rdd缓存的内存占用，降低cache操作内存占比。 对应的 算子函数的内存占比，就提升了。这个时候，可能就减少minor gc 的频率，同时减少full gc的频率，对性能的 提升 有一定帮助的。 堆外内存和连接等待时长的调整其实这两个参数主要是为了解决一些Spark作业运行时候出现的一些错误信息而进行调整的 堆外内存a) 问题提出有时候，如果你的spark作业处理的数据量特别特别大，几亿数据量；然后spark作业一运行就会出现类似shuffle file cannot find，executor、task lost，out of memory（内存溢出）等这样的错误。这是因为可能是说executor的堆外内存不太够用，导致executor在运行的过程中，可能会内存溢出；然后可能导致后续的stage的task在运行的时候，可能要从一些executor中去拉取shuffle map output文件，但是executor可能已经挂掉了，关联的blockmanager也没有了；所以可能会报shuffle output file not found；resubmitting task；executor lost 这样的错误；最终导致spark作业彻底崩溃。 上述情况下，就可以去考虑调节一下executor的堆外内存。也许就可以避免报错；此外，有时，堆外内存调节的比较大的时候，对于性能来说，也会带来一定的提升。b) 解决方案：--conf spark.yarn.executor.memoryOverhead=2048在spark-submit脚本里面添加如上配置。默认情况下，这个堆外内存上限大概是300多M；我们通常项目中真正处理大数据的时候，这里都会出现问题导致spark作业反复崩溃无法运行；此时就会去调节这个参数，到至少1G或者更大的内存。通常这个参数调节上去以后，就会避免掉某些OOM的异常问题，同时呢，会让整体spark作业的性能，得到较大的提升。 连接等待时长的调整a) 问题提出：由于JVM内存过小，导致频繁的Minor gc，有时候更会触犯full gc，一旦出发full gc；此时所有程序暂停，导致无法建立网络连接；spark默认的网络连接的超时时长是60s；如果卡住60s都无法建立连接的话，那么就宣告失败了。碰到一种情况，有时候报错信息会出现一串类似file id not found，file lost的错误。这种情况下，很有可能是task需要处理的那份数据的executor在正在进行gc。所以拉取数据的时候，建立不了连接。然后超过默认60s以后，直接宣告失败。几次都拉取不到数据的话，可能会导致spark作业的崩溃。也可能会导致DAGScheduler，反复提交几次stage。TaskScheduler，反复提交几次task。大大延长我们的spark作业的运行时间。b) 解决方案：--conf spark.core.connection.ack.wait.timeout=300在spark-submit脚本中添加如上参数，调节这个值比较大以后，通常来说，可以避免部分的偶尔出现的某某文件拉取失败，某某文件lost掉的错误。 查看gcspark-env.sh: JAVA_OPTS=” -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps” https://www.cnblogs.com/jcchoiling/p/6494652.html参考:https://www.jianshu.com/p/e4557bf9186bhttps://www.cnblogs.com/lifeone/p/6434356.htmlhttps://www.cnblogs.com/dreamfly2016/p/5720180.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDD]]></title>
    <url>%2F2019%2F02%2F14%2FRDD%2F</url>
    <content type="text"><![CDATA[特点Resillient Distributed Dataset，即弹性分布式数据集RDD的内部属性 通过RDD的内部属性，用户可以获取相应的元数据信息。通过这些信息可以支持更复杂的算法或优化。 1）分区列表：通过分区列表可以找到一个RDD中包含的所有分区及其所在地址。 2）计算每个分片的函数：通过函数可以对每个数据块进行RDD需要进行的用户自定义函数运算。 3）对父RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖。 4）可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。 5）可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。(存储的是一个表，可以将处理的分区“本地化”) 分区— partitions计算函数— computer(p,context)依赖— dependencies()分区策略(Pair RDD)– partitioner()本地性策略— preferredLocations(p) 12345678910//只计算一次 protected def getPartitions: Array[Partition] //对一个分片进行计算，得出一个可遍历的结果 def compute(split: Partition, context: TaskContext): Iterator[T] //只计算一次，计算RDD对父RDD的依赖 protected def getDependencies: Seq[Dependency[_]] = deps //可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce @transient val partitioner: Option[Partitioner] = None //可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置 protected def getPreferredLocations(split: Partition): Seq[String] = Nil 1）RDD的特点 1）创建：只能通过转换 ( transformation ，如map/filter/groupBy/join 等，区别于动作 action) 从两种数据源中创建 RDD 1 ）稳定存储中的数据； 2 ）其他 RDD。 2）只读：状态不可变，不能修改。 3）分区：支持使 RDD 中的元素根据那个 key 来分区 ( partitioning ) ，保存到多个结点上。还原时只会重新计算丢失分区的数据，而不会影响整个系统。 4）路径：在 RDD 中叫世族或血统 ( lineage ) ，即 RDD 有充足的信息关于它是如何从其他 RDD 产生而来的。 5）持久化：支持将会被重用的 RDD 缓存 ( 如 in-memory 或溢出到磁盘 )。 6）延迟计算： Spark 也会延迟计算 RDD ，使其能够将转换管道化 (pipeline transformation)。 7）操作：丰富的转换（transformation）和动作 ( action ) ， count/reduce/collect/save 等。 执行了多少次transformation操作，RDD都不会真正执行运算（记录lineage），只有当action操作被执行时，运算才会触发。https://blog.csdn.net/guohecang/article/details/51736572 本地性策略一直在用spark但是没有考虑过优化的看过来。分布式计算系统的精粹在于移动计算而非移动数据，但是在实际的计算过程中，总存在着移动数据的情况，除非是在集群的所有节点上都保存数据的副本。移动数据，将数据从一个节点移动到另一个节点进行计算，不但消耗了网络IO，也消耗了磁盘IO，降低了整个计算的效率。为了提高数据的本地性，除了优化算法（也就是修改spark内存，难度有点高），就是合理设置数据的副本。设置数据的副本，这需要通过配置参数并长期观察运行状态才能获取的一个经验值。Spark中的数据本地性有三种： PROCESS_LOCAL：进程本地化，代码和数据在同一个进程中，也就是在同一个executor中；计算数据的task由executor执行，数据在executor的BlockManager中；性能最好NODE_LOCAL：节点本地化，代码和数据在同一个节点中；比如说，数据作为一个HDFS block块，就在节点上，而task在节点上某个executor中运行；或者是，数据和task在一个节点上的不同executor中；数据需要在进程间进行传输NO_PREF：对于task来说，数据从哪里获取都一样，没有好坏之分,比如从数据库中获取数据RACK_LOCAL：机架本地化，数据和task在一个机架的两个节点上；数据需要通过网络在节点之间进行传输ANY：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差 通常读取数据PROCESS_LOCAL&gt;NODE_LOCAL&gt;ANY，尽量使数据以PROCESS_LOCAL或NODE_LOCAL方式读取。其中PROCESS_LOCAL还和cache有关，如果RDD经常用的话将该RDD cache到内存中，注意，由于cache是lazy的，所以必须通过一个action的触发，才能真正的将该RDD cache到内存中。 如何配置Locality呢？可以统一采用spark.locality.wait来设置（例如设置5000ms）。当然可以分别设置spark.locality.wait.process、spark.locality.wait.node、spark.locality.wait.rack等；一般的具体设置是Locality优先级越高则可以设置越高的等待超时时间。12new SparkConf() .set(&quot;spark.locality.wait&quot;, &quot;10&quot;) 如果数据是PROCESS_LOCAL，但是此时并没有空闲的Core来运行我们的Task，此时Task就要等待，例如等待3000ms，3000ms内如果能够运行待运行的Task则直接运行，如果超过了3000ms，此时数据本地性就要退而求其次采用NODE_LOCAL的方式。同样的道理，NODE_LOCAL也会有等待的超时时间，以此类推…对于ANY的情况，默认情况状态下性能会非常低下，此时强烈建议使用Tachyon，例如在百度云上，为了确保计算速度，就在计算集群和存储集群之间加入了Tachyon,通过Tachyon来从远程抓取数据，而Spark基于Tachyon来进行计算，这就更好的满足了数据本地性。 数据本地性任务分配的源码在 taskSetManager.scala 。https://blog.csdn.net/oitebody/article/details/80137721数据本地性的副作用https://www.jianshu.com/p/a1d0824053d8 参考:https://www.cnblogs.com/yourarebest/p/5122372.htmlhttps://www.cnblogs.com/jackie2016/p/5643100.htmlhttp://blog.sina.com.cn/s/blog_9ca9623b0102w8pc.htmlhttps://bit1129.iteye.com/blog/2186084https://www.jianshu.com/p/a1d0824053d8https://blog.csdn.net/u013939918/article/details/60897191]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark算子]]></title>
    <url>%2F2019%2F02%2F13%2Fspark%E7%AE%97%E5%AD%90%2F</url>
    <content type="text"><![CDATA[sortByKey该函数会对原始RDD中的数据进行Shuffle操作，从而实现排序。这个函数中,传入两个參数,ascending表示是升序还是降序,默认true表示升序.第二个參数是运行排序使用的partition的个数,默认是当前RDD的partition个数. groupByKey、reduceByKey与sortByKeygroupByKey把相同的key的数据分组到一个集合序列当中： [(“hello”,1), (“world”,1), (“hello”,1), (“fly”,1), (“hello”,1), (“world”,1)] –&gt; [(“hello”,(1,1,1)),(“word”,(1,1)),(“fly”,(1))] reduceByKey把相同的key的数据聚合到一起并进行相应的计算： [(“hello”,1), (“world”,1), (“hello”,1), (“fly”,1), (“hello”,1), (“world”,1)] add–&gt; [(“hello”,3）,(“word”,2),(“fly”,1)] sortByKey按key的大小排序，默认为升序排序： [(3,”hello”）,(2,”word”),(1,”fly”)] –&gt; [(1,”fly”)，(2,”word”)，(3,”hello”)] 123456789101112131415161718192021222324252627282930313233343536373839 from pyspark import SparkConf, SparkContextfrom operator import addconf = SparkConf()sc = SparkContext(conf=conf)def func_by_key(): data = [ &quot;hello world&quot;, &quot;hello fly&quot;, &quot;hello world&quot;, &quot;hello fly&quot;, &quot;hello fly&quot;, &quot;hello fly&quot; ] data_rdd = sc.parallelize(data) word_rdd = data_rdd.flatMap(lambda s: s.split(&quot; &quot;)).map(lambda x: (x, 1)) group_by_key_rdd = word_rdd.groupByKey() print(&quot;groupByKey:&#123;&#125;&quot;.format(group_by_key_rdd.mapValues(list).collect())) print(&quot;groupByKey mapValues(len):&#123;&#125;&quot;.format( group_by_key_rdd.mapValues(len).collect() )) reduce_by_key_rdd = word_rdd.reduceByKey(add) print(&quot;reduceByKey:&#123;&#125;&quot;.format(reduce_by_key_rdd.collect())) print(&quot;sortByKey:&#123;&#125;&quot;.format(reduce_by_key_rdd.map( lambda x: (x[1], x[0]) ).sortByKey().map(lambda x: (x[0], x[1])).collect()))func_by_key()sc.stop()&quot;&quot;&quot;result：groupByKey:[(&apos;fly&apos;, [1, 1, 1, 1]), (&apos;world&apos;, [1, 1]), (&apos;hello&apos;, [1, 1, 1, 1, 1, 1])]groupByKey mapValues(len):[(&apos;fly&apos;, 4), (&apos;world&apos;, 2), (&apos;hello&apos;, 6)]reduceByKey:[(&apos;fly&apos;, 4), (&apos;world&apos;, 2), (&apos;hello&apos;, 6)]sortByKey:[(2, &apos;world&apos;), (4, &apos;fly&apos;), (6, &apos;hello&apos;)]&quot;&quot;&quot; 从结果可以看出，groupByKey对分组后的每个key的value做mapValues(len)后的结果与reduceByKey的结果一致，即：如果分组后要对每一个key所对应的值进行操作则应直接用reduceByKey；sortByKey是按key排序，如果要对value排序，可以交换key与value的位置，再排序。https://www.cnblogs.com/FG123/p/9746830.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive orcfile]]></title>
    <url>%2F2019%2F02%2F13%2Forcfile%2F</url>
    <content type="text"><![CDATA[rcfile在传统的数据库系统中，主要有三种数据存储方式： 水平的行存储结构： 行存储模式就是把一整行存在一起，包含所有的列，这是最常见的模式。这种结构能很好的适应动态的查询，比如 select a from tableA 和 select a, b, c, d, e, f, g from tableA 这样两个查询其实查询的开销差不多，都需要把所有的行读进来过一遍，拿出需要的列。而且这种情况下，属于同一行的数据都在同一个 HDFS 块上，重建一行数据的成本比较低。但是这样做有两个主要的弱点：a）当一行中有很多列，而我们只需要其中很少的几列时，我们也不得不把一行中所有的列读进来，然后从中取出一些列。这样大大降低了查询执行的效率。b）基于多个列做压缩时，由于不同的列数据类型和取值范围不同，压缩比不会太高。 垂直的列存储结构： 列存储是将每列单独存储或者将某几个列作为列组存在一起。列存储在执行查询时可以避免读取不必要的列。而且一般同列的数据类型一致，取值范围相对多列混合更小，在这种情况下压缩数据能达到比较高的压缩比。但是这种结构在重建出行时比较费劲，尤其适当一行的多个列不在一个 HDFS 块上的时候。 比如我们从第一个 DataNode 上拿到 column A，从第二个 DataNode 上拿到了 column B，又从第三个 DataNode 上拿到了 column C，当要把 A，B，C 拼成一行时，就需要把这三个列放到一起重建出行，需要比较大的网络开销和运算开销。 混合的 PAX 存储结构： PAX 结构是将行存储和列存储混合使用的一种结构，主要是传统数据库中提高 CPU 缓存利用率的一种方法，并不能直接用到 HDFS 中。但是 RCFile 也是继承自它的思想，先按行存再按列存。 RCFile 的设计理念是“ first horizontally-partition, then vertically-partition”，这使得 RCFile 同时具有 Row-Store 和 Column-Store 两者的所有优势。RCFile 的主要特点是：1). 保证数据记录的所有字段都在集群中同一个节点上；2). 集群节点上每一列数据单独压缩，使得 RCFile 的数据压缩率非常高；3). 在数据读取的时候只读取需要使用的数据列，不使用的数据列永远不会去读取。在每一个 HDFS block 中，RCFile 以 Row-Group 的形式组织和管理数据，一个 HDFS block 可包含 Row-Group 的数量取决于 Row-Group 和 HDFS block 的大小。在 RCFile 中同一个表的所有 Row-Group 具有相同的大小。 每个 Row-Group 由三个数据段组成。 1). 第一个数据段是一个同步标志，它放在 Row-Group 的开头，RCFile 使用同步标志来隔离 HDFS block 中两个连续的 Row-Group；2). 第二个数据段是 Row-Group 的元数据头部信息，Row-Group 里数据记录的数量、每个数据列占用多少字节、每个数据列的每个字段占用多少字节等等信息都保存在这里；3). 第三个数据段是表数据段，表数据段采用 Column-Store 的方式组织数据，在表数据段里，同一列的所有字段被连续的存储在一起。 数据的布局： 首先根据 HDFS 的结构，一个表可以由多个 HDFS 块构成。在每个 HDFS 块中，RCFile 以 row group 为基本单位组织数据，一个表多所有 row group 大小一致，一个 HDFS 块中可以包含多个 row group。每个 row group 包含三个部分，第一部分是 sync marker，用来区分一个 HDFS 块中两个连续多 row group。第二部分是 row group 的 metadata header，记录每个 row group 中有多少行数据，每个列数据有多少字节，以及每列一行数据的字节数。第三部分就是 row group 中的实际数据，这里是按列存储的。 数据的压缩： 在 metadata header 部分用 RLE (Run Length Encoding) 方法压缩，因为对于记录每个列数据中一行数据的字节数是一样的，这些数字重复连续出现，因此用这种方法压缩比比较高。在压缩实际数据时，每列单独压缩。 数据的追加写： RCFile 会在内存里维护每个列的数据，叫 column holder，当一条记录加入时，首先会被打散成多个列，人后追加到每列对应的 column holder，同时更新 metadata header 部分。可以通过记录数或者缓冲区大小控制内存中 column holder 的大小。当记录数或缓冲大小超过限制，就会先压缩 metadata header，再压缩 column holder，然后写到 HDFS。 数据的读取和惰性解压（lazy decompression）： RCFile 在处理一个 row group 时，只会读取 metadata header 和需要的列，合理利用列存储在 I/O 方面的优势。而且即使在查询中出现的列技术读进内存也不一定会被解压缩，只有但确定该列数据需要用时才会去解压，也就是惰性解压（lazy decompression）。例如对于 select a from tableA where b = 1，会先解压 b 列，如果对于整个 row group 中的 b 列，值都不为 1，那么就没必要对这个 row group 对 a 列去解压，因为整个 row group 都跳过了。 row group 的大小： row group 太小肯定是不能充分发挥列存储的优势，但是太大也会有问题。首先，论文中实验指出，当 row group 超过某一阈值时，很难再获得更高当压缩比。其次，row group 太大会降低 lazy decompression 带来的益处，还是拿 select a from tableA where b = 1 来说，如果一个 row group 里有一行 b = 1，我们还是要解压 a 列，从而根据 metadata header 中的信息找到 b = 1 的那行的 a 列的值，如果此时我们把 row group 设小一点，假设就设成 1，这样对于 b &lt;&gt; 1 的行都不需要解压 a 列。最后论文中给出一个一般性的建议，建议将 row group 设成 4MB。 ORCfileORC（OptimizedRC File）存储源自于RC（RecordColumnar File）这种存储格式，RC是一种列式存储引擎，对schema演化（修改schema需要重新生成数据）支持较差，而ORC是对RC改进，但它仍对schema演化支持较差，主要是在压缩编码，查询性能方面做了优化。RC/ORC最初是在Hive中得到使用，最后发展势头不错，独立成一个单独的项目。Hive 1.x版本对事务和update操作的支持，便是基于ORC实现的（其他存储格式暂不支持）。ORC发展到今天，已经具备一些非常高级的feature，比如支持update操作，支持ACID，支持struct，array复杂类型。你可以使用复杂类型构建一个类似于parquet的嵌套式数据架构，但当层数非常多时，写起来非常麻烦和复杂，而parquet提供的schema表达方式更容易表示出多级嵌套的数据类型。 ORC File包含一组组的行数据，称为stripes，除此之外，ORC File的file footer还包含一些额外的辅助信息。在ORC File文件的最后，有一个被称为postscript的区，它主要是用来存储压缩参数及压缩页脚的大小。在默认情况下，一个stripe的大小为250MB。大尺寸的stripes使得从HDFS读数据更高效。在file footer里面包含了该ORC File文件中stripes的信息，每个stripe中有多少行，以及每列的数据类型。当然，它里面还包含了列级别的一些聚合的结果，比如：count, min, max, and sum。这里贴一下文档上的图： 可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于之前的rcfile里的RowGroup概念，不过大小由4MB-&gt;250MB，这样应该能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data,Row Data,Stripe Footer：每个Stripe都包含index data、row data以及stripe footer，Stripe footer包含流位置的目录:1，Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset，据说还包括每个Column的max和min值，具体没细看代码。2，Row Data：存的是具体的数据，和RCfile一样，先取部分行，然后对这些行按列进行存储。与RCfile不同的地方在于每个列进行了编码，分成多个Stream来存储，具体如何编码在下一篇解析里会讲。3，Stripe Footer：存的是各个Stream的类型，长度等信息。 每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。 ORCFILE主要特点：混合存储结构，先按行存储，一组行数据叫stripes，stripes内部按列式存储。支持各种复杂的数据类型，比如： datetime, decimal, 以及一些复杂类型(struct, list, map, and union)；在文件中存储了一些轻量级的索引数据；基于数据类型的块模式压缩：a、integer类型的列用行程长度编码(run-length encoding)b、String类型的列用字典编码(dictionary encoding)； 参考：http://lxw1234.com/archives/2016/04/630.htmhttps://blog.csdn.net/dabokele/article/details/51542327https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties#ConfigurationProperties-ORCFileFormat http://www.360doc.com/content/15/0526/16/20466010_473411459.shtml]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive 排序与分发的各种By]]></title>
    <url>%2F2019%2F02%2F13%2Fhive-%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[排序与分发的各种By与传统关系型数据库最大的区别就是处理数据的能力这种能力最大的体现就是排序与分发的原理order by 是全局排序，只有一个reduce，数据量多时速度慢sort by 是随机分发到一个reduce然后reduce内部排序，一般不会单独使用;，也就是说保证了局部有序（每个reducer出来的数据是有序的，但是不能保证所有的数据是有序的，除非只有一个reducer）好处是：执行了局部排序之后可以为接下去的全局排序提高不少的效率（其实就是做一次归并排序就可以做到全局排序了）distribute by 是根据 distribute by 的字段把相应的记录分发到那个reduce cluster by是distribute by + sort by的简写 group by是对检索结果的保留行进行单纯分组，一般总爱和聚合函数一块用例如AVG（），COUNT（），max（），main（）等一块用。distribute by不可以。 partition by虽然也具有分组功能，但同时也具有其他的功能,一般和窗口函数一起使用。]]></content>
  </entry>
  <entry>
    <title><![CDATA[beeline]]></title>
    <url>%2F2019%2F02%2F13%2Fbeeline%2F</url>
    <content type="text"><![CDATA[1beeline -u &quot;jdbc:hive2://xxxxx;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=datahiveserver2_zk&quot; -n username -p &quot;password&quot; --color=true --silent=false --showHeader=false --outputformat=csv 格式参数:–outputformat[table/vertical/csv/tsv/dsv/csv2/tsv2]为了便于输出结果解析，建议把输出格式设置成普通文本，否则输出格式默认为table参考：https://my.oschina.net/guol/blog/875875]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拉链表]]></title>
    <url>%2F2019%2F01%2F29%2F%E6%8B%89%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[1、历史全量表2、每日更新表所谓拉链，就是记录历史。记录一个事物从开始，一直到当前状态的所有变化的信息。要确定拉链表的时间粒度，比如说拉链表每天只取一个状态，也就是说如果一天有3个状态变更，我们只取最后一个状态，这种天粒度的表其实已经能解决大部分的问题了；流水表存放的是一个用户的变更记录，比如在一张流水表中，一天的数据中，会存放一个用户的每条修改记录，但是在拉链表中只有一条记录。这是拉链表设计时需要注意的一个粒度问题。我们当然也可以设置的粒度更小一些，一般按天就足够（如果一个订单在一天内有多次状态变化，则只会记录最后一个状态的历史）。查询性能拉链表当然也会遇到查询性能的问题，比如说我们存放了5年的拉链数据，那么这张表势必会比较大，当查询的时候性能就比较低了，个人认为两个思路来解决：1.在一些查询引擎中，我们对start_date和end_date做索引，这样能提高不少性能。2.保留部分历史数据，比如说我们一张表里面存放全量的拉链表数据，然后再对外暴露一张只提供近3个月数据的拉链表。 每日的用户增量假设我们每天都会获得一份切片数据，我们可以通过取两天切片数据的不同来作为每日更新表，这种情况下我们可以对所有的字段先进行concat，再取md5，这样就ok了12345678910111213141516171819202122INSERT OVERWRITE TABLE dws.user_hisSELECT * FROM( SELECT A.user_num, A.mobile, A.reg_date, A.t_start_time, CASE WHEN A.t_end_time = &apos;9999-12-31&apos; AND B.user_num IS NOT NULL THEN &apos;2017-01-01&apos; //昨天是历史最新，今天有数据，则昨天为历史 ELSE A.t_end_time //保留昨天的状态 END AS t_end_time FROM dws.user_his AS A LEFT JOIN ods.user_update AS B ON A.user_num = B.user_numUNION SELECT C.user_num, C.mobile, C.reg_date, &apos;2017-01-02&apos; AS t_start_time, &apos;9999-12-31&apos; AS t_end_time FROM ods.user_update AS C) AS T 增量表：每天的新增数据，增量数据是上次导出之后的新数据(如果只是某些字段被更新而不是新增记录则不被包括)。拉链表：维护历史状态，以及最新状态数据的一种表，拉链表根据拉链粒度的不同，实际上相当于快照，只不过做了优化，去除了一部分不变的记录而已,通过拉链表可以很方便的还原出拉链时点的客户记录。每日更新表：假设我们每天都会获得一份切片数据，我们可以通过取两天切片数据的不同来作为每日更新表，这种情况下我们可以对所有的字段先进行concat，再取md5，这样就ok了拉链表比每日更新表更容易得到最新有效的final记录https://www.cnblogs.com/zuifangxiu/articles/6475179.html拉链表回滚]]></content>
      <categories>
        <category>dw</category>
      </categories>
      <tags>
        <tag>dw</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[层次桥接表]]></title>
    <url>%2F2019%2F01%2F29%2F%E5%B1%82%E6%AC%A1%E6%A1%A5%E6%8E%A5%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[初版： 扁平化: 层次桥接表 下钻 上钻er图：]]></content>
      <categories>
        <category>dw</category>
      </categories>
      <tags>
        <tag>dw</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[桥接表]]></title>
    <url>%2F2019%2F01%2F28%2F%E6%A1%A5%E6%8E%A5%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[采用桥接表有利有弊，桥接表增加了误用模式的可能性，可能会大致不准确的结果，而拒绝桥接表则严重影响了维度解决方案应有的分析能力。所以只有了解它，才能在各种情况下为平衡能力与风险做出正确的抉择。 标准的一对多关系： 多值维度：如果每个订单有多个销售人员。面对多值维度，有两个基本选择：位置设计或桥接表设计。1、事实表中多个键值，位置设计是指预留空值字段用于新增字段，缺点是难以扩展，很快就会发现表的列用光了，要增加新列很困难。仅支持数量有限的关系。2、利用桥接表消除无法扩展和存在大量空值的情况，但是生成的表。 可能会导致双重或多重计算。3、为了解决查询复杂的情况，有时不需要新建桥接表，直接将多个维度用字符串分割，然后存储解决。如存|Ann|Henry| 区域是销售salesrep表的字段。 是关系型数据库的问题。在设计大数据数仓时可以考虑其他这种方式规避。问题解决：1、不提倡2、增加分配因子。不提倡多数情况下，分配因子不存在，可用性较低。3、使用主成员补充桥接表，桥接表不直接对外。不能最终解决问题。不提倡。4、缓慢变化维的问题。5、交叉，同样是对一些关系型分析工具的妥协。不提倡 多值属性理解上，是利用支架表和桥接表，解决方式和问题与多值维度雷同。 参考：star schema完全参考手册https://blog.csdn.net/u012073449/article/list/3?t=1http://book.51cto.com/art/201207/346044.htm多对多维度或多值维度维度表和事实表之间的标准关系是一对多关系，这意味着维度表中的一行记录会连接事实表中的多行记录，但是事实表中的一行记录在维度表中只关联一行记录。这种关系很重要，因为它防止了重复计数。幸运的是，在大多数情况下都是这种一对多关系。在现实世界中还存在比一对多关系更复杂的两种常见情况：事实表和维度表之间的多对多关系。维度表之间的多对多关系。这两种情况本质是相同的，但事实表和维度表之间的多对多关系少了唯一描述事实和维度组的中间维度。对于这两种情况，我们介绍一种称为桥接表的中间表，以支持更复杂的多对多关系。 1、事实表和维度表之间的多对多关系在多个维度表的值可以赋给单个事实事务时，事实表和维度表之间通常是多对多关系。一个常见的示例是多个销售代表可以参与给定的销售事务，这种情形经常发生在涉及大宗交易的复杂销售事件中(例如计算机系统)。精确地处理这种情况需要创建一个桥接表，将销售代表组合成一个组。SalesRepGroup桥接表如图2-4所示。 ETL过程需要针对每条引入的事实表记录中的销售代表组合，在桥接表中查找相应的销售代表组键。如果该销售代表组键不存在，就添加一个新的销售代表组。注意图2-4所示的桥接表有重复计数的风险。如果按照销售代表累加销售量，那么每个销售代表都会对总销售量做出贡献。对某些分析而言结果是正确的，但对于其他情况仍会有重复计数的问题。要解决这个问题，可以向桥接表中添加加权因子列。加权因子是一个分数值，所有的销售代表组的加权因子累加起来为1。将加权因子和累加事实相乘，以按照每个销售代表在分组中的比重分配事实。注意可能需要在Orders和SalesRepGroup之间添加一个SalesRepGroupKey表，以支持真正的主键-外键关系。这会把这个事实-维度实例变成维度-维度实例。但是不提倡使用加权因子，因为如果新增分类，历史数据不好恢复，而且如果加权因子没有达成共识，就没办法设置了。 2、维度之间的多对多关系从分析的角度来看，维度之间的多对多关系是一个很重要的概念，大多数维度都不是完全相互独立的。维度之间的独立性是连续的，而不是有或没有这两种截然不同的状态。例如在连续的一端，零售店这条链状关系的库存维度和产品维度是相对独立的，而不是绝对独立的。一些库存方式不适合某些产品。其他维度之间的关系则紧密得多，但是由于存在多对多关系，因此很难将其组合成单个维度。例如在银行系统中，账户和顾客之间有直接关系，但不是一对一的关系。一个账户可以有一个或多个签名确认的顾客，一个顾客也可有多个账户。银行通常从账户的角度来处理数据；MonthAccountSnapshot(月账快照)是金融机构中常见的一种事实表。因为账户和顾客之间存在的多对多关系，这种更多关注账户的系统就很难按照顾客来查看账户。一种方法是创建CustomerGroup桥接表来连接事实表，例如前面多对多示例中的SalesRepGroup表。较好的方法是利用账户和顾客之间的关系，如图2-5所示。 账户和顾客维度之间的AccountToCustomer桥接表可以捕获多对多关系，并且有几个显著的优点。首先，源系统中的关系是已知的，因此创建桥接表比手动构建SalesRepGroup维度表更容易。其次，账户-顾客关系自身就非常有趣。AccountToCustomer桥接表可以回答诸如”每个顾客的平均账户数量是多少？”这样的问题，而无须连接任何事实表。桥接表经常是底层业务过程的标志，特别是在需要跟踪桥接表随时间而产生的变化(即关系本身是类型2变化)时。对顾客和账户来说，业务过程可能称为账户维护，其中一项事务可能称作”添加签名人”。如果3个顾客与同一个账户关联，在源系统中该账户就会有3个”Add(添加)”事务。通常这些事务和它们表示的业务过程还不是很重要，不需要在DW/BI系统中通过它们自身的事实表来跟踪。然而，这些关系和它们产生的变化对分析业务来说是相当重要的。我们在维度模型中把它们包含为渐变维度，在一些情况下包含为桥接表。]]></content>
      <categories>
        <category>dw</category>
      </categories>
      <tags>
        <tag>dw</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive默认分号提交问题]]></title>
    <url>%2F2019%2F01%2F28%2Fhive%E9%BB%98%E8%AE%A4%E5%88%86%E5%8F%B7%E6%8F%90%E4%BA%A4%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[将分号改为\0731234567891011121314151617181920212223CREATE EXTERNAL TABLE `table`( `a` string COMMENT &apos;&apos;, `b` string COMMENT &apos;&apos;, `c` string COMMENT &apos;&apos;, `d` int COMMENT &apos;&apos;, `e` string COMMENT &apos;&apos;, `f` string COMMENT &apos;&apos;, `g` string COMMENT &apos;&apos;, `h` int COMMENT &apos;&apos;)PARTITIONED BY ( `dt` string)ROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.RegexSerDe&apos;WITH SERDEPROPERTIES ( &apos;input.regex&apos;=&apos;(\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125;\\s*\\d&#123;2&#125;:\\d&#123;2&#125;:\\d&#123;2&#125;).\\d&#123;3&#125;\\s*[A-Z]&#123;3,5&#125;\\s*L:([a-zA-Z0-9_\\-]&#123;1,50&#125;),D:([A-Z0-9_]&#123;1,5&#125;),Mod:([0-9]&#123;1,2&#125;),([a-zA-Z0-9_]&#123;10,30&#125;):([A-Z]&#123;1&#125;),ClientCode:([a-zA-z\\d\\-_\\.\\|]&#123;1,100&#125;)(?:,RuleVersion:(\\d&#123;1,3&#125;))\073&apos;, &apos;output.format.string&apos;=&apos;%1$s %2$s %3$s %4$s %5$s %6$s %7$s &apos; )STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos;OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;LOCATION &apos;hdfs://qrcluster/user/abtest/log/abtest_sdk_log_daycombine&apos;]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支架表]]></title>
    <url>%2F2019%2F01%2F28%2F%E6%94%AF%E6%9E%B6%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[支架表看起来和雪花模式有些类似。采用支架表的原因是：1、支架表中的维度没有基本维度表中的分析价值大，使用也不是很频繁。2、其次如果有维度在基本维度中有很大的冗余(基本维度记录中都含有重复字段)，那么将不常用的放到支架表中可以节省空间。备注：可以使用支架表，但是只可偶尔为之，不要常用。如果您的设计包含大量的支架表，就应该拉响警报。您可能会陷入过度规范化设计的麻烦之中。]]></content>
      <categories>
        <category>dw</category>
      </categories>
      <tags>
        <tag>dw</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里onedata]]></title>
    <url>%2F2019%2F01%2F28%2F%E9%98%BF%E9%87%8Conedata%2F</url>
    <content type="text"><![CDATA[OneData体系分为：1、数据规范定义体系2、数据模型规范设计3、ETL规范研发以及支撑整个体系从方法到实施的工具体系。 落地实现数据规范定义将此前个性化的数据指标进行规范定义，抽象成：原子指标、时间周期、其他修饰词等三个要素。例如，以往业务方提出的需求是：最近7天的成交。而实际上，这个指标在规范定义中，应该结构化分解成为： 原子指标（支付订单金额 ）+修饰词-时间周期（最近7天）+修饰词-卖家类型（淘宝） 上文只有抽象指标的方法，应该还包括字段规范，表名规范派生指标的定义也很重要，因为数据运营人员经常会查最近7，最近一月的指标，每次都从基础事实表中，查询的量会很大。 数据模型架构 理解为dw的分层设计 将数据分为ODS（操作数据）层、CDM（公共维度模型）层、ADS（应用数据）层。 其中：ODS层主要功能 这里先介绍一下阿里云数加大数据计算服务MaxCompute，是一种快速、完全托管的TB/PB级dw解决方案，适用于多种数据处理场景，如日志分析，dw，机器学习，个性化推荐和生物信息等。 同步：结构化数据增量或全量同步到数加MaxCompute（原ODPS）；结构化：非结构化(日志)结构化处理并存储到MaxCompute（原ODPS）；累积历史、清洗：根据数据业务需求及稽核和审计要求保存历史数据、数据清洗； CDM层主要功能 CDM层又细分为DWD层和DWS层，分别是明细宽表层和公共汇总数据层，采取维度模型方法基础，更多采用一些维度退化手法，减少事实表和维度表的关联，容易维度到事实表强化明细事实表的易用性；同时在汇总数据层，加强指标的维度退化，采取更多宽表化的手段构建公共指标数据层，提升公共指标的复用性，减少重复的加工。ADS层主要功能 个性化指标加工：不公用性；复杂性（指数型、比值型、排名型指标）基于应用的数据组装：大宽表集市、横表转纵表、趋势指标串 其模型架构图如上图，阿里通过构建全域的公共层数据，极大的控制了数据规模的增长趋势，同时在整体的数据研发效率，成本节约、性能改进方面都有不错的结果。 研发流程和工具落地实现将OneData体系贯穿于整个研发流程的每个环节中，并通过研发工具来进行保障。 实施效果1、数据标准统一：数据指标口径一致，各种场景下看到的数据一致性得到保障2、支撑多个业务，极大扩展性：服务了集团内部45个BU的业务，满足不同业务的个性化需求3、统一数据服务：建立了统一的数据服务层，其中离线数据日均调用次数超过22亿；实时数据调用日均超过11亿4、计算、存储成本：指标口径复用性强，将原本30000多个指标精简到3000个；模型分层、粒度清晰，数据表从之前的25000张精简到不超过3000张。5、研发成本：通过数据分域、模型分层，强调工程师之间的分工和协作，不再需要从头到尾每个细节都了解一遍，节省了工程师的时间和精力。参考：https://yq.aliyun.com/articles/67011 明细宽表层-面向业务建模过程： &emsp;&emsp;事务型事实宽表，周期性快照事实宽表，累计快照事实宽表公共汇总款表层-面向分析主题建模: &emsp;&emsp;流量，订单，商品，用户 上面两层都是基于公共维度表的基础上 维度退化手法指标维度退化大宽表集市、横表转纵表、趋势指标串个性化指标加工：不公用性；复杂性（指数型、比值型、排名型指标）]]></content>
      <categories>
        <category>dw</category>
      </categories>
      <tags>
        <tag>dw</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hiveserver2 jdbc]]></title>
    <url>%2F2019%2F01%2F28%2Fhiveserver2%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185import org.apache.log4j.Logger;import java.io.BufferedWriter;import java.io.FileWriter;import java.io.IOException;import java.sql.*;public class HiveJdbcCli &#123; //网上写 org.apache.hadoop.hive.jdbc.HiveDriver ,新版本不能这样写 private static String driverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;; //这里是hive2，网上其他人都写hive,在高版本中会报错// private static String url = &quot;jdbc:hive2://master:10000/default&quot;; private static String url = &quot;jdbc:hive2://xxxxx/intercarhive;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=datahiveserver2_zk&quot;; private static String user = &quot;xxxxx&quot;; private static String password = &quot;xxxxxx&quot;; private static String sql = &quot;&quot;; private static ResultSet res; private static final Logger log = Logger.getLogger(HiveJdbcCli.class); public static void main(String[] args) &#123; Connection conn = null; Statement stmt = null; try &#123; conn = getConn(); stmt = conn.createStatement(); // 第一步:存在就先删除 String tableName = dropTable(stmt); stmt.close(); stmt = conn.createStatement(); // 第二步:不存在就创建 createTable(stmt, tableName); stmt.close(); stmt = conn.createStatement(); // 第三步:查看创建的表 showTables(stmt, tableName); stmt.close(); stmt = conn.createStatement(); // 执行describe table操作 describeTables(stmt, tableName); stmt.close(); stmt = conn.createStatement(); selectData2File(stmt, &quot; pf_test.hdfs_fsimage_file_orc&quot;, &quot;/d/test/fsimage.txt&quot;); stmt.close(); stmt = conn.createStatement();// // 执行load data into table操作// loadData(stmt, tableName);//// // 执行 select * query 操作// selectData(stmt, tableName);//// // 执行 regular hive query 统计操作// countData(stmt, tableName); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); log.error(driverName + &quot; not found!&quot;, e); System.exit(1); &#125; catch (SQLException e) &#123; e.printStackTrace(); log.error(&quot;Connection error!&quot;, e); System.exit(1); &#125; finally &#123; try &#123; if (conn != null) &#123; conn.close(); conn = null; &#125; if (stmt != null) &#123; stmt.close(); stmt = null; &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125; private static void countData(Statement stmt, String tableName) throws SQLException &#123; sql = &quot;select count(1) from &quot; + tableName; System.out.println(&quot;Running:&quot; + sql); res = stmt.executeQuery(sql); System.out.println(&quot;执行“regular hive query”运行结果:&quot;); while (res.next()) &#123; System.out.println(&quot;count ------&gt;&quot; + res.getString(1)); &#125; &#125; private static void selectData(Statement stmt, String tableName) throws SQLException &#123; sql = &quot;select * from &quot; + tableName + &quot; limit 100&quot;; System.out.println(&quot;Running:&quot; + sql); res = stmt.executeQuery(sql); System.out.println(&quot;执行 select * query 运行结果:&quot;); while (res.next()) &#123; System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2)); &#125; &#125; private static void selectData2File(Statement stmt, String tableName, String fileName) &#123; sql = &quot;select * from &quot; + tableName + &quot; limit 10000000&quot;; System.out.println(&quot;Running:&quot; + sql); try &#123; res = stmt.executeQuery(sql); System.out.println(&quot;执行 select * query 运行结果:&quot;); BufferedWriter bw = new BufferedWriter(new FileWriter(fileName)); ResultSetMetaData rsmd = res.getMetaData(); int columnsNumber = rsmd.getColumnCount(); while (res.next()) &#123;// System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2)); for (int i = 1; i &lt;= columnsNumber; i++) &#123; bw.write(res.getString(i) + &quot;\t&quot;); &#125; bw.write(&quot;\n&quot;); &#125; bw.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; private static void loadData(Statement stmt, String tableName) throws SQLException &#123; //目录 ，我的是hive安装的机子的虚拟机的home目录下 String filepath = &quot;user.txt&quot;; sql = &quot;load data local inpath &apos;&quot; + filepath + &quot;&apos; into table &quot; + tableName; System.out.println(&quot;Running:&quot; + sql); stmt.execute(sql); &#125; private static void describeTables(Statement stmt, String tableName) throws SQLException &#123; sql = &quot;describe &quot; + tableName; System.out.println(&quot;Running:&quot; + sql); res = stmt.executeQuery(sql); System.out.println(&quot;执行 describe table 运行结果:&quot;); while (res.next()) &#123; System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2)); &#125; &#125; private static void showTables(Statement stmt, String tableName) throws SQLException &#123; sql = &quot;show tables &apos;&quot; + tableName + &quot;&apos;&quot;; System.out.println(&quot;Running:&quot; + sql); res = stmt.executeQuery(sql); System.out.println(&quot;执行 show tables 运行结果:&quot;); if (res.next()) &#123; System.out.println(res.getString(1)); &#125; &#125; private static void createTable(Statement stmt, String tableName) throws SQLException &#123; sql = &quot;create table &quot; + tableName + &quot; (key int, value string) row format delimited fields terminated by &apos;\t&apos;&quot;; stmt.execute(sql); &#125; private static String dropTable(Statement stmt) throws SQLException &#123; // 创建的表名 String tableName = &quot;testHive&quot;; sql = &quot;drop table if exists &quot; + tableName; stmt.execute(sql); return tableName; &#125; private static Connection getConn() throws ClassNotFoundException, SQLException &#123; Class.forName(driverName); Connection conn = DriverManager.getConnection(url, user, password); return conn; &#125;&#125; 1234567891011121314151617&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.45&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive使用和优化]]></title>
    <url>%2F2019%2F01%2F28%2Fhive%E4%BD%BF%E7%94%A8%E5%92%8C%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[使用complex type array 和 map 类型创建表时指定类型和分隔符 123456create table test1 (f1 array&lt;int&gt;, f2 map&lt;string,int&gt;) ROW FORMAT DELIMITED COLLECTION ITEMS TERMINATED BY &apos;|&apos; MAP KEYS TERMINATED BY &apos;:&apos;; 文件中按照表定义的分隔符来分割，字段的分隔符默认为^A，可以在建表时通过 FIELDS TERMINATED BY ‘^A’ 指定 1234cat /home/hadoop/hivetest1.txt 1|2|3^Aa:1|b:2 11|12|13^Aaa:1|bb:2 把文件 load 到表中load data local inpath &#39;/home/hadoop/hivetest1.txt&#39; into table test1; 在查询中使用array 和 map 类型select f1[0],f2[&#39;aa&#39;] from test1; select * from test1 where f1[0]=11 and f2[&#39;aa&#39;]=1; 处理json字符串create table test(json_str string);test表的json_str字段的内容为： 1234567&#123;&quot;a&quot;:&quot;1&quot;,&quot;b&quot;:&quot;2&quot;,&quot;c&quot;:[&quot;aa&quot;,&quot;11&quot;,&#123;&quot;aaa&quot;:&quot;111&quot;,&quot;bbb&quot;:&quot;222&quot;&#125;]&#125; select get_json_object(json_str,&apos;$.b&apos;) from test; select get_json_object(json_str,&apos;$.c[0]&apos;) from test; select get_json_object(json_str,&apos;$.c[2].aaa&apos;) from test; 动态分区12set hive.exec.dynamic.partition=true; set hive.exec.dynamic.partition.mode=nonstrict; 另外，如果默认值不够大，根据需要调大以下三个参数，否则会报错 123set hive.exec.max.dynamic.partitions=2000 set hive.exec.max.dynamic.partitions.pernode=1000 set hive.exec.max.created.files=110000; 这样，从另外一个表把数据导入到目标表时，就会自动按照指定字段分区。例如： insert overwrite table A partition(reportdate) select * from B;表A要先创建，并且以reportdate作为分区的字段。在上面sql中不用指定reportdate的值（如果是非动态分区则需要指定）。表B最后一个字段作为分区的字段，会自动根据最后一个字段的值自动分区。 map join 优化将hive.auto.convert.join设置为true set hive.auto.convert.join=true;设置进行mapjoin的小表的阈值（即当小表的大小 少于 一定该阈值时 hive 会做 mapjoin 优化，如果不设置，则采用默认值） 12set hive.mapjoin.smalltable.filesize=25000000; hive.mapjoin.bucket.cache.size=100 每一个key有多少个value值缓存在内存中hive.mapjoin.cache.numrows=25000有多少行cache在内存中 insert into/overwrite多表插入的优化通过对原表扫描一遍，插入到不同的目的表中 1234from table insert overwrite table dest1 select * where col=condition ; insert overwrite table dest2 select * where col=condition ; insert overwrite table dest3 select * where col=condition ; 对limit语句的优化1234hive.limit.optimize.enable=false hive.limit.row.max.size=100000 hive.limit.optimize.limit.file=10 hive.limit.optimize.fetch.max=50000 对reduce个数的设置hive默认是根据输入的大小来设置，每个reducer处理的数据量是1G。如果有10G的输入数据，则hive自动生成10个reducer。默认情况下一个reducer处理1G的数据，这样一个reducer处理的数据量太大，可以改小一些。 123hiive.exec.reducers.bytes.per.reducer=1G hive.exec.reducers.max=999 mapred.reduce.tasks 数据倾斜优化有编译器的优化和运行期的优化 1234hive.groupby.skewindata=false hive.optimize.skewjoin.compiletime=false hive.optimize.skewjoin=false hive.skewjoin.key=100000 文件压缩对中间结果的压缩和最终结果的压缩 12hive.exec.compress.output=false hive.exec.compress.intermediate=false 并行执行一个hive sql语句编译成多个MR作业，没有依赖关系的作业可以并行执行。 12hive.exec.parallel=false hive.exec.parallel.thread.number=8 合并结果的小文件对于hive的结果中是小文件的，会再起一个MR作业合并小文件。 1234hive.merge.mapfiles=true hive.merge.mapredfiles=false hive.merge.size.per.task=256000000 hive.merge.smallfiles.avgsize=16000000 推断执行1hive.mapred.reduce.tasks.speculative.execution=true 设置reduce个数显示设置reduce的个数，或者每个reduce处理的数据的大小（默认1G的值很多时候有些大，可以设置小一些，同时reduce的内存也要相应小一些，提高并行度）。 123mapred.reduce.tasks=-1 hive.exec.reducers.bytes.per.reducer=1000000000（1G） hive.exec.reducers.max=999 map端聚合hive.map.aggr=truegroup by语句时现在map聚合一次，减少传输到reduce的数据，就是mapreduce的combiner。默认是打开的。 列剪裁列剪裁优化，只对需要的列进行处理，忽略其他的列，减少数据的处理量。默认是打开的。 hive.optimize.cp=true 本地模式一些sql处理的数据量比较少，或者计算量比较少，可以在本地运行而不是MR作业运行，这样性能会更好些，也节约hadoop集群的资源。 12hive.exec.mode.local.auto=false hive.fetch.task.conversion=minimal 测试模式1234hive.test.mode=false hive.test.mode.prefix=test_ hive.test.mode.samplefreq=32 hive.test.mode.nosamplelist=table1,table2,table3 用于测试，通过采样减少输入的数据，结果表前面加前缀“test_” 严格模式1hive.mapred.mode=nonstrict 通过严格模式，使一些不严格的用法不通过，防止潜在的错误。No partition being picked up for a query.Comparing bigints and strings.Comparing bigints and doubles.Orderby without limit. jvm重用JVM重用是hadoop调优参数的内容，对hive的性能具有非常大的 影响，特别是对于很难避免小文件的场景或者task特别多的场景，这类场景大多数执行时间都很短。hadoop默认配置是使用派生JVM来执行map和 reduce任务的，这是jvm的启动过程可能会造成相当大的开销，尤其是执行的job包含有成千上万个task任务的情况。 JVM重用可以使得JVM实例在同一个JOB中重新使用N次，N的值可以在Hadoop的mapre-site.xml文件中进行设置 1mapred.job.reuse.jvm.num.tasks 也可在hive的执行设置：1set mapred.job.reuse.jvm.num.tasks=10; JVM的一个缺点是，开启JVM重用将会一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡“的job中有几个 reduce task 执行的时间要比其他reduce task消耗的时间多得多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。 hive参数优化之默认启用本地模式启动hive本地模式参数，一般建议将其设置为true，即时刻启用：12hive (chavin)&gt; set hive.exec.mode.local.auto; hive.exec.mode.local.auto=false 推测执行相关配置1234567891011hive (default)&gt; set mapred.map.tasks.speculative.execution;mapred.map.tasks.speculative.execution=truehive (default)&gt; set mapred.reduce.tasks.speculative.execution;mapred.reduce.tasks.speculative.execution=truehive (default)&gt; set hive.mapred.reduce.tasks.speculative.execution;hive.mapred.reduce.tasks.speculative.execution=true 单个mapreduce中运行多个group by参数hive.multigroupby.singlemr控制师徒将查询中的多个group by组装到单个mapreduce任务中。如果启用这个优化，那么需要一组常用的group by键：12345678910111213例子：select Provice,city,county,count(rainfall) from area where data=&quot;2018-09-02&quot; group by provice,city,countselect Provice,count(rainfall) from area where data=&quot;2018-09-02&quot; group by provice #使用multi group byfrom area insert overwrite table temp1 select Provice,city,county,count(rainfall) from area where data=&quot;2018-09-02&quot; group by provice,city,count insert overwrite table temp2 select Provice,count(rainfall) from area where data=&quot;2018-09-02&quot; group by provice 聚合优化：启用参数：hive.map.aggr=true默认开启 参数hive.fetch.task.conversion的调优：默认值：hive.fetch.task.conversion=minimal 建议值：set hive.fetch.task.conversion=more;原理： 对于简单的不需要聚合的类似 SELECT from LIMIT n 语句，不需要起 MapReduce job ，直接通过 Fetch task 获取数据https://blog.csdn.net/liyaohhh/article/details/50675267 参考：https://www.cnblogs.com/duanxingxing/p/4535842.htmlhttps://blog.csdn.net/z_l_l_m/article/details/8773505]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink vs spark]]></title>
    <url>%2F2019%2F01%2F28%2Fflink-vs-spark%2F</url>
    <content type="text"><![CDATA[概念在 spark 中， 批处理使用 RDD， 流处理使用 DStream（内部使用RDD实现）， 所有底层统一都使用 RDD的抽象实现在 flink 中， 批处理使用 Dataset， 流处理使用 DataStreams， 听起来类似 RDD 和 DStreams， 但其实不是， 不同点在于 1、flink 中的 Dataset 代表着执行计划， 而spark 的 RDD 仅仅是一个 java 对象，spark中的dataframes 才有执行计划， flink 和 spark 两者最基础的东西， Dataset 和 RDD， 是不同的。 一个是经过优化器优化的， 一个没有。 flink 中的 Dataset 类似 spark 中经过优化的 Dataframe 概念， spark 1.6 中引入的dataset（跟 flink 中的 Dataset 重名了，两者类似） 最终应该会代替RDD的抽象吧。 2、在 spark 中， DStream 和 Dataframe 等都是基于 RDD 的封装， 然而 flink 中的 Dataset 和 DataStream 则是独立实现的， 尽管两者间尽量保持相同的 API， 但是你很难统一起来， 至少没有 spark 中那样优雅， 这个大方向， flink 能不能做到就难说了。我们不能统一 DataSet 和 DataStreams， 尽管 flink 有和 spark 相同的抽象，但是内部实现是不同的。 内存管理spark 1.5 之前， spark 一直都是使用 java jvm 堆来保存对象， 虽然有利于启动项目， 但是经常会产生 OOM 和 gc 问题， 所以 1.5 开始， spark 开始引入 自己管理内存的 tungsten 项目。 Flink 从第一天起就自己管理内存， spark 就是从这学的吧， 不仅保存数据使用 binary data， 而且可以直接在binary data 上进行操作。 spark 1.5 开始也可以直接在binary data 进行 dataframe API 提供的操作了。 自己管理内存， 直接使用分配的binary data而不是JVM objects 可以得到更高的性能 和 更好的资源利用。 流式处理在 spark 的眼里， streaming 是特殊的 batch， 在 flink 眼里， batch 是特殊的 streaming， 主要的区别在于1、实时 vs 准实时 ，flink 提供 event 事件级别的延迟， 可以认为是实时的， 类似于 storm 的模型， 而 spark 中， 是微批处理， 不能提供事件级别的延迟， 我们可以称之为准实时。2、flink 则可以灵活的支持窗口， 支持带有事件时间的窗口（Window）操作是flink 的亮点， 你可以选择使用处理时间还是事件时间进行窗口操作， 这种灵活性是 spark 所不如的。3、spark 来自于 Map/Reduce 时代， 崇尚 运算追着数据走， 数据可以是内存中的数组， 也可以是磁盘中的文件， 可以进行很好的容错。Flink 的模型中， 数据是追着运算走的， 算子位于节点上， 数据从中流过， 类似于 akka-streams 中的概念。]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[状态计算]]></title>
    <url>%2F2019%2F01%2F28%2F%E7%8A%B6%E6%80%81%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[spark流计算的数据是以窗口的形式，源源不断的流过来的。如果每个窗口之间的数据都有联系的话，那么就需要对前一个窗口的数据做状态管理。spark有提供了两种模型来达到这样的功能，一个是updateStateByKey，另一个是mapWithState ，后者属于Spark1.6之后的版本特性，性能是前者的数十倍。基本的wordcount123456789101112131415161718192021222324252627282930313233343536package com.scala.testimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;object WC &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() //程序在运行时receiver会独占一个线程,所以streaming程序至少要两个线程,防止starvation scenario conf.setAppName(&quot;WordCount&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) //所有流功能的主要入口 val smc : StreamingContext = new StreamingContext(sc, Seconds(5)) //指定从TCP源数据流的离散流,接收到的每一行数据都是一行文本 val lines : ReceiverInputDStream[String] = smc.socketTextStream(&quot;localhost&quot;,6666) //将接收到的文本压平,转换,聚合 val dStream : DStream[(String, Int)] = lines.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_) dStream.print() // Spark Streaming 只有建立在启动时才会执行计算，在它已经开始之后，并没有真正地处理// -------------------------- //启动计算 smc.start(); //等待计算终止 smc.awaitTermination(); //true 会把内部的sparkcontext同时停止 //false 只会停止streamingcontext 不会停sparkcontext smc.stop(true); &#125;&#125; updateStateByKey1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.scala.testimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.util.parsing.json.JSON/** * * https://blog.csdn.net/cyony/article/details/79653357 * nc -lk 6666 * 样例数据 * &#123;&quot;name&quot;:&quot;cyony1&quot;,&quot;score&quot;:&quot;90&quot;,&quot;sex&quot;:&quot;1&quot;&#125; * * &#123;&quot;name&quot;:&quot;cyony2&quot;,&quot;score&quot;:&quot;76&quot;,&quot;sex&quot;:&quot;0&quot;&#125; * * updateStateByKey这种模型，每次窗口触发，都会将两个RDD执行cogroup操作，，非常的耗时。而且checkpoint dir也会很大 * */object WC_stateful &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() //程序在运行时receiver会独占一个线程,所以streaming程序至少要两个线程,防止starvation scenario conf.setAppName(&quot;WordCount&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) //所有流功能的主要入口 val smc : StreamingContext = new StreamingContext(sc, Seconds(5)) //定义checkpoint目录 smc.checkpoint(&quot;./wc_stateful&quot;) //指定从TCP源数据流的离散流,接收到的每一行数据都是一行文本 val lines : ReceiverInputDStream[String] = smc.socketTextStream(&quot;localhost&quot;,6666) //定义updateStateByKey更新函数 val updateFunc = (currentValue:Seq[Int],preValue:Option[Int]) =&gt; &#123; Some(currentValue.sum + preValue.getOrElse(0)) &#125; //将接收到的文本压平,转换,聚合 lines.map(JSON.parseFull(_).get.asInstanceOf[Map[String,String]]) .map(map =&gt; (map.get(&quot;sex&quot;).get.toInt,map.get(&quot;score&quot;).get.toInt)) .reduceByKey(_+_) .updateStateByKey(updateFunc) .print()// dStream.print() // Spark Streaming 只有建立在启动时才会执行计算，在它已经开始之后，并没有真正地处理// -------------------------- //启动计算 smc.start(); //等待计算终止 smc.awaitTermination(); //true 会把内部的sparkcontext同时停止 //false 只会停止streamingcontext 不会停sparkcontext smc.stop(true); &#125;&#125; mapWithState123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package com.scala.testimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.streaming.&#123;Seconds, State, StateSpec, StreamingContext&#125;import org.apache.spark.streaming.dstream.ReceiverInputDStreamimport scala.util.parsing.json.JSON/** * * https://blog.csdn.net/cyony/article/details/79653357 * nc -lk 6666 * 样例数据 * &#123;&quot;name&quot;:&quot;cyony1&quot;,&quot;score&quot;:&quot;90&quot;,&quot;sex&quot;:&quot;1&quot;&#125; * * &#123;&quot;name&quot;:&quot;cyony2&quot;,&quot;score&quot;:&quot;76&quot;,&quot;sex&quot;:&quot;0&quot;&#125; * * 如果当前窗口期没有新的数据过来，mapstate方式是根本不会触发状态更新操作的，但是updateState方式就会触发更新操作。 * 这个和他的模型原理有关，进一步佐证了updateState方式会每次都执行cogroup操作RDD，生成新的RDD。 * * https://www.jianshu.com/p/1463bc1d81b5 * https://blog.csdn.net/zangdaiyang1991/article/details/84099722 * http://www.cnblogs.com/DT-Spark/articles/5616560.html * */object WC_mapWithState &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() //程序在运行时receiver会独占一个线程,所以streaming程序至少要两个线程,防止starvation scenario conf.setAppName(&quot;WordCount&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) //所有流功能的主要入口 val smc : StreamingContext = new StreamingContext(sc, Seconds(5)) //定义checkpoint目录 smc.checkpoint(&quot;./wc_mapWithState&quot;) //指定从TCP源数据流的离散流,接收到的每一行数据都是一行文本 val lines : ReceiverInputDStream[String] = smc.socketTextStream(&quot;localhost&quot;,6666) //定义MapWithState更新函数 val mappingFun = (sex: Int, score: Option[Int], state: State[Int]) =&gt; &#123; val sum = score.getOrElse(0) + state.getOption().getOrElse(0) state.update(sum) (sex, sum) &#125; //将接收到的文本压平,转换,聚合 lines.map(JSON.parseFull(_).get.asInstanceOf[Map[String, String]]) .map(map =&gt; (map.get(&quot;sex&quot;).get.toInt, map.get(&quot;score&quot;).get.toInt)).reduceByKey(_ + _) .mapWithState(StateSpec.function(mappingFun)) .print() // dStream.print() // Spark Streaming 只有建立在启动时才会执行计算，在它已经开始之后，并没有真正地处理 // -------------------------- //启动计算 smc.start(); //等待计算终止 smc.awaitTermination(); //true 会把内部的sparkcontext同时停止 //false 只会停止streamingcontext 不会停sparkcontext smc.stop(true); &#125;&#125; 使用Redis管理状态我们不使用Spark自身的缓存机制来存储状态，而是使用Redis来存储状态。来一批新数据，先去redis上读取它们的上一个状态，然后更新写回Redis，逻辑非常简单，如下图所示 在实际实现过程中，为了避免对同一个key有多次get/set请求，所以在更新状态前，使用groupByKey对相同key的记录做个归并，对于前面描述的问题，我们可以先这样做：12val liveDStream = ... // (userId, clickId)liveDStream.groupByKey().mapPartitions(...) 为了减少访问Redis的次数，我们使用pipeline的方式批量访问，即在一个分区内，一个一个批次的get/set，以提高Redis的访问性能，那么我们的更新逻辑就可以做到mapPartitions里面，如下代码所示。12345678910111213141516171819202122232425262728293031323334353637383940414243val updateAndflush = ( records: Seq[(Long, Set(Int))], states: Seq[Response[String]], pipeline: Pipeline) =&gt; &#123; pipeline.sync() // wait for getting var i = 0 while (i &lt; records.size) &#123; val (userId, values) = records(i) // 从字符串中解析出上一个状态中的点击列表 val oldValues: Set[Int] = parseFrom(states(i).get()) val newValues = values ++ oldValues // toString函数将Set[Int]编码为字符串 pipeline.setex(userId.toString, 3600, toString(newValues)) i += 1 &#125; pipeline.sync() // wait for setting&#125;val mappingFunc = (iter: Iterator[(Long, Iterable[Int])]) =&gt; &#123; val jedis = ConnectionPool.getConnection() val pipeline = jedis.pipelined() val records = ArrayBuffer.empty[(Long, Set(Int))] val states = ArrayBuffer.empty[Response[String]] while (iter.hasNext) &#123; val (userId, values) = iter.next() records += ((userId, values.toSet)) states += pipeline.get(userId.toString) if (records.size == batchSize) &#123; updateAndflush(records, states, pipeline) records.clear() states.clear() &#125; &#125; updateAndflush(records, states, pipeline) Iterator[Int]()&#125;liveDStream.groupByKey() .mapPartitions(mappingFunc) .foreachRDD &#123; rdd =&gt; rdd.foreach(_ =&gt; Unit) // force action&#125; 上述代码没有加容错等操作，仅描述实现逻辑，可以看到，函数mappingFunc会对每个分区的数据处理，实际计算时，会累计到batchSize才去访问Redis并更新，以降低访问Redis的频率。这样就不再需要cache和checkpoint了，程序挂了，快速拉起来即可，不需要从checkpoint处恢复状态，同时可以节省相当大的计算资源。123456spark.streaming.blockInterval=10000spark.streaming.backpressure.enabled=truespark.streaming.receiver.maxRate=5000spark.yarn.maxAppAttempts=4spark.speculation=truespark.task.maxFailures=8 参考：https://blog.csdn.net/struggle3014/article/details/79792695]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[幂等操作]]></title>
    <url>%2F2019%2F01%2F28%2F%E5%B9%82%E7%AD%89%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[最近很多人都在谈论幂等性，好吧，这回我也来聊聊这个话题，光看着俩字，一开始的确有点一头雾水，语文不好嘛，词太专业嘛，对吧 现如今我们的系统大多拆分为分布式SOA，或者微服务，一套系统中包含了多个子系统服务，而一个子系统服务往往会去调用另一个服务，而服务调用服务无非就是使用RPC通信或者restful，既然是通信，那么就有可能再服务器处理完毕后返回结果的时候挂掉，这个时候用户端发现很久没有反应，那么就会多次点击按钮，这样请求有多次，那么处理数据的结果是否要统一呢？那是肯定的！尤其再支付场景。幂等性：就是用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用。举个最简单的例子，那就是支付，用户购买商品使用约支付，支付扣款成功，但是返回结果的时候网络异常，此时钱已经扣了，用户再次点击按钮，此时会进行第二次扣款，返回结果成功，用户查询余额返发现多扣钱了，流水记录也变成了两条．．．在以前的单应用系统中，我们只需要把数据操作放入事务中即可，发生错误立即回滚，但是再响应客户端的时候也有可能出现网络中断或者异常等等。在增删改查4个操作中，尤为注意就是增加或者修改，查询对于结果是不会有改变的，删除只会进行一次，用户多次点击产生的结果一样修改在大多场景下结果一样增加在重复提交的场景下会出现那么如何设计接口才能做到幂等呢？ 方法一、单次支付请求，也就是直接支付了，不需要额外的数据库操作了，这个时候发起异步请求创建一个唯一的ticketId，就是门票，这张门票只能使用一次就作废，具体步骤如下：异步请求获取门票调用支付，传入门票根据门票ID查询此次操作是否存在，如果存在则表示该操作已经执行过，直接返回结果；如果不存在，支付扣款，保存结果返回结果到客户端如果步骤4通信失败，用户再次发起请求，那么最终结果还是一样的 方法二、分布式环境下各个服务相互调用这边就要举例我们的系统了，我们支付的时候先要扣款，然后更新订单，这个地方就涉及到了订单服务以及支付服务了。用户调用支付，扣款成功后，更新对应订单状态，然后再保存流水。而在这个地方就没必要使用门票ticketId了，因为会比较闲的麻烦（支付状态：未支付，已支付）步骤：1、查询订单支付状态2、如果已经支付，直接返回结果3、如果未支付，则支付扣款并且保存流水4、返回支付结果如果步骤4通信失败，用户再次发起请求，那么最终结果还是一样的对于做过支付的朋友，幂等，也可以称之为冲正，保证客户端与服务端的交易一致性，避免多次扣款。 最后来看一下我们的订单流程，虽然不是很复杂，但是最后在支付环境是一定要实现幂等性的 可以简单理解成upsert操作，有则更新，无则插入参考https://www.cnblogs.com/leechenxiang/p/6626629.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming如何应对 Exactly once 语义（kafka）]]></title>
    <url>%2F2019%2F01%2F28%2FSpark-Streaming%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9-Exactly-once-%E8%AF%AD%E4%B9%89%2F</url>
    <content type="text"><![CDATA[Spark Streaming（以下简写SS）Exactly once语义（以下简写EO） 首先EO表示可以精准控制到某一条记录，但由于SS是基于rdd和batch的，所以SS的EO可以认为是针对一个批次的的精准控制(控制各个批次间是否重复和漏读)。涉及到三部分都保证 exactly once 的语义。1、(数据源)上游是否EO到SS2、(数据处理)SS作为整体是否保证了EO3、(数据存储)SS是否将数据EO地写出到了下游 涉及到 上游是否EO到SS对于 接收数据，主要取决于上游数据源的特性。例如，从 HDFS 这类支持容错的文件系统中读取文件，能够直接支持 Exactly-once 语义。如果上游消息系统支持 ACK（如RabbitMQ），我们就可以结合 Spark 的 Write Ahead Log 特性来实现 At-least-once 语义。对于非可靠的数据接收器（如 socketTextStream），当 Worker 或 Driver 节点发生故障时就会产生数据丢失，提供的语义也是未知的。而 Kafka 消息系统是基于偏移量（Offset）的，它的 Direct API 可以提供 Exactly-once 语义。 官方在创建 DirectKafkaInputStream(Kafka direct api)时只需要输入消费 Kafka 的 From Offset，然后其自行获取本次消费的 End Offset，也就是当前最新的 Offset。保存的 Offset 是本批次的 End Offset，下次消费从上次的 End Offset 开始消费。 当程序宕机或重启任务后，这其中存在一些问题。如果在数据处理完成前存储 Offset，则可能存在作业处理数据失败与作业宕机等情况，重启后会无法追溯上次处理的数据导致数据出现丢失。如果在数据处理完成后存储 Offset，但是存储 Offset 过程中发生失败或作业宕机等情况，则在重启后会重复消费上次已经消费过的数据。 而且此时又无法保证重启后消费的数据与宕机前的数据量相同数据相当，这又会引入另外一个问题，如果是基于聚合统计指标作更新操作，这会带来无法判断上次数据是否已经更新成功。 参考文章中给出的解决方案是，保证在创建 DirectKafkaInputStream 可以同时输入 From Offset 与 End Offset，并且我们在存储 Kafka Offset 的时候保存了每个批次的起始Offset 与结束 Offset。这样的设计使得后面用户在后面对于第一个批次的数据处理非常灵活可变：1、如果用户直接忽略第一个批次的数据，那此时保证的是 at most once 的语义，因为我们无法获知重启前的最后一个批次数据操作是否有成功完成。2、如果用户依照原有逻辑处理第一个批次的数据，不对其做去重操作，那此时保证的是 at least once 的语义，最终结果中可能存在重复数据；3、最后如果用户想要实现 exactly once，muise spark core 提供了根据topic、partition 与 offset 生成 UID 的功能。只要确保两个批次消费的 Offset 相同，则最终生成的 UID 也相同，用户可以根据此 UID 作为判断上个批次数据是否有存储成功的依据。下面简单的给出了重启后第一个批次操作的行为。 参考:http://www.10tiao.com/html/522/201809/2651425155/1.html实际上如果是聚合操作，完全可以引入状态计算，而不需要修改源码。或者将Offset，存储到redis中，每次存redis中读取,见其他文章https://blog.csdn.net/qq_32252917/article/details/78827126 。本文只对EO做讨论，状态计算在其他文章做介绍。EO只要明确保证能拿到上次成功结束的offset就可以了(保证数据零丢失)，至于后面是否会被重复计算部分，可以根据业务做不同的处理(根据输出做幂等设计)。 控制offset实际上是解决数据丢失如下的主要场景：SS在使用Receiver收到数据时(非Kafka direct api)，通过Driver的调度，Executor开始计算数据的时候如果Driver突然奔溃（导致Executor会被Kill掉），此时Executor会被Kill掉，那么Executor中的数据就会丢失，此时就必须通过例如WAL机制让所有的数据通过类似HDFS的方式进行安全性容错处理，从而解决Executor被Kill掉后导致数据丢失可以通过WAL机制恢复回来。此时数据可以零丢失，但并不能保证Exactly Once，如果Receiver接收且保存起来后没来得及更新updateOffsets时，就会导致数据被重复处理。所以本文讨论的EO是用户自己能精准控制offset而非交给框架去处理。 SS处理数据是否保证了EO在使用 Spark RDD 对数据进行 转换或汇总 时，我们可以天然获得 Exactly-once 语义，因为 RDD 本身就是一种具备容错性、不变性、以及计算确定性的数据结构。只要数据来源是可用的，且处理过程中没有副作用（Side effect），我们就能一直得到相同的计算结果。 SS内部的实现机制是spark core基于RDD模型的，RDD为保证计算过程中数据不丢失使用了checkpoint机制，也就是说其计算逻辑是RDD的变换过程，也就是DAG，可以在计算过程中的任何一个阶段（也就是这个阶段的RDD）上使用checkpoint方法，就可以保证当后续计算失败，可以从这个checkpoint重新算起，使得计算延续下去。当Spark Streaming场景下，其天然会进行batch操作，也就是说kafka过来的数据，每秒（一个固定batch的时间周期）会对当前kafka中的数据产生一个RDD，那么后续计算就是在这个RDD上进行的。只需要在kafkaRDD这个位置合理使用了checkpoint（这一点在前面已经讲过，可以保证）就能保证SS内部的Exactly once。 SS是否将数据EO地写出到了下游结果输出 默认符合 At-least-once 语义，因为 foreachRDD 方法可能会因为 Worker 节点失效而执行多次，从而重复写入外部存储。我们有两种方式解决这一问题，幂等更新和事务更新。下面我们将深入探讨这两种方式。参考：https://blog.csdn.net/qq_32252917/article/details/78827126首先输出操作是具有At-least Once语义的，也就是说SS可以保证需要输出的数据一定会输出出去，只不过由于失败等原因可能会输出多次。那么如何保证Exactly once？第一种“鸵鸟做法”，就是期望下游（数据）具有幂等特性。比如相同数据写 hdfs 同一个文件，这本身就是幂等操作，保证了多次操作最终获取的值还是相同；HBase、ElasticSearch 与 redis 等都能够实现幂等操作。对于关系型数据库的操作一般都是能够支持事务性操作。第二种使用事务更新，简要代码如下：1234567dstream.foreachRDD &#123; (rdd, time) =&gt; rdd.foreachPartition &#123; partitionIterator =&gt; val partitionId = TaskContext.get.partitionId() val uniqueId = generateUniqueId(time.milliseconds, partitionId) // use this uniqueId to transactionally commit the data in partitionIterator &#125;&#125; 这样保证同一个partition要么一起更新成功，要么一起失败，通过uniqueId来标识这一次的更新，这就要求下游支持事务机制。如果不采用幂等或者事务。可以采用如下方案，除了数据主动(重启服务)出错外，还会遇到如下问题。关于Spark Streaming数据输出多次重写及解决方案： 为什么会有这个问题，因为SparkStreaming在计算的时候基于SparkCore，SparkCore天生会做以下事情导致SparkStreaming的结果（部分）重复输出: 1.Task重试； 2.慢任务推测； 3.Stage重复； 4.Job重试；会导致数据的丢失。对应的解决方案： 1.一个任务失败就是job 失败，设置spark.task.maxFailures次数为1； 2.设置spark.speculation为关闭状态（因为慢任务推测其实非常消耗性能，所以关闭后可以显著的提高Spark Streaming处理性能） 3.Spark streaming on kafka的话，假如job失败后可以设置kafka的auto.offset.reset为largest的方式会自动恢复job的执行。参考：https://zybuluo.com/marlin/note/486917http://spark.apache.org/docs/latest/streaming-programming-guide.html#semantics-of-output-operationshttps://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka/]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive小文件合并]]></title>
    <url>%2F2019%2F01%2F26%2FHive%E5%B0%8F%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6%2F</url>
    <content type="text"><![CDATA[HDFS中的文件、目录和块都映射为一个对象，存储在NameNode服务器内存中，通常占用150个字节。 如果有1千万个文件，就需要消耗大约3G的内存空间。如果是10亿个文件呢，简直不可想象。所以我们要了解一下,hadoop 处理小文件的各种方案，然后选择一种适合的方案来解决本的小文件问题。 此外，HDFS读写小文件时也会更加耗时，因为每次都需要从NameNode获取元信息，并与对应的DataNode建立连接。对于MapReduce程序来说，小文件还会增加Mapper的个数，job作为一个独立的jvm实例，每个job只处理很少的数据，其开启和停止的开销可能会大大超过实际的任务处理时间，浪费了大量的调度时间。当然，这个问题可以通过使用CombinedInputFile和JVM重用来解决. 参考:https://blog.csdn.net/WYpersist/article/details/80043816 输入文件合并输入合并。即在Map前合并小文件。这个方法即可以解决之前小文件数太多，导致mapper数太多的问题；还可以防止输出小文件合数太多的问题（因为mr只有map时，mapper数就是输出的文件个数）。文件合并失效，且job只有map时，map的个数就是文件个数；通过控制map大小控制map个数，以控制输出文件个数。set hive.hadoop.supports.splittable.combineinputformat=true; 开关set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 执行Map前进行小文件合并set mapred.max.split.size=2048000000; 2G 每个Map最大输入大小set mapred.min.split.size.per.node=2048000000; 一个节点上split的至少的大小 ，决定了多个data node上的文件是否需要合并set mapred.min.split.size.per.rack=2048000000; 一个交换机下split的至少的大小，决定了多个交换机上的文件是否需要合并MR-Job 默认的输入格式 FileInputFormat 为每一个小文件生成一个切片。CombineFileInputFormat 通过将多个“小文件”合并为一个”切片”（在形成切片的过程中也考虑同一节点、同一机架的数据本地性），让每一个 Mapper 任务可以处理更多的数据，从而提高 MR 任务的执行速度。 解读：CombineFileInputFormat类：https://www.cnblogs.com/skyl/p/4754999.html 输出文件合并输出合并。即在输出结果的时候合并小文件动态分区好用，但是会产生很多小文件。原因就在于，假设初始有N个mapper,最后生成了m个分区，最终会有多少个文件生成呢？答案是N*m,是的，每一个mapper会生成m个文件，就是每个分区都会对应一个文件，这样的话你算一下。所以小文件就会成倍的产生。怎么解决这个问题，通常处理方式也是像上面那样，让数据尽量聚到少量reducer里面。但是有时候虽然动态分区不会产生reducer,但是也就意味着最后没有进行文件合并,我们也可以用distribute by rand()这句来保证数据聚类到相同的reducer。参考：https://www.iteblog.com/archives/1533.html 但是如果是多层分区呢，且二层分区数据量差异很大，虽然也可以使用上面的方式但仍然有可能不均匀，此时需要扩展，采用如下方式distribute by if(productType in (&#39;h&#39;,&#39;f&#39;,&#39;t&#39;),floor(rand() * 10),1) 其他合并方式，配置参数set hive.merg.xxxx在文件合并 和 压缩 并存时会失效，即只对text或者seq文件生效，对压缩格式如orc等会不适用，就有些鸡肋参考：https://meihuakaile.github.io/2018/10/19/hive%E5%B0%8F%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6/ 上面的方式只是解决使用和规避问题，如果是已经有很多小文件，那么只有压缩一条路可走了，问题就转变为如何更快更优的解决各种格式的压缩问题了。]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk]]></title>
    <url>%2F2019%2F01%2F26%2Fawk%2F</url>
    <content type="text"><![CDATA[awk是行处理器: 相比较屏幕处理的优点，在处理庞大文件时不会出现内存溢出或是处理缓慢的问题，通常用来格式化文本信息awk处理过程: 依次对每一行进行处理，然后输出 awk命令形式:awk [-F|-f|-v] ‘BEGIN{} // {command1; command2} END{}’ file [-F|-f|-v] 大参数，-F指定分隔符，-f调用脚本，-v定义变量 var=value‘ ‘ 引用代码块BEGIN 初始化代码块，在对每一行进行处理之前，初始化代码，主要是引用全局变量，设置FS分隔符// 匹配代码块，可以是字符串或正则表达式{} 命令代码块，包含一条或多条命令； 多条命令使用分号分隔END 结尾代码块，在对每一行进行处理之后再执行的代码块，主要是进行最终计算或输出结尾摘要信息特殊要点: $0 表示整个当前行$1 每行第一个字段 NF 字段数量变量；即每行的字段个数，Number FieldNR 每行的记录号，多文件记录递增；即行号 Number Record FNR 与NR类似，不过多文件记录不递增，每个文件都从1开始 \t 制表符\n 换行符 FS BEGIN时定义分隔符RS输入的记录分隔符， 默认为换行符(即文本是按一行一行输入) ~ 匹配，与==相比不是精确比较!~ 不匹配，不精确比较 == 等于，必须全部相等，精确比较!= 不等于，精确比较 &amp;&amp; 逻辑与|| 逻辑或 + 匹配时表示1个或1个以上 /[0-9][0-9]+/ 两个或两个以上数字/[0-9][0-9]*/ 一个或一个以上数字 FILENAME 文件名 OFS输出字段分隔符， 默认也是空格，可以改为制表符等ORS 输出的记录分隔符，默认为换行符,即处理结果也是一行一行输出到屏幕 -F’[:#/]’ 定义三个分隔符 IF语句 必须用在{}中，且比较内容用()扩起来 awk -F: ‘{if($1~/mail/) print $1}’ /etc/passwd //简写 awk -F: ‘{if($1~/mail/) {print $1}}’ /etc/passwd //全写 awk -F: ‘{if($1~/mail/) {print $1} else {print $2}}’ /etc/passwd //if…else… 原文链接 : http://blog.chinaunix.net/uid-23302288-id-3785105.html]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell统计]]></title>
    <url>%2F2019%2F01%2F26%2Fshell%E7%BB%9F%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[统计 列出当天访问次数最多的IP前20个命令：cut -d- -f 1 /usr/local/apache2/logs/access_log |uniq -c | sort -rn | head -20 查看当天有多少个IP访问：awk ‘{print $1}’ log_file|sort|uniq|wc -l 查看某一个页面被访问的次数;grep “/index.php” log_file | wc -l 查看每一个IP访问了多少个页面：awk ‘{++S[$1]} END {for (a in S) print a,S[a]}’ log_file 将每个IP访问的页面数进行从小到大排序：awk ‘{++S[$1]} END {for (a in S) print S[a],a}’ log_file | sort -n 查看某一个IP访问了哪些页面：grep ^111.111.111.111 log_file| awk ‘{print $1,$7}’ 去掉搜索引擎统计当天的页面：awk ‘{print $12,$1}’ log_file | grep ^\”Mozilla | awk ‘{print $2}’ |sort | uniq | wc -l 查看2009年6月21日14时这一个小时内有多少IP访问：awk ‘{print $4,$1}’ log_file | grep 21/Jun/2009:14 | awk ‘{print $2}’| sort | uniq | wc -l 统计访问日志里每个ip访问次数[root@qr logs]# cat a.sh#!/bin/bash#将28/Jan/2015全天的访问日志放到a.txt文本cat access.log |sed -rn ‘/28\/Jan\/2015/p’ &gt; a.txt #统计a.txt里面有多少个ip访问cat a.txt |awk ‘{print $1}’|sort |uniq &gt; ipnum.txt#通过shell统计每个ip访问次数for i in `cat ipnum.txt`doiptj=`cat access.log |grep $i | grep -v 400 |wc -l`echo “ip地址”$i”在2015-01-28日全天(24小时)累计成功请求”$iptj”次，平均每分钟请求次数为：”$(($iptj/1440)) &gt;&gt; result.txtdone 把100天前的文件打包并且删除find [path] -type f -mtime +100 -exec tar rvf tmp.tar –remove-files {} \; 查找find . -name ‘.sh’ | xargs grep -in ‘48 ‘ 查找所有”.h”文件find /PATH -name “*.h” 查找所有”.h”文件中的含有”helloworld”字符串的文件find /PATH -name “.h” -exec grep -in “helloworld” {} \;find /PATH -name “.h” | xargs grep -in “helloworld” 查找所有”.h”和”.c”文件中的含有”helloworld”字符串的文件find /PATH /( -name “.h” -or -name “.c” /) -exec grep -in “helloworld” {} \; 查找非备份文件中的含有”helloworld”字符串的文件find /PATH /( -not -name “*~” /) -exec grep -in “helloworld” {} \;注：/PATH为查找路径，默认为当前路径。带-exec参数时必须以\;结尾，否则会提示“find: 遗漏“-exec”的参数”。 运维 lsof |grep deleted注：这个deleted表示该已经删除了的文件，但是文件句柄未释放,这个命令会把所有的未释放文件句柄的进程列出来 swap使用排序 1for i in $( cd /proc;ls |grep &quot;^[0-9]&quot;|awk &apos; $0 &gt;100&apos;) ;do sudo awk &apos;/^Swap:/&#123;a=a+$2&#125;END&#123;print &apos;&quot;$i&quot;&apos;,a/1024&quot;M&quot;&#125;&apos; /proc/$i/smaps 2&gt;/dev/null ; done | sort -k2nr | head -10 curl -O下载 正则bizType=([^&amp;]+)bizType=(.*?)[&amp;|$]]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Top*]]></title>
    <url>%2F2019%2F01%2F26%2FTop%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sed操作]]></title>
    <url>%2F2019%2F01%2F26%2Fsed%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[一次性给文件多行加注释在vim 视图模式下：2,5 s/^/#/或者直接使用sed，命令如下：sed -i ‘2,5s/^/#/‘ filename 注释取消反之，将2~5行带#注释取消：：2,5 s/^#//或者sed -i ‘2,5s/^#//‘ filename 去掉空行sed -i ‘/^$/d’ df.txt 将每一行拖尾的“空白字符”（空格，制表符）删除sed ‘s/ *$//‘ df.txt &gt;cwm.txt 将每一行中的前导和拖尾的空白字符删除sed ‘s/^ //;s/ $//‘ df.txt &gt;cwm.txt vi下全文替换:%s/1/2/g 全文替换“1,20” ：表示从第1行到20行；“%” ：表示整个文件，同“1,$”；“. ,$” ：从当前行到文件尾；]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有效电话号码]]></title>
    <url>%2F2019%2F01%2F26%2F%E6%9C%89%E6%95%88%E7%94%B5%E8%AF%9D%E5%8F%B7%E7%A0%81%2F</url>
    <content type="text"><![CDATA[给定一个包含电话号码列表（一行一个电话号码）的文本文件 file.txt，写一个 bash 脚本输出所有有效的电话号码。 你可以假设一个有效的电话号码必须满足以下两种格式： (xxx) xxx-xxxx 或 xxx-xxx-xxxx。（x 表示一个数字） 你也可以假设每行前后没有多余的空格字符。 示例: 假设 file.txt 内容如下： 987-123-4567123 456 7890(123) 456-7890你的脚本应当输出下列有效的电话号码： 987-123-4567(123) 456-7890 1egrep -o &quot;(^[0-9]&#123;3&#125;-[0-9]&#123;3&#125;-[0-9]&#123;4&#125;$)|(^\([0-9][0-9][0-9]\)\s[0-9]&#123;3&#125;-[0-9]&#123;4&#125;$)&quot; file.txt]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[转置文件]]></title>
    <url>%2F2019%2F01%2F26%2F%E8%BD%AC%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[给定一个文件 file.txt，转置它的内容。 你可以假设每行列数相同，并且每个字段由 ‘ ‘ 分隔. 示例: 假设 file.txt 文件内容如下： name agealice 21ryan 30应当输出： name alice ryanage 21 30 1cat file.txt | awk &apos;&#123;for(i=1;i&lt;=NF;i++)&#123;if(NR==1)&#123;res[i]=$i&#125;else&#123;res[i]=res[i]&quot; &quot;$i&#125;&#125;&#125;END&#123;for(i=1;i&lt;=NF;i++)&#123;print res[i]&#125;&#125;&apos; awk是一行一行处理数据，if(NR==1){res[i]=$i}优先初始化第一行的res[]数组，长度为第一行的元素格式，后面每行在对应位置元素后面追加。]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第十行]]></title>
    <url>%2F2019%2F01%2F26%2F%E7%AC%AC%E5%8D%81%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[给定一个文本文件 file.txt，请只打印这个文件中的第十行。 示例: 假设 file.txt 有如下内容： Line 1Line 2Line 3Line 4Line 5Line 6Line 7Line 8Line 9Line 10你的脚本应当显示第十行： Line 10说明: 如果文件少于十行，你应当输出什么？ 至少有三种不同的解法，请尝试尽可能多的方法来解题。 12tail -n +10 file.txt | head -n 1awk &apos;NR==10&#123;print $0&#125;&apos; file.txt]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计词频]]></title>
    <url>%2F2019%2F01%2F26%2F%E7%BB%9F%E8%AE%A1%E8%AF%8D%E9%A2%91%2F</url>
    <content type="text"><![CDATA[cat words.txt | tr -s &quot; &quot; &quot;\n&quot; |sort |uniq -c | sort -r -n | awk &#39;{print $2,$1}&#39;tr -s 将重复出现字符串压缩为一个字符串“ “ “\n” 将空格替换为换行uniq 命令删除文件中的重复行。uniq 命令读取由 InFile 参数指定的标准输入或文件。该命令首先比较相邻的行，然后除去第二行和该行的后续副本。重复的行一定相邻。（所以一定要在发出 uniq 命令之前，请使用 sort 命令使所有重复行相邻。）sort -n按照数值排序sort -r降序排序sort file.txt | uniq -c -c或–count在每列旁边显示该行重复出现的次数 另:删除字符asdtr -d ‘asd’删除空行tr -s ‘\n’sort -u 去重,如果只有sort是不会去重sort file.txt | uniq -u -u或——unique：仅显示出一次的行列；只显示单一行sort file.txt | uniq -d -d或–repeated：仅显示重复出现的行列]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[left semi join]]></title>
    <url>%2F2019%2F01%2F25%2Fhive-LeftSemiJoin%2F</url>
    <content type="text"><![CDATA[参考:https://blog.csdn.net/happyrocking/article/details/79885071]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[join倾斜]]></title>
    <url>%2F2019%2F01%2F25%2Fhive-join%E5%80%BE%E6%96%9C%2F</url>
    <content type="text"><![CDATA[map join使用 map join 在 map 端就先进行 join ，免得到reduce 时卡住 Auto Map Join还记得原理中提到的物理优化器？Physical Optimizer么？它的其中一个功能就是把Join优化成Auto Map Join 图上左边是优化前的，右边是优化后的 优化过程是把Join作业前面加上一个条件选择器ConditionalTask和一个分支。左边的分支是MapJoin，右边的分支是Common Join(Reduce Join) 看看左边的分支是不是和我们上上一张图很像？ 这个时候，我们在执行的时候，就由这个Conditional Task 进行实时路径选择，遇到小于25兆走左边，大于25兆走右边。所谓，男的走左边，女的走右边，人妖走中间。 在比较新版的Hive中，Auto Mapjoin是默认开启的。如果没有开启，可以使用一个开关， set hive.auto.convert.join=true 开启。当然，Join也会遇到和上面的Group By一样的倾斜问题。 Ｈive 也可以通过像Group By一样两道作业的模式单独处理一行或者多行倾斜的数据。即采用下面的方式。 hive.optimize.skewjoinhive 中设定12set hive.optimize.skewjoin = true; set hive.skewjoin.key = skew_key_threshold （default = 100000） 可以就按官方默认的1个reduce 只处理1G 的算法，那么skew_key_threshold= 1G/平均行长.或者默认直接设成250000000 (差不多算平均行长4个字节) 其原理是就在Reduce Join过程，把超过十万条的倾斜键的行写到文件里，回头再起一道Join单行的Map Join作业来单独收拾它们。最后把结果取并集就是了 对full outer join无效。 其他方法 null或者某个无效字符太多导致数据倾斜。null=null结果是null，即false，在join时关联不上，join之前去掉不影响结果；‘’关联得上，但是不需要时产生不必要的脏数据，可以在join之前把key为null/‘’的值去掉。因为null关联不上如果null有用不能去掉，可以用下面两种方法。办法（1）用union all 12345Select * From log a Join users b On a.user_id is not null And a.user_id = b.user_id Union all Select * from log a where a.user_id is null; 办法（2）赋予null值新的随机值123456Select * from log a left Join bmw_users b on case when a.user_id is null then concat(&apos;dp_hive&apos;,rand()) else a.user_id end = b.user_id; 方法1的log读取两次，jobs是2。方法2的job数是1。这个优化适合无效id(比如-99,’’,null等)产生的倾斜问题。把空值的key变成一个字符串加上随机数，就能把倾斜的数据分到不同的reduce上 ,解决数据倾斜问题。 不同数据类型关联也会产生数据倾斜。场景：一张表s8的日志，每个商品一条记录，要和商品表关联。但关联却碰到倾斜的问题，s8的日志中有字符串商品id,也有数字的商品id,类型是string的，但商品中的数字id是bigint的。问题原因：把s8的商品id转成数字id做hash来分配reduce，所以字符串id的s8日志，都到一个reduce上了，解决的方法验证了这个猜测。解决方法：把数字类型转换成字符串类型 123Select * from s8_log a Left outer join r_auction_auctions b On a.auction_id = cast(b.auction_id as string); 大表Join的数据偏斜MapReduce编程模型下开发代码需要考虑数据偏斜的问题，Hive代码也是一样。数据偏斜的原因包括以下两点： 1. Map输出key数量极少，导致reduce端退化为单机作业。 2. Map输出key分布不均，少量key对应大量value，导致reduce端单机瓶颈。Hive中我们使用MapJoin解决数据偏斜的问题，即将其中的某个小表（全量）分发到所有Map端的内存进行Join，从而避免了reduce。这要求分发的表可以被全量载入内存。极限情况下，Join两边的表都是大表，就无法使用MapJoin。这种问题最为棘手，目前已知的解决思路有两种： 如果是上述情况1，考虑先对Join中的一个表去重，以此结果过滤无用信息。这样一般会将其中一个大表转化为小表，再使用MapJoin 。一个实例是广告投放效果分析， 例如将广告投放者信息表i中的信息填充到广告曝光日志表w中，使用投放者id关联。因为实际广告投放者数量很少（但是投放者信息表i很大），因此可以考虑先在w表中去重查询所有实际广告投放者id列表，以此Join过滤表i，这一结果必然是一个小表，就可以使用MapJoin。 12345 select /*+mapjoin(x)*/* from log a left outer join ( select /*+mapjoin(c)*/d.* from ( select distinct user_id from log ) c join users d on c.user_id = d.user_id) x on a.user_id = b.user_id; 如果是上述情况2，考虑切分Join中的一个表为多片，以便将切片全部载入内存，然后采用多次MapJoin得到结果。 1234567891011select * from ( select w.id, w.time, w.amount, i1.name, i1.loc, i1.cat from w left outer join i sampletable(1 out of 2 on id) i1 ) union all ( select w.id, w.time, w.amount, i2.name, i2.loc, i2.cat from w left outer join i sampletable(1 out of 2 on id) i2 ) ); 以下语句实现了left outer join逻辑：12345678select t1.id, t1.time, t1.amount, coalease(t1.name, t2.name), coalease(t1.loc, t2.loc), coalease(t1.cat, t2.cat) from ( select w.id, w.time, w.amount, i1.name, i1.loc, i1.cat from w left outer join i sampletable(1 out of 2 on id) i1 ) t1 left outer join i sampletable(2 out of 2 on id) t2; 上述语句使用Hive的sample table特性对表做切分。 如果A表关联B表在某个key上倾斜，B是码表，那么可以将B表放大1000倍，然后对A表的key字段加上一个1000以内hash后缀，然后和放大后的b表关联，可以解决问题。 参考:https://blog.csdn.net/lw_ghy/article/details/51469753https://www.cnblogs.com/skyl/p/4855099.htmlhttps://cwiki.apache.org/confluence/display/Hive/Skewed+Join+Optimization]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[group by数据倾斜]]></title>
    <url>%2F2019%2F01%2F25%2Fhive_groupby%E5%80%BE%E6%96%9C%2F</url>
    <content type="text"><![CDATA[group by数据倾斜倾斜原因：select count(distinct name) from user时 使用distinct会将所有的name值都shuffle到一个reducer里面。特别的有select uid, count(distinct name) from user group by uid; 即count distinct + （group by）的情况。 优化：（1）主要是把count distinct改变成group by。改变上面的sql为select uid, count(name) from (select uid, name from user group by uid, name)t group by uid.（2）给group by 字段加随机数打散，聚合，之后把随机数去掉，再次聚合（有点类似下面的参数SET hive.groupby.skewindata=true;）1234567891011121314151617181920212223242526272829select split(uid, &apos;_&apos;)[0] uid, sum(names) from( select concat_ws(&apos;-&apos;, uid, substr(rand()*10, 1, 1)) uid, count(name) names from ( select uid, name ---此处去重，且不会倾斜 from user group by uid, name )a group by concat_ws(&apos;-&apos;, uid, substr(rand()*10, 1, 1)))bgroup by split(uid, &apos;_&apos;)[0]等同于下面select split(uid, &apos;_&apos;)[0] uid, sum(names) from( select uid,count(name) names from ( select concat_ws(&apos;-&apos;, uid, substr(rand()*10, 1, 1)) uid, name from ( select uid, name from user group by uid, name )a ) c group by uid )bgroup by split(uid, &apos;_&apos;)[0] (3)SET hive.groupby.skewindata=true; 当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，该Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作. set hive.map.aggr=true; 在mapper端部分聚合，相当于Combiner 。而Map-Side聚合，一般在聚合函数sum,count时使用。 无论你使用Map端，或者两道作业。其原理都是通过部分聚合来来减少数据量。能不能部分聚合，部分聚合能不能有效减少数据量，通常与UDAF，也就是聚合函数有关。也就是只对代数聚合函数有效，对整体聚合函数无效。所谓代数聚合函数，就是由部分结果可以汇总出整体结果的函数，如count，sum。 所谓整体聚合函数，就是无法由部分结果汇总出整体结果的函数，如avg，mean。 比如，sum, count，知道部分结果可以加和得到最终结果。 而对于，mean，avg，知道部分数据的中位数或者平均数，是求不出整体数据的中位数和平均数的。 set hive.groupby.mapaggr.checkinterval=100000；–这个是group的键对应的记录条数超过这个值则会进行分拆,值根据具体数据量设置。hive.map.aggr.hash.min.reduction=0.5(默认)预先取100000条数据聚合,如果聚合后的条数/100000&gt;0.5，则不再聚合 使用Hive的过程中，我们习惯性用set hive.groupby.skewindata=true来避免因数据倾斜造成的计算效率问题，但是每个设置都是把双刃剑，最近调研了下相关问题，现总结如下： 从下表可以看出，skewindata配置真正发生作用，只会在以下三种情况下，能够将1个job转化为2个job：select count distinct … from …select a,count() from … group by a 只针对单列有效select count(),count(distinct …) from 此处没有group by如下sql会报错select count(*),count(distinct …) from … group by a 此处有group by需要改为select a,sum(1),count(distinct …) from … group by a 参考：https://meihuakaile.github.io/2018/10/19/hive%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/https://blog.csdn.net/dxl342/article/details/77886577https://blog.csdn.net/lw_ghy/article/details/51469753]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark性能优化指南-基础篇]]></title>
    <url>%2F2018%2F02%2F22%2FSpark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97-%E5%9F%BA%E7%A1%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。 开发调优Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。 原则一：避免创建重复的RDD通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。 我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。 一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")rdd1.map(...)val rdd2 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")rdd2.reduce(...)// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt")rdd1.map(...)rdd1.reduce(...)``` ## 原则二：尽可能复用同一个RDD 除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。 ```java// 错误的做法。// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。JavaPairRDD&lt;Long, String&gt; rdd1 = ...JavaRDD&lt;String&gt; rdd2 = rdd1.map(...)// 分别对rdd1和rdd2执行了不同的算子操作。rdd1.reduceByKey(...)rdd2.map(...)// 正确的做法。// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。// 其实在这种情况下完全可以复用同一个RDD。// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。JavaPairRDD&lt;Long, String&gt; rdd1 = ...rdd1.reduceByKey(...)rdd1.map(tuple._2...)// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。 原则三：对多次使用的RDD进行持久化当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。 Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。 因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。12345678910111213141516171819202122232425262728293031323334353637383940414243// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。// 正确的做法。// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt").cache()rdd1.map(...)rdd1.reduce(...)// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt").persist(StorageLevel.MEMORY_AND_DISK_SER)rdd1.map(...)rdd1.reduce(...)``` 对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。 ## 原则四：尽量避免使用shuffle类算子 如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。 shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。 因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。 **Broadcast与map进行join代码示例** ```java// 传统的join操作会导致shuffle操作。// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。val rdd3 = rdd1.join(rdd2)// Broadcast+map的join操作，不会导致shuffle操作。// 使用Broadcast将一个数据量较小的RDD作为广播变量。val rdd2Data = rdd2.collect()val rdd2DataBroadcast = sc.broadcast(rdd2Data)// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。val rdd3 = rdd1.map(rdd2DataBroadcast...)// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。 原则五：使用map-side预聚合的shuffle操作&#160;&#160;如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。 &#160;&#160;所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者&#160;&#160;aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。 &#160;&#160;比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。 原则六：使用高性能的算子除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。使用reduceByKey/aggregateByKey替代groupByKey详情见“原则五：使用map-side预聚合的shuffle操作”。使用mapPartitions替代普通mapmapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！使用foreachPartitions替代foreach原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。使用filter之后进行coalesce操作通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。使用repartitionAndSortWithinPartitions替代repartition与sort类操作repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。 原则七：广播大变量&#160;&#160;有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。 &#160;&#160;在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。 &#160;&#160;因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。123456789101112// 以下代码在算子函数中，使用了外部的变量。// 此时没有做任何特殊操作，每个task都会有一份list1的副本。val list1 = ...rdd1.map(list1...)// 以下代码将list1封装成了Broadcast类型的广播变量。// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。// 每个Executor内存中，就只会驻留一份广播变量副本。val list1 = ...val list1Broadcast = sc.broadcast(list1)rdd1.map(list1Broadcast...) 原则八：使用Kryo优化序列化性能在Spark中，主要有三个地方涉及到了序列化： 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解） 将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口 使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。 &#160;&#160;对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦. &#160;&#160;以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：123456// 创建SparkConf对象。val conf = new SparkConf().setMaster(...).setAppName(...)// 设置序列化器为KryoSerializer。conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")// 注册要序列化的自定义类型。conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2])) 原则九：优化数据结构Java中，有三种类型比较耗费内存： 对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间 字符串，每个字符串内部都有一个字符数组以及长度等额外信息 集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry &#160;&#160;因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。 &#160;&#160;但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。 资源调优在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。 Spark作业基本运行原理详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。 在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。 Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。 当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。 因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。 task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。 以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。 原则：资源参数调优了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。 num-executors 参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。 参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。 executor-memory 参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。 参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，就代表了你的Spark作业申请到的总内存量（也就是所有Executor进程的内存总和），这个量是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的总内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。 executor-cores 参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。 参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。 driver-memory 参数说明：该参数用于设置Driver进程的内存。 参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。 spark.default.parallelism 参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。 参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。 spark.storage.memoryFraction 参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。 参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 spark.shuffle.memoryFraction 参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。 参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。123456789./bin/spark-submit \ --master yarn-cluster \ --num-executors 100 \ --executor-memory 6G \ --executor-cores 4 \ --driver-memory 1G \ --conf spark.default.parallelism=1000 \ --conf spark.storage.memoryFraction=0.5 \ --conf spark.shuffle.memoryFraction=0.3 \]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark性能优化指南—— 数据倾斜调优]]></title>
    <url>%2F2018%2F02%2F22%2FSpark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97%E2%80%94%E2%80%94-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[数据倾斜调优 绝大多数task执行得都非常快，但个别task执行极慢。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时。这种情况很常见。 原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。 数据倾斜发生的原理数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。 因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。 下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。 如何定位导致数据倾斜的代码数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。 某个task执行特别慢的情况首先要看的，就是数据倾斜发生在第几个stage中。 如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。 比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。 这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。 stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。 stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131val conf = new SparkConf()val sc = new SparkContext(conf)val lines = sc.textFile("hdfs://...")val words = lines.flatMap(_.split(" "))val pairs = words.map((_, 1))val wordCounts = pairs.reduceByKey(_ + _)wordCounts.collect().foreach(println(_))``` 通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由educeByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。 ### 某个task莫名其妙内存溢出的情况这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。 但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。 ## 查看导致数据倾斜的key的数据分布情况 知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。 此时根据你执行操作的情况不同，可以有很多种查看key分布的方式： 1. 如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。 2. 如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。 举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。 ```javaval sampledPairs = pairs.sample(false, 0.1)val sampledWordCounts = sampledPairs.countByKey()sampledWordCounts.foreach(println(_))``` ## 数据倾斜的解决方案### 解决方案一：使用Hive ETL预处理数据方案适用场景：导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。 方案实现思路：此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。 方案实现原理：这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。 方案优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。 方案缺点：治标不治本，Hive ETL中还是会发生数据倾斜。 方案实践经验：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。 项目实践经验：在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。 ### 解决方案二：过滤少数导致倾斜的key 方案适用场景：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。 方案实现思路：如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。 方案实现原理：将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。 方案优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。 方案缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。 方案实践经验：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。 ### 解决方案三：提高shuffle操作的并行度 方案适用场景：如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。 方案实现思路：在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。 方案实现原理：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。 方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。 方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。 方案实践经验：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。 ![输入图片说明](https://static.oschina.net/uploads/img/201607/12180745_BiEM.png "在这里输入图片标题") ### 解决方案四：两阶段聚合（局部聚合+全局聚合） 方案适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。 方案实现思路：这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。 方案实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。 方案优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。 方案缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。 ![输入图片说明](https://static.oschina.net/uploads/img/201607/12181157_u65t.png "在这里输入图片标题") ```java// 第一步，给RDD中的每个key都打上一个随机前缀。JavaPairRDD&lt;String, Long&gt; randomPrefixRdd = rdd.mapToPair( new PairFunction&lt;Tuple2&lt;Long,Long&gt;, String, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(10); return new Tuple2&lt;String, Long&gt;(prefix + "_" + tuple._1, tuple._2); &#125; &#125;);// 第二步，对打上随机前缀的key进行局部聚合。JavaPairRDD&lt;String, Long&gt; localAggrRdd = randomPrefixRdd.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;);// 第三步，去除RDD中每个key的随机前缀。JavaPairRDD&lt;Long, Long&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair( new PairFunction&lt;Tuple2&lt;String,Long&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;String, Long&gt; tuple) throws Exception &#123; long originalKey = Long.valueOf(tuple._1.split("_")[1]); return new Tuple2&lt;Long, Long&gt;(originalKey, tuple._2); &#125; &#125;);// 第四步，对去除了随机前缀的RDD进行全局聚合。JavaPairRDD&lt;Long, Long&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;); 解决方案五：将reduce join转为map join方案适用场景：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。 方案实现思路：不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。 方案实现原理：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。 方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。 方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。123456789101112131415161718192021222324252627282930313233// 首先将数据量比较小的RDD的数据，collect到Driver中来。List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。// 可以尽可能节省内存空间，并且减少网络传输性能开销。final Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);// 对另外一个RDD执行map类操作，而不再是join类操作。JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; // 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。 List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value(); // 可以将rdd1的数据转换为一个Map，便于后面进行join操作。 Map&lt;Long, Row&gt; rdd1DataMap = new HashMap&lt;Long, Row&gt;(); for(Tuple2&lt;Long, Row&gt; data : rdd1Data) &#123; rdd1DataMap.put(data._1, data._2); &#125; // 获取当前RDD数据的key以及value。 String key = tuple._1; String value = tuple._2; // 从rdd1数据Map中，根据key获取到可以join到的数据。 Row rdd1Value = rdd1DataMap.get(key); return new Tuple2&lt;String, String&gt;(key, new Tuple2&lt;String, Row&gt;(value, rdd1Value)); &#125; &#125;);// 这里得提示一下。// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。// rdd2中每条数据都可能会返回多条join后的数据。 解决方案六：采样倾斜key并分拆join操作方案适用场景：两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。 方案实现思路： 对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。 然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。 接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。 再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。 而另外两个普通的RDD就照常join即可。 最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。 方案实现原理：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。 方案优点：对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。 方案缺点：如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。JavaPairRDD&lt;Long, String&gt; sampledRDD = rdd1.sample(false, 0.1);// 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。// 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。// 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。JavaPairRDD&lt;Long, Long&gt; mappedSampledRDD = sampledRDD.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return new Tuple2&lt;Long, Long&gt;(tuple._1, 1L); &#125; &#125;);JavaPairRDD&lt;Long, Long&gt; countedSampledRDD = mappedSampledRDD.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;);JavaPairRDD&lt;Long, Long&gt; reversedSampledRDD = countedSampledRDD.mapToPair( new PairFunction&lt;Tuple2&lt;Long,Long&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple) throws Exception &#123; return new Tuple2&lt;Long, Long&gt;(tuple._2, tuple._1); &#125; &#125;);final Long skewedUserid = reversedSampledRDD.sortByKey(false).take(1).get(0)._2;// 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。JavaPairRDD&lt;Long, String&gt; skewedRDD = rdd1.filter( new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return tuple._1.equals(skewedUserid); &#125; &#125;);// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。JavaPairRDD&lt;Long, String&gt; commonRDD = rdd1.filter( new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; return !tuple._1.equals(skewedUserid); &#125; &#125;);// rdd2，就是那个所有key的分布相对较为均匀的rdd。// 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。// 对扩容的每条数据，都打上0～100的前缀。JavaPairRDD&lt;String, Row&gt; skewedRdd2 = rdd2.filter( new Function&lt;Tuple2&lt;Long,Row&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; return tuple._1.equals(skewedUserid); &#125; &#125;).flatMapToPair(new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call( Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; Random random = new Random(); List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;(); for(int i = 0; i &lt; 100; i++) &#123; list.add(new Tuple2&lt;String, Row&gt;(i + "_" + tuple._1, tuple._2)); &#125; return list; &#125; &#125;);// 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。// 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD1 = skewedRDD.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(100); return new Tuple2&lt;String, String&gt;(prefix + "_" + tuple._1, tuple._2); &#125; &#125;) .join(skewedUserid2infoRDD) .mapToPair(new PairFunction&lt;Tuple2&lt;String,Tuple2&lt;String,Row&gt;&gt;, Long, Tuple2&lt;String, Row&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt; call( Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; tuple) throws Exception &#123; long key = Long.valueOf(tuple._1.split("_")[1]); return new Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt;(key, tuple._2); &#125; &#125;);// 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD2 = commonRDD.join(rdd2);// 将倾斜key join后的结果与普通key join后的结果，uinon起来。// 就是最终的join结果。JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD = joinedRDD1.union(joinedRDD2); 解决方案七：使用随机前缀和扩容RDD进行join方案适用场景：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。 方案实现思路： 该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。 然后将该RDD的每条数据都打上一个n以内的随机前缀。 同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。 最后将两个处理后的RDD进行join即可。 方案实现原理：将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。 方案优点：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。 方案缺点：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。 方案实践经验：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。123456789101112131415161718192021222324252627282930// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。JavaPairRDD&lt;String, Row&gt; expandedRDD = rdd1.flatMapToPair( new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123; List&lt;Tuple2&lt;String, Row&gt;&gt; list = new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;(); for(int i = 0; i &lt; 100; i++) &#123; list.add(new Tuple2&lt;String, Row&gt;(0 + "_" + tuple._1, tuple._2)); &#125; return list; &#125; &#125;);// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。JavaPairRDD&lt;String, String&gt; mappedRDD = rdd2.mapToPair( new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123; Random random = new Random(); int prefix = random.nextInt(100); return new Tuple2&lt;String, String&gt;(prefix + "_" + tuple._1, tuple._2); &#125; &#125;);// 将两个处理后的RDD进行join即可。JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD); 解决方案八：多种方案组合使用在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，预处理一部分数据，并过滤一部分数据来缓解；其次可以对某些shuffle操作提升并行度，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark性能优化指南—— shuffle调优]]></title>
    <url>%2F2018%2F02%2F22%2FSpark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97%E2%80%94%E2%80%94-shuffle%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。 ShuffleManager发展概述在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。 在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。 因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。 下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。 HashShuffleManager运行原理未经优化的HashShuffleManager下图说明了未经优化的HashShuffleManager的原理。这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。 我们先从shuffle write开始说起。shuffle write阶段，主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。 那么每个执行shuffle write的task，要为下一个stage创建多少个磁盘文件呢？很简单，下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。 接着我们来说说shuffle read。shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。 shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。 优化后的HashShuffleManager下图说明了优化后的HashShuffleManager的原理。这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。 开启consolidate机制之后，在shuffle write过程中，task就不是为下游stage的每个task创建一个磁盘文件了。此时会出现shuffleFileGroup的概念，每个shuffleFileGroup会对应一批磁盘文件，磁盘文件的数量与下游stage的task数量是相同的。一个Executor上有多少个CPU core，就可以并行执行多少个task。而第一批并行执行的每个task都会创建一个shuffleFileGroup，并将数据写入对应的磁盘文件内。 当Executor的CPU core执行完一批task，接着执行下一批task时，下一批task就会复用之前已有的shuffleFileGroup，包括其中的磁盘文件。也就是说，此时task会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate机制允许不同的task复用同一批磁盘文件，这样就可以有效将多个task的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升shuffle write的性能。 假设第二个stage有100个task，第一个stage有50个task，总共还是有10个Executor，每个Executor执行5个task。那么原本使用未经优化的HashShuffleManager时，每个Executor会产生500个磁盘文件，所有Executor会产生5000个磁盘文件的。但是此时经过优化之后，每个Executor创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。也就是说，每个Executor此时只会创建100个磁盘文件，所有Executor只会创建1000个磁盘文件。 SortShuffleManager运行原理SortShuffleManager的运行机制主要分成两种，一种是普通运行机制，另一种是bypass运行机制。当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。 普通运行机制下图说明了普通的SortShuffleManager的原理。在该模式下，数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存；如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。 在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。 一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。 SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于每个task最终只有一个磁盘文件，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件。 bypass运行机制下图说明了bypass SortShuffleManager的原理。bypass运行机制的触发条件如下： shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。 不是聚合类的shuffle算子（比如reduceByKey）。 此时task会为每个下游task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。 该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。 而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。 shuffle相关参数调优以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。spark.shuffle.file.buffer 默认值：32k 参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 spark.reducer.maxSizeInFlight 默认值：48m 参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 spark.shuffle.io.maxRetries 默认值：3 参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。 调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。 spark.shuffle.io.retryWait 默认值：5s 参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。 调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。 spark.shuffle.memoryFraction 默认值：0.2 参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。 调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。 spark.shuffle.manager 默认值：sort 参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。 调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。 spark.shuffle.sort.bypassMergeThreshold 默认值：200 参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。 调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。 spark.shuffle.consolidateFiles 默认值：false 参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。 调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。 参考：https://tech.meituan.com/404.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署redis集群]]></title>
    <url>%2F2018%2F02%2F21%2F%E9%83%A8%E7%BD%B2redis%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[Dockerfile 123456789101112131415161718192021222324252627282930FROM alpine:latestMAINTAINER lifei#换源RUN echo 'http://mirrors.ustc.edu.cn/alpine/edge/main' &gt; /etc/apk/repositoriesRUN echo '@community http://mirrors.ustc.edu.cn/alpine/edge/community' &gt;&gt; /etc/apk/repositoriesRUN echo '@testing http://mirrors.ustc.edu.cn/alpine/edge/testing' &gt;&gt; /etc/apk/repositoriesRUN apk update# 修正时区ENV TIMEZONE Asia/ShanghaiRUN apk add tzdataRUN ln -snf /usr/share/zoneinfo/$TIMEZONE /etc/localtimeRUN echo $TIMEZONE &gt; /etc/timezone#修改一些系统设置RUN echo 'vm.overcommit_memory = 1' &gt;&gt; /etc/sysctl.confRUN echo "echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled" &gt;&gt; /etc/rc.local#开始安装 redisRUN apk add redisCOPY start_redis.sh /usr/local/RUN chmod +x /usr/local/start_redis.shRUN chown redis:redis /usr/local/start_redis.shEXPOSE 6379ENTRYPOINT ["sh","/usr/local/start_redis.sh"]CMD ["master"] start_redis.sh 12345678910111213141516redis_role=$1sed -i 's/protected-mode yes/protected-mode no/g' /etc/redis.confsed -i 's/daemonize yes/daemonize no/g' /etc/redis.confsed -i 's/bind 127.0.0.1/bind 0.0.0.0/g' /etc/redis.confecho "Start Redis by "if [ $redis_role = "master" ]; then echo "master" redis-server /etc/redis.confelif [ $redis_role = "slave" ]; then echo "slave" sed -i 's/# slaveof &lt;masterip&gt; &lt;masterport&gt;/slaveof redis-master 6379/g' /etc/redis.conf redis-server /etc/redis.confelse echo "unknow role!"fi 在上面的目录下 1234567891011docker run -d --name=redis-master -p=6379:6379 lifei2199/redis masterdocker run -d --name=node1 --link=redis-master:redis-master -p=7001:6379 lifei2199/redis slavedocker run -d --name=node2 --link=redis-master:redis-master -p=7002:6379 lifei2199/redis slavedocker run -d --name=node3 --link=redis-master:redis-master -p=7003:6379 lifei2199/redis slavedocker ps -adocker exec -i redis-master /bin/shredis-cliinfo replicationset username lifei2199get username 关闭 12345678停用全部运行中的容器:docker stop $(docker ps -q)删除全部容器：docker rm $(docker ps -aq)一条命令实现停用并删除容器：docker stop $(docker ps -q) &amp; docker rm $(docker ps -aq)]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[维度建模]]></title>
    <url>%2F2018%2F02%2F14%2F%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1%2F</url>
    <content type="text"><![CDATA[概念维度表：说明数据，维度是指可指定不同值的对象的描述性属性或特征。例如，地理位置的维度可以包括“纬度”、“经度”或“城市名称”。“城市名称”维度的值可以为“旧金山”、“柏林”或“新加坡”。维度是现实世界中的对象或者概念。 事实表：其实质就是通过一些指标值和各种维度外键来确定一个事实的。发生在现实世界中的操作型事件，其所产生的可度量数值，存储在事实表中。从最低的粒度级别来看，事实表行对应一个度量事件，反之亦然。事实表的关键是度量值dw架构中的中央表，它包含联系事实与维度表的数字度量值和键。 事务事实表/原子事实表/交易事实表聚合事实表周期快照事实表累积快照事实表参考:https://blog.csdn.net/weixin_42478648/article/details/85290881 维度模型也有着一些缺点，比如数据的一致性很难保证，数据的冗余，大量的维度信息处理等，但这些相对于有点来讲都是可接受的，而且也可以通过其他方式避免和简化的。但是有了总线架构和一致性维度、一致性事实就不一样了以维表为总线，事实表以维表为基础的总线矩阵，意味着建设出来的架构正是总线式架构。 总线架构一致性维度就好比企业范围内的一组总线，不同数据集市的事实的就好比插在这组总线上的元件。这也是称之为总线架构的原因。实际设计过程中，我们通常把总线架构列表成矩阵的形式，其中列为一致性维度，行为不同的业务处理过程，即事实，在交叉点上打上标记表示该业务处理过程与该维度相关。这个矩阵也称为总线矩阵（Bus Matrix）。 总线矩阵的每一行代表一个业务过程，并且至少定义了一个事实表和相应的维度。通常，总线矩阵的一行会产生几个相关的事实表，由此可以从不同角度跟踪业务过程。 总线矩阵是企业商业智能数据的路标，必须为任何企业范围的DW/BI工作创建总线矩阵。对数据建模人员和数据管理员来说，创建得到企业认可的一致性维度是一项艰巨的挑战。使用单个维度表来描述企业的产品、顾客或设备，意味着公司必须认同每个维度表的定义方式，包括属性列表、属性名称、体系结构及定义和派生表中每个属性所需的业务规则。从企业战略来看，这是艰巨的工作，其难度与员工人数和部门数量成正比。但是，这一过程是必需的。 一致性维度在多维体系结构中，没有物理上的dw，由物理上的数据集市组合成逻辑上的dw。而且数据集市的建立是可以逐步完成的，最终组合在一起，成为一个dw。如果分步建立数据集市的过程出现了问题，数据集市就会变成孤立的集市，不能组合成dw，而一致性维度的提出正式为了解决这个问题。一致性维度的范围是总线架构中的维度，即可能会在多个数据集市中都存在的维度，这个范围的选取需要架构师来决定。一致性维度的内容和普通维度并没有本质上区别，都是经过数据清洗和整合后的结果。 一致性维度建立的地点是多维体系结构的后台（Back Room），即数据准备区。在多维体系结构的dw项目组内需要有专门的维度设计师，他的职责就是建立维度和维护维度的一致性。在后台建立好的维度同步复制到各个数据集市。这样所有数据集市的这部分维度都是完全相同的。建立新的数据集市时，需要在后台进行一致性维度处理，根据情况来决定是否新增和修改一致性维度，然后同步复制到各个数据集市。这是不同数据集市维度保持一致的要点。在同一个集市内，一致性维度的意思是两个维度如果有关系，要么就是完全一样的，要么就是一个维度在数学意义上是另一个维度的子集。例如，如果建立月维度话，月维度的各种描述必须与日期维度中的完全一致，最常用的做法就是在日期维度上建立视图生成月维度。这样月维度就可以是日期维度的子集，在后续钻取等操作时可以保持一致。如果维度表中的数据量较大，出于效率的考虑，应该建立物化视图或者实际的物理表。这样，维度保持一致后，事实就可以保存在各个数据集市中。虽然在物理上是独立的，但在逻辑上由一致性维度使所有的数据集市是联系在一起，随时可以进行交叉探察等操作，也就组成了dw。 一致性事实在建立多个数据集市时，完成一致性维度的工作就已经完成了一致性的80%－90%的工作量。余下的工作就是建立一致性事实。 一致性事实和一致性维度有些不同，一致性维度是由专人维护在后台（Back Room），发生修改时同步复制到每个数据集市，而事实表一般不会在多个数据集市间复制。需要查询多个数据集市中的事实时，一般通过交叉探查（drill across）来实现。 为了能在多个数据集市间进行交叉探查，一致性事实主要需要保证两点。第一个是KPI的定义及计算方法要一致，第二个是事实的单位要一致性。如果业务要求或事实上就不能保持一致的话，建议不同单位的事实分开建立字段保存。 这样，一致性维度将多个数据集市结合在一起，一致性事实保证不同数据集市间的事实数据可以交叉探查，一个分布式的dw就建成了。 多维体系结构Multidimensional Architecture（MD）有三个关键性概念：总线架构（Bus Architecture），一致性维度（Conformed Dimension）和一致性事实（Conformed Fact）。 多维体系结构主要包括后台（Back Room）和前台（Front Room）两部分。1、后台也称为数据准备区（Staging Area），是MD架构的最为核心的部件。在后台，是一致性维度的产生、保存和分发的场所。同时，代理键也在后台产生。2、前台是MD架构对外的接口，包括两种主要的数据集市，一种是原子数据集市，另一种是聚集数据集市。原子数据集市保存着最低粒度的细节数据，数据以星型结构来进行数据存储。聚集数据集市的粒度通常比原子数据集市要高，和原子数据集市一样，聚集数据集市也是以星型结构来进行数据存储。前台还包括像查询管理、活动监控等为了提供dw的性能和质量的服务。在多维体系结构（MD） 的dw架构中，主导思想是分步建立dw，由数据集市组合成企业的dw。但是，在建立第一个数据集市前，架构师首先要做的就是设计出在整个企业 内具有统一解释的标准化的维度和事实，即一致性维度和一致性事实。而开发团队必须严格的按照这个体系结构来进行数据集市的迭代开发。 维度模型设计过程选择业务过程，即选择主题声明粒度确定维度确定事实 星座模型星座模型是星型模型延伸而来，星型模型是基于一张事实表的，而星座模型是基于多张事实表的，而且共享维度信息。 通过构建一致性维度，来建设星座模型，也是很好的选择。比如同一主题的细节表和汇总表共享维度，不同主题的事实表，可以通过在维度上互相补充来生成可以共享的维度。以维表为总线，事实表以维表为基础的总线矩阵，意味着建设出来的架构正是总线式架构。]]></content>
      <categories>
        <category>dw</category>
      </categories>
      <tags>
        <tag>dw</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark 二次排序]]></title>
    <url>%2F2018%2F02%2F13%2Fspark-%E4%BA%8C%E6%AC%A1%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[数据：40 2040 1040 3040 530 3030 2030 1030 4050 2050 5050 1050 601234567891011121314151617181920212223242526272829303132333435363738394041package com.scala.test.core.secondsortimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object SecondarySort &#123; def main(args:Array[String]): Unit =&#123; val conf = new SparkConf().setAppName(&quot;SecondarySort&quot;).setMaster(&quot;local[*]&quot;) val sc = new SparkContext(conf) val line = sc.textFile(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/secondarysort/sort.txt&quot;) val data = line.map(x =&gt; &#123; val line : Array[String] = x.split(&quot; &quot;) (line(0),line(1)) &#125;) val rdd = data.groupByKey().sortByKey()//todo groupbykey和sortByKey// (40,CompactBuffer(20, 10, 30, 5))// (50,CompactBuffer(20, 50, 10, 60))// (30,CompactBuffer(30, 20, 10, 40)) //todo 之前多条现在变成3条了,改变了原来的条数// val result = rdd.map(item =&gt; (item._1, item._2.toList.sortWith(_.toInt&lt;_.toInt)))//todo values转list并排序 //todo 不改变原来的条数 val result = rdd.flatMap(item =&gt; &#123; val list = item._2.toList.sortWith(_.toInt&lt;_.toInt) list.map(x =&gt; (item._1,x)) &#125;)// 保存// result.saveAsTextFile(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/secondarysort/out&quot;)// 打印 result.collect().foreach(println) sc.stop() &#125;&#125;``` 上面的方式，groupbykey之后把相同的key聚合在一起排序，实际上是内存排序，这种方法可能导致归约器耗尽内存。如果数量很少可以用。和mapreduce的setGroupingComparatorClass方式不同,mapreduce在reduce中没有排序操作，是用框架中的排序进行。 使用repartitionAndSortWithinPartitions： package com.scala.test.core.secondsort import org.apache.spark.{HashPartitioner, Partitioner, SparkConf, SparkContext} object SecondarySortRepartition { def main(args: Array[String]): Unit = { implicit val caseInsensitiveOrdering = new Ordering[Int] { override def compare(a: Int, b: Int) = b.compareTo(a) } import org.apache.spark.Partitioner class KeyBasePartitioner(partitions: Int) extends Partitioner { require(partitions &gt;= 0, s&quot;Number of partitions ($partitions) cannot be negative.&quot;) override def numPartitions: Int = partitions override def getPartition(key: Any): Int = { val k = key.asInstanceOf[SecondarySort] Math.abs(k.one.hashCode() % numPartitions) } } val conf = new SparkConf().setAppName(&quot;SecondarySortRepartition&quot;).setMaster(&quot;local[*]&quot;) val sc = new SparkContext(conf) val line = sc.textFile(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/secondarysort/sort.txt&quot;) class SecondarySort(val one :Int,val two : Int) extends Ordered[SecondarySort] with Serializable { override def compare(that: SecondarySort): Int = { if(this.one-that.one != 0){ this.one-that.one }else{ this.two-that.two } } } line.map(x =&gt; { val xy=x.split(&quot; &quot;) (new SecondarySort(Integer.parseInt(xy(0)),Integer.parseInt(xy(1)).toInt),x) } ) .repartitionAndSortWithinPartitions(new KeyBasePartitioner(3)) .saveAsTextFile(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/secondarysort/out&quot;) // .foreach(println) sc.stop() }}`当spark的分区数大于线程数时，spark仍会按照一个一个分区单独处理，而不会像MapReduce设置setGroupingComparatorClass。可以通过设置.setMaster(“local[2]”)和new KeyBasePartitioner(3)来验证。 spark 1.2之后引入了一个高质量的算子 repartitionAndSortWithinPartitions?。该算子为spark的Shuffle增加了sort。假如，后面再跟mapPartitions算子的话，其算子就是针对已经按照key排序的分区，这就有点像mr的意思了。与groupbykey不同的是，数据不会一次装入内存，而是使用迭代器一次一条记录从磁盘加载。这种方式最小化了内存压力。 定义partitioner的方式可以参考https://www.bbsmax.com/A/LPdoVrl253/repartitionAndSortWithinPartitions算子比先分区在排序效率高https://blog.csdn.net/luofazha2012/article/details/80587128 sortbykey VS repartitionAndSortWithinPartitionshttps://blog.csdn.net/wyqwilliam/article/details/81627603]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark wordcount]]></title>
    <url>%2F2018%2F02%2F12%2Fspark-wordcount%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718package com.scala.test.coreimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object WordCount &#123; def main(args : Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(&quot;wordCount&quot;).setMaster(&quot;local[*]&quot;) val sc = new SparkContext(conf) val line = sc.textFile(&quot;/Users/lifei/Desktop/ab/a.txt&quot;);//todo wholeTextFiles 可以读取目录// 直接打印// line.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collect().foreach(println)// 保存到文件 line.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;/Users/lifei/Desktop/ab/lala&quot;) sc.stop(); &#125;&#125;]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[报表查询]]></title>
    <url>%2F2018%2F02%2F12%2Fhive%E7%BA%A7%E8%81%94%E6%8A%A5%E8%A1%A8%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[级联报表查询123456789101112131415161718192021A,2015-01,5A,2015-01,15B,2015-01,5A,2015-01,8B,2015-01,25A,2015-01,5C,2015-01,10C,2015-01,20A,2015-02,4A,2015-02,6C,2015-02,30C,2015-02,10B,2015-02,10B,2015-02,5A,2015-03,14A,2015-03,6B,2015-03,20B,2015-03,25C,2015-03,10C,2015-03,20``` create table t_access_times(username string,month string,counts int)row format delimited fields terminated by ‘,’;1![upload successful](/images/pasted-52.png) – 窗口分析函数– 窗口的定义：– 针对窗口中数据的操作– 哪些数据select uid, month, amount,sum (amount) over(partition by uid order by month rows between unbounded preceding and current row) as accumulatefrom t_access_amount; 1234参考： https://www.cnblogs.com/arjenlee/p/9692312.html#auto_id_81# 留存 次日，7日内，30日内留存：selecta.uid,casewhen diff(b.dt,a.dt) = 1 then ‘2,7,30’when diff(b.dt,a.dt) &lt; 7 then ‘7,30’when diff(b.dt,a.dt) =’2018-06-02’and dt&lt;=’2018-07-02’) ainner join(select uid,dt from coming_table where dt&gt;=’2018-06-02’and dt&lt;=’2018-07-02’) bon a.uid = b.uid然后行转列`]]></content>
      <categories>
        <category>dw</category>
      </categories>
      <tags>
        <tag>dw</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive模型]]></title>
    <url>%2F2018%2F02%2F12%2Fhive%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Jobtracker是hadoop1.x中的组件，它的功能相当于： Resourcemanager+MRAppMaster TaskTracker 相当于： Nodemanager + yarnchild hive里面有两种服务模式一种是cli模式，一种是hiveserver2，分别对应的启动入口 cli：hive-cli/org.apache.hadoop.hive.cli.CliDriver.javahiveserver2：hiveservice/org.apache.hive.service.server.HiveServer2.java,直接用debug或者run运行调试 hive sql 的执行顺序from… where…. group by… select…having … order by… 源码 compiler包括parser解释器和SemanticAnalyzer语义解析器。 无论使用CLI、Thrift Server、JDBC还是自定义的提交工具，最终的HQL都会传给Driver实例，执行Driver.run()方法。从这种设计也可以看出，如果您要开发一套自定义的Hive作业提交工具，最好的方式是引用Driver实例，调用相关方法进行开发。 而Driver.run()方法，获得了这样一个HQL，则会执行两个重要的步骤：编译和执行，即Driver.complie()和Driver.execute()。对于Driver.comile()来说，其实就是调用parse和optimizer包中的相关模块，执行语法解析、语义分析、优化；对于Driver.run()来说，其实就是调用exec包中的相关模块，将解析后的执行计划执行，如果解析后的结果是一个查询计划，那么通常的作法就是提交一系列的MapReduce作业。 以查询的执行为例，整个Hive的流程是非常简单的一条直线，由上到下进行。 Join操作左边为小表应该将条目少的表/子查询放在 Join 操作符的左边原因是在 Join 操作的 Reduce 阶段，位于 Join 操作符左边的表的内容会被加载进内存，将条目少的表放在左边，可以有效减少发生 OOM 错误的几率 参考：https://blog.csdn.net/dante_003/article/details/73789910https://www.jianshu.com/p/892cc8985c9chttps://blog.csdn.net/wf1982/article/details/9122543https://blog.csdn.net/wzq6578702/article/details/71250081]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[join]]></title>
    <url>%2F2018%2F02%2F11%2Fjoin%2F</url>
    <content type="text"><![CDATA[测试数据user.txt (用户id,用户名)1 用户12 用户23 用户3 more post.txt (用户id,帖子id,标题)1 1 贴子11 2 贴子22 3 帖子34 4 贴子45 5 贴子55 6 贴子65 7 贴子7123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149package com.qr.mr.join;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.ArrayList;import java.util.List;/** * * user.txt 用户表(用户id,用户名) * * post.txt 帖子表(用户id,帖子id,标题) */public class Join &#123;// 类型 U表示用户,P表示帖子 public static class UserMap extends Mapper&lt;Object, Text,Text,Text&gt; &#123; public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; String[] line = value.toString().split(&quot;,&quot;); context.write(new Text(line[0]),new Text(&quot;U,&quot;+value.toString())); &#125; &#125; public static class PostMap extends Mapper&lt;Object, Text,Text,Text&gt; &#123; public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; String[] line = value.toString().split(&quot;,&quot;); context.write(new Text(line[0]),new Text(&quot;P,&quot;+value.toString())); &#125; &#125; public static class Reduce extends Reducer&lt;Text,Text,Text,Text&gt; &#123; private List&lt;String&gt; users = new ArrayList&lt;String&gt;(); private List&lt;String&gt; posts = new ArrayList&lt;String&gt;(); private String joinType; @Override protected void setup(Context context) throws IOException, InterruptedException &#123; super.setup(context); joinType = context.getConfiguration().get(&quot;joinType&quot;); &#125; public void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; //todo users.clear(); posts.clear(); for (Text v:values) &#123; if (v.toString().contains(&quot;U,&quot;))&#123; users.add(v.toString().substring(2)); &#125; else &#123; posts.add(v.toString().substring(2)); &#125; &#125; if (joinType.equals(&quot;innerJoin&quot;))&#123; if (users.size() &gt; 0 &amp;&amp; posts.size() &gt;0)&#123; for (String user:users)&#123; for (String post:posts)&#123; context.write(new Text(user),new Text(post)); &#125; &#125; &#125; &#125; if (joinType.equals(&quot;leftOuter&quot;))&#123; for (String user:users) &#123; if (posts.size() &gt; 0)&#123;//todo for (String post:posts) &#123; context.write(new Text(user),new Text(post)); &#125; &#125; else &#123; context.write(new Text(user),new Text(&quot;,,&quot;)); &#125; &#125; &#125; if (joinType.equals(&quot;rightOuter&quot;))&#123; for (String post:posts) &#123; if (users.size() &gt; 0)&#123; for (String user:users) &#123; context.write(new Text(user),new Text(post)); &#125; &#125; else &#123; context.write(new Text(&quot;,&quot;),new Text(post)); &#125; &#125; &#125; if (joinType.equals(&quot;allOuter&quot;))&#123; if (users.size() &gt; 0)&#123; for (String user:users) &#123; if (posts.size() &gt; 0)&#123; for (String post:posts) &#123; context.write(new Text(user),new Text(post)); &#125; &#125; else &#123; context.write(new Text(user),new Text(&quot;,,&quot;)); &#125; &#125; &#125; else &#123; for (String post:posts) &#123; context.write(new Text(&quot;,&quot;),new Text(post)); &#125; &#125; &#125; if (joinType.equals(&quot;anti&quot;))&#123; if (users.size() == 0 || posts.size() == 0)&#123;//todo for (String user:users) &#123; context.write(new Text(user),new Text(&quot;,,&quot;)); &#125; for (String post:posts) &#123; context.write(new Text(&quot;,&quot;),new Text(post)); &#125; &#125; &#125; &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;Join&quot;); job.setJarByClass(Join.class); //设置连接类型 //innerJoin,leftOuter,rightOuter,allOuter,anti job.getConfiguration().set(&quot;joinType&quot;,&quot;allOuter&quot;);// job.setMapperClass(Map.class); //todo 不能单独设置 job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); //todo MultipleInputs.addInputPath(job,new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/join/in/user.txt&quot;), TextInputFormat.class,UserMap.class); MultipleInputs.addInputPath(job,new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/join/in/post.txt&quot;), TextInputFormat.class,PostMap.class); Path outpath = new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/join/out&quot;); FileSystem fs = outpath.getFileSystem(job.getConfiguration()); if (fs.exists(outpath))&#123; fs.delete(outpath,true); &#125; FileOutputFormat.setOutputPath(job,outpath); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[倒排]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%80%92%E6%8E%92%2F</url>
    <content type="text"><![CDATA[题意hdfs 上有三个文件，内容下上面左面框中所示。右框中为处理完成后的结果文件。倒排索引（Inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档检索系统中最常用的数据结构。通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。倒排索引主要由两个部分组成：“单词词典”和“倒排文件”这个任务与传统的倒排索引任务不同的地方是加上了每个文件中的频数。 实现思路首先关注结果中有文件名称，这个我们有两种方式处理：1、自定义InputFormat，在其中的自定义RecordReader中，直接通过InputSplit得到Path，继而得到FileName;2、在Mapper中，通过上下文可以取到Split，也可以得到fileName。这个任务中我们使用第二种方式，得到filename.在mapper中，得到filename 及 word，封装到一个自定义keu中。value 使用IntWritable。在map 中直接输出值为1的IntWritable对象。对进入reduce函数中的key进行分组控制，要求按word相同的进入同一次reduce调用。所以需要自定义GroupingComparator。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160package com.qr.mr.invertedsort;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.*;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import org.apache.log4j.Logger;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import java.util.LinkedHashMap;/** * http://blog.itpub.net/30066956/viewspace-2120238/ */public class InvertedSort &#123; static class WordKey implements WritableComparable&lt;WordKey&gt; &#123; private String fileName; private String word; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(fileName); out.writeUTF(word); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.fileName = in.readUTF(); this.word = in.readUTF(); &#125; @Override public int compareTo(WordKey key) &#123; int r = word.compareTo(key.word); if(r==0) r = fileName.compareTo(key.fileName); return r; &#125; public String getFileName() &#123; return fileName; &#125; public void setFileName(String fileName) &#123; this.fileName = fileName; &#125; public String getWord() &#123; return word; &#125; public void setWord(String word) &#123; this.word = word; &#125; &#125; public static class IndexInvertedMapper extends Mapper&lt;LongWritable, Text,WordKey, IntWritable&gt; &#123; private WordKey newKey = new WordKey(); private IntWritable ONE = new IntWritable(1); private String fileName ; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; newKey.setFileName(fileName); String words [] = value.toString().split(&quot; &quot;); for(String w:words)&#123; newKey.setWord(w); context.write(newKey, ONE); &#125; &#125; @Override protected void setup(Context context) throws IOException, InterruptedException &#123; FileSplit inputSplit = (FileSplit) context.getInputSplit(); fileName = inputSplit.getPath().getName(); &#125; &#125; public static class IndexInvertedReducer extends Reducer&lt;WordKey,IntWritable,Text,Text&gt; &#123; private Text outputKey = new Text(); @Override protected void reduce(WordKey key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; outputKey.set(key.getWord()); LinkedHashMap&lt;String,Integer&gt; map = new LinkedHashMap&lt;String,Integer&gt;(); for(IntWritable v :values)&#123; if(map.containsKey(key.getFileName()))&#123; map.put(key.getFileName(), map.get(key.getFileName())+ v.get()); &#125; else&#123; map.put(key.getFileName(), v.get()); &#125; &#125; StringBuilder sb = new StringBuilder(); sb.append(&quot;&#123;&quot;); for(String k: map.keySet())&#123; sb.append(&quot;(&quot;).append(k).append(&quot;,&quot;).append(map.get(k)).append(&quot;)&quot;).append(&quot;,&quot;); &#125; sb.deleteCharAt(sb.length()-1).append(&quot;&#125;&quot;); context.write(outputKey, new Text(sb.toString())); &#125; &#125; public static class IndexInvertedGroupingComparator extends WritableComparator &#123; Logger log = Logger.getLogger(getClass()); public IndexInvertedGroupingComparator()&#123; super(WordKey.class,true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; WordKey key1 = (WordKey) a; WordKey key2 = (WordKey) b; log.info(&quot;==============key1.getWord().compareTo(key2.getWord()):&quot;+key1.getWord().compareTo(key2.getWord())); return key1.getWord().compareTo(key2.getWord()); &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;IndexInvertedJob&quot;); job.setJarByClass(InvertedSort.class); Path in = new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/invertedsort/in&quot;); Path out = new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/invertedsort/out&quot;); FileSystem.get(conf).delete(out,true); FileInputFormat.setInputPaths(job, in); FileOutputFormat.setOutputPath(job, out); job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); job.setMapperClass(IndexInvertedMapper.class); job.setMapOutputKeyClass(WordKey.class); job.setMapOutputValueClass(IntWritable.class); job.setReducerClass(IndexInvertedReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); job.setGroupingComparatorClass(IndexInvertedGroupingComparator.class); System.exit(job.waitForCompletion(true)? 0 : 1); &#125;&#125; 参考:http://blog.itpub.net/30066956/viewspace-2120238/]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二次排序]]></title>
    <url>%2F2018%2F02%2F11%2F%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[输入文件 sort.txt 内容为 40 20 40 10 40 30 40 5 30 30 30 20 30 10 30 40 50 20 50 50 50 10 50 60 输出文件的内容（从小到大排序）如下 30 10 30 20 30 30 30 40 -------- 40 5 40 10 40 20 40 30 -------- 50 10 50 20 50 50 50 60 从输出的结果可以看出Key实现了从小到大的排序，同时相同Key的Value也实现了从小到大的排序，这就是二次排序的结果 在本例中要比较两次。先按照第一字段排序，然后再对第一字段相同的按照第二字段排序。根据这一点，我们可以构造一个复合类IntPair ，它有两个字段，先利用分区对第一字段排序，再利用分区内的比较对第二字段排序。二次排序的流程分为以下几步。 1、自定义 key 2、自定义分区 3、Key的比较类 4、定义分组类函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159 package com.qr.mr.secondarysort;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.*;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Partitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import java.util.*;public class SecondarySort &#123;// 1、自定义 key public static class IntPair implements WritableComparable&lt;IntPair&gt; &#123; private int first = 0; private int second = 0; /** * Read the two integers. * Encoded as: MIN_VALUE -&amp;gt; 0, 0 -&amp;gt; -MIN_VALUE, MAX_VALUE-&amp;gt; -1 */ @Override public void readFields(DataInput in) throws IOException &#123; first = in.readInt(); second = in.readInt(); &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(first); out.writeInt(second); &#125; @Override public int hashCode() &#123; return first * 157 + second; &#125; @Override public boolean equals(Object right) &#123; if (right instanceof IntPair) &#123; IntPair r = (IntPair) right; return r.first == first &amp;&amp; r.second == second; &#125; else &#123; return false; &#125; &#125; @Override public int compareTo(IntPair o) &#123; if (first != o.first) &#123; return first &lt; o.first ? -1 : 1; &#125; else if (second != o.second) &#123; return second &lt; o.second ? -1 : 1; &#125; else &#123; return 0; &#125; &#125; /** * Set the left and right values. */ public void set(int left, int right) &#123; first = left; second = right; &#125; public int getFirst() &#123; return first; &#125; public int getSecond() &#123; return second; &#125; &#125;// 2、自定义分区 public static class FirstPartitioner extends Partitioner&lt;IntPair,IntWritable&gt; &#123; @Override public int getPartition(IntPair key, IntWritable value, int numPartitions) &#123; return Math.abs(key.getFirst() * 127) % numPartitions; &#125; &#125;// 3、Key的比较类 public static class FirstGroupingComparator implements RawComparator&lt;IntPair&gt; &#123; @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123; return WritableComparator.compareBytes(b1, s1, Integer.SIZE/8, b2, s2, Integer.SIZE/8); &#125; @Override public int compare(IntPair o1, IntPair o2) &#123; int l = o1.getFirst(); int r = o2.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); &#125; &#125; public static class MapClass extends Mapper&lt;Object, Text,IntPair, IntWritable&gt;&#123; private final IntPair key = new IntPair(); private final IntWritable value = new IntWritable(); @Override public void map(Object inKey, Text inValue, Context context) throws IOException, InterruptedException &#123; StringTokenizer itr = new StringTokenizer(inValue.toString()); int left = 0; int right = 0; if (itr.hasMoreTokens()) &#123; left = Integer.parseInt(itr.nextToken()); if (itr.hasMoreTokens()) &#123; right = Integer.parseInt(itr.nextToken()); &#125; key.set(left, right); value.set(right); context.write(key, value); &#125; &#125; &#125; public static class Reduce extends Reducer&lt;IntPair,IntWritable,Text,IntWritable&gt;&#123; private static final Text SEPARATOR = new Text(&quot;------------------------------------------------&quot;); private final Text first = new Text(); public void reduce(IntPair key,Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; context.write(SEPARATOR, null); first.set(Integer.toString(key.getFirst())); for(IntWritable value: values) &#123; context.write(first, value); &#125; &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, &quot;secondary sort&quot;); job.setJarByClass(SecondarySort.class); job.setMapperClass(MapClass.class); job.setReducerClass(Reduce.class); // group and partition by the first int in the pair job.setPartitionerClass(FirstPartitioner.class); job.setGroupingComparatorClass(FirstGroupingComparator.class); // the map output is IntPair, IntWritable job.setMapOutputKeyClass(IntPair.class); job.setMapOutputValueClass(IntWritable.class); // the reduce output is Text, IntWritable job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/secondarysort/sort.txt&quot;)); FileOutputFormat.setOutputPath(job, new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/secondarysort/out&quot;)); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全排序]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%85%A8%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[错误写法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.qr.mr.datasort;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;/** * http://www.aboutyun.com/thread-7046-1-1.html * * 这个实例仅仅要求对输入数据进行排序，熟悉MapReduce过程的读者会很快想到在MapReduce过程中就有排序，是否可以利用这个默认的排序，而不需要自己再实现具体的排序呢？ * 答案是肯定的。 * * 但是在使用之前首先需要了解它的默认排序规则。它是按照key值进行排序的， * 如果key为封装int的IntWritable类型，那么MapReduce按照数字大小对key排序， * 如果key为封装为String的Text类型，那么MapReduce按照字典顺序对字符串排序。 * * 了解了这个细节，我们就知道应该使用封装int的IntWritable型数据结构了。 * 也就是在map中将读入的数据转化成IntWritable型，然后作为key值输出（value任意）。 * reduce拿到&lt;key，value-list&gt;之后，将输入的key作为value输出，并根据value-list中元素的个数决定输出的次数。 * 输出的key（即代码中的linenum）是一个全局变量，它统计当前key的位次。 * * 需要注意的是这个程序中没有配置Combiner，也就是在MapReduce过程中不使用Combiner。 * 这主要是因为使用map和reduce就已经能够完成任务了。 * * * https://blog.csdn.net/evo_steven/article/details/17139123 */public class DataSort &#123; public static class Map extends Mapper&lt;Object, Text, IntWritable,Text&gt; &#123;// private static IntWritable data = null;//todo 此处不能这样写，否则会报错 public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; String line = value.toString(); context.write(new IntWritable(Integer.parseInt(line)),new Text(&quot;&quot;)); &#125; &#125; public static class Reduce extends Reducer&lt;IntWritable,Text,IntWritable,IntWritable&gt; &#123;// 每个reduce中也只是单独的全局变量，并非整个集群的全局变量，// 一旦加入job.setNumReduceTasks(2);就会有两个文件，会出现错误结果 private static IntWritable count = new IntWritable(1); //todo IntWritable public void reduce(IntWritable key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException &#123; for (Text val:values) &#123; context.write(count,key); count = new IntWritable(count.get() + 1); &#125; &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;DataSort&quot;); job.setJarByClass(DataSort.class); job.setNumReduceTasks(2); //todo 如果有此处，实际环境中会报错 job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(IntWritable.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job,new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/datasort/in&quot;)); FileOutputFormat.setOutputPath(job,new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/datasort/out&quot;)); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 有问题的做法，缺少setPartitionerClass设计模式中的全排序实现思路是 两次JOB，第一次做分区，第二次做排序，也用到了setPartitionerClass，reduce只负责输出Partitinoner的作用除了快速找到key对应的reducer，更重要的一点是：这个Partitioner控制了排序的总体有序！ 正确做法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package com.qr.mr.datasort;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.*;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.partition.InputSampler;import org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner;import java.io.IOException;public class DataSortNew &#123; public static class Map extends Mapper&lt;Text, Text, Text,IntWritable&gt; &#123; public void map(Text key,Text value,Context context) throws IOException, InterruptedException &#123; context.write(key,new IntWritable(Integer.parseInt(key.toString())));//todo key是Text,value也是key &#125; &#125; public static class Reduce extends Reducer&lt;Text,IntWritable,IntWritable, NullWritable&gt; &#123;//todo 此处的key也一定要跟着改为Text public void reduce(Text key,Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; for (IntWritable val:values) &#123; context.write(val,NullWritable.get()); &#125; &#125; &#125; public static class KeyComparator extends WritableComparator &#123; protected KeyComparator() &#123; super(Text.class, true); &#125; @Override public int compare(WritableComparable w1, WritableComparable w2) &#123; int v1 = Integer.parseInt(w1.toString()); int v2 = Integer.parseInt(w2.toString()); return v1 - v2; &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); conf.set(&quot;mapreduce.totalorderpartitioner.naturalorder&quot;, &quot;false&quot;); Job job = Job.getInstance(conf,&quot;DataSortNew&quot;); job.setJarByClass(DataSortNew.class); FileInputFormat.addInputPath(job,new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/datasort/in&quot;)); FileOutputFormat.setOutputPath(job,new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/datasort/out&quot;)); job.setInputFormatClass(KeyValueTextInputFormat.class); job.setSortComparatorClass(KeyComparator.class);//todo job.setNumReduceTasks(100); //todo job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); job.setOutputValueClass(NullWritable.class);// 在新版本的Hadoop中，内置了三个采样器： SplitSampler，RandomSampler和IntervalSampler。这三个采样器都是InputSampler类的静态内部类，并且都实现了InputSampler类的内部接口Sample// https://flyingdutchman.iteye.com/blog/1878962// 0.01-----------------每个样本被抽到的概率// 1000------------------样本数// 100--------------------分区数 String partitionPath=&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/datasort/ss/sampler&quot;; TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(partitionPath)); InputSampler.RandomSampler&lt;Text,Text&gt; sampler =new InputSampler.RandomSampler&lt;&gt;(0.01,1000,100); InputSampler.writePartitionFile(job,sampler); job.setPartitionerClass(TotalOrderPartitioner.class);//todo job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 参考:https://www.iteblog.com/archives/2146.htmlhttps://www.iteblog.com/archives/2147.htmlhttps://flyingdutchman.iteye.com/blog/1878962]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[去重]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%8E%BB%E9%87%8D%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.qr.mr.removeduplicate;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;/** * http://www.aboutyun.com/thread-7041-1-1.html */public class RemoveDuplicate &#123; public static class Map extends Mapper&lt;Object,Text, Text,Text&gt;&#123; private static Text line = new Text(); public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123;//todo line = value; context.write(line,new Text(&quot;&quot;)); &#125; &#125; public static class Reduce extends Reducer&lt;Text,Text,Text,Text&gt;&#123; public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException &#123;//todo context.write(key,new Text(&quot;&quot;)); &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;//todo Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;RemoveDuplicate&quot;);//todo job.setJarByClass(RemoveDuplicate.class); job.setMapperClass(Map.class); job.setCombinerClass(Reduce.class);//todo job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.addInputPath(job,new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/removeduplicate/in&quot;)); FileOutputFormat.setOutputPath(job,new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/removeduplicate/out&quot;)); System.exit(job.waitForCompletion(true) ? 0: 1); &#125;&#125;]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[共同好友]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%85%B1%E5%90%8C%E5%A5%BD%E5%8F%8B%2F</url>
    <content type="text"><![CDATA[题意给出A-O个人中每个人的好友列表，求出哪些人两两之间有共同好友，以及他们的共同好友都有谁 原始文件：A:B,C,D,F,E,OB:A,C,E,KC:F,A,D,ID:A,E,F,LE:B,C,D,M,LF:A,B,C,D,E,O,MG:A,C,D,E,FH:A,C,D,E,OI:A,OJ:B,OK:A,C,DL:D,E,FM:E,F,GO:A,H,I,J输出格式：A-B: C,E… 思路此题旨在求两人之间的共同好友，原信息是&lt;人，该人的所有好友&gt;，因此首先以好友为键，人为值，交给reduce找出拥有此好友的所有人；再将这些人中两两配对作为键，之前的键（好友）作为值交给reduce去合并简而言之我打算分成两个步骤，两次迭代1）求出每一个人都是哪些人的共同好友2）把这些人（用共同好友的人）作为key,其好友作为value输出 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131package com.qr.mr.sharefriends;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.Arrays;import java.util.HashSet;import java.util.Set;public class ShareFriends &#123; public static class Map1 extends Mapper&lt;Object, Text,Text,Text&gt;&#123; /** * 第一阶段的map函数主要完成以下任务 * 1.遍历原始文件中每行&lt;所有朋友&gt;信息 * 2.遍历“朋友”集合，以每个“朋友”为键，原来的“人”为值 即输出&lt;朋友,人&gt; */ public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; String lines = value.toString(); String cur = lines.split(&quot;:&quot;)[0]; String[] friends = lines.split(&quot;:&quot;)[1].split(&quot;,&quot;); for (String f:friends) &#123; context.write(new Text(f),new Text(cur)); &#125; &#125; &#125; /** * 第一阶段的reduce函数主要完成以下任务 * 1.对所有传过来的&lt;朋友，list(人)&gt;进行拼接，输出&lt;朋友,拥有这名朋友的所有人&gt; */ public static class Reduce1 extends Reducer&lt;Text,Text,Text,Text&gt;&#123; public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException &#123; StringBuffer sb = new StringBuffer(); for (Text v:values) &#123; sb.append(v.toString()).append(&quot;,&quot;); &#125; sb.deleteCharAt(sb.length() - 1); context.write(key,new Text(sb.toString())); &#125; &#125; /** * * //todo 如果A是B的好友，B也一定是A的好友，则不需要第一层job * * 第二阶段的map函数主要完成以下任务 * 1.将上一阶段reduce输出的&lt;朋友,拥有这名朋友的所有人&gt;信息中的 “拥有这名朋友的所有人”进行排序 ，以防出现B-C C-B这样的重复 * 2.将 “拥有这名朋友的所有人”进行两两配对，并将配对后的字符串当做键，“朋友”当做值输出，即输出&lt;人-人，共同朋友&gt; * */ public static class Map2 extends Mapper&lt;Object, Text,Text,Text&gt;&#123; public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] friend_persons = line.split(&quot;\t&quot;); String friend = friend_persons[0];//todo String[] persons = friend_persons[1].split(&quot;,&quot;); Arrays.sort(persons); //todo 排序,以防出现B-C C-B这样的重复 //两两配对 for(int i=0;i&lt;persons.length-1;i++)&#123; for(int j=i+1;j&lt;persons.length;j++)&#123; context.write(new Text(persons[i]+&quot;-&quot;+persons[j]+&quot;:&quot;), new Text(friend));//todo friend值作为共同好友value &#125; &#125; &#125; &#125; /** * 第二阶段的reduce函数主要完成以下任务 * 1.&lt;人-人，list(共同朋友)&gt; 中的“共同好友”进行拼接 最后输出&lt;人-人，两人的所有共同好友&gt; */ public static class Reduce2 extends Reducer&lt;Text,Text,Text,Text&gt;&#123; public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException &#123; Set&lt;String&gt; set = new HashSet&lt;String&gt;(); StringBuffer sb = new StringBuffer();//todo for(Text friend : values)&#123; if(!set.contains(friend.toString())) set.add(friend.toString()); &#125; for(String friend : set)&#123; sb.append(friend.toString()).append(&quot;,&quot;); &#125; sb.deleteCharAt(sb.length()-1); context.write(key, new Text(sb.toString())); &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); //第一阶段 Job job1 = Job.getInstance(conf,&quot;shareFriends&quot;); job1.setJarByClass(ShareFriends.class); job1.setMapperClass(Map1.class); job1.setReducerClass(Reduce1.class); job1.setOutputKeyClass(Text.class); job1.setOutputValueClass(Text.class); FileInputFormat.setInputPaths(job1, new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/sharefriends/in&quot;)); FileOutputFormat.setOutputPath(job1, new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/sharefriends/out/01&quot;)); boolean res1 = job1.waitForCompletion(true); //第二阶段 Job job2 = Job.getInstance(conf); job2.setMapperClass(Map2.class); job2.setReducerClass(Reduce2.class); job2.setOutputKeyClass(Text.class); job2.setOutputValueClass(Text.class); FileInputFormat.setInputPaths(job2, new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/sharefriends/out/01&quot;)); FileOutputFormat.setOutputPath(job2, new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/sharefriends/out/02&quot;)); boolean res2 = job2.waitForCompletion(true); System.exit(res2?0:1); &#125;&#125; 参考：https://blog.csdn.net/u012808902/article/details/77513188]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用单表关联在父子关系中求解祖孙关系]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%88%A9%E7%94%A8%E5%8D%95%E8%A1%A8%E5%85%B3%E8%81%94%E5%9C%A8%E7%88%B6%E5%AD%90%E5%85%B3%E7%B3%BB%E4%B8%AD%E6%B1%82%E8%A7%A3%E7%88%B7%E5%AD%99%E5%85%B3%2F</url>
    <content type="text"><![CDATA[首先是有如下数据，设定左边是右边的儿子，右边是左边的父母 Tom LucyTom JackJone LucyJone JackLucy MaryLucy BenJack AliceJack JesseTerry AliceTerry JessePhilip TerryPhilip AlmaMark TerryMark Alma要求输出如下所示的爷孙关系，左边是右边的孙子，右边是左边的祖父母：Tom JesseTom AliceJone JesseJone AliceJone BenJone MaryTom BenTom MaryPhilip AlicePhilip JesseMark AliceMark Jesse 要利用Mapreduce解决这个问题，主要思想如下：1、在Map阶段，将父子关系与相反的子父关系，同时在各个value前补上前缀-与+标识此key-value中的value是正序还是逆序产生的，之后进入context。 下图通过其中的Jone-Lucy Lucy-Mary，求解出Jone-Mary来举例2、MapReduce会自动将同一个key的不同的value值，组合在一起，推到Reduce阶段。在value数组中，跟住前缀，我们可以轻松得知，哪个是爷，哪个是孙。 因此对各个values数组中各个项的前缀进行输出。可以看得出，整个过程Key一直被作为连接的桥梁来用。形成一个单表关联的运算。因此代码如下，根据上面的思想很容易得出如下的代码了，需要注意的是，输出流输入流都要设置成Text，因为全程都是在对Text而不是IntWritable进行操作：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package com.qr.mr.Grandson;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.ArrayList;import java.util.List;/** * 求祖父母，外祖父母四个人，或者更多人 */public class GrandChildren &#123; public static class Map extends Mapper&lt;Object,Text,Text, Text&gt; &#123; public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; String[] record = value.toString().split(&quot; &quot;); context.write(new Text(record[0]),new Text(record[1]+&quot;,Father&quot;)); context.write(new Text(record[1]),new Text(record[0]+&quot;,Son&quot;)); &#125; &#125; public static class Reduce extends Reducer&lt;Text,Text,Text,Text&gt;&#123; public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException &#123; List&lt;String&gt; grandpa = new ArrayList&lt;&gt;(); List&lt;String&gt; grandson = new ArrayList&lt;&gt;(); for (Text v:values) &#123; if (v.toString().contains(&quot;Father&quot;))&#123; grandpa.add(v.toString().split(&quot;,&quot;)[0]); &#125; else if (v.toString().contains(&quot;Son&quot;))&#123; grandson.add(v.toString().split(&quot;,&quot;)[0]); &#125; &#125; for (String s:grandson) &#123; for (String g:grandpa) &#123; context.write(new Text(s),new Text(g)); &#125; &#125; &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;Grandson&quot;); job.setJarByClass(GrandChildren.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.addInputPath(job,new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/Grandson/in&quot;)); FileOutputFormat.setOutputPath(job,new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/Grandson/out&quot;)); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用job嵌套，多重Mapreduce，求解二度人脉]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%88%A9%E7%94%A8job%E5%B5%8C%E5%A5%97%EF%BC%8C%E5%A4%9A%E9%87%8DMapreduce%EF%BC%8C%E6%B1%82%E8%A7%A3%E4%BA%8C%E5%BA%A6%E4%BA%BA%E8%84%89%2F</url>
    <content type="text"><![CDATA[题意Tom LucyTom JackJone LucyJone JackLucy MaryLucy BenJack AliceJack JesseTerry AliceTerry JessePhilip TerryPhilip AlmaMark TerryMark Alma 只是这次是假设一个没有重复，也就是不会出现Tom Lucy-Lucy Tom这样的好友关系表。任务是求其其中的二度人脉、潜在好友，也就是如下图：比如I认识C、G、H，但C不认识G，那么C-G就是一对潜在好友，但G-H早就认识了，因此不算为潜在好友。最终得出的是，如下的一个，没有重复的，首字母接近A再前的，也就Tom Mary-MaryTom只输出Mary Tom，因为M比T更接近A： Alice JesseAlice JoneAlice MarkAlice PhilipAlice TomAlma TerryBen JoneBen MaryBen TomJack LucyJack TerryJesse JoneJesse MarkJesse PhilipJesse TomJone MaryJone TomMark PhilipMary Tom 思路如下首先，我们进行第一个MapReduce：1、一个输入行，产生一对互逆的关系，压入context 例如Tom Lucy这个输入行就在Map阶段搞出Tom Lucy-Lucy Tom这样的互逆关系。 2、之后Map-reduce会自动对context中相同的key合并在一起。 例如由于存在Tom Lucy、Tom Jack，显然会产生一个Tom:{Lucy,Jack} 这是Reduce阶段以开始的键值对。 3、这个键值对相当于Tom所认识的人。先进行如下的输出，1代表Tom的一度人脉 Tom Lucy 1Tom Jack 1 潜在好友显然会在{Lucy,Jack}这个Tom所认识的人产生，对这个数组做笛卡尔乘积，形成关系：{&lt;Lucy,Lucy&gt;,&lt;Jack,Jack&gt;,&lt;Lucy,Jack&gt;,&lt;Jack,Lucy&gt;} 将存在自反性，前项首字母大于后项剔除，也就是&lt;Lucy,Lucy&gt;这类无意义的剔除，&lt;Lucy,Jack&gt;,&lt;Jack,Lucy&gt;认定为一个关系，将剩余关系进行如下的输出，其中2代表Tom的二度人脉，也就是所谓的潜在好友： Lucy Jack 2 此时，第一个MapReduce，输出如下： Alice Jack 1Alice Terry 1Jack Terry 2Alma Philip 1Alma Mark 1Mark Philip 2Ben Lucy 1Jack Jesse 1Jack Tom 1Jack Jone 1Alice Jack 1Jesse Tom 2Jesse Jone 2Jone Tom 2Alice Jesse 2Alice Tom 2Alice Jone 2Jesse Terry 1Jack Jesse 1Jack Terry 2Jack Jone 1Jone Lucy 1Jack Lucy 2Ben Lucy 1Jone Lucy 1Lucy Tom 1Lucy Mary 1Ben Jone 2Ben Tom 2Ben Mary 2Jone Tom 2Jone Mary 2Mary Tom 2Alma Mark 1Mark Terry 1Alma Terry 2Lucy Mary 1Philip Terry 1Alma Philip 1Alma Terry 2Philip Terry 1Alice Terry 1Mark Terry 1Jesse Terry 1Alice Philip 2Alice Mark 2Alice Jesse 2Mark Philip 2Jesse Philip 2Jesse Mark 2Jack Tom 1Lucy Tom 1Jack Lucy 2这时，形式已经很明朗了 再进行第二个Mapreduce任务是剔除本身就存在的关系，也就是在潜在好友中剔除本身就认识的关系。 将上述第一个Mapreduce的输出，关系作为key，后面的X度人脉这个1、2值作为value，进行Mapreduce的处理。 那么例如这个关系，之所以会被认定为潜在好友，是因为它所对应的值数组，里面一个1都没有，全是2，也就是它们本来不是一度人脉，而这对，不能成为潜在好友，因为他们所对应的值数组，里面有1，存在任意一对一度人脉、直接认识，就绝对不能被认定为二度人脉。 将被认定为二度人脉的关系输出，就得到最终结果。 代码package com.qr.mr.deg2friend; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import java.io.IOException; import java.util.ArrayList; import java.util.List; import java.util.Random; //https://www.cnblogs.com/hxsyl/p/6127843.html public class Deg2Friend { public static class Map extends Mapper&lt;Object,Text,Text,Text&gt; { public void map(Object key,Text value,Context context) throws IOException, InterruptedException { String[] line = value.toString().split(&quot;,&quot;); context.write(new Text(line[0]),new Text(line[1])); context.write(new Text(line[1]),new Text(line[0])); } } public static class Reduce extends Reducer&lt;Text,Text,Text,Text&gt;{ public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException { List&lt;String&gt; potentialFriends = new ArrayList&lt;&gt;(); //这里一定要用string存，而不是用Text，Text只是一个类似指针，一个MapReduce的引用 //换成用Text来存，你会惊讶，为何我存的值都一模一样的？ for (Text v:values) { potentialFriends.add(v.toString()); if (key.toString().compareTo(v.toString()) &lt; 0){// 确保首字母大者再前，如Tom Alice则输出Alice Tom context.write(new Text(key + &quot;\t&quot; + v),new Text(&quot;1&quot;)); } else { context.write(new Text(v + &quot;\t&quot; + key),new Text(&quot;1&quot;)); } } for (String p1:potentialFriends) { for (String p2:potentialFriends) { if (p1.compareTo(p2) &lt; 0){//todo 将存在自反性，前项首字母大于后项的关系剔除 context.write(new Text(p1 +&quot;\t&quot; + p2),new Text(&quot;2&quot;)); } } } } } public static class Map2 extends Mapper&lt;Object,Text,Text,Text&gt;{ public void map(Object key,Text value,Context context) throws IOException, InterruptedException { String[] line = value.toString().split(&quot;\t&quot;);//输入文件，键值对的分隔符为\t context.write(new Text(line[0] + &quot;\t&quot; + line[1]), new Text(line[2]));//关系作为key，后面的X度人脉这个1、2值作为value } } public static class Reduce2 extends Reducer&lt;Text, Text, Text, Text&gt; { public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException { //检查合并之后是否存在任意一对一度人脉 boolean isPotentialFriend = true; for (Text v:values) { if (v.toString().equals(&quot;1&quot;)){ isPotentialFriend = false; break; } } if (isPotentialFriend){ String[] potentialFriends = key.toString().split(&quot;\t&quot;); context.write(new Text(potentialFriends[0]),new Text(potentialFriends[1])); } } } public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException { Configuration conf = new Configuration(); // 判断output文件夹是否存在，如果存在则删除 Path outPath = new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/deg2friend/out&quot;); FileSystem fs = outPath.getFileSystem(conf); if (fs.exists(outPath)){ fs.delete(outPath,true);// true的意思是，就算output有东西，也一带删除 } Job job = Job.getInstance(conf,&quot;deg2friend&quot;); job.setJarByClass(Deg2Friend.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class);//todo keyClass job.setOutputValueClass(Text.class);//todo valueClass // 原来当mapper与reducer的输出类型一致时可以用 job.setOutputKeyClass(theClass)与job.setOutputValueClass //(theClass)这两个进行配置就行，但是当mapper用于reducer两个的输出类型不一致的时候就需要分别进行配置了。 // job.setMapOutputKeyClass(Text.class); //todo 测试只有map的情况 // job.setMapOutputValueClass(Text.class); FileInputFormat.addInputPath(job,new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/deg2friend/in&quot;)); Path tempDir = new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/deg2friend/out/&quot; + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)) ); FileOutputFormat.setOutputPath(job, tempDir); if (job.waitForCompletion(true)){ Job job2 = Job.getInstance(conf,&quot;job2&quot;); job2.setMapperClass(Map2.class); job2.setReducerClass(Reduce2.class); job2.setOutputKeyClass(Text.class); job2.setOutputValueClass(Text.class); FileInputFormat.addInputPath(job2,tempDir); FileOutputFormat.setOutputPath(job2,new Path(&quot;/Users/lifei/qrproject/hiveudf/src/main/java/com/qr/mr/deg2friend/out/final&quot;)); System.exit(job2.waitForCompletion(true) ? 0 : 1); } } } 参考:https://blog.csdn.net/yongh701/article/details/50630498]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordcount]]></title>
    <url>%2F2018%2F02%2F11%2Fwordcount%2F</url>
    <content type="text"><![CDATA[最基本的，需要手动写出。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.qr.mr.wordcount;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.StringTokenizer;/** * StringTokenizer * https://www.cnblogs.com/gnivor/p/4509268.html * * * http://hadoop.apache.org/docs/r1.0.4/cn/mapred_tutorial.html#Map%2FReduce+-+%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2 * http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Payload * */public class WordCount &#123; public static class TokenizerMapper extends Mapper&lt;Object, Text,Text, IntWritable&gt;&#123; private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; StringTokenizer itr = new StringTokenizer(value.toString());//StringTokenizer while (itr.hasMoreTokens())&#123; word.set(itr.nextToken()); context.write(word,one); &#125; &#125; &#125; public static class IntSumReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt;&#123; private IntWritable result = new IntWritable(); public void reduce(Text key,Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val:values) &#123; sum += val.get(); &#125; result.set(sum); context.write(key,result); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;word count&quot;); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job,new Path(&quot;/Users/lifei/Desktop/ab/a.txt&quot;)); FileOutputFormat.setOutputPath(job,new Path(&quot;/Users/lifei/Desktop/ab_out&quot;)); System.exit(job.waitForCompletion(true)? 0 : 1); &#125;&#125;]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of39 二叉树]]></title>
    <url>%2F2017%2F02%2F25%2Fof39%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997998999100010011002100310041005100610071008100910101011101210131014101510161017101810191020102110221023102410251026102710281029103010311032103310341035103610371038103910401041104210431044104510461047104810491050105110521053105410551056105710581059106010611062106310641065106610671068106910701071107210731074107510761077107810791080108110821083108410851086108710881089109010911092109310941095109610971098109911001101110211031104110511061107110811091110111111121113111411151116111711181119112011211122112311241125112611271128112911301131113211331134113511361137113811391140114111421143114411451146114711481149115011511152115311541155115611571158package offer;import java.util.*;/** * 二叉树 * http://blog.csdn.net/luckyxiaoqiang/article/details/7518888 轻松搞定面试中的二叉树题目 * http://www.cnblogs.com/Jax/archive/2009/12/28/1633691.html 算法大全（3） 二叉树 * * TODO: 一定要能熟练地写出所有问题的递归和非递归做法！ * * 1. 求二叉树中的节点个数: getNodeNumRec（递归），getNodeNum（迭代） * 2. 求二叉树的深度: getDepthRec（递归），getDepth * 3. 求二叉树的宽度 * 4. 前序遍历，中序遍历，后序遍历: preorderTraversalRec, preorderTraversal, inorderTraversalRec, postorderTraversalRec * 5. 分层遍历二叉树（按层次从上往下，从左往右）: levelTraversal, levelTraversalRec（递归解法！） * * 5. 将二叉查找树变为有序的双向链表: convertBST2DLLRec, convertBST2DLL * 6. 求二叉树第K层的节点个数：getNodeNumKthLevelRec, getNodeNumKthLevel * 7. 求二叉树中叶子节点的个数：getNodeNumLeafRec, getNodeNumLeaf * 8. 判断两棵二叉树是否相同的树：isSameRec, isSame * 9. 判断二叉树是不是平衡二叉树：isAVLRec * 10. 求二叉树的镜像（破坏和不破坏原来的树两种情况）：mirrorRec, mirrorCopyRec * 10.1 判断两个树是否互相镜像：isMirrorRec * 11. 求二叉树中两个节点的最低公共祖先节点：getLastCommonParent, getLastCommonParentRec, getLastCommonParentRec2 * 12. 求二叉树中节点的最大距离：getMaxDistanceRec * 13. 由前序遍历序列和中序遍历序列重建二叉树：rebuildBinaryTreeRec * 14.判断二叉树是不是完全二叉树：isCompleteBinaryTree, isCompleteBinaryTreeRec * */public class Of39 &#123; private static class TreeNode &#123; int val; public TreeNode left; public TreeNode right; public TreeNode(int val) &#123; this.val = val; &#125; &#125; /** * 求二叉树中的节点个数递归解法： O(n) * （1）如果二叉树为空，节点个数为0 * （2）如果二叉树不为空，二叉树节点个数 = 左子树节点个数 + 右子树节点个数 + 1 */ public static int getNodeNumRec(TreeNode root) &#123; if (root == null) &#123; return 0; &#125; else &#123; return getNodeNumRec(root.left) + getNodeNumRec(root.right) + 1; //左 右节点加上主节点1为总数 &#125; &#125; /** * 求二叉树中的节点个数迭代解法O(n)：基本思想同LevelOrderTraversal， * 即用一个Queue，在Java里面可以用LinkedList来模拟 */ public static int getNodeNum(TreeNode root) &#123; if(root == null)&#123; return 0; &#125; int count = 1; //初始化为1 Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.add(root); //将根节点先放到队列中 while(!queue.isEmpty())&#123; TreeNode cur = queue.remove(); //返回队头元素并移除 if(cur.left != null)&#123; // 如果有左孩子，加到队尾 queue.add(cur.left); count++; &#125; if(cur.right != null)&#123; // 如果有右孩子，加到队尾 queue.add(cur.right); count++; &#125; &#125; return count; &#125; /** * 求二叉树的深度（高度） 递归解法： O(n) * （1）如果二叉树为空，二叉树的深度为0 * （2）如果二叉树不为空，二叉树的深度 = max(左子树深度， 右子树深度) + 1 * * * maxDepth() 1. 如果树为空，那么返回0 2. 否则 (a) 递归得到左子树的最大高度 例如，调用maxDepth( tree-&gt; left-subtree ) (b) 递归得到右子树的最大高度 例如，调用maxDepth( tree-&gt; right-subtree ) (c) 对于当前节点，取左右子树高度的最大值并加1。 max_depth = max(左子树的最大高度, 右子树的最大高度) + 1 (d) 返回max_depth */ public static int getDepthRec(TreeNode root) &#123; if (root == null) &#123; return 0; &#125; int leftDepth = getDepthRec(root.left); int rightDepth = getDepthRec(root.right); return Math.max(leftDepth, rightDepth) + 1; // +1是因为根节点已经是一层了,否则root==null直接是0了 &#125; /** * 求二叉树的深度（高度） 迭代解法： O(n) * 基本思想同LevelOrderTraversal，还是用一个Queue */ public static int getDepth(TreeNode root) &#123; if(root == null)&#123; return 0; &#125; int depth = 0; // 深度 int currentLevelNodes = 1; // 当前Level，node的数量 int nextLevelNodes = 0; // 下一层Level，node的数量 LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.add(root); //将根节点放入队列 while( !queue.isEmpty() )&#123; //判断队列是否为空 TreeNode cur = queue.remove(); // 从队头位置移除 currentLevelNodes--; // 减少当前Level node的数量 if(cur.left != null)&#123; // 如果有左孩子，加到队尾 queue.add(cur.left); nextLevelNodes++; // 并增加下一层Level node的数量 &#125; if(cur.right != null)&#123; // 如果有右孩子，加到队尾 queue.add(cur.right); nextLevelNodes++; &#125; if(currentLevelNodes == 0)&#123; // 说明已经遍历完当前层的所有节点 ***********该判断是层级的界限,遍历完一层,则高度+1 depth++; // 增加高度 ***********高度+1 currentLevelNodes = nextLevelNodes; // 初始化下一层的遍历 ***********遍历下一层 nextLevelNodes = 0; &#125; &#125; return depth; &#125; /** * 若某一层的节点数不少于其他层次的节点数，那么该节点数即为二叉树的宽度。 * * @param root * @return */ public static int getWidth(TreeNode root) &#123; if(root == null)&#123; return 0; &#125; int maxWidth = 1; //初始化值为1 LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.add(root); //将根节点放入队列 while (true)&#123; //第一层循环 int len = queue.size(); //获取当前层节点的个数 if (len == 0) //队列为空,跳出第一层循环 break; while (len &gt; 0)&#123; //第二层循环,循环所有节点,将下一层节点放到队列中 TreeNode t = queue.removeFirst(); //弹出第一个节点 https://www.yiibai.com/java/util/linkedlist_removefirst.html len--; //弹出一个结点-1，为0就退循环 if (t.left != null)&#123; queue.add(t.left); &#125; if (t.right != null)&#123; queue.add(t.right); &#125; &#125; maxWidth = Math.max(maxWidth,queue.size()); &#125; return maxWidth; &#125; /** * 前序遍历，中序遍历，后序遍历 前序遍历递归解法： * （1）如果二叉树为空，空操作 * （2）如果二叉树不为空，访问根节点，前序遍历左子树，前序遍历右子树 */ public static void preorderTraversalRec(TreeNode root) &#123; if (root == null) &#123; return; &#125; System.out.print(root.val + &quot; &quot;); preorderTraversalRec(root.left); //左子树 preorderTraversalRec(root.right); //右子树 &#125; /** * 前序遍历:根左右 * 前序遍历迭代解法：用一个辅助stack，总是把右孩子先放进栈 */ public static void preorderTraversal(TreeNode root) &#123; if(root == null)&#123; return; &#125; Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;(); // 辅助stack stack.push(root); while( !stack.isEmpty() )&#123; TreeNode cur = stack.pop(); // 出栈栈顶元素 System.out.print(cur.val + &quot; &quot;); //todo 打印出遍历结果 // 关键点：要先压入右孩子，再压入左孩子，这样在出栈时会先打印左孩子再打印右孩子 if(cur.right != null)&#123; stack.push(cur.right); &#125; if(cur.left != null)&#123; stack.push(cur.left); &#125; &#125; &#125; /** * 中序遍历递归解法 * （1）如果二叉树为空，空操作。 * （2）如果二叉树不为空，中序遍历左子树，访问根节点，中序遍历右子树 */ public static void inorderTraversalRec(TreeNode root) &#123; if (root == null) &#123; return; &#125; inorderTraversalRec(root.left); System.out.print(root.val + &quot; &quot;); inorderTraversalRec(root.right); &#125; /** * 中序遍历迭代解法 ，用栈先把根节点的所有左孩子都添加到栈内， * 然后输出栈顶元素，再处理栈顶元素的右子树 * http://www.youtube.com/watch?v=50v1sJkjxoc * * 还有一种方法能不用递归和栈，基于线索二叉树的方法，较麻烦以后补上 * http://www.geeksforgeeks.org/inorder-tree-traversal-without-recursion-and-without-stack/ */ public static void inorderTraversal(TreeNode root)&#123; //中序 是左中右 栈中首先存的是一路靠左的节点,到达最底部之后,取右边的,放到栈中,一旦某个环节结束,就跳出返到双亲节点 if(root == null)&#123; return; &#125; Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;();// 辅助stack TreeNode cur = root; while( true )&#123;//第一层循环 while(cur != null)&#123; //第二层循环,先找到最左孩子 先添加一个非空节点所有的左孩子到栈 stack.push(cur); cur = cur.left; &#125; if(stack.isEmpty())&#123; break; // 跳出第一层循环 &#125; // 因为此时已经没有左孩子了，所以输出栈顶元素,即最左孩子 cur = stack.pop(); System.out.print(cur.val + &quot; &quot;);//todo 打印遍历结果 cur = cur.right; //处理最左孩子的右子树 &#125; &#125; /** * 后序遍历递归解法 * （1）如果二叉树为空，空操作 * （2）如果二叉树不为空，后序遍历左子树，后序遍历右子树，访问根节点 */ public static void postorderTraversalRec(TreeNode root) &#123; if (root == null) &#123; return; &#125; postorderTraversalRec(root.left); postorderTraversalRec(root.right); System.out.print(root.val + &quot; &quot;); &#125; /** * 后序遍历:左右根 * 后序遍历迭代解法 * http://www.youtube.com/watch?v=hv-mJUs5mvU * todo 重点重点重点重点重点重点重点重点重点重点 * https://www.cnblogs.com/turnips/p/5096578.html */ public static void postorderTraversal(TreeNode root) &#123; if (root == null) &#123; return; &#125; Stack&lt;TreeNode&gt; s = new Stack&lt;TreeNode&gt;(); // 第一个stack用于添加node和它的左右孩子 Stack&lt;TreeNode&gt; output = new Stack&lt;TreeNode&gt;(); // 第二个stack用于翻转第一个stack输出 s.push(root); while( !s.isEmpty() )&#123; // 确保所有元素都被翻转转移到第二个stack TreeNode cur = s.pop(); // 把栈顶元素添加到第二个stack output.push(cur); // 关键点：要先压入左孩子，再压入右孩子，这样在出栈时会先弹出右孩子再弹出左孩子 //这一点和前序遍历相反 if(cur.left != null)&#123; // 把栈顶元素的左孩子和右孩子分别添加入第一个stack s.push(cur.left); &#125; if(cur.right != null)&#123; s.push(cur.right); &#125; &#125; while( !output.isEmpty() )&#123; // 遍历输出第二个stack，即为后序遍历 System.out.print(output.pop().val + &quot; &quot;); &#125; &#125; /** * 分层遍历二叉树（按层次从上往下，从左往右）迭代 * 相当于广度优先搜索，使用队列实现。队列初始化，将根节点压入队列。当队列不为空，进行如下操作：弹出一个节点 * ，访问，若左子节点或右子节点不为空，将其压入队列 */ public static void levelTraversal(TreeNode root) &#123; if (root == null) &#123; return; &#125; LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.push(root); while (!queue.isEmpty()) &#123; TreeNode cur = queue.removeFirst(); System.out.print(cur.val + &quot; &quot;); if (cur.left != null) &#123; queue.add(cur.left); &#125; if (cur.right != null) &#123; queue.add(cur.right); &#125; &#125; &#125; /** * 分层遍历二叉树（递归） * 很少有人会用递归去做level traversal * 基本思想是用一个大的ArrayList，里面包含了每一层的ArrayList。 * 大的ArrayList的size和level有关系 * */ public static void levelTraversalRec(TreeNode root) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; ret = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); dfs(root, 0, ret); System.out.println(ret); &#125; private static void dfs(TreeNode root, int level, ArrayList&lt;ArrayList&lt;Integer&gt;&gt; ret)&#123; if(root == null)&#123; return; &#125; // 添加一个新的ArrayList表示新的一层 if(level &gt;= ret.size())&#123; ret.add(new ArrayList&lt;Integer&gt;()); &#125; ret.get(level).add(root.val); // 把节点添加到表示那一层的ArrayList里 dfs(root.left, level+1, ret); // 递归处理下一层的左子树和右子树 dfs(root.right, level+1, ret); &#125; /** * 将二叉查找树变为有序的双向链表 要求不能创建新节点，只调整指针。 * 递归解法： * 参考了http://stackoverflow.com/questions/11511898/converting-a-binary-search-tree-to-doubly-linked-list#answer-11530016 * 感觉是最清晰的递归解法，但要注意递归完，root会在链表的中间位置，因此要手动 * 把root移到链表头或链表尾 */ public static TreeNode convertBST2DLLRec(TreeNode root) &#123; root = convertBST2DLLSubRec(root); // root会在链表的中间位置，因此要手动把root移到链表头 while(root.left != null)&#123; root = root.left; &#125; return root; &#125; /** * 递归转换BST为双向链表(DLL) */ public static TreeNode convertBST2DLLSubRec(TreeNode root)&#123; if(root==null || (root.left==null &amp;&amp; root.right==null))&#123; return root; &#125; TreeNode tmp = null; if(root.left != null)&#123; // 处理左子树 tmp = convertBST2DLLSubRec(root.left); while(tmp.right != null)&#123; // 寻找最右节点 tmp = tmp.right; &#125; tmp.right = root; // 把左子树处理后结果和root连接 root.left = tmp; &#125; if(root.right != null)&#123; // 处理右子树 tmp = convertBST2DLLSubRec(root.right); while(tmp.left != null)&#123; // 寻找最左节点 tmp = tmp.left; &#125; tmp.left = root; // 把右子树处理后结果和root连接 root.right = tmp; &#125; return root; &#125; /** * 将二叉查找树变为有序的双向链表 迭代解法 // * 类似inorder traversal的做法 */ public static TreeNode convertBST2DLL(TreeNode root) &#123; if(root == null)&#123; return null; &#125; Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;(); TreeNode cur = root; // 指向当前处理节点 TreeNode old = null; // 指向前一个处理的节点 TreeNode head = null; // 链表头 while( true )&#123; while(cur != null)&#123; // 先添加一个非空节点所有的左孩子到栈 stack.push(cur); cur = cur.left; &#125; if(stack.isEmpty())&#123; break; &#125; // 因为此时已经没有左孩子了，所以输出栈顶元素 cur = stack.pop(); if(old != null)&#123; old.right = cur; &#125; if(head == null)&#123; // /第一个节点为双向链表头节点 head = cur; &#125; old = cur; // 更新old cur = cur.right; // 准备处理右子树 &#125; return head; &#125; /** * 求二叉树第K层的节点个数 递归解法： * （1）如果二叉树为空或者k&lt;1返回0 * （2）如果二叉树不为空并且k==1，返回1 * （3）如果二叉树不为空且k&gt;1，返回root左子树中k-1层的节点个数与root右子树k-1层节点个数之和 * * 求以root为根的k层节点数目 等价于 求以root左孩子为根的k-1层（因为少了root那一层）节点数目 加上 * 以root右孩子为根的k-1层（因为少了root那一层）节点数目 * * 所以遇到树，先把它拆成左子树和右子树，把问题降解 * */ public static int getNodeNumKthLevelRec(TreeNode root, int k) &#123; if (root == null || k &lt; 1) &#123; return 0; &#125; if (k == 1) &#123; return 1; &#125; int numLeft = getNodeNumKthLevelRec(root.left, k - 1); // 求root左子树的k-1层节点数 int numRight = getNodeNumKthLevelRec(root.right, k - 1); // 求root右子树的k-1层节点数 return numLeft + numRight; &#125; /** * 求二叉树第K层的节点个数 迭代解法： * 同getDepth的迭代解法 */ public static int getNodeNumKthLevel(TreeNode root, int k)&#123; if(root == null)&#123; return 0; &#125; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.add(root); int i = 1; int currentLevelNodes = 1; // 当前Level，node的数量 int nextLevelNodes = 0; // 下一层Level，node的数量 while( !queue.isEmpty() &amp;&amp; i&lt;k)&#123; TreeNode cur = queue.remove(); // 从队头位置移除 currentLevelNodes--; // 减少当前Level node的数量 if(cur.left != null)&#123; // 如果有左孩子，加到队尾 queue.add(cur.left); nextLevelNodes++; // 并增加下一层Level node的数量 &#125; if(cur.right != null)&#123; // 如果有右孩子，加到队尾 queue.add(cur.right); nextLevelNodes++; &#125; if(currentLevelNodes == 0)&#123; // 说明已经遍历完当前层的所有节点 currentLevelNodes = nextLevelNodes; // 初始化下一层的遍历 nextLevelNodes = 0; i++; // 进入到下一层 &#125; &#125; return currentLevelNodes; &#125; /** * 求二叉树中叶子节点的个数（递归） */ public static int getNodeNumLeafRec(TreeNode root) &#123; // 当root不存在，返回空 if (root == null) &#123; return 0; &#125; // 当为叶子节点时返回1 if (root.left == null &amp;&amp; root.right == null) &#123; return 1; &#125; // 把一个树拆成左子树和右子树之和，原理同上一题 return getNodeNumLeafRec(root.left) + getNodeNumLeafRec(root.right); &#125; /** * 求二叉树中叶子节点的个数（迭代） * 还是基于Level order traversal */ public static int getNodeNumLeaf(TreeNode root) &#123; if(root == null)&#123; return 0; &#125; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.add(root); int leafNodes = 0; // 记录上一个Level，node的数量 while( !queue.isEmpty() )&#123; TreeNode cur = queue.remove(); // 从队头位置移除 if(cur.left != null)&#123; // 如果有左孩子，加到队尾 queue.add(cur.left); &#125; if(cur.right != null)&#123; // 如果有右孩子，加到队尾 queue.add(cur.right); &#125; if(cur.left==null &amp;&amp; cur.right==null)&#123; // 叶子节点 leafNodes++; &#125; &#125; return leafNodes; &#125; /** * 判断两棵二叉树是否相同的树。 * 递归解法： * （1）如果两棵二叉树都为空，返回真 * （2）如果两棵二叉树一棵为空，另一棵不为空，返回假 * （3）如果两棵二叉树都不为空，如果对应的左子树和右子树都同构返回真，其他返回假 */ public static boolean isSameRec(TreeNode r1, TreeNode r2) &#123; // 如果两棵二叉树都为空，返回真 if (r1 == null &amp;&amp; r2 == null) &#123; return true; &#125; // 如果两棵二叉树一棵为空，另一棵不为空，返回假 else if (r1 == null || r2 == null) &#123; return false; &#125; if(r1.val != r2.val)&#123; return false; &#125; boolean leftRes = isSameRec(r1.left, r2.left); // 比较对应左子树 boolean rightRes = isSameRec(r1.right, r2.right); // 比较对应右子树 return leftRes &amp;&amp; rightRes; &#125; /** * 判断两棵二叉树是否相同的树（迭代） * 遍历一遍即可，这里用preorder */ public static boolean isSame(TreeNode r1, TreeNode r2) &#123; // 如果两个树都是空树，则返回true if(r1==null &amp;&amp; r2==null)&#123; return true; &#125; // 如果有一棵树是空树，另一颗不是，则返回false if(r1==null || r2==null)&#123; return false; &#125; Stack&lt;TreeNode&gt; s1 = new Stack&lt;TreeNode&gt;(); Stack&lt;TreeNode&gt; s2 = new Stack&lt;TreeNode&gt;(); s1.push(r1); s2.push(r2); while(!s1.isEmpty() &amp;&amp; !s2.isEmpty())&#123; TreeNode n1 = s1.pop(); TreeNode n2 = s2.pop(); if(n1==null &amp;&amp; n2==null)&#123; continue; &#125;else if(n1!=null &amp;&amp; n2!=null &amp;&amp; n1.val==n2.val)&#123; s1.push(n1.right); s1.push(n1.left); s2.push(n2.right); s2.push(n2.left); &#125;else&#123; return false; &#125; &#125; return true; &#125; /** * 判断二叉树是不是平衡二叉树 递归解法： * （1）如果二叉树为空，返回真 * （2）如果二叉树不为空，如果左子树和右子树都是AVL树并且左子树和右子树高度相差不大于1，返回真，其他返回假 */ public static boolean isAVLRec(TreeNode root) &#123; if(root == null)&#123; // 如果二叉树为空，返回真 return true; &#125; // 如果左子树和右子树高度相差大于1，则非平衡二叉树, getDepthRec()是前面实现过的求树高度的方法 if(Math.abs(getDepthRec(root.left) - getDepthRec(root.right)) &gt; 1)&#123; return false; &#125; // 递归判断左子树和右子树是否为平衡二叉树 return isAVLRec(root.left) &amp;&amp; isAVLRec(root.right); &#125; /** * 求二叉树的镜像 递归解法： * （1）如果二叉树为空，返回空 * （2）如果二叉树不为空，求左子树和右子树的镜像，然后交换左子树和右子树 */ // 1. 破坏原来的树，把原来的树改成其镜像 public static TreeNode mirrorRec(TreeNode root) &#123; if (root == null) &#123; return null; &#125; TreeNode left = mirrorRec(root.left); TreeNode right = mirrorRec(root.right); root.left = right; root.right = left; return root; &#125; // 2. 不能破坏原来的树，返回一个新的镜像树 public static TreeNode mirrorCopyRec(TreeNode root)&#123; if(root == null)&#123; return null; &#125; TreeNode newNode = new TreeNode(root.val); newNode.left = mirrorCopyRec(root.right); newNode.right = mirrorCopyRec(root.left); return newNode; &#125; // 3. 判断两个树是否互相镜像 public static boolean isMirrorRec(TreeNode r1, TreeNode r2)&#123; // 如果两个树都是空树，则返回true if(r1==null &amp;&amp; r2==null)&#123; return true; &#125; // 如果有一棵树是空树，另一颗不是，则返回false if(r1==null || r2==null)&#123; return false; &#125; // 如果两个树都非空树，则先比较根节点 if(r1.val != r2.val)&#123; return false; &#125; // 递归比较r1的左子树的镜像是不是r2右子树 和 // r1的右子树的镜像是不是r2左子树 return isMirrorRec(r1.left, r2.right) &amp;&amp; isMirrorRec(r1.right, r2.left); &#125; // 1. 破坏原来的树，把原来的树改成其镜像 public static void mirror(TreeNode root) &#123; if(root == null)&#123; return; &#125; Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;(); stack.push(root); while( !stack.isEmpty() )&#123; TreeNode cur = stack.pop(); // 交换左右孩子 TreeNode tmp = cur.right; cur.right = cur.left; cur.left = tmp; if(cur.right != null)&#123; stack.push(cur.right); &#125; if(cur.left != null)&#123; stack.push(cur.left); &#125; &#125; &#125; // 2. 不能破坏原来的树，返回一个新的镜像树 public static TreeNode mirrorCopy(TreeNode root)&#123; if(root == null)&#123; return null; &#125; Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;(); Stack&lt;TreeNode&gt; newStack = new Stack&lt;TreeNode&gt;(); stack.push(root); TreeNode newRoot = new TreeNode(root.val); newStack.push(newRoot); while( !stack.isEmpty() )&#123; TreeNode cur = stack.pop(); TreeNode newCur = newStack.pop(); if(cur.right != null)&#123; stack.push(cur.right); newCur.left = new TreeNode(cur.right.val); newStack.push(newCur.left); &#125; if(cur.left != null)&#123; stack.push(cur.left); newCur.right = new TreeNode(cur.left.val); newStack.push(newCur.right); &#125; &#125; return newRoot; &#125; /** * 求二叉树中两个节点的最低公共祖先节点 * 递归解法： * （1）如果两个节点分别在根节点的左子树和右子树，则返回根节点 * （2）如果两个节点都在左子树，则递归处理左子树；如果两个节点都在右子树，则递归处理右子树 */ public static TreeNode getLastCommonParentRec(TreeNode root, TreeNode n1, TreeNode n2) &#123; if (findNodeRec(root.left, n1)) &#123; // 如果n1在树的左子树 if (findNodeRec(root.right, n2)) &#123; // 如果n2在树的右子树 return root; // 返回根节点 &#125; else &#123; // 如果n2也在树的左子树 return getLastCommonParentRec(root.left, n1, n2); // 递归处理 &#125; &#125; else &#123; // 如果n1在树的右子树 if (findNodeRec(root.left, n2)) &#123; // 如果n2在左子树 return root; &#125; else &#123; // 如果n2在右子树 return getLastCommonParentRec(root.right, n1, n2); // 递归处理 &#125; &#125; &#125; // 帮助方法，递归判断一个点是否在树里 private static boolean findNodeRec(TreeNode root, TreeNode node) &#123; if (root == null || node == null) &#123; return false; &#125; if (root == node) &#123; return true; &#125; // 先尝试在左子树中查找 boolean found = findNodeRec(root.left, node); if (!found) &#123; // 如果查找不到，再在右子树中查找 found = findNodeRec(root.right, node); &#125; return found; &#125; // 求二叉树中两个节点的最低公共祖先节点 （更加简洁版的递归） public static TreeNode getLastCommonParentRec2(TreeNode root, TreeNode n1, TreeNode n2) &#123; if(root == null)&#123; return null; &#125; // 如果有一个match，则说明当前node就是要找的最低公共祖先 if(root.equals(n1) || root.equals(n2))&#123; return root; &#125; TreeNode commonInLeft = getLastCommonParentRec2(root.left, n1, n2); TreeNode commonInRight = getLastCommonParentRec2(root.right, n1, n2); // 如果一个左子树找到，一个在右子树找到，则说明root是唯一可能的最低公共祖先 if(commonInLeft!=null &amp;&amp; commonInRight!=null)&#123; return root; &#125; // 其他情况是要不然在左子树要不然在右子树 if(commonInLeft != null)&#123; return commonInLeft; &#125; return commonInRight; &#125; /** * 非递归解法： * 先求从根节点到两个节点的路径，然后再比较对应路径的节点就行，最后一个相同的节点也就是他们在二叉树中的最低公共祖先节点 */ public static TreeNode getLastCommonParent(TreeNode root, TreeNode n1, TreeNode n2) &#123; if (root == null || n1 == null || n2 == null) &#123; return null; &#125; ArrayList&lt;TreeNode&gt; p1 = new ArrayList&lt;TreeNode&gt;(); boolean res1 = getNodePath(root, n1, p1); ArrayList&lt;TreeNode&gt; p2 = new ArrayList&lt;TreeNode&gt;(); boolean res2 = getNodePath(root, n2, p2); if (!res1 || !res2) &#123; return null; &#125; TreeNode last = null; Iterator&lt;TreeNode&gt; iter1 = p1.iterator(); Iterator&lt;TreeNode&gt; iter2 = p2.iterator(); while (iter1.hasNext() &amp;&amp; iter2.hasNext()) &#123; TreeNode tmp1 = iter1.next(); TreeNode tmp2 = iter2.next(); if (tmp1 == tmp2) &#123; last = tmp1; &#125; else &#123; // 直到遇到非公共节点 break; &#125; &#125; return last; &#125; // 把从根节点到node路径上所有的点都添加到path中 private static boolean getNodePath(TreeNode root, TreeNode node, ArrayList&lt;TreeNode&gt; path) &#123; if (root == null) &#123; return false; &#125; path.add(root); // 把这个节点加到路径中 if (root == node) &#123; return true; &#125; boolean found = false; found = getNodePath(root.left, node, path); // 先在左子树中找 if (!found) &#123; // 如果没找到，再在右子树找 found = getNodePath(root.right, node, path); &#125; if (!found) &#123; // 如果实在没找到证明这个节点不在路径中，说明刚才添加进去的不是路径上的节点，删掉！ path.remove(root); &#125; return found; &#125; /** * 求二叉树中节点的最大距离 即二叉树中相距最远的两个节点之间的距离。 (distance / diameter) * 递归解法： * （1）如果二叉树为空，返回0，同时记录左子树和右子树的深度，都为0 * （2）如果二叉树不为空，最大距离要么是左子树中的最大距离，要么是右子树中的最大距离， * 要么是左子树节点中到根节点的最大距离+右子树节点中到根节点的最大距离， * 同时记录左子树和右子树节点中到根节点的最大距离。 * * http://www.cnblogs.com/miloyip/archive/2010/02/25/1673114.html * * 计算一个二叉树的最大距离有两个情况: 情况A: 路径经过左子树的最深节点，通过根节点，再到右子树的最深节点。 情况B: 路径不穿过根节点，而是左子树或右子树的最大距离路径，取其大者。 只需要计算这两个情况的路径距离，并取其大者，就是该二叉树的最大距离 */ public static Result getMaxDistanceRec(TreeNode root)&#123; if(root == null)&#123; Result empty = new Result(0, -1); // 目的是让调用方 +1 后，把当前的不存在的 (NULL) 子树当成最大深度为 0 return empty; &#125; // 计算出左右子树分别最大距离 Result lmd = getMaxDistanceRec(root.left); Result rmd = getMaxDistanceRec(root.right); Result res = new Result(); res.maxDepth = Math.max(lmd.maxDepth, rmd.maxDepth) + 1; // 当前最大深度 // 取情况A和情况B中较大值 res.maxDistance = Math.max( lmd.maxDepth+rmd.maxDepth, Math.max(lmd.maxDistance, rmd.maxDistance) ); return res; &#125; private static class Result&#123; int maxDistance; int maxDepth; public Result() &#123; &#125; public Result(int maxDistance, int maxDepth) &#123; this.maxDistance = maxDistance; this.maxDepth = maxDepth; &#125; &#125; /** * 13. 由前序遍历序列和中序遍历序列重建二叉树（递归） * 感觉这篇是讲的最为清晰的: * http://crackinterviewtoday.wordpress.com/2010/03/15/rebuild-a-binary-tree-from-inorder-and-preorder-traversals/ * 文中还提到一种避免开额外空间的方法，等下次补上 */ public static TreeNode rebuildBinaryTreeRec(List&lt;Integer&gt; preOrder, List&lt;Integer&gt; inOrder)&#123; TreeNode root = null; List&lt;Integer&gt; leftPreOrder; List&lt;Integer&gt; rightPreOrder; List&lt;Integer&gt; leftInorder; List&lt;Integer&gt; rightInorder; int inorderPos; int preorderPos; if ((preOrder.size() != 0) &amp;&amp; (inOrder.size() != 0)) &#123; // 把preorder的第一个元素作为root root = new TreeNode(preOrder.get(0)); // Based upon the current node data seperate the traversals into leftPreorder, rightPreorder, // leftInorder, rightInorder lists // 因为知道root节点了，所以根据root节点位置，把preorder，inorder分别划分为 root左侧 和 右侧 的两个子区间 inorderPos = inOrder.indexOf(preOrder.get(0)); // inorder序列的分割点 leftInorder = inOrder.subList(0, inorderPos); rightInorder = inOrder.subList(inorderPos + 1, inOrder.size()); preorderPos = leftInorder.size(); // preorder序列的分割点 leftPreOrder = preOrder.subList(1, preorderPos + 1); rightPreOrder = preOrder.subList(preorderPos + 1, preOrder.size()); root.left = rebuildBinaryTreeRec(leftPreOrder, leftInorder); // root的左子树就是preorder和inorder的左侧区间而形成的树 root.right = rebuildBinaryTreeRec(rightPreOrder, rightInorder); // root的右子树就是preorder和inorder的右侧区间而形成的树 &#125; return root; &#125; /** 14. 判断二叉树是不是完全二叉树（迭代） 若设二叉树的深度为h，除第 h 层外，其它各层 (1～h-1) 的结点数都达到最大个数， 第 h 层所有的结点都连续集中在最左边，这就是完全二叉树。 有如下算法，按层次（从上到下，从左到右）遍历二叉树，当遇到一个节点的左子树为空时， 则该节点右子树必须为空，且后面遍历的节点左右子树都必须为空，否则不是完全二叉树。 */ public static boolean isCompleteBinaryTree(TreeNode root)&#123; if(root == null)&#123; return false; &#125; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.add(root); boolean mustHaveNoChild = false; boolean result = true; while( !queue.isEmpty() )&#123; TreeNode cur = queue.remove(); if(mustHaveNoChild)&#123; // 已经出现了有空子树的节点了，后面出现的必须为叶节点（左右子树都为空） if(cur.left!=null || cur.right!=null)&#123; result = false; break; &#125; &#125; else &#123; if(cur.left!=null &amp;&amp; cur.right!=null)&#123; // 如果左子树和右子树都非空，则继续遍历 queue.add(cur.left); queue.add(cur.right); &#125;else if(cur.left!=null &amp;&amp; cur.right==null)&#123; // 如果左子树非空但右子树为空，说明已经出现空节点，之后必须都为空子树 mustHaveNoChild = true; queue.add(cur.left); &#125;else if(cur.left==null &amp;&amp; cur.right!=null)&#123; // 如果左子树为空但右子树非空，说明这棵树已经不是完全二叉完全树！ result = false; break; &#125;else&#123; // 如果左右子树都为空，则后面的必须也都为空子树 mustHaveNoChild = true; &#125; &#125; &#125; return result; &#125; /** * 14. 判断二叉树是不是完全二叉树（递归） * http://stackoverflow.com/questions/1442674/how-to-determine-whether-a-binary-tree-is-complete * */ public static boolean isCompleteBinaryTreeRec(TreeNode root)&#123;// Pair notComplete = new Pair(-1, false);// return !isCompleteBinaryTreeSubRec(root).equalsTo(notComplete); return isCompleteBinaryTreeSubRec(root).height != -1; &#125; // 递归判断是否满树（完美） public static boolean isPerfectBinaryTreeRec(TreeNode root)&#123; return isCompleteBinaryTreeSubRec(root).isFull; &#125; // 递归，要创建一个Pair class来保存树的高度和是否已满的信息 public static Pair isCompleteBinaryTreeSubRec(TreeNode root)&#123; if(root == null)&#123; return new Pair(0, true); &#125; Pair left = isCompleteBinaryTreeSubRec(root.left); Pair right = isCompleteBinaryTreeSubRec(root.right); // 左树满节点，而且左右树相同高度，则是唯一可能形成满树（若右树也是满节点）的情况 if(left.isFull &amp;&amp; left.height==right.height)&#123; return new Pair(1+left.height, right.isFull); &#125; // 左树非满，但右树是满节点，且左树高度比右树高一 // 注意到如果其左树为非完全树，则它的高度已经被设置成-1， // 因此不可能满足第二个条件！ if(right.isFull &amp;&amp; left.height==right.height+1)&#123; return new Pair(1+left.height, false); &#125; // 其他情况都是非完全树，直接设置高度为-1 return new Pair(-1, false); &#125; private static class Pair&#123; int height; // 树的高度 boolean isFull; // 是否是个满树 public Pair(int height, boolean isFull) &#123; this.height = height; this.isFull = isFull; &#125; public boolean equalsTo(Pair obj)&#123; return this.height==obj.height &amp;&amp; this.isFull==obj.isFull; &#125; &#125; public static void main(String[] args) &#123; TreeNode r1 = new TreeNode(1); TreeNode r2 = new TreeNode(2); TreeNode r3 = new TreeNode(3); TreeNode r4 = new TreeNode(4); TreeNode r5 = new TreeNode(5); TreeNode r6 = new TreeNode(6); r1.left = r2; r1.right = r3; r2.left = r4; r2.right = r5; r3.right = r6; //求节点数 高度// System.out.println(getNodeNumRec(r1));// System.out.println(getNodeNum(r1)); System.out.println(getDepthRec(r1));// System.out.println(getDepth(r1));// System.out.println(getWidth(r1)); //前中后遍历// preorderTraversalRec(r1); //1 2 4 5 3 6// System.out.println();// preorderTraversal(r1);// System.out.println();// inorderTraversalRec(r1); //4 2 5 1 3 6// System.out.println();// inorderTraversal(r1);// System.out.println();// postorderTraversalRec(r1); //4 5 2 6 3 1// System.out.println();// postorderTraversal(r1);// System.out.println(); //层序遍历// levelTraversal(r1);// System.out.println();// levelTraversalRec(r1);// System.out.println();// TreeNode tmp = convertBSTRec(r1);// while(true)&#123;// if(tmp == null)&#123;// break;// &#125;// System.out.print(tmp.val + &quot; &quot;);// if(tmp.right == null)&#123;// break;// &#125;// tmp = tmp.right;// &#125;// System.out.println();// while(true)&#123;// if(tmp == null)&#123;// break;// &#125;// System.out.print(tmp.val + &quot; &quot;);// if(tmp.left == null)&#123;// break;// &#125;// tmp = tmp.left;// &#125;// TreeNode tmp = convertBST2DLL(r1);// while(true)&#123;// if(tmp == null)&#123;// break;// &#125;// System.out.print(tmp.val + &quot; &quot;);// if(tmp.right == null)&#123;// break;// &#125;// tmp = tmp.right;// &#125;// System.out.println(getNodeNumKthLevelRec(r1, 2));// System.out.println(getNodeNumKthLevel(r1, 2));// System.out.println(getNodeNumLeafRec(r1));// System.out.println(getNodeNumLeaf(r1));// System.out.println(isSame(r1, r1));// inorderTraversal(r1);// System.out.println();// mirror(r1);// TreeNode mirrorRoot = mirrorCopy(r1);// inorderTraversal(mirrorRoot);// System.out.println(isCompleteBinaryTree(r1));// System.out.println(isCompleteBinaryTreeRec(r1)); &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of37 求两个单链表相交的第一个节点]]></title>
    <url>%2F2017%2F02%2F25%2Fof37-%E6%B1%82%E4%B8%A4%E4%B8%AA%E5%8D%95%E9%93%BE%E8%A1%A8%E7%9B%B8%E4%BA%A4%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E8%8A%82%E7%82%B9%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package offer;public class Of37 &#123; /** * 求两个单链表相交的第一个节点 对第一个链表遍历，计算长度len1，同时保存最后一个节点的地址。 * 对第二个链表遍历，计算长度len2，同时检查最后一个节点是否和第一个链表的最后一个节点相同，若不相同，不相交，结束。 * 两个链表均从头节点开始，假设len1大于len2 * ，那么将第一个链表先遍历len1-len2个节点，此时两个链表当前节点到第一个相交节点的距离就相等了，然后一起向后遍历，直到两个节点的地址相同。 * 时间复杂度，O(len1+len2) * * ---- len2 * |__________ * | * --------- len1 * |---|&lt;- len1-len2 */ public static Node getFirstCommonNode(Node head1, Node head2) &#123; if (head1 == null || head2 == null) &#123; return null; &#125; int len1 = 1; Node tail1 = head1; while (tail1.next != null) &#123; tail1 = tail1.next; len1++; &#125; int len2 = 1; Node tail2 = head2; while (tail2.next != null) &#123; tail2 = tail2.next; len2++; &#125; // 不相交直接返回NULL if (tail1 != tail2) &#123; return null; &#125; Node n1 = head1; Node n2 = head2; // 略过较长链表多余的部分 if (len1 &gt; len2) &#123; int k = len1 - len2; while (k != 0) &#123; n1 = n1.next; k--; &#125; &#125; else &#123; int k = len2 - len1; while (k != 0) &#123; n2 = n2.next; k--; &#125; &#125; // 一起向后遍历，直到找到交点 while (n1 != n2) &#123; n1 = n1.next; n2 = n2.next; &#125; return n1; &#125; private static class Node &#123; int val; Node next; public Node(int val) &#123; this.val = val; &#125; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of36 求数组的逆序对]]></title>
    <url>%2F2017%2F02%2F25%2Fof36%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package offer;public class Of36 &#123; public static int inversePairs(int[] data) &#123; if (data == null || data.length &lt; 1) &#123; throw new IllegalArgumentException(&quot;Array arg should contain at least a value&quot;); &#125; int[] copy = new int[data.length]; System.arraycopy(data, 0, copy, 0, data.length); return inversePairsCore(data, copy, 0, data.length - 1); &#125; private static int inversePairsCore(int[] data, int[] copy, int start, int end) &#123; if (start == end) &#123; copy[start] = data[start]; return 0; &#125; int mid = (end + start) / 2; int left = inversePairsCore(copy, data, start, mid);//todo copy data交换顺序 int right = inversePairsCore(copy, data, mid + 1, end); int i = mid;// 前半段的最后一个数字的下标 int j = end;// 后半段最后一个数字的下标 int indexCopy = end;//todo 辅助数组复制的数组的最后一个数字的下标 int count = 0;// 逆序数 while (i &gt;= start &amp;&amp; j &gt;= mid + 1) &#123; if (data[i] &gt; data[j]) &#123;//出现逆序对 copy[indexCopy] = data[i];//copy为排序后的数组,调整顺序 indexCopy--; i--; count += j - mid; //对应的逆序数 todo ??? 为什么不是count++ 先用两个指针分别指向两个子数组的末尾，并每次比较两个指针指向的数字。如果第一个子数组中的数字大于第二个子数组中的数字，则构成逆序对，并且逆序对的数目等于第二个子数组中剩余数字的个数,因为第二子数组也是排序好的，大于高位的，自然也大于低位的// count++;//该情况下有问题6, 5, 4, 3, 2, 1 &#125; else &#123;//没有出现逆序对 copy[indexCopy] = data[j]; indexCopy--; j--; &#125; &#125; for (; i &gt;= start; i--) &#123; copy[indexCopy] = data[i]; indexCopy--; i--; &#125; for (; j &gt;= mid + 1; j--) &#123; copy[indexCopy] = data[j]; indexCopy--; j--; &#125; return count + left + right; &#125; public static void main(String[] args) &#123; int[] data = &#123;1, 2, 3, 4, 7, 6, 5&#125;; System.out.println(inversePairs(data)); int[] data2 = &#123;6, 5, 4, 3, 2, 1&#125;; System.out.println(inversePairs(data2)); // 15 int[] data3 = &#123;1, 2, 3, 4, 5, 6&#125;; System.out.println(inversePairs(data3)); // 0 int[] data4 = &#123;1&#125;; System.out.println(inversePairs(data4)); // 0 int[] data5 = &#123;1, 2&#125;; System.out.println(inversePairs(data5)); // 0 int[] data6 = &#123;2, 1&#125;; System.out.println(inversePairs(data6)); // 1 int[] data7 = &#123;1, 2, 1, 2, 1&#125;; System.out.println(inversePairs(data7)); // 3 &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基本字符串压缩Java实现]]></title>
    <url>%2F2017%2F02%2F25%2F%E5%9F%BA%E6%9C%AC%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8E%8B%E7%BC%A9Java%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031package com.lifeibigdata.bilibili;/** * Created by lifei on 2018/6/22. */public class ZipString &#123; public void zipString(String origString)&#123; int low = 0,heigh = 0; int length = origString.length(); StringBuffer sb = new StringBuffer(); char c = &apos; &apos;; int count = 0; while (low &lt; length)&#123; heigh = low;//todo 从低位开始遍历 c = origString.charAt(heigh); while (heigh &lt; length &amp;&amp; origString.charAt(heigh) == c)&#123; heigh++; &#125; count = heigh - low; sb.append(c).append(count); low = heigh;//todo 从高位开始遍历 &#125; System.out.println(sb.toString()); &#125; public static void main(String[] args) &#123; ZipString zs = new ZipString(); zs.zipString(&quot;aabcccccaaa&quot;);//a2b1c5a3 &#125;&#125;]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of31 求所有子数组的和的最大值]]></title>
    <url>%2F2017%2F02%2F25%2Fof31-%E6%B1%82%E6%89%80%E6%9C%89%E5%AD%90%E6%95%B0%E7%BB%84%E7%9A%84%E5%92%8C%E7%9A%84%E6%9C%80%E5%A4%A7%E5%80%BC%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798package offer;/** * 题目：输入一个整型数组，数组里有正数也有负数。数组中一个或连续的多个整数组成一个子数组。求所有子数组的和的最大值。要求时间复杂度为 O(n)。 */public class Of31 &#123; static int from; static int to; public static void main(String[] args) &#123; int a[] = &#123;1,-2,3,10,-4,7,2,-5&#125;; int m = maxSubarray2(a); System.out.println(m+&quot;---&quot;+from+&quot;---&quot;+to); &#125; private static int maxSubarray(int[] a) &#123; if (a == null || a.length &lt;= 0) return 0; int sum = a[0]; int result = sum; for (int i = 0; i &lt; a.length; i++) &#123; if (sum &gt; 0)&#123; sum += a[i]; &#125; else &#123; sum = a[i]; &#125; result = Math.max(sum,result);//只计算和 &#125; return result; &#125; private static int maxSubarray2(int[] a) &#123; if (a == null || a.length &lt;= 0)&#123; from = to = -1; return 0; &#125; int sum = a[0]; int result = sum; int fromNew = 0; //新的子数组起点 for (int i = 0; i &lt; a.length; i++) &#123; if (sum &gt; 0)&#123; sum += a[i]; &#125; else &#123; sum = a[i]; fromNew = i; &#125; if (result &lt; sum)&#123;//计算from to result = sum; from = fromNew; to = i; &#125; &#125; return result; &#125; private static int maxSubarray3(int[] a,int n) &#123; if (n == 1) return a[0]; int mid = n &gt;&gt; 1; //0...mid - 1 mid...n - 1 int answer = Math.max(maxSubarray3(a,mid),maxSubarray3(a,n - mid)); int now = a[mid - 1],may = now; for (int i = mid - 2; i &gt;=0 ; --i) &#123; may = Math.max(may,now+=a[i]); &#125; now = may; for (int i = mid; i &lt; n; ++i) &#123; may = Math.max(may,now+=a[i]); &#125; return Math.max(answer,may); &#125; private static int maxSubarray4(int[] a) &#123; int[] dp = new int[a.length]; dp[0] = a[0]; int answer = dp[0]; for (int i = 1; i &lt; a.length; i++) &#123; dp[i] = Math.max(dp[i - 1] + a[i],a[i]); answer = Math.max(answer,dp[i]); &#125; return answer; &#125; private static int maxSubarray5(int[] a) &#123; int sum = a[0]; int minSum = Math.min(0,sum); int answer = a[0]; for (int i = 1; i &lt; a.length; i++) &#123; sum += a[i]; answer = Math.max(answer,sum - minSum); minSum = Math.min(minSum,sum); &#125; return answer; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PriorityQueue]]></title>
    <url>%2F2017%2F02%2F25%2FPriorityQueue%2F</url>
    <content type="text"><![CDATA[优先队列，Priority是优先的意思，实际上这个队列就是具有“优先级”。既然具有优先级的特性，那么就得有个前后排序的“规则”。所以其接受的类需要实现Comparable 接口。 构造函数123456PriorityQueue()PriorityQueue(Collection&lt;? extends E&gt; c)PriorityQueue(int initialCapacity)PriorityQueue(int initialCapacity, Comparator&lt;? super E&gt; comparator)PriorityQueue(PriorityQueue&lt;? extends E&gt; c)PriorityQueue(SortedSet&lt;? extends E&gt; c) 常用功能函数 用法poll出来的时候是按顺序出队的，poll方法返回的总是队列中排序最高的。 安全性查看PriorityQueue类的源码，会发现增加操作，并不是原子操作。没有使用任何锁。那么，如果是在多线程环境，肯定是不安全的。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of30 输入n个整数，找出其中最小的k个数]]></title>
    <url>%2F2017%2F02%2F25%2Fof30-%E8%BE%93%E5%85%A5n%E4%B8%AA%E6%95%B4%E6%95%B0%EF%BC%8C%E6%89%BE%E5%87%BA%E5%85%B6%E4%B8%AD%E6%9C%80%E5%B0%8F%E7%9A%84k%E4%B8%AA%E6%95%B0%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130package offer;import java.util.Arrays;import java.util.PriorityQueue;/** * Created by lifei on 2017/4/2. * 题目：输入n个整数，找出其中最小的k个数。 * 例如输入4，5，1，6，2，7，3，8这8个数字，则最小的4个数字是1，2，3，4 * * 解法一：O(n)的算法，只有当我们可以修改输入的数组时可用 * 解法二：O(nlogk)的算法，特别适用处理海量数据 * * 在最大堆中，根节点的值总是大于它的子树中的任意结点的值。于是我们每次可以在O(1）得到已有的k个数字中的最大值，但需要O(logk)时间完成删除及插入操作 */public class Of30 &#123; public static void main(String[] args) &#123; int[] arr = &#123;4,5,1,6,2,7,3,8&#125;;// int[] output = getLeastNumbers(arr,4);// for (int x:output) &#123;// System.out.print(x+&quot;,&quot;);// &#125;// getLeastNumbers2(arr,4); int x = findK(arr,2); System.out.println(x); &#125; private static int[] getLeastNumbers(int[] input, int k) &#123; if (input.length == 0 &amp;&amp; k &lt;= 0) return null; int[] output = new int[k]; int start = 0; int end = input.length - 1; int index = Of29.partition(input,start,end); while (index != k - 1)&#123; if (index &gt; k - 1)&#123; end = index - 1; index = Of29.partition(input,start,end); &#125; else &#123; start = index + 1; index = Of29.partition(input,start,end); &#125; &#125; for (int i = 0; i &lt; k;i++) output[i] = input[i]; return output; &#125; /** * 查找一个数组中的第K大的元素 * 调用这个Math.Random()函数能够返回带正号的double值，该值大于等于0.0且小于1.0，即取值范围是[0.0,1.0)的左闭右开区间，返回值是一个伪随机选择的数，在该范围内（近似）均匀分布 * * http://blog.csdn.net/hzh_csdn/article/details/53446211 * * PriorityQueue是从JDK1.5开始提供的新的数据结构接口。 如果不提供Comparator的话，优先队列中元素默认按自然顺序排列，也就是数字默认是小的在队列头，字符串则按字典序排列。 https://github.com/CarpenterLee/JCFInternals/tree/master/markdown https://my.oschina.net/leejun2005/blog/135085 * @param input * @param k * @return */ private static int findK(int[] input, int k) &#123; PriorityQueue&lt;Integer&gt; q = new PriorityQueue&lt;&gt;(); for (int i:input) &#123; q.offer(i); if (q.size() &gt; k)&#123;//时钟保证队列中有k个元素，如果超过则弹出最大的 q.poll(); &#125; &#125; return q.peek(); &#125; /** * 第二种做法 * @param input * @param k */ private static void getLeastNumbers2(int[] input, int k) &#123; if (input == null || k &lt; 0 || k &gt; input.length)return; //根据输入数组前k个数简历最大堆 //从k+1个数开始与根节点比较 //大于根节点，舍去 //小于，取代根节点，重建最大堆 int[] kArray = Arrays.copyOfRange(input,0,k); heapSort(kArray);//使用数组构建大顶堆 for(int i = k;i&lt;input.length;i++)&#123; if(input[i]&lt;kArray[k-1])&#123; kArray[k-1] = input[i]; heapSort(kArray); &#125; &#125; for(int i:kArray) System.out.print(i); &#125; private static void heapSort(int[] arr) &#123; for (int i = 0; i &lt; arr.length - 1; i++) &#123; buildMaxHeap(arr,arr.length - i - 1); swap(arr,0,arr.length - i - 1); &#125; &#125; //新建大顶堆 private static void buildMaxHeap(int[] arr, int lastIndex) &#123; for (int i = (lastIndex -1) /2; i &gt;=0 ; i--) &#123; int k = i; while (2*k +1 &lt;= lastIndex)&#123; int biggerIndex = 2*k + 1; if (biggerIndex &lt; lastIndex)&#123; if (arr[biggerIndex] &lt; arr[biggerIndex +1])&#123; biggerIndex++; &#125; &#125; if (arr[k] &lt; arr[biggerIndex])&#123; swap(arr,k,biggerIndex); k = biggerIndex; &#125; else &#123; break; &#125; &#125; &#125; &#125; private static void swap(int[] arr, int i, int j) &#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of29 数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字]]></title>
    <url>%2F2017%2F02%2F25%2Fof29-%E6%95%B0%E7%BB%84%E4%B8%AD%E6%9C%89%E4%B8%80%E4%B8%AA%E6%95%B0%E5%AD%97%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%E8%B6%85%E8%BF%87%E6%95%B0%E7%BB%84%E9%95%BF%E5%BA%A6%E7%9A%84%E4%B8%80%E5%8D%8A%EF%BC%8C%E8%AF%B7%E6%89%BE%E5%87%BA%E8%BF%99%E4%B8%AA%E6%95%B0%E5%AD%97%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package offer;/** * Created by lifei on 2017/4/2. * 题目：数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字 * * 解法一：基于Partition函数的O(n)算法 求中位数 * 解法二：根据数组的特点找出O(n)的算法 * 本题采用第一种实现方式。 * https://baijiahao.baidu.com/po/feed/share?wfr=spider&amp;for=pc&amp;context=%7B%22sourceFrom%22%3A%22bjh%22%2C%22nid%22%3A%22news_3577540174517580868%22%7D * http://blog.csdn.net/morewindows/article/details/6684558 * 快速排序http://www.cnblogs.com/surgewong/p/3381438.html * * * 第二种方式见 * http://wiki.jikexueyuan.com/project/for-offer/question-twenty-nine.html */public class Of29 &#123; public static void main(String[] args) &#123; int[] arr = &#123;1,2,3,3,2,2,2,2&#125;; int x = moreThanHalfNum(arr); System.out.println(x); &#125; private static int moreThanHalfNum(int[] arr) &#123; if (arr.length == 0) return -1; int length = arr.length; int middle = length &gt;&gt; 1;//todo middle int start = 0; int end = length - 1; int index = partition(arr,start,end); while (index != middle)&#123; if (index &gt; middle)&#123; end = index - 1; index = partition(arr,start,end); &#125; else &#123; start = index + 1; index = partition(arr,start,end); &#125; &#125; int result = arr[middle]; if (!checkMoreThanHalf(arr,result))&#123; result = -1; &#125; return result; &#125; private static boolean checkMoreThanHalf(int[] arr, int number) &#123; int times = 0; for (int i = 0; i &lt; arr.length; i++) &#123; if (arr[i] == number)&#123; times++; &#125; &#125; boolean isMoreThanHalf = true; if (times * 2 &lt;= arr.length)&#123; isMoreThanHalf = false; &#125; return isMoreThanHalf; &#125; static int partition(int[] arr, int left, int right) &#123; int result = arr[left]; if (left &gt; right) return -1; while (left &lt; right)&#123; while (left &lt; right &amp;&amp; arr[right] &gt;= result)&#123; right--; &#125; arr[left] = arr[right]; while (left &lt; right &amp;&amp; arr[left] &lt; result)&#123; left++; &#125; arr[right] = arr[left]; &#125; arr[left] = result; return left; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of28 输入一个字符串，打印出该字符串中字符的所有排列。]]></title>
    <url>%2F2017%2F02%2F25%2Fof28-%E8%BE%93%E5%85%A5%E4%B8%80%E4%B8%AA%E5%AD%97%E7%AC%A6%E4%B8%B2%EF%BC%8C%E6%89%93%E5%8D%B0%E5%87%BA%E8%AF%A5%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E5%AD%97%E7%AC%A6%E7%9A%84%E6%89%80%E6%9C%89%E6%8E%92%E5%88%97%E3%80%82%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940package offer;/** * Created by lifei on 2017/3/30. * 题目：输入一个字符串，打印出该字符串中字符的所有排列。 * * 思路: * （a)把字符串分为两部分，一部分是字符串的第一个字符，另一部分是第一个字符以后的所有字符。接下来我们求阴影部分的字符串的排列。 * （b）拿第一个字符和它后面的字符逐个交换。(即轮流当首字母) */public class Of28 &#123; public static void main(String[] args) &#123; String str = &quot;abcd&quot;; char[] chs = str.toCharArray(); permutation(chs,0,4); &#125; private static void permutation(char[] chs, int index, int size) &#123; if (index == size)&#123; for (char x:chs) &#123; System.out.print(x+&quot;,&quot;); &#125; System.out.println(); &#125; else &#123; for (int i = index; i &lt; size; i++) &#123; if (i != index &amp;&amp; chs[i] == chs[index]) continue;//排除重复 swap(chs,i,index);//首次是交换 0 0索引, 1 0;2 0 permutation(chs,index + 1,size); swap(chs,i,index); &#125; &#125; &#125; private static void swap(char[] chs, int idx1, int idx2) &#123; char temp = chs[idx1]; chs[idx1] = chs[idx2]; chs[idx2] = temp; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of27 输入一颗二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。]]></title>
    <url>%2F2017%2F02%2F25%2Fof27-%E8%BE%93%E5%85%A5%E4%B8%80%E9%A2%97%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%EF%BC%8C%E5%B0%86%E8%AF%A5%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E8%BD%AC%E6%8D%A2%E6%88%90%E4%B8%80%E4%B8%AA%E6%8E%92%E5%BA%8F%E7%9A%84%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8%E3%80%82%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package offer;/** * Created by lifei on 2017/3/29. * 题目：输入一颗二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。 * todo 要求不能创建新的结点，只能调整树中结点指针的指向。 * * 中序遍历,结果从小到大 * 每个节点保持只有两个指针 * * http://wiki.jikexueyuan.com/project/for-offer/question-twenty-seven.html */public class Of27 &#123; static class TreeNode&#123; int data; TreeNode left; TreeNode right; public TreeNode(int data) &#123; this.data = data; &#125; &#125; public static void main(String[] args) &#123; TreeNode root = new TreeNode(10); TreeNode t6 = new TreeNode(6); TreeNode t14 = new TreeNode(14); TreeNode t4 = new TreeNode(4); TreeNode t8 = new TreeNode(8); TreeNode t12 = new TreeNode(12); TreeNode t16 = new TreeNode(16); root.left = t6; root.right = t14; t6.left = t4; t6.right = t8; t14.left = t12; t14.right = t16; TreeNode head = convert(root); while (head != null)&#123; System.out.print(head.data+&quot;,&quot;); head = head.right;//todo 此处必须是right &#125; &#125; private static TreeNode convert(TreeNode root) &#123; TreeNode[] lastnode = new TreeNode[1];//用于保存处理过程中的双向链表的尾结点 convert(root,lastnode); TreeNode head = lastnode[0];// 找到双向链表的头结点 while (head != null &amp;&amp; head.left != null)&#123; head = head.left; &#125; return head; &#125;// node 当前的根结点// lastNode 已经处理好的双向链表的尾结点，使用一个长度为1的数组，类似C++中的二级指针 private static void convert(TreeNode root, TreeNode[] lastnode) &#123;//采用中序遍历 lastnode表示已经转换好列表的最后一个节点 if (root == null)return; TreeNode current = root; if (current.left != null)//如果有左子树就先处理左子树 convert(current.left,lastnode); current.left = lastnode[0];// 将当前结点的前驱指向已经处理好的双向链表（由当前结点的左子树构成）的尾结点 // 如果左子树转换成的双向链表不为空，设置尾结点的后继 if (lastnode[0] != null) lastnode[0].right = current; lastnode[0] = current;//todo 记录当前结点为尾结点 //处理右子树 if (current.right != null) convert(current.right,lastnode); &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>recursion</tag>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[while read读取文件内容]]></title>
    <url>%2F2017%2F02%2F25%2Fwhile-read%2F</url>
    <content type="text"><![CDATA[默认文件中每列按照tab分割1234while read ip user passdo echo &quot;$ip--$user--$pass&quot;done &lt; ip.txt 1234cat ip.txt | while read ip user passdo echo &quot;$ip--$user--$pass&quot;done 使用IFS作为分隔符读文件 说明：默认情况下IFS是空格，如果需要使用其它的需要重新赋值 IFS=:例如： cat test12chen:222:gogojie:333:hehe 123456#!/bin/bashIFS=:cat test | while read a1 a2 a3do echo &quot;$a1--$a2--$a3&quot;done]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of25 输入一颗二叉树和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。从树的根节点开始往下一直到叶结点所经过的所有的结点形成一条路径。]]></title>
    <url>%2F2017%2F02%2F25%2Fof25%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package offer;import java.util.Stack;/** * Created by lifei on 2017/3/29. * 题目：输入一颗二叉树和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。从树的根节点开始往下一直到叶结点所经过的所有的结点形成一条路径。 * * 当用前序遍历的方式访问到某一节点时，我们把该结点添加到路径上，并累加该结点的值。 * 如果该结点为叶结点并且路径中结点值的和刚好为输入的整数，则当前的路径符合要求，我们把它打印出来。 * 如果当前的结点不是叶结点，则继续访问它的子节点。当前结点访问结束后，递归函数将自动回到它的父节点。 * 因此我们在函数退出之前要在路径上删除当前结点并减去当前结点的值，以确保返回父节点时路径刚好是从根节点到父节点的路径。 * * 我们不难看出保存路径的数据结构实际上是一个栈，因此路径要与递归调用状态一致，而递归调用的本质上是一个压栈和出栈的过程。 */public class Of25 &#123; static class TreeNode&#123; int data; TreeNode left; TreeNode right; public TreeNode(int data) &#123; this.data = data; &#125; &#125; public static void main(String[] args) &#123; TreeNode root = new TreeNode(10); TreeNode n5 = new TreeNode(5); TreeNode n12 = new TreeNode(12); TreeNode n4 = new TreeNode(4); TreeNode n7 = new TreeNode(7); root.left = n5; root.right = n12; n5.left = n4; n5.right = n7; findPath(root,22); &#125; private static void findPath(TreeNode root, int k) &#123; if (root == null) return; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); findPath(root,k,stack); &#125; private static void findPath(TreeNode root, int k, Stack&lt;Integer&gt; path) &#123; if (root == null) return; if (root.left == null &amp;&amp; root.right == null)&#123;//叶子节点 if (root.data == k)&#123;//todo k为每次减后剩余的值,如果和叶子节点的值一样,则满足条件 System.out.println(&quot;路径开始:&quot;); for (int i : path) System.out.printf(i+&quot;,&quot;); System.out.println(root.data); &#125; &#125; else &#123;//非叶子节点 path.push(root.data); findPath(root.left,k - root.data,path); findPath(root.right,k - root.data,path); path.pop(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>recursion</tag>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of24 输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果]]></title>
    <url>%2F2017%2F02%2F25%2Fof24%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package offer;/** * Created by lifei on 2017/3/28. * 题目：输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则返回true，否则返回false。假设输入的数组的任意两个数字都互不相同。 * * 在后序遍历得到的序列中，最后一个数字是树的根节点的值。 * 数组中前面的数字可以分为两部分： * 第一部分是左子树结点的值，它们都比根节点的值小； * 第二部分是右子树结点的值，他们都比根节点的值大。 */public class Of24 &#123; public static void main(String[] args) throws Exception &#123; int[] arr = &#123;5,7,6,9,11,10,8&#125;; boolean b = verifySequence(arr,0,arr.length - 1); System.out.println(b); &#125; private static boolean verifySequence(int[] arr, int start, int end) throws Exception &#123; if (arr == null || arr.length &lt; 2)return true;//只有一个或0个节点,或者为null if (start &lt; 0)&#123; throw new Exception(&quot;first can&apos;t be less than 0&quot;); &#125; if (end &gt; arr.length)&#123; throw new Exception(&quot;last can&apos;t be greater than the count of the element.&quot;); &#125; int root = arr[end]; int i = start;//在二叉搜索树中左子树的结点小于根节点，TODO 首次start从0开始 for (; i &lt; end; i++) &#123; if (arr[i] &gt; root) break; &#125; int j = i; for (; j &lt; end;j++)&#123; if (arr[j] &lt; root) return false;//在二叉搜索树中右子树的结点大于根节点 &#125; boolean left = true;//判断左子树是不是二叉搜索树 if (i &gt; start)&#123; left = verifySequence(arr,start,i - 1); &#125; boolean right = true;//判断右子树是不是二叉搜索树 if (i &lt; end)&#123; right = verifySequence(arr,i,end - 1); &#125; return (left &amp;&amp; right);//todo &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>recursion</tag>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of23 层序遍历二叉树]]></title>
    <url>%2F2017%2F02%2F25%2Fof23-%E5%B1%82%E5%BA%8F%E9%81%8D%E5%8E%86%E4%BA%8C%E5%8F%89%E6%A0%91%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123package offer;import java.util.*;/** * Created by lifei on 2017/3/28. * 题目：从上往下打印二叉树的每个结点，同一层的结点按照从左到右的顺序打印 * */public class Of23 &#123; static class TreeNode&#123; int data; TreeNode left; TreeNode right; public TreeNode(int data) &#123; this.data = data; &#125; &#125; public static void main(String[] args) &#123; TreeNode root1 = new TreeNode(8); TreeNode t1 = new TreeNode(6); TreeNode t2 = new TreeNode(10); TreeNode t3 = new TreeNode(5); TreeNode t4 = new TreeNode(7); TreeNode t5 = new TreeNode(9); TreeNode t6 = new TreeNode(11); root1.left = t1; root1.right = t2; t1.left = t3; t1.right = t4; t2.left = t5; t2.right = t6;// printTree(root1);//层序遍历// System.out.println(&quot;\n------------------&quot;);// printTree2(root1);//递归遍历// printTree3(root1);//前序遍历// printTree4(root1);//中序遍历// System.out.println(&quot;\n------------------&quot;); printTree5(root1);//后序遍历 &#125; private static void printTree(TreeNode root1) &#123; if (root1 == null) return; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;();//todo 使用队列 queue.add(root1); while (!queue.isEmpty())&#123; TreeNode node = queue.poll(); System.out.print(node.data+&quot;,&quot;); if (node.left != null)&#123; queue.add(node.left); &#125; if (node.right != null)&#123; queue.add(node.right); &#125; &#125; &#125; private static void printTree5(TreeNode root1) &#123; if (root1 == null)return; Stack&lt;TreeNode&gt; s = new Stack&lt;&gt;(); Stack&lt;TreeNode&gt; output = new Stack&lt;&gt;(); s.add(root1); while (!s.isEmpty())&#123; TreeNode cur = s.pop(); output.push(cur);//第一个进栈,最后弹出,进栈顺序是根右左 弹出顺序是左右根,即后序遍历 if (cur.left != null)//left 先进栈,后弹出,所以会优先遍历右子树 s.add(cur.left); if (cur.right != null)//right 后进栈,所以会先弹出,优先遍历 s.add(cur.right); &#125; while (!output.isEmpty())&#123; System.out.print(output.pop().data+&quot;,&quot;); &#125; &#125; private static void printTree4(TreeNode root1) &#123; if (root1 == null) return; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode cur = root1;//只是遍历,不改变原来的结构,所以建立游标 while (true)&#123; while (cur != null)&#123; stack.add(cur); cur = cur.left; &#125; if (stack.isEmpty())&#123; break; &#125; cur = stack.pop();//此时已经没有左孩子了,所以输出栈顶元素,此时的cur也是根节点,满足左中右原则 System.out.print(cur.data+&quot;,&quot;); cur = cur.right;//处理最左孩子的右子树 &#125; &#125; private static void printTree3(TreeNode root1) &#123; if (root1 == null) return; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.add(root1); while (!stack.isEmpty())&#123; //todo 使用栈 TreeNode node = stack.pop(); if (node.right != null)//todo 先将右子节点压栈,才能优先弹出左子节点 stack.add(node.right); if (node.left != null) stack.add(node.left); &#125; &#125; private static void printTree2(TreeNode root1) &#123; if (root1 != null) System.out.println(root1.data); if (root1.left != null) printTree2(root1.left); if (root1.right != null) printTree2(root1.right); &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of19 请完成一个函数，输入一个二叉树，该函数输出它的镜像]]></title>
    <url>%2F2017%2F02%2F25%2Fof19-%E8%AF%B7%E5%AE%8C%E6%88%90%E4%B8%80%E4%B8%AA%E5%87%BD%E6%95%B0%EF%BC%8C%E8%BE%93%E5%85%A5%E4%B8%80%E4%B8%AA%E4%BA%8C%E5%8F%89%E6%A0%91%EF%BC%8C%E8%AF%A5%E5%87%BD%E6%95%B0%E8%BE%93%E5%87%BA%E5%AE%83%E7%9A%84%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package offer;/** * Created by lifei on 2017/3/28. * 题目：请完成一个函数，输入一个二叉树，该函数输出它的镜像 * * 我们先前序遍历这棵树的每个结点，如果遍历的结点有子节点，就交换它的两个子节点，当交换完所有的非叶子结点的左右子节点之后，我们就得到了镜像 * */public class Of19 &#123; static class TreeNode&#123; int data; TreeNode left; TreeNode right; public TreeNode(int data) &#123; this.data = data; &#125; &#125; public static void main(String[] args) &#123; TreeNode root1 = new TreeNode(8); TreeNode t1 = new TreeNode(8); TreeNode t2 = new TreeNode(7); TreeNode t3 = new TreeNode(9); TreeNode t4 = new TreeNode(2); TreeNode t5 = new TreeNode(4); TreeNode t6 = new TreeNode(7); root1.left = t1; root1.right = t2; t1.left = t3; t1.right = t4; t4.left = t5; t4.right = t6; mirror(root1); &#125; private static void mirror(TreeNode root1) &#123; if (root1 == null) return; if (root1.left == null &amp;&amp; root1.right == null)&#123; return; &#125; TreeNode tmp = root1.left; root1.left = root1.right; root1.right = tmp; if (root1.left != null)&#123; mirror(root1.left);//使用递归 &#125; if (root1.right != null)&#123; mirror(root1.right); &#125; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>recursion</tag>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of18 输入两棵二叉树A和B，判断B是不是A的子结构。]]></title>
    <url>%2F2017%2F02%2F25%2Fof18-%E8%BE%93%E5%85%A5%E4%B8%A4%E6%A3%B5%E4%BA%8C%E5%8F%89%E6%A0%91A%E5%92%8CB%EF%BC%8C%E5%88%A4%E6%96%ADB%E6%98%AF%E4%B8%8D%E6%98%AFA%E7%9A%84%E5%AD%90%E7%BB%93%E6%9E%84%E3%80%82%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package offer;/** * Created by lifei on 2017/3/27. * 题目：输入两棵二叉树A和B，判断B是不是A的子结构。 * */public class Of18 &#123; static class TreeNode&#123; int data; TreeNode left; TreeNode right; public TreeNode(int data) &#123; this.data = data; &#125; &#125; static boolean hasSubTree(TreeNode root1,TreeNode root2)&#123; if (root2 == null) return true; if (root1 == null) return false; boolean result = false; if (root1.data == root2.data)&#123; result = doesTree1HaveTree2(root1,root2); &#125; if (!result) result = hasSubTree(root1.left,root2);//递归 if (!result) result = hasSubTree(root1.right,root2); return result; &#125; private static boolean doesTree1HaveTree2(TreeNode root1, TreeNode root2) &#123; if (root2 == null) return true; if (root1 == null) return false; if (root1.data != root2.data) return false; return doesTree1HaveTree2(root1.left,root2.left) &amp;&amp; doesTree1HaveTree2(root1.right,root2.right);//递归 &#125; public static void main(String[] args) &#123; TreeNode root1 = new TreeNode(8); TreeNode t1 = new TreeNode(8); TreeNode t2 = new TreeNode(7); TreeNode t3 = new TreeNode(9); TreeNode t4 = new TreeNode(2); TreeNode t5 = new TreeNode(4); TreeNode t6 = new TreeNode(7); root1.left = t1; root1.right = t2; t1.left = t3; t1.right = t4; t4.left = t5; t4.right = t6; TreeNode root2 = new TreeNode(8); TreeNode t7 = new TreeNode(9); TreeNode t8 = new TreeNode(2); root2.left = t7; root2.right = t8; System.out.println(hasSubTree(t1,t2));; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>recursion</tag>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of17 合并两个排序的链表]]></title>
    <url>%2F2017%2F02%2F25%2Fof17n%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E7%9A%84%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package offer;/** * Created by lifei on 2017/3/27. * 题目:合并两个排序的链表 */public class Of17 &#123; static class Node &#123; int data; Node nextNode; public Node(int data) &#123; this.data = data; &#125; &#125; public static void main(String[] args) &#123; Node h1 = new Node(1); Node n3 = new Node(3); Node n5 = new Node(5); Node n7 = new Node(6); h1.nextNode = n3; n3.nextNode = n5; n5.nextNode = n7; Node h2 = new Node(2); Node n4 = new Node(4); Node n6 = new Node(6); Node n8 = new Node(8); h2.nextNode = n4; n4.nextNode = n6; n6.nextNode = n8; Node root = merge(h1,h2); while (root != null)&#123; System.out.println(root.data); root = root.nextNode; &#125; &#125; private static Node merge(Node h1, Node h2) &#123; if (h1 == null) return h2; if (h2 == null) return h1; Node newHead = null; if (h1.data &gt; h2.data)&#123; newHead = h2; newHead.nextNode = merge(h1,h2.nextNode);//使用递归求解 &#125; else &#123; newHead = h1; newHead.nextNode = merge(h1.nextNode,h2); &#125; return newHead; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>recursion</tag>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of16 定义一个函数，输入一个链表的头结点，反转该链表并输出反转后链表的头结点。]]></title>
    <url>%2F2017%2F02%2F25%2Fof16%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package offer;/** * Created by lifei on 2017/3/27. * 题目：定义一个函数，输入一个链表的头结点，反转该链表并输出反转后链表的头结点。 * * todo 翻转链表,而不是从尾开始打印 * */public class Of16 &#123; static class Node &#123; int data; Node nextNode; public Node(int data) &#123; this.data = data; &#125; &#125; public static void main(String[] args) &#123; Node head = new Node(0); Node node1 = new Node(1); Node node2 = new Node(2); Node node3 = new Node(3); head.nextNode = node1; node1.nextNode = node2; node2.nextNode = node3; Node root = reverseList(head); while (root != null)&#123; System.out.println(root.data); root = root.nextNode; &#125; &#125; private static Node reverseList(Node head) &#123; if (head == null)return null; Node pre = null; Node now = head; while (now != null)&#123; Node next = now.nextNode; //保存下一个结点 now.nextNode = pre; //当前结点指向前一个结点 //为下一轮循环做准备 pre = now; //前任结点 到现任节点 now = next; //现任节点到下一结点 &#125; return pre; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of15 输入一个链表，输出该链表中倒数第k哥结点]]></title>
    <url>%2F2017%2F02%2F25%2Fof15%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package offer;/** * Created by lifei on 2017/3/27. * 题目： 输入一个链表，输出该链表中倒数第k哥结点 * */public class Of15 &#123; static class Node &#123; int data; Node nextNode; public Node(int data) &#123; this.data = data; &#125; &#125; public static void main(String[] args) &#123; Node head = new Node(0); Node node1 = new Node(1); Node node2 = new Node(2); Node node3 = new Node(3); head.nextNode = node1; node1.nextNode = node2; node2.nextNode = node3; Node kthNode = findKthNode(head,2); System.out.println(kthNode.data); &#125; private static Node findKthNode(Node head, int k) &#123; if (head == null || k == 0)&#123; return null; &#125; Node p = head; Node q = null; for (int i = 0; i &lt; k -1; i++)&#123;//首先将前置节点跑到k;不必在一个循环中同时对p q做处理,容易出现问题 if (p.nextNode != null)&#123; p = p.nextNode; &#125; else &#123; return null;//todo 长度不满足为k时，返回null &#125; &#125; q = head; while (p.nextNode != null)&#123; p = p.nextNode; q = q.nextNode; &#125; return q; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of14 输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有奇数位于数组的前半部分，所有偶数位于数组的后半部分。]]></title>
    <url>%2F2017%2F02%2F25%2Fof14%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839package offer;/** * Created by lifei on 2017/3/27. * 题目：输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有奇数位于数组的前半部分，所有偶数位于数组的后半部分。 * */public class Of14 &#123; public static void main(String[] args) &#123; int[] arr = &#123;1,2,3,4,5&#125;; order(arr); for (int x : arr)&#123; System.out.println(x); &#125; &#125; private static void order(int[] arr) &#123; int p = 0; int q = arr.length - 1; while (q - p &gt; 1) &#123; if (isEven(arr[p]) &amp;&amp; isEven(arr[q])) &#123;//两个都是偶数,q是正确的位置，所以q需要--，从高位走向低位 q--; &#125; else if (isEven(arr[p]) &amp;&amp; !isEven(arr[q])) &#123;//q是偶数,p是奇数需要替换 int t = arr[q]; arr[q] = arr[p]; arr[p] = t; &#125; else if (!isEven(arr[p]) &amp;&amp; isEven(arr[q])) &#123;//p是奇数,q是偶数 p++; q--; &#125; else &#123;//都是奇数 p++; &#125; &#125; &#125; static boolean isEven(int n)&#123;//是否是偶数 return (n &amp; 1) == 0; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of13 给定单向链表的头指针和一个节点指针，定义一个函数在O(1)时间删除该节点]]></title>
    <url>%2F2017%2F02%2F24%2Fof13%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package offer;/** * Created by lifei on 2017/3/27. * 题目：给定单向链表的头指针和一个节点指针，定义一个函数在O(1)时间删除该节点。 * * */public class Of13 &#123; static class Node&#123; int data; Node nextNode; public Node(int data) &#123; this.data = data; &#125; &#125; public static void main(String[] args) &#123; Node head = new Node(0); Node node1 = new Node(1); Node node2 = new Node(2); Node node3 = new Node(3); head.nextNode = node1; node1.nextNode = node2; node2.nextNode = node3; Node root = removeNode(head,node3); while (root != null)&#123; System.out.println(root.data); root = root.nextNode; &#125; &#125; private static Node removeNode(Node head, Node delNode) &#123; if (head == null || delNode == null) return null; Node newhead = head; if (head == delNode)&#123; newhead = head.nextNode; head = null; &#125; else &#123; if (delNode.nextNode == null)&#123;//delnode while (head.nextNode.nextNode != null)&#123;//在倒数第二个节点停止 ... node node null head = head.nextNode; &#125; head.nextNode = null; &#125; else &#123; delNode.data = delNode.nextNode.data; delNode.nextNode = delNode.nextNode.nextNode; &#125; &#125; return newhead; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of9 写一个函数，输入n，求斐波那契数列的第n项]]></title>
    <url>%2F2017%2F02%2F24%2Fof9%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243package offer;/** * Created by lifei on 2017/3/27. * 题目一：写一个函数，输入n，求斐波那契数列的第n项 * n = 0 时 f(n) = 0； * n = 1 时 f(n) = 1; * n &gt; 1 时 f(n) = f(n - 1) - f(n - 2) ; */public class Of9 &#123; static long fibonacii(int n)&#123; long result = 0l; long preone = 1l; long pretwo = 0l; if (n ==0)&#123; return pretwo; &#125; if (n == 1)&#123; return preone; &#125; for (int i = 2; i &lt;=n ; i++) &#123; result = preone + pretwo;//结果保证由最后两个值得到 pretwo = preone; preone = result; &#125; return result; &#125; /** * 扩展部分： 一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 用Fib(n)表示青蛙跳上n阶台阶的跳法数，青蛙一次性跳上n阶台阶的跳法数，是定Fib(0)=1; 当n = 1时，只有一种跳法，即1阶跳：Fib(1） = 1； 当n = 2时，有两种跳法，一阶跳和二阶跳：Fib(2) = Fib(1)+FIb(0) = 2; 当n =3时，有三种跳法，第一次跳出一阶后，后面还有Fib(3-1)中跳法；第一次跳出二阶后，后面还有Fib(3-2)中跳法；第一次跳出三阶后，后面还有Fib(3-3)中跳法 Fib(3)= Fib(2)+Fib(1)+Fib(0) = 4 当n= n时，共有n种跳法方式，第一次跳出一阶后，后面还有Fib(n-1）种跳法；第一次跳出二阶后，后面还有Fib（n-2)种跳法，第一次跳出n阶后，后面还有Fib(n-n)种体哦啊发。 Fib(n) = Fib(n-1)+Fib(n-2)+Fib(n-3)+..........+Fib(n-n) = Fib(0)+Fib(1)+Fib(2)+.......+Fib(n-1) 又因为Fib(n-1)=Fib(0)+Fib(1)+Fib(2)+.......+Fib(n-2) 两式相减得：Fib(n)-Fib(n-1)=Fib(n-1) =====》 Fib(n) = 2*Fib(n-1) n &gt;= 2 */&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of8 输入一个递增的排序的数组的一个旋转，输出旋转数组的最小元素]]></title>
    <url>%2F2017%2F02%2F24%2Fof8%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package offer;/** * Created by lifei on 2017/3/26. * 题目:把一个数组最开始的若干个元素搬到数组的末尾，我们称之为旋转。 输入一个递增的排序的数组的一个旋转，输出旋转数组的最小元素。 * * 例如数组｛3，4，5，1，2｝为&#123;1,2,3,4,5&#125;的一个旋转，该数组的最小元素为1 * * 我们注意到旋转之后的数组实际上可以划分为两个排序的子数组，而且前面的子数组的元素都是大于或者等于后面子数组的元素 * * 使用二分查找发,如果遇到三个值相等,minOrder使用minOrder查找 * */public class Of8 &#123; public static void main(String[] args) &#123;// int[] arr = &#123;3,4,5,1,2&#125;; int[] arr = &#123;2,2,2,2,2,0,1,2,2&#125;; System.out.println(minInReverseList(arr));; &#125; private static int minInReverseList(int[] arr) &#123; if (arr == null) return -1; int leftIndex = 0; int rightIndex = arr.length - 1; int midIndex = leftIndex; while (arr[leftIndex] &gt;= arr[rightIndex])&#123;//数组的左侧大于或等于右侧的数据 if (rightIndex - leftIndex &lt;= 1)&#123; midIndex = rightIndex; break; &#125; midIndex = (leftIndex + rightIndex)/2;//得到中间的数据项 if (arr[leftIndex] == arr[rightIndex] &amp;&amp; arr[midIndex] == arr[leftIndex])&#123;//中间的数据项和两边的数据项相等，举例就是2,2,2,2,2,0,1,2,2这种情况 return minOrder(arr,leftIndex,rightIndex); &#125; if (arr[midIndex] &gt;= arr[leftIndex]) leftIndex = midIndex; if (arr[midIndex] &lt; arr[rightIndex]) rightIndex = midIndex; &#125; return arr[midIndex]; &#125; private static int minOrder(int[] arr, int leftIndex, int rightIndex) &#123; int result = arr[leftIndex]; for (int i = leftIndex + 1; i &lt; rightIndex; i++) &#123; if (result &gt; arr[i]) result = arr[i];//todo &#125; return result; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of7 用两个栈实现队列与用两个队列实现栈]]></title>
    <url>%2F2017%2F02%2F24%2Fof7%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package offer;import java.util.LinkedList;import java.util.Stack;/** * Created by lifei on 2017/3/26. * 题目: 用两个栈实现队列与用两个队列实现栈 * * http://blog.csdn.net/jsqfengbao/article/details/47089355 */public class Of7 &#123; //用两个栈实现队列 //todo 一个栈进,一个栈出;当出栈被清空之后,将进栈弹出加到出栈中;如果出栈不为空，则直接从出栈弹出 Stack&lt;Integer&gt; inStack = new Stack&lt;Integer&gt;(); Stack&lt;Integer&gt; outStack = new Stack&lt;Integer&gt;(); public void appendTail(int i)&#123; inStack.push(i); &#125; public int delHead()&#123; if (outStack.isEmpty())&#123;//出栈为空 while (!inStack.isEmpty())&#123;//进栈不空 outStack.push(inStack.pop());//将进栈补入出栈 &#125; &#125; if (outStack.isEmpty())&#123; try &#123; throw new Exception(&quot;队列为空，不能删除&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; return outStack.pop(); &#125; //用两个队列实现栈 //两个队列都为空为push到q1,否则哪个为空插入哪个 //始终保证一个队列为空，弹出项为不为空队列的队尾元素 LinkedList&lt;String&gt; q1 = new LinkedList&lt;&gt;(); LinkedList&lt;String&gt; q2 = new LinkedList&lt;&gt;(); public String push(String str)&#123; if(q1.size() ==0 &amp;&amp; q2.size() == 0)&#123; q1.addLast(str); &#125;else if(q1.size()!=0)&#123; q1.addLast(str); &#125;else if(q2.size()!=0)&#123; q2.addLast(str); &#125; return str; &#125; public String pop()&#123; String re =null; if(q1.size() == 0 &amp;&amp; q2.size() == 0)&#123;//如果两个队列为空则返回null return null; &#125; if(q2.size() == 0)&#123; while(q1.size() &gt;0)&#123; re = q1.removeFirst(); if(q1.size() != 0)&#123; q2.addLast(re); &#125; &#125; &#125; else if(q1.size() == 0)&#123; while(q2.size() &gt;0)&#123; re = q2.removeFirst(); if(q2.size()!=0)&#123; q1.addLast(re); &#125; &#125; &#125; return re; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of6 输入某二叉树的前序遍历和中序遍历的结果，请重新构造出该二叉树]]></title>
    <url>%2F2017%2F02%2F24%2Fof6%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126package offer;/** * Created by lifei on 2017/3/26. * 题目：输入某二叉树的前序遍历和中序遍历的结果，请重新构造出该二叉树 * 1 * 2 3 * 4 5 6 * 7 8 * * 前序:1 2 4 7 3 5 6 8 * 后序:4 7 2 1 5 3 8 6 * * http://blog.csdn.net/jsqfengbao/article/details/47088947 */public class Of6 &#123; static class TreeNode&#123; int val; TreeNode left; TreeNode right; public TreeNode(int val) &#123; this.val = val; &#125; &#125; /** * * @param preOrder 前序遍历数组 * @param inOrder 中序遍历数组 * @param length 数组长度 * @return 根结点 */ public static TreeNode construct(int[] preOrder,int[] inOrder,int length)&#123; if (preOrder == null || inOrder == null || length &lt;= 0)&#123; return null; &#125; try &#123; return constructCore(preOrder,0,preOrder.length - 1, inOrder,0,inOrder.length - 1); &#125; catch (Exception e)&#123; e.printStackTrace(); &#125; return null; &#125; /** * * @param preOrder 前序遍历序列 * @param startPreIndex 前序序列开始位置 * @param endPreIndex 前序序列结束位置 * @param inOrder 中序遍历序列 * @param startInIndex 中序序列开始位置 * @param endInIndex 中序序列结束位置 * @return 根结点 */ private static TreeNode constructCore(int[] preOrder, int startPreIndex, int endPreIndex, int[] inOrder, int startInIndex, int endInIndex) &#123; int rootVal = preOrder[startPreIndex]; System.out.println(&quot;rootVal = &quot;+rootVal); TreeNode root = new TreeNode(rootVal); //只有一个元素 if (startPreIndex == endPreIndex)&#123; if (startInIndex == endInIndex &amp;&amp; preOrder[startPreIndex] == inOrder[startInIndex])&#123; System.out.println(&quot;only one element&quot;); return root; &#125; else &#123; try &#123; throw new Exception(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; //todo 在中序遍历中找到根节点的索引 int rootInIndex = startInIndex; while (rootInIndex &lt;= endInIndex &amp;&amp; inOrder[rootInIndex] != rootVal)&#123; ++rootInIndex; &#125; if (rootInIndex == endInIndex &amp;&amp; inOrder[rootInIndex] != rootVal)&#123; try &#123; throw new Exception(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; int leftLength = rootInIndex - startInIndex;//todo 中序遍历的左半部分 int leftPreOrderEndIndex = startPreIndex + leftLength;//todo 前序遍历的左子树 if (leftLength &gt; 0)&#123; //构建左子树 root.left = constructCore( preOrder,startPreIndex + 1, leftPreOrderEndIndex, inOrder,startInIndex,rootInIndex - 1); &#125; if (leftLength &lt; endPreIndex - startPreIndex)&#123; //右子树有元素,构建右子树 root.right = constructCore(preOrder,leftPreOrderEndIndex + 1, endPreIndex, inOrder,rootInIndex + 1,endInIndex); &#125; return root; &#125; public static void main(String[] args) &#123; int[] preOrder = &#123;1,2,4,7,3,5,6,8&#125;; int[] inOrder = &#123;4,7,2,1,5,3,8,6&#125;; printPreOrder(construct(preOrder,inOrder,preOrder.length)); &#125; private static void printPreOrder(TreeNode root) &#123; if (root == null)&#123; return; &#125; else &#123; System.out.println(root.val + &quot; &quot;); &#125; if (root.left != null) printPreOrder(root.left); if (root.right != null) printPreOrder(root.right); &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Of5 翻转链表]]></title>
    <url>%2F2017%2F02%2F24%2FOf5%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package offer;/** * Created by lifei on 2017/3/26. * 题目：输入一个链表的头结点，从尾到头反过来打印出每个节点的值。 * * 思路:通常打印是一个只读操作，我们不希望打印时修改内容。假设面试官也要求这个题目不能改变链表的结构。 接下来我们想到解决这个问题肯定要遍历链表。遍历的顺序是从头到尾的顺序，可输出的顺序却是从尾到头。也就是说第一个遍历的节点最后一个输出， 而最后一个遍历到的节点第一个输出。这就是典型的&apos;后进先出“，我们可以从栈实现这种顺序。没经过一个节点的时候，把该节点放到一个栈中。 当遍历完整的链表后，再从栈顶开始逐个输出节点的值，此时输出的节点的顺序已经反转过来了。 既然想到了用栈来实现这个函数，而递归在本身上就是一个栈结构，于是自然就想到了用递归来实现。 要实现反过来输出链表，我们每访问到一个节点的时候，先递归输出后面的节点，再输出该节点本身，这样链表的输出结果就反过来了。 * * 栈和递归之间的关系 */public class Of5 &#123; static class Node&#123; int data; Node nextNode; public Node(int data) &#123; this.data = data; &#125; &#125; public static void main(String[] args)&#123; Node head = new Node(0); Node node1 = new Node(1); Node node2 = new Node(2); Node node3 = new Node(3); head.nextNode = node1; node1.nextNode = node2; node2.nextNode = node3; reverseRec(head); &#125; private static void reverseRec(Node head) &#123; if (head != null)&#123; if (head.nextNode != null)&#123; reverseRec(head.nextNode);//todo 此处是递归,一直在嵌套 &#125; System.out.println(head.data); &#125; else &#123; System.out.println(&quot;list is null!&quot;); &#125; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of4 把空格替换成“%20”]]></title>
    <url>%2F2017%2F02%2F24%2Fof4%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546package offer;/** * Created by lifei on 2017/3/26. * 题目：请实现一个函数，把字符串中的每个空格替换成“%20”。例如输入“We are happy&quot;，则输出”We%20are%20happy&quot;. * * java replace,StringBuilder * * 本题目考察更多的是数组替换,而非字符串替换 * * 思路: * 1.先统计中空格的个数,计算替换之后的长度 * 2.分配两个指针 */public class Of4 &#123; public static void main(String[] args)&#123; String str = &quot;we are happy.&quot;; char[] chars = str.toCharArray(); int blankNum = 0; for (char c:chars) &#123; if (c == &apos; &apos;)blankNum++; &#125; if (blankNum == 0)&#123; System.out.println(str); return; &#125; int oldlength = str.length(); int newlength = oldlength + 2*blankNum;//获取新数组长度 char[] newChars = new char[newlength]; System.arraycopy(chars,0,newChars,0,oldlength); int p = oldlength - 1;//todo 从后向前遍历 int q = newlength - 1; while (p &gt;= 0)&#123; if (chars[p] != &apos; &apos;)&#123; newChars[q--] = chars[p--]; &#125; else &#123; newChars[q--] = &apos;0&apos;; newChars[q--] = &apos;2&apos;; newChars[q--] = &apos;%&apos;; p--; &#125; &#125; System.out.println(new String(newChars)); &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[of3 从有序的二维数组中查找]]></title>
    <url>%2F2017%2F02%2F24%2Fof3%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243package offer;/** * Created by lifei on 2017/3/26. * 题目：在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下的数序排列。 * 请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数 * */public class Of3 &#123; public static void main(String[] args)&#123; int[][] mat = &#123; &#123;1,2,8,9&#125;, &#123;2,4,9,12&#125;, &#123;4,7,10,13&#125;, &#123;6,8,11,15&#125; &#125;; System.out.println(findKey(mat,2)); &#125; public static boolean findKey(int[][] array,int key)&#123; int rows = array.length; int cols = 0; if (rows != 0)&#123; cols = array[0].length; &#125; boolean find = false; if (rows &gt; 0 &amp;&amp; cols &gt; 0)&#123; //todo 从第一行的最后一列开始,使用while比较合适 int row = 0; int col = cols - 1; while (row &lt;= rows - 1 &amp;&amp; col &gt;= 0)&#123; if (array[row][col] &gt; key)&#123; col--; &#125; else if (array[row][col] &lt; key)&#123; row++; &#125; else &#123; find = true; break; &#125; &#125; &#125; return find; &#125;&#125;]]></content>
      <categories>
        <category>sword</category>
      </categories>
      <tags>
        <tag>sword</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark SQL与DataFrame-董]]></title>
    <url>%2F2017%2F02%2F22%2FSpark-SQL%E4%B8%8EDataFrame%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark应用经验与程序调优-董]]></title>
    <url>%2F2017%2F02%2F22%2FSpark%E5%BA%94%E7%94%A8%E7%BB%8F%E9%AA%8C%E4%B8%8E%E7%A8%8B%E5%BA%8F%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark计算引擎剖析-董]]></title>
    <url>%2F2017%2F02%2F22%2FSpark%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8E%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[生成逻辑查询计划，再转化为物理查询计划(stage+task)每个action就是一个jobdriver的三个阶段是单机的，只有第四个阶段是分布式的shuffleDependency会产生m*n个连接数shuffle:]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark程序设计-董]]></title>
    <url>%2F2017%2F02%2F22%2Fspark%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[Spark官方代码 http://spark.apache.org/examples.html Scala代码实例 https://github.com/apache/spark/tree/master/examples/src/ main/scala/org/apache/spark/examples Java代码实例 https://github.com/apache/spark/tree/master/examples/src/ main/java/org/apache/spark/examples Python官方代码 https://github.com/apache/spark/tree/master/examples/src/ main/python 实例1:分布式估算Pi 实例2:log query 实例3:逻辑回归简易电影受众系统]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis缓存]]></title>
    <url>%2F2017%2F02%2F21%2Fmybatis%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[缓存首先我们要知道什么是查询缓存？查询缓存又有什么作用？功能：mybatis提供查询缓存，用于减轻数据压力，提高数据库性能。用图来表示如下图： 一级缓存是SqlSession级别的缓存。在操作数据库时需要构造 sqlSession对象，在对象中有一个数据结构（HashMap）用于存储缓存数据。不同的sqlSession之间的缓存数据区域（HashMap）是互相不影响的。所以在这种情况下，是不能实现跨表的session共享的二级缓存是mapper级别的缓存，多个SqlSession去操作同一个Mapper的sql语句，多个SqlSession可以共用二级缓存，二级缓存是横跨跨SqlSession的。相信不用我再给大家去解释什么事Mapper了吧！ 一级缓存Mybatis对缓存提供支持，但是在没有配置的默认情况下，它只开启一级缓存，一级缓存只是相对于同一个SqlSession而言。首先从图上，我们可以看出，一级缓存区域是根据SqlSession为单位划分的。每次查询都会先从缓存区与找，如果找不到就会从数据库查询数据，然后将查询到的数据写入一级缓存中。Mybatis内部存储缓存使用一个HashMap，key为hashCode+sqlId+sql语句。而value值就是从查询出来映射生成的java对象。而为了保证缓存里面的数据肯定是准确数据避免脏读，每次我们进行数据修改后（增删改）就会执行commit操作，清空缓存区域。说到这里，我们来做一下测试：123456789101112//获取sessionSqlSession session = sqlSessionFactory.openSession();//获限mapper接口实例UserMapper userMapper = session.getMapper(UserMapper.class);//第一次查询User user1 = userMapper.findUserById(1);System.out.println(user1);//第二次查询，由于是同一个session则不再向数据发出语句直接从缓存取出User user2 = userMapper.findUserById(1);System.out.println(user2);//关闭sessionsession.close(); 而当我们提交Commit之后呢？12345678910111213141516//获取sessionSqlSession session = sqlSessionFactory.openSession();//获限mapper接口实例UserMapper userMapper = session.getMapper(UserMapper.class);//第一次查询User user1 = userMapper.findUserById(1);System.out.println(user1);//在同一个session执行更新User user_update = new User();user_update.setId(1);user_update.setUsername(&quot;张三&quot;);userMapper.updateUser(user_update);session.commit();//第二次查询，虽然是同一个session但是由于执行了更新操作session的缓存被清空，这里重新发出sql操作User user2 = userMapper.findUserById(1);System.out.println(user2); 由此可见，Mybatis的一级缓存是存在与SqlSession中，可以提高我们的查询性能，但是不能实现多sql的session的共享。 二级缓存 二级缓存区域是根据mapper的namespace划分的，相同namespace的mapper查询数据放在同一个区域，如果使用mapper代理方法每个mapper的namespace都不同，此时可以理解为二级缓存区域是根据mapper划分，也就是根据命名空间来划分的，如果两个mapper文件的命名空间一样，那样，他们就可以共享一个mapper缓存。 每次查询会先从缓存区域找，如果找不到从数据库查询，查询到数据将数据写入缓存。 Mybatis内部存储缓存使用一个HashMap，key为hashCode+sqlId+Sql语句。value为从查询出来映射生成的Java对象sqlSession执行insert、update、delete等操作commit提交后会清空缓存区域。 开启二级缓存 然后还要在Mapper映射文件中添加一行：1&lt;cache/&gt; &lt;!--&lt;span &gt;表示此mapper开启二级缓存--&gt; 假如说，已开启二级缓存的Mapper中有个statement要求禁用怎么办，那也不难，只需要在statement中设置useCache=false就可以禁用当前select语句的二级缓存，也就是每次都会生成sql去查询ps：默认情况下默认是true，也就是默认使用二级缓存1&lt;select id=&quot;findOrderListResultMap&quot; resultMap=&quot;ordersUserMap&quot; useCache=&quot;false&quot;&gt; 刷新缓存：在mapper的同一个namespace中，如果有其他insert、update、delete操作后都需要执行刷新缓存操作，来避免脏读。这时我们只需要设置statement配置中的flushCache=“true“属性，就会默认刷新缓存，相反如果是false就不会了。当然，不管开不开缓存刷新功能，你要是手动更改数据库表，那都肯定不能避免脏读的发生，那就属于手贱了。1&lt;insert id=&quot;insertUser&quot; parameterType=&quot;cn.ssm.mybatis.po.User&quot; flushCache=&quot;true&quot;&gt; 那既然能够刷新缓存，能定时刷新吗？也就是设置时间间隔来刷新缓存，答案是肯定的。我们在mapper映射文件中添加来表示开启缓存，那接下来，只需要我们在配置flushinterval（刷新间隔）就哦了：1&lt;cache eviction=&quot;FIFO&quot; flushInterval=&quot;60000&quot; size=&quot;512&quot; readOnly=&quot;true&quot;/&gt; flushInterval（刷新间隔）可以被设置为任意的正整数，而且它们代表一个合理的毫秒形式的时间段。默认情况是不设置，也就是没有刷新间隔，缓存仅仅调用语句时刷新。size（引用数目）可以被设置为任意正整数，要记住你缓存的对象数目和你运行环境的可用内存资源数目。默认值是1024。readOnly（只读）属性可以被设置为true或false。只读的缓存会给所有调用者返回缓存对象的相同实例。因此这些对象不能被修改。这提供了很重要的性能优势。可读写的缓存会返回缓存对象的拷贝（通过序列化）。这会慢一些，但是安全，因此默认是false。 而这个例子更高级的配置创建了一个 FIFO 缓存,并每隔 60 秒刷新,存数结果对象或列表的 512 个引用,而且返回的对象被认为是只读的,因此在不同线程中的调用者之间修改它们会导致冲突。可用的收回策略有, 默认的是 LRU: LRU – 最近最少使用的:移除最长时间不被使用的对象。 FIFO – 先进先出:按对象进入缓存的顺序来移除它们。 SOFT – 软引用:移除基于垃圾回收器状态和软引用规则的对象。 WEAK – 弱引用:更积极地移除基于垃圾收集器状态和弱引用规则的对象。 Mybatis这么好，如何应用呢？ 因为这是一种缓存机制嘛，只有相对于实时性要求不高的需求才会使用缓存机制，它也一样。对于访问多的查询请求且用户对查询结果实时性要求不高，此时可采用mybatis二级缓存技术降低数据库访问量，提高访问速度业务场景比如：耗时较高的统计分析sql、电话账单查询sql等。实现方法如下：通过设置刷新间隔时间，由mybatis每隔一段时间自动清空缓存，根据数据变化频率设置缓存刷新间隔flushInterval，比如设置为30分钟、60分钟、24小时等，根据需求而定。 可是，好归好，Mybatis也有它一定的局限性。那就是Mybatis对于细粒度的数据级别的缓存实现的不是太好，也就是如果同Mapper下的商品类别繁多的话，他不能实现只刷新某固定商品的信息，而只能全盘刷新。当时将这块的时候我想过让Mapper水平分区不就行了，可是后来说到Mybatis的二级缓存是以命名空间划分的或者说是以Mapper划分，不管我们怎么水平划分，只要命名空间一样，那就只共享一个二级缓存域，当刷新的时候还是会全Mapper更新一遍。参考https://blog.csdn.net/liweizhong193516/article/details/53639350https://www.cnblogs.com/DoubleEggs/p/6243223.htmlhttps://www.cnblogs.com/happyflyingpig/p/7739749.html]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单例模式]]></title>
    <url>%2F2017%2F02%2F21%2F%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[单例应用 单例模式的定义单例模式确保某个类只有一个实例，而且自行实例化并向整个系统提供这个实例。 单例模式的特点单例类只能有一个实例。单例类必须自己创建自己的唯一实例。单例类必须给所有其他对象提供这一实例。 单例模式的应用 在计算机系统中，线程池、缓存、日志对象、对话框、打印机、显卡的驱动程序对象常被设计成单例。 这些应用都或多或少具有资源管理器的功能。每台计算机可以有若干个打印机，但只能有一个Printer Spooler，以避免两个打印作业同时输出到打印机中。每台计算机可以有若干通信端口，系统应当集中管理这些通信端口，以避免一个通信端口同时被两个请求同时调用。总之，选择单例模式就是为了避免不一致状态。 单例模式的Java代码单例模式分为懒汉式（需要才去创建对象）和饿汉式（创建类的实例时就去创建对象）。饿汉模式我们知道饿汉式的实现是线程安全的，没有延迟加载（Lazy Loading），下面我们深入研究下为什么。 饿汉式的实现如下：1234567public class Singleton &#123; private static Singleton singleton = new Singleton(); private Singleton() &#123;&#125; public static Singleton getSignleton()&#123; return singleton; &#125;&#125; 在介绍饿汉式时我们大多会说这种实现是线程安全的，实例在类加载时实例化，有JVM保证线程安全。虚拟机是怎么保证饿汉式实现的线程安全？ 先介绍类生命周期的7个阶段，前面的5个阶段为类加载阶段，每个阶段作用不详细介绍了：首先，singleton 作为类成员变量的实例化发生在类Singleton 类加载的初始化阶段，初始化阶段是执行类构造器clinit() 方法的过程。 clinit()方法是由编译器自动收集类中的所有类变量（static）的赋值动作和静态语句块（static{}）块中的语句合并产生的。因此，private static Singleton singleton = new Singleton();也会被放入到这个方法中。（这是第一步先放到方法中） 虚拟机会保证一个类的clinit()方法在多线程环境中被正确的加锁、同步，如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的clinit()方法，其他线程都需要阻塞等待，直到活动线程执行clinit()方法完毕。需要注意的是，其他线程虽然会被阻塞，但如果执行clinit()方法的那条线程退出clinit()方法后，其他线程唤醒后不会再次进入clinit()方法。同一个类加载器下，一个类型只会初始化一次。（保证线程安全）回答了线程安全的问题，我们再看看这个实现为什么不是延迟加载的？（什么时候初始化） 什么情况下需要开始类加载过程的第一个阶段——加载，Java虚拟机规范中没有进行强制的约束，这点可以交给虚拟机的具体实现来自由把握。 但是对于初始化阶段，虚拟机规范则是严格规定了有且只有以下5种情况必须立即对类进行“初始化”（而加载、验证、准备自然需要在此之前完成）。 遇到new, getstatic, putstatic, invoke static这4条字节码指令时，如果类没有进行过初始化，由需要先触发其初始化。生成这4条指令的最常见的Java代码场景是：使用new关键字实例化对象，读取或设置一个类的静态字段（被fina修饰的静态字段除外，其已在编译期把值放入了常量池中），以及调用一个类的静态方法。 使用java.lang.reflect包的方法 对类进行反射时，如果类没有进行过初始化，由需要先触发其初始化。（反射） 初始化一个类的时候，如果发现其父类还没有进行初始化，由先触发其父类的初始化。（父类没有初始化） 当虚拟机启动时，用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟杨会先初始化这个主类。（main方法） 当使用JDK 1.7的动态语言支持时，如果一个java.lang.invoke.MethodHandle实例最后的解析结果REF_getstatic、REF_putStatic、REF_invokeStatic的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化。（句柄） 注意到第一条中的new字节码指令会触发初始化，因为private static Singleton singleton = new Singleton();中就有使用new关键字的情况，可知一旦触发初始化clinit() 方法执行，singleton 就会被分配内存完成实例化。单例模式下大部分情况下是调用静态方法getSignleton()被触发初始化，但是也不能100%保证，上述5种情况下，任何一种都会触发初始化，于是就能解释为什么饿汉式不是延迟加载了。 枚举下面我们用一个枚举实现单个数据源例子来简单验证一下：声明一个枚举，用于获取数据库连接。12345678910public enum DataSourceEnum &#123; DATASOURCE; private DBConnection connection = null; private DataSourceEnum() &#123; connection = new DBConnection(); &#125; public DBConnection getConnection() &#123; return connection; &#125;&#125; 下面深入了解一下为什么枚举会满足线程安全、序列化等标准。在JDK5 中提供了大量的语法糖，枚举就是其中一种。所谓 语法糖（Syntactic Sugar），也称糖衣语法，是由英国计算机学家 Peter.J.Landin 发明的一个术语，指在计算机语言中添加的某种语法，这种语法对语言的功能并没有影响，但是但是更方便程序员使用。只是在编译器上做了手脚，却没有提供对应的指令集来处理它。就拿枚举来说，其实Enum就是一个普通的类，它继承自java.lang.Enum类。12345678910public enum DataSourceEnum &#123; DATASOURCE;&#125; 把上面枚举编译后的字节码反编译，得到的代码如下：public final class DataSourceEnum extends Enum&lt;DataSourceEnum&gt; &#123; public static final DataSourceEnum DATASOURCE; public static DataSourceEnum[] values(); public static DataSourceEnum valueOf(String s); static &#123;&#125;;&#125; 由反编译后的代码可知，DATASOURCE 被声明为 static 的，根据在【单例深思】饿汉式与类加载 中所描述的类加载过程，可以知道虚拟机会保证一个类的clinit() 方法在多线程环境中被正确的加锁、同步。所以，枚举实现是在实例化时是线程安全。接下来看看序列化问题： Java规范中规定，每一个枚举类型极其定义的枚举变量在JVM中都是唯一的，因此在枚举类型的序列化和反序列化上，Java做了特殊的规定。 在序列化的时候Java仅仅是将枚举对象的name属性输出到结果中，反序列化的时候则是通过 java.lang.Enum 的 valueOf() 方法来根据名字查找枚举对象。也就是说，以下面枚举为例，序列化的时候只将 DATASOURCE 这个名称输出，反序列化的时候再通过这个名称，查找对于的枚举类型，因此反序列化后的实例也会和之前被序列化的对象实例相同。123public enum DataSourceEnum &#123; DATASOURCE;&#125; 由此可知，枚举天生保证序列化单例。 懒汉模式饿汉模式属性实例化对象12345678910//饿汉模式：线程安全，耗费资源。public class HugerSingletonTest &#123; //该对象的引用不可修改 private static final HugerSingletonTest ourInstance = new HugerSingletonTest(); public static HugerSingletonTest getInstance() &#123; return ourInstance; &#125; private HugerSingletonTest() &#123; &#125;&#125; 这种写法如果完美的话，就没必要在啰嗦那么多双检锁的问题了。缺点是它不是一种懒加载模式（lazy initialization），单例会在加载类后一开始就被初始化，即使客户端没有调用 getInstance()方法。饿汉式的创建方式在一些场景中将无法使用：譬如 Singleton 实例的创建是依赖参数或者配置文件的，在 getInstance() 之前必须调用某个方法设置参数给它，那样这种单例写法就无法使用了。 在静态代码块实例对象123456789101112public class Singleton &#123; private static Singleton ourInstance; static &#123;//...此处可以从配置文件中加载资源 ourInstance = new Singleton();//在静态块中 &#125; public static Singleton getInstance() &#123; return ourInstance; &#125; private Singleton() &#123; &#125;&#125; 分析：饿汉式单例模式只要调用了该类，就会实例化一个对象，但有时我们并只需要调用该类中的一个方法，而不需要实例化一个对象，所以饿汉式是比较消耗资源的。 懒汉模式非线程安全 1234567891011 public class Singleton &#123; private static Singleton ourInstance; public static Singleton getInstance() &#123; if (null == ourInstance) &#123; ourInstance = new Singleton();//不在静态块中 &#125; return ourInstance; &#125; private Singleton() &#123; &#125;&#125; 分析：如果有两个线程同时调用getInstance()方法，则会创建两个实例化对象。所以是非线程安全的。 线程安全：给方法加锁1234567891011public class Singleton &#123; private static Singleton ourInstance; public synchronized static Singleton getInstance() &#123; if (null == ourInstance) &#123; ourInstance = new Singleton(); &#125; return ourInstance; &#125; private Singleton() &#123; &#125;&#125; 分析：如果有多个线程调用getInstance()方法，当一个线程获取该方法，而其它线程必须等待，消耗资源。 线程安全：双重检查锁（同步代码块）1234567891011121314public class Singleton &#123; private volatile static Singleton instance; //声明成 volatile，也要声明为static，但是没有new private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (instance == null) &#123; synchronized (Singleton.class) &#123;//todo synchronized if (instance == null) &#123; instance = new Singleton();//不在静态块中 &#125; &#125; &#125; return instance; &#125;&#125; 分析：为什么需要双重检查锁呢？因为第一次检查是确保之前是一个空对象，而非空对象就不需要同步了，空对象的线程然后进入同步代码块，如果不加第二次空对象检查，两个线程同时获取同步代码块，一个线程进入同步代码块，另一个线程就会等待，而这两个线程就会创建两个实例化对象，所以需要在线程进入同步代码块后再次进行空对象检查，才能确保只创建一个实例化对象。 线程安全：静态内部类12345678910public class Singleton &#123; private static class SingletonHodler &#123; private static Singleton ourInstance = new Singleton(); &#125; public synchronized static Singleton getInstance() &#123; return SingletonHodler.ourInstance; &#125; private Singleton() &#123; &#125;&#125; 分析：利用静态内部类，都个线程在调用getInstance方法时会创建一个实例化对象。 线程安全：枚举12345enum SingletonTest &#123; INSTANCE; public void whateverMethod() &#123; &#125;&#125; 分析：枚举的方式是《Effective Java》书中提倡的方式，它不仅能避免多线程同步问题，而且还能防止反序列化重新创建新的对象，但是在枚举中的其他任何方法的线程安全由程序员自己负责。还有防止上面的通过反射机制调用私用构造器。不过，由于Java1.5中才加入enum特性。 指令重排序我们再来思考一个问题，就是懒汉式的双重检查版本的单例模式，它一定是线程安全的吗？我会毫不犹豫的告诉你—不一定，因为在JVM的编译过程中会存在指令重排序的问题,(除非你已经加上volatile)。其实创建一个对象，往往包含三个过程。对于singleton = new Singleton()，这不是一个原子操作，在 JVM 中包含的三个过程。1231&gt;给 singleton 分配内存2&gt;调用 Singleton 的构造函数来初始化成员变量，形成实例3&gt;将singleton对象指向分配的内存空间（执行完这步 singleton才是非 null 了） 但是，由于JVM会进行指令重排序，所以上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是 1-3-2，则在 3 执行完毕、2 未执行之前，被l另一个线程抢占了，这时 instance 已经是非 null 了（但却没有初始化），所以这个线程会直接返回 instance，然后使用，那肯定就会报错了。 针对这种情况，我们有什么解决方法呢？那就是把singleton声明成 volatile，改进后的懒汉式线程安全（双重检查锁）的12345678910111213141516public class Singleton &#123; //volatile的作用是：保证可见性、禁止指令重排序，但不能保证原子性 private volatile static Singleton ourInstance; public synchronized static Singleton getInstance() &#123; if (null == ourInstance) &#123; synchronized (Singleton.class) &#123; if (null == ourInstance) &#123; ourInstance = new Singleton(); &#125; &#125; &#125; return ourInstance; &#125; private Singleton() &#123; &#125;&#125; 静态内部类推荐使用，这种方法也是《Effective Java》上所推荐的。123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 这种写法仍然使用JVM本身机制保证了线程安全问题；由于 SingletonHolder 是私有的，除了 getInstance() 之外没有办法访问它，因此它是懒汉式的；同时读取实例的时候不会进行同步，没有性能缺陷；也不依赖 JDK 版本。一般情况下直接使用饿汉式就好了，如果明确要求要懒加载（lazy initialization）会倾向于使用静态内部类，如果涉及到反序列化创建对象时会试着使用枚举的方式来实现单例。 总结 静态内部类能够结构饿汉模式晚加载，再加上静态代码块的方式就可以解决从配置文件加载资源的问题。 懒加载可以解决加载资源的问题，但是要注意双重检锁 枚举可以反序列化 饿汉模式最简单]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>单例模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[I/O]]></title>
    <url>%2F2017%2F02%2F21%2FI-O%2F</url>
    <content type="text"><![CDATA[BIO:JDK1.4以前我们使用都是BIO 阻塞IO阻塞到我们的读写方法,阻塞到线程来提供性能.对于线程的开销本来就是性能的浪费.NIO:jdk1.4 linux 多路复用技术(select模式) 实现IO事件的轮询方式:同步非阻塞的模式.这个种方式目前是主流的网络通信模式.Mina，netty mina2.0 netty5.0—网络通信框架.比我直接写NIO要容易些 并且代码可读性更好AIO:jdk1.7 (NIO2) 才是实现真正的异步aio，学习linux epoll模式AIO使用的比较少,大家可以认真的学习一些思想 小结:1)BIO阻塞的IO 2）NIO select+非阻塞 同步非阻塞 3)异步非阻塞IO 3.NIO AIO原理的解读 对于网络通信而言NIO，AIO并没有改变网通通信的基本步骤，只是在其原来的基础上(serverscoket,socket)做了一个改进. Socket &lt;—-建立连接需要三次握手—–&gt; serversocket 对于三次握手的方式建立稳定的连接性能开销比较大.解决方案从思想上来说比较容易 就是减少连接的次数.对我们的读写通信管道进行一个抽象. 4.NIO原理 通过selctor（选择器）就相当管家 ，管理所有的IO事件 Connction accept 客服端和服务端的读写.—–IO事件 selctor（选择器）如何进行管理IO事件当IO事件注册给我们的选择器的时候 选择器会给他们分配一个key（可以简单的理解成一个时间的标签） 当IO事件完成过通过key值来找到相应的管道 然后通过管道发送数据和接收数据等操作. 数据缓冲区：通过bytebuffer，提供很多读写的方法 put（） get（） 服务端：ServerSocketChannel客服端: SocketChannel选择器: Selector selector=Select.open();这样就打开了我们的选择器. 4.Selectionkey: 可以通过它来判断IO事件是否已经就绪. key.isAccptable:是否可以接受客户端的连接 Key.isconnctionable:是否可以连接服务端 Key.isreadable():缓冲区是否可读 Key.iswriteable():缓冲区是否可写 5.如何获得事件的keys Selectionkey keys= Selector.selectedkeys(); 6.如何注册 channel.regist(Selector,Selectionkey.OP_Write); channel.regist(Selector,Selectionkey.OP_Read); channel.regist(Selector,Selectionkey.OP_Connct); channel.regist(Selector,Selectionkey.OP_Accept); 6.AIO: 服务端:AsynchronousServerSocketChannel客服端:AsynchronousSocketChannel用户处理器:CompletionHandler接口,这个接口实现应用程序向操作系统发起IO请求,当完成后处理具体逻辑，否则做自己该做的事情.]]></content>
      <categories>
        <category>netty</category>
      </categories>
      <tags>
        <tag>netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker部署mesos]]></title>
    <url>%2F2017%2F02%2F21%2Fdocker%E9%83%A8%E7%BD%B2mesos%2F</url>
    <content type="text"><![CDATA[http://dockone.io/article/136]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>mesos</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume接收kafka source落地本地]]></title>
    <url>%2F2017%2F02%2F21%2Fflume%E6%8E%A5%E6%94%B6kafka-source%E8%90%BD%E5%9C%B0%E6%9C%AC%E5%9C%B0%2F</url>
    <content type="text"><![CDATA[flume接收kafka source落地本地，然后上传hdfs，避免flume直接上传hdfsmaven工程pom.xml1234567891011&lt;version.flume&gt;1.7.0&lt;/version.flume&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;$&#123;version.flume&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-configuration&lt;/artifactId&gt; &lt;version&gt;$&#123;version.flume&#125;&lt;/version&gt;&lt;/dependency&gt; java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453package com.qnr.qav.flume;/** * Created by lifei on 2018/7/31. */import java.io.File;import java.util.concurrent.atomic.AtomicInteger;public class PathManagerExtra &#123; private long seriesTimestamp; private String baseDirectory; private AtomicInteger fileIndex; private File currentFile; private String pefix; private String suffix; public PathManagerExtra() &#123; seriesTimestamp = System.currentTimeMillis(); fileIndex = new AtomicInteger(); &#125; public File nextFile() &#123; //(1) /usr/local/flume/xxxxpjmLog/%Y%m%d 将%Y%m%d替换为年月日 并返回(此处为省事整串替换，配置文件中的也必须写成%Y%m%d&lt;span style="font-family: Arial, Helvetica, sans-serif;"&gt;)&lt;/span&gt; String dirStr = SinkPjmDefinedUtils.getRealPath(baseDirectory); //(2) flume_bjxd02.%Y%m%d%H%M将%Y%m%d%H%M替换为年月日时分 String pefixStr = SinkPjmDefinedUtils.getRealPathFilePrefix(pefix); //(3) 拼文件全路径/data/logs/flume/allpjm/20150115/flume_bjxd02.201501151029.1421288975655.log // （写文件中需要添加.tmp后缀） String filePath = dirStr+pefixStr+"."+System.currentTimeMillis()+suffix+".tmp"; currentFile = SinkPjmDefinedUtils.CreateFolderAndFile(dirStr, filePath); return currentFile; &#125; /* public File nextFile() &#123; currentFile = new File(baseDirectory, seriesTimestamp + "-" + fileIndex.incrementAndGet()); return currentFile; &#125; */ public File getCurrentFile() &#123; if (currentFile == null) &#123; return nextFile(); &#125; return currentFile; &#125; public void rotate() &#123; currentFile = null; &#125; public String getBaseDirectory() &#123; return baseDirectory; &#125; public void setBaseDirectory(String baseDirectory) &#123; this.baseDirectory = baseDirectory; &#125; public long getSeriesTimestamp() &#123; return seriesTimestamp; &#125; public AtomicInteger getFileIndex() &#123; return fileIndex; &#125; public String getPefix() &#123; return pefix; &#125; public void setPefix(String pefix) &#123; this.pefix = pefix; &#125; public String getSuffix() &#123; return suffix; &#125; public void setSuffix(String suffix) &#123; this.suffix = suffix; &#125;&#125;package com.qnr.qav.flume;import java.io.File;import java.text.SimpleDateFormat;import java.util.Date;/** * Created by lifei on 2018/7/31. */public class SinkPjmDefinedUtils &#123; /** * 功能：替换文件夹路径中的%Y%m%d &lt;br/&gt; * * @author pjm &lt;br/&gt; * @version 2015-1-15 上午09:44:46 &lt;br/&gt; */ public static String getRealPath(String path)&#123; if (path.contains("%Y%m%d%H")) &#123; Date today = new Date(); SimpleDateFormat formatter = new SimpleDateFormat("yyyyMMddHH"); String formattedDate = formatter.format(today); System.out.println(formattedDate); path = path.replace("%Y%m%d%H", formattedDate); &#125; return path; &#125; /** * 功能： 文件前缀替换&lt;br/&gt; * * @author pjm &lt;br/&gt; * @version 2015-1-15 上午09:45:32 &lt;br/&gt; */ public static String getRealPathFilePrefix(String path)&#123; if (path.contains("%Y%m%d%H%M")) &#123; Date today = new Date(); SimpleDateFormat formatter = new SimpleDateFormat("yyyyMMddHHmm"); String formattedDate = formatter.format(today); System.out.println(formattedDate); path = path.replace("%Y%m%d%H%M", formattedDate); &#125; return path; &#125; /** * 功能： 创建文件和文件夹，并返回文件&lt;br/&gt; * * @author pjm &lt;br/&gt; * @version 2015-1-15 上午09:45:48 &lt;br/&gt; */ public static File CreateFolderAndFile(String dirpath,String filepath)&#123;//String dirpath = "/data/logs/flume/All/20150115/";//String filepath = "/data/logs/flume/All/20150115/flume_bjxd04.201501150900.1421283612463.log";//String dirpath = "/usr/local/flume/AllLog/20150115/";//String filepath = "/usr/local/flume/AllLog/20150115/flume_bjxd04.201501150900.1421283612463.log"; File dirFile = new File(dirpath); // 创建文件夹 if (!dirFile.exists()) &#123; dirFile.mkdirs(); &#125; File f = new File(filepath);/* // 创建文件 if (!f.exists()) &#123; try &#123; f.createNewFile();// f.createTempFile("kkk2", ".java", dirFile); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;*/ return f; &#125;&#125;package com.qnr.qav.flume;import com.google.common.base.Preconditions;import com.google.common.util.concurrent.ThreadFactoryBuilder;import org.apache.commons.io.FileUtils;import org.apache.flume.*;import org.apache.flume.conf.Configurable;import org.apache.flume.instrumentation.SinkCounter;import org.apache.flume.serialization.EventSerializer;import org.apache.flume.serialization.EventSerializerFactory;import org.apache.flume.sink.AbstractSink;import org.apache.flume.sink.RollingFileSink;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.*;import java.util.concurrent.Executors;import java.util.concurrent.ScheduledExecutorService;import java.util.concurrent.TimeUnit;/** * Created by lifei on 2018/7/31. */public class RollingFileSinkExtra extends AbstractSink implements Configurable &#123; private static final Logger logger = LoggerFactory .getLogger(RollingFileSink.class); private static final long defaultRollInterval = 30; private static final int defaultBatchSize = 100; private int batchSize = defaultBatchSize; private String directory; //在 RollingFileSink类 是 private File directory; 因为此处需要替换 年月日等 定义为String private long rollInterval; private OutputStream outputStream; private ScheduledExecutorService rollService; private String serializerType; private Context serializerContext; private EventSerializer serializer; private SinkCounter sinkCounter; private PathManagerExtra pathController; private volatile boolean shouldRotate; private String pefix; private String suffix; public RollingFileSinkExtra() &#123; pathController = new PathManagerExtra(); shouldRotate = false; &#125; @Override public void configure(Context context) &#123;// &lt;span style="white-space:pre"&gt; //获取配置参数sink.directory&lt;/span&gt; sink.rollInterval sink.filePrefix sink.fileSuffix directory = context.getString("sink.directory"); String rollInterval = context.getString("sink.rollInterval"); pefix = context.getString("sink.filePrefix"); suffix = context.getString("sink.fileSuffix"); serializerType = context.getString("sink.serializer", "TEXT"); serializerContext = new Context(context.getSubProperties("sink." + EventSerializer.CTX_PREFIX)); Preconditions.checkArgument(directory != null, "Directory may not be null"); Preconditions.checkNotNull(serializerType, "Serializer type is undefined"); if (rollInterval == null) &#123; this.rollInterval = defaultRollInterval; &#125; else &#123; this.rollInterval = Long.parseLong(rollInterval); &#125; batchSize = context.getInteger("sink.batchSize", defaultBatchSize); if (sinkCounter == null) &#123; sinkCounter = new SinkCounter(getName()); &#125; &#125; @Override public void start() &#123; logger.info("Starting &#123;&#125;...", this); sinkCounter.start(); super.start(); pathController.setBaseDirectory(directory); pathController.setPefix(pefix); pathController.setSuffix(suffix); if (rollInterval &gt; 0) &#123; rollService = Executors.newScheduledThreadPool( 1, new ThreadFactoryBuilder().setNameFormat( "rollingFileSink-roller-" + Thread.currentThread().getId() + "-%d") .build()); /* * Every N seconds, mark that it's time to rotate. We purposefully * do NOT touch anything other than the indicator flag to avoid * error handling issues (e.g. IO exceptions occuring in two * different threads. Resist the urge to actually perform rotation * in a separate thread! */ rollService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; logger.debug("Marking time to rotate file &#123;&#125;", pathController.getCurrentFile()); shouldRotate = true; &#125; &#125;, rollInterval, rollInterval, TimeUnit.SECONDS); &#125; else &#123; logger.info("RollInterval is not valid, file rolling will not happen."); &#125; logger.info("RollingFileSink &#123;&#125; started.", getName()); &#125; @Override public Status process() throws EventDeliveryException &#123; if (shouldRotate) &#123; // shouldRotate为真，表示当前文件停止Roll，再生成新的文件执行写入 logger.debug("Time to rotate &#123;&#125;", pathController.getCurrentFile()); if (outputStream != null) &#123; logger.debug("Closing file &#123;&#125;", pathController.getCurrentFile()); try &#123; serializer.flush(); serializer.beforeClose(); outputStream.close(); sinkCounter.incrementConnectionClosedCount(); shouldRotate = false; &#125; catch (Exception e) &#123; sinkCounter.incrementConnectionFailedCount(); throw new EventDeliveryException("Unable to rotate file " + pathController.getCurrentFile() + " while delivering event", e); &#125; finally &#123; serializer = null; outputStream = null; &#125; ////去掉文件后缀名（文件在写入的过程中默认给加了.tmp作为区分，文件写完需要去掉这个后缀） File ff = pathController.getCurrentFile(); try &#123; FileUtils.moveFile( ff, new File(ff.getAbsolutePath().substring(0, ff.getAbsolutePath().indexOf(".tmp")))); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; pathController.rotate(); &#125; &#125; if (outputStream == null) &#123; File currentFile = pathController.getCurrentFile(); logger.debug("Opening output stream for file &#123;&#125;", currentFile); try &#123; outputStream = new BufferedOutputStream(new FileOutputStream( currentFile)); serializer = EventSerializerFactory.getInstance(serializerType, serializerContext, outputStream); serializer.afterCreate(); sinkCounter.incrementConnectionCreatedCount(); &#125; catch (IOException e) &#123; sinkCounter.incrementConnectionFailedCount(); throw new EventDeliveryException("Failed to open file " + pathController.getCurrentFile() + " while delivering event", e); &#125; &#125; Channel channel = getChannel(); Transaction transaction = channel.getTransaction(); Event event = null; Status result = Status.READY; try &#123; transaction.begin(); int eventAttemptCounter = 0; for (int i = 0; i &lt; batchSize; i++) &#123; event = channel.take(); if (event != null) &#123; sinkCounter.incrementEventDrainAttemptCount(); eventAttemptCounter++; serializer.write(event); /* * FIXME: Feature: Rotate on size and time by checking bytes * written and setting shouldRotate = true if we're past a * threshold. */ /* * FIXME: Feature: Control flush interval based on time or * number of events. For now, we're super-conservative and * flush on each write. */ &#125; else &#123; // No events found, request back-off semantics from runner result = Status.BACKOFF; break; &#125; &#125; serializer.flush(); outputStream.flush(); transaction.commit(); sinkCounter.addToEventDrainSuccessCount(eventAttemptCounter); &#125; catch (Exception ex) &#123; transaction.rollback(); throw new EventDeliveryException("Failed to process transaction", ex); &#125; finally &#123; transaction.close(); &#125; return result; &#125; @Override public void stop() &#123; logger.info("RollingFile sink &#123;&#125; stopping...", getName()); sinkCounter.stop(); super.stop(); if (outputStream != null) &#123; logger.debug("Closing file &#123;&#125;", pathController.getCurrentFile()); try &#123; serializer.flush(); serializer.beforeClose(); outputStream.close(); sinkCounter.incrementConnectionClosedCount(); &#125; catch (IOException e) &#123; sinkCounter.incrementConnectionFailedCount(); logger.error( "Unable to close output stream. Exception follows.", e); &#125; finally &#123; outputStream = null; serializer = null; &#125; &#125; if (rollInterval &gt; 0) &#123; rollService.shutdown(); while (!rollService.isTerminated()) &#123; try &#123; rollService.awaitTermination(1, TimeUnit.SECONDS); &#125; catch (InterruptedException e) &#123; logger.debug( "Interrupted while waiting for roll service to stop. " + "Please report this.", e); &#125; &#125; &#125; logger.info("RollingFile sink &#123;&#125; stopped. Event metrics: &#123;&#125;", getName(), sinkCounter); &#125; public String getDirectory() &#123; return directory; &#125; public void setDirectory(String directory) &#123; this.directory = directory; &#125; public long getRollInterval() &#123; return rollInterval; &#125; public void setRollInterval(long rollInterval) &#123; this.rollInterval = rollInterval; &#125;&#125; flume配置老版本kakfa zookeeperConnect配置1234567891011121314151617181920212223242526272829303132agent.sources = source1agent.channels = memoryChannelagent.sinks = k1#sourceagent.sources.source1.type = org.apache.flume.source.kafka.KafkaSourceagent.sources.source1.zookeeperConnect = xxxxxxxxx.com:2181agent.sources.source1.topic = custom_wireless_m_pub_loganalystsagent.sources.source1.groupId = test-group3agent.sources.source1.batchSize = 1000agent.sources.source1.batchDurationMillis = 1000#channelagent.channels.memoryChannel.type = memoryagent.channels.memoryChannel.capacity = 10000agent.channels.memoryChannel.transactionCapacity = 10000#sink#agent.sinks.k1.type = file_roll#agent.sinks.k1.channel = c1#agent.sinks.k1.sink.directory = /home/q/performance/apache-flume-1.7.0-bin/testdir/agent.sinks.k1.type = com.qnr.qav.flume.RollingFileSinkExtraagent.sinks.k1.sink.directory = /home/q/performance/shell/data/%Y%m%d%H/agent.sinks.k1.sink.filePrefix = performance.%Y%m%d%H%Magent.sinks.k1.sink.fileSuffix = .logagent.sinks.k1.sink.rollInterval = 60#assembleagent.sources.source1.channels = memoryChannelagent.sinks.k1.channel = memoryChannel 启动命令12345678910111213bin/flume-ng agent --conf conf/ --conf-file conf/file.conf --name agent -Dflume.root.logger=INFO,console &gt; run.log 2&gt;&amp;1 &amp;zookeeper-3.3.6.jarhadoop-yarn-common-2.1.0-beta.jarhadoop-yarn-api-2.1.0-beta.jarhadoop-mapreduce-client-core-2.1.0-beta.jarhadoop-common-2.2.0.jarhadoop-auth-2.2.0.jarhadoop-annotations-2.2.0.jarcommons-configuration-1.6.jarhadoop-hdfs-2.2.0.jarudf-1.0.jar put2hdfs.sh12345678910111213141516171819202122232425262728293031#!/bin/bashsource /etc/profiledt="$(date -d "$1 3 min ago " +'%Y-%m-%d')"h="$(date -d "$1 3 min ago " +'%H')"hour="$(date -d "$1 3 min ago " +'%Y%m%d%H')"min="$(date -d "$1 3 min ago " +'%Y%m%d%H%M')"pt="/home/q/performance/shell/data/$&#123;hour&#125;"chmod a+w $&#123;pt&#125;cd $ptpwdcontext=`ls $pt | grep "performance.$&#123;min&#125;"`if [ "$context" = "" ];then echo "$context not exists!!!! skip" exit 1fiecho "&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;process: $pt/$context&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;"#获取文件的大小FILE_SIZE=`ls -l $context | awk '&#123;print $5&#125;' `echo "&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;size: $FILE_SIZE&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;"#判断文件大小 如果为0直接删除if [ $FILE_SIZE -ne 0 ];then #压缩文件 gzip $context SQL=" LOAD DATA LOCAL INPATH '$&#123;pt&#125;/$&#123;context&#125;.gz' INTO TABLE orig_performance_all PARTITION (dt='$&#123;dt&#125;',hour='$&#123;h&#125;') " echo "$SQL" hive -e "use wirelessdata;$&#123;SQL&#125;;" || exit 1firm -rf $&#123;context&#125;.gzecho "&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;done&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;"exit 0 参考： https://my.oschina.net/leejun2005/blog/288136http://blog.csdn.net/wang_ying_198/article/details/51144026]]></content>
      <categories>
        <category>flume</category>
      </categories>
      <tags>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink slot]]></title>
    <url>%2F2017%2F02%2F21%2Fflink-slot%2F</url>
    <content type="text"><![CDATA[slot在flink里面可以认为是资源组，Flink是通过将任务分成子任务并且将这些子任务分配到slot来并行执行程序。每个Flink TaskManager在集群中提供处理槽。 插槽的数量通常与每个TaskManager的可用CPU内核数成比例。一般情况下你的slot数是你每个TM的cpu的核数。 1、启动Flink应用程序时，(在CLI情况下)用户可以提供用于该作业的默认插槽数，通过-p来指定并行数。1./bin/flink run -p 10 ../examples/*WordCount-java*.jar 2、在程序中可以通过env.setParallelism()来设定，此时设定的是整个程序的并行度12345678final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setParallelism(3); DataStream&lt;String&gt; text = [...]DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCounts = [...]wordCounts.print(); env.execute(&quot;Word Count Example&quot;); 3、也可以为单个操作符(Operator)设置编程API中的slot数目123456789101112final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStream&lt;String&gt; text = [...]DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wordCounts = text .flatMap(new LineSplitter()) .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1).setParallelism(5); wordCounts.print(); env.execute(&quot;Word Count Example&quot;); 4、可以在Flink的配置文件里面去设置这个值(flink-conf.yaml)1parallelism.default property 参考https://blog.csdn.net/a6822342/article/details/77531000]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[richfunction 广播变量]]></title>
    <url>%2F2017%2F02%2F21%2Frichfunction-%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435package com.examples.infoworld.helloword;import org.apache.flink.api.common.functions.RichMapFunction;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.configuration.Configuration;import java.util.Collection;public class RichfunctionTest2 &#123; public static void main(String[] args) throws Exception &#123; ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet&lt;Integer&gt; ds1 = env.fromElements(1,2,3 ); DataSet&lt;String&gt; ds2 = env.fromElements(&quot;a&quot;,&quot;b&quot; ); DataSet&lt;String&gt; ds3 = ds2.map(new RichMapFunction&lt;String, String&gt;() &#123; private Collection&lt;Integer&gt; broadcastSet; @Override public void open(Configuration conf) throws Exception &#123; broadcastSet = getRuntimeContext().getBroadcastVariable(&quot;ds1&quot;); &#125; @Override public String map(String s) throws Exception &#123; for (Integer i:broadcastSet)&#123; System.out.println(i); &#125; return s; &#125; &#125;).withBroadcastSet(ds1,&quot;ds1&quot;); ds3.print(); &#125;&#125;]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[richfunction 传递参数]]></title>
    <url>%2F2017%2F02%2F21%2Frichfunction-%E4%BC%A0%E9%80%92%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.examples.infoworld.helloword;/** *RichFuction除了提供原来MapFuction的方法之外，还提供open, close, getRuntimeContext 和setRuntimeContext方法， * 这些功能可用于参数化函数（传递参数），创建和完成本地状态，访问广播变量以及访问运行时信息以及有关迭代中的信息 * */import org.apache.flink.api.common.functions.RichFilterFunction;import org.apache.flink.api.java.DataSet;import org.apache.flink.api.java.ExecutionEnvironment;import org.apache.flink.configuration.Configuration;public class RichfunctionTest &#123;// ①传递参数 public static void main(String[] args) throws Exception &#123; //基于DataSet而非DataStream ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); DataSet&lt;Integer&gt; ds = env.fromElements(1, 2, 3); Configuration conf = new Configuration(); conf.setInteger(&quot;limit&quot;,2); DataSet&lt;Integer&gt; ds1 = ds.filter(new RichFilterFunction&lt;Integer&gt;() &#123; //获取limit private int limit; // rich function新增方法 @Override public void open(Configuration conf) throws Exception &#123; limit = conf.getInteger(&quot;limit&quot;, 0); &#125; //map operator默认方法 @Override public boolean filter(Integer integer) throws Exception &#123; return integer &gt; limit; &#125; &#125;).withParameters(conf);//可以将Configuration中的limit参数的值传递进RichFuction里面，通过后面withParameters方法传递进去 ds1.print();//从configuration中获取了limit的值，并设定了fliter的阈值是2，从而过滤了1，2 &#125;&#125;]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java scala混排]]></title>
    <url>%2F2017%2F02%2F21%2Fjava-scala%E6%B7%B7%E6%8E%92%2F</url>
    <content type="text"><![CDATA[12345678910111213141516package com.examples.infoworld.helloword;public class Cal &#123; public int add(int a,int b)&#123; return a+b; &#125;&#125;import com.dataartisans.examples.infoworld.helloword.Calobject TestApp &#123; def main(args: Array[String]): Unit = &#123; print(new Cal().add(2,4)) &#125;&#125;]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wc]]></title>
    <url>%2F2017%2F02%2F21%2Fwc%2F</url>
    <content type="text"><![CDATA[java v112345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.examples.infoworld.helloword;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.util.Collector;public class wc &#123; public static void main(String[] args) throws Exception&#123; //环境 final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //source DataStreamSource&lt;String&gt; lines = env.socketTextStream(&quot;localhost&quot;, 9999); //transformation SingleOutputStreamOperator&lt;WC&gt; sum = lines .flatMap(new FlatMapFunction&lt;String, WC&gt;() &#123; @Override public void flatMap(String s, Collector&lt;WC&gt; collector) throws Exception &#123; for (String word : s.split(&quot;,&quot;)) &#123; collector.collect(new WC(word, 1L)); &#125; &#125; &#125;) .keyBy(&quot;word&quot;) .timeWindow(Time.seconds(4), Time.seconds(2)) .sum(&quot;count&quot;); sum.print().setParallelism(1); env.execute(&quot;wc&quot;); &#125; public static class WC&#123; private String word; private Long count; public WC(String word, Long count) &#123; this.word = word; this.count = count; &#125; public WC() &#123; &#125; public String getWord() &#123; return word; &#125; public void setWord(String word) &#123; this.word = word; &#125; public Long getCount() &#123; return count; &#125; public void setCount(Long count) &#123; this.count = count; &#125; @Override public String toString() &#123; return &quot;WC&#123;&quot; + &quot;word=&apos;&quot; + word + &apos;\&apos;&apos; + &quot;, count=&quot; + count + &apos;&#125;&apos;; &#125; &#125;&#125; 注意引包是注意区分flink-streaming-java和flink-streaming-scala的区别，此工程为java工程所以只需要flink-streaming-java nc -lk 9999先启动，然后在启动工程 java v21234567891011121314151617181920212223242526272829303132333435package com.examples.infoworld.helloword;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.windowing.time.Time;import org.apache.flink.util.Collector;import scala.Tuple2;public class wc2 &#123; public static void main(String[] args) &#123; ParameterTool tool = ParameterTool.fromArgs(args); int port = tool.getInt(&quot;port&quot;,9999); String hostname = tool.get(&quot;host&quot;,&quot;localhost&quot;); //环境 final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //source DataStreamSource&lt;String&gt; lines = env.socketTextStream(hostname, port); //代替wcl类，Tuple1表示一个参数，Tuple2表示两个参数，。。。。 lines.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String,Integer&gt;&gt;() &#123; @Override public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) throws Exception &#123; for (String word:s.split(&quot;,&quot;))&#123; out.collect(new Tuple2&lt;String,Integer&gt;(word,1)); &#125; &#125; &#125;) .keyBy(0) .timeWindow(Time.seconds(4),Time.seconds(1)) .sum(1); &#125;&#125; scala12345678910111213141516171819202122232425262728293031package com.test.scala//隐式转换import org.apache.flink.streaming.api.scala._import org.apache.flink.streaming.api.windowing.time.Timeobject ScalaWc &#123; def main(args: Array[String]): Unit = &#123; //step 1 创建env val env = StreamExecutionEnvironment.getExecutionEnvironment //step2 创建source val lines = env.socketTextStream(&quot;localhost&quot;,9999) //step3 transformations val results = lines.flatMap(x =&gt; x.split(&quot;,&quot;)) .map(x =&gt; wc(x,1)) .keyBy(&quot;word&quot;) .timeWindow(Time.seconds(4),Time.seconds(2)) .sum(&quot;count&quot;) //step4 sink results.print().setParallelism(1) //step exe env.execute(&quot;ScalaWc&quot;) &#125; case class wc(word:String ,count:Long)&#125;]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[传数据]]></title>
    <url>%2F2017%2F02%2F20%2F%E4%BC%A0%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[1）进入待分享的目录2）执行命令python -m SimpleHTTPServer 端口号 注意：不填端口号则默认使用8000端口。3）浏览器访问该主机的地址：http://IP:端口号/python -m SimpleHTTPServer 8000 wget –limit-rate=30m -r http://hostname:8000/xxx.jar]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网卡命令]]></title>
    <url>%2F2017%2F02%2F20%2F%E7%BD%91%E5%8D%A1%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[ip -f inet addr show 参考：http://www.361way.com/ifconfig-ip-ip/1835.html]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Table is neither enables nor disabled in HBase]]></title>
    <url>%2F2017%2F02%2F20%2FTable-is-neither-enables-nor-disabled-in-HBase%2F</url>
    <content type="text"><![CDATA[问题：操作hbase表时报以下错误12345678Attached is the Exeption I got: hbase(main):002:0&gt; disable &apos;x&apos;ERROR: org.apache.hadoop.hbase.TableNotEnabledException: org.apache.hadoop.hbase.TableNotEnabledException: x at org.apache.hadoop.hbase.master.handler.DisableTableHandler.(DisableTableHandler.java:75) at org.apache.hadoop.hbase.master.HMaster.disableTable(HMaster.java:1154) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364) at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1336)Here is some help for this command: Start disable of named table: e.g. &quot;hbase&gt; disable &apos;t1&apos;&quot;hbase(main):003:0&gt; enable &apos;x&apos;ERROR: org.apache.hadoop.hbase.TableNotDisabledException: org.apache.hadoop.hbase.TableNotDisabledException: x at org.apache.hadoop.hbase.master.handler.EnableTableHandler.(EnableTableHandler.java:74) at org.apache.hadoop.hbase.master.HMaster.enableTable(HMaster.java:1142) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364) at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1336) 解决方案：1、登陆zk，然后将这个表信息进行删除delete /hbase/xxxx/table/atpco:ttf_addon2、重启master，切换master]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[master和regionserver时间不同步]]></title>
    <url>%2F2017%2F02%2F20%2Fmaster%E5%92%8Cregionserver%E6%97%B6%E9%97%B4%E4%B8%8D%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[regionserver错误日志中出现：122014-12-05 11:26:17,119 INFO [regionserver60020] regionserver.HRegionServer: STOPPED: Unhandled: org.apache.hadoop.hbase.ClockOutOfSyncException: Server hostname,60020,1417749974947 has been rejected; Reported time is too far out of sync with master. Time difference of 41562ms &gt; max allowed of 30000ms hbase.master.maxclockskew参数决定了master和regionserver时间同步的最大差值 使用ntpdate重新同步时钟即可]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase慢查询日志分析]]></title>
    <url>%2F2017%2F02%2F20%2Fhbase%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[当hbase出现慢查询时，可能会出现这么一个warning日志，具体如下12015-08-08 09:39:32,532 WARN [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): &#123;&quot;processingtimems&quot;:40939,&quot;call&quot;:&quot;Scan(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanRequest)&quot;,&quot;client&quot;:&quot;xxx.xxx.xxx.xxx:53959&quot;,&quot;starttimems&quot;:1438997931593,&quot;queuetimems&quot;:0,&quot;class&quot;:&quot;HRegionServer&quot;,&quot;responsesize&quot;:65,&quot;method&quot;:&quot;Scan&quot;&#125; 其中processingtimems表示该慢查询执行的时间，单位是msstarttimems表示该慢查询开始的时间，因为精确到ms，所以可以去除后3位来进行时间格式转换client表示该查询来源ip，通过这个ip可以查出相关接口人，联系处理method表示该慢查询执行的方法，例如上面的例子表示的是scan操作 对于慢查询，可以通过参数hbase.ipc.warn.response.time调整执行多长时间的慢查询才打印出日志，默认是10秒]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用ImportTsv来进行批量数据导入]]></title>
    <url>%2F2017%2F02%2F20%2F%E4%BD%BF%E7%94%A8ImportTsv%E6%9D%A5%E8%BF%9B%E8%A1%8C%E6%89%B9%E9%87%8F%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%2F</url>
    <content type="text"><![CDATA[前期准备，hadoop需要开启MapReduce 第一种方法一、操作步骤1、创建tsv文件，文件格式如下abc|siaogisss|gaiogo 2、将tsv文件put到hdfs中./bin/hdfs dfs -put a.tsv /tmp/a.tsv 3、使用ImportTsv来进行导入1./bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=&apos;|&apos; -Dimporttsv.columns=HBASE_ROW_KEY,cf:a t /tmp/a.tsv 如果使用客户端，在hbase的conf下面建了一个yarn-site.xml123456789101112131415&lt;?xml version=&quot;1.0&quot;?&gt;&lt;configuration&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;&lt;value&gt;hostname:8032&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;&lt;value&gt;hostname:8030&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;&lt;value&gt;hostname:8031&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 把这些拷进去就可以了1sudo -uhadoop ./bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=&quot;,&quot; -Dimporttsv.columns=HBASE_ROW_KEY,contact_info:pc user_tag1 /home/q/user_tag/user_test.tsv 二、线上导入的案例命令 表结构为create &#39;algo_router:user_tag&#39;,{NAME =&gt; &#39;contact_info&#39;},{NAME =&gt; &#39;contact_tag&#39;}导入语句为1./bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=&quot;,&quot; -Dimporttsv.columns=HBASE_ROW_KEY,contact_info:rate,contact_info:price,contact_tag:tag &apos;algo_router:user_tag&apos; /temp/user_tag.tsv 第二种方法（先生成HFILE文件再导入）假设datatsv，数据如下row1,a,crow2,c,d 生成HFILE文件1./bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=&apos;,&apos; -Dimporttsv.columns=HBASE_ROW_KEY,cf:c1,cf:c2 -Dimporttsv.bulk.output=/output table_pucong hdfs://mycluster:8020/tmp/datatsv 执行完结果显示1234./bin/hdfs dfs -ls /outputFound 2 itemsrw-rr- 1 hadoop supergroup 0 2015-08-10 22:08 /output/_SUCCESSdrwxr-xr-x - hadoop supergroup 0 2015-08-10 22:08 /output/cf 将生成的FILE文件转移成hbase的table1./bin/hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles hdfs://mycluster:8020/output table_pucong]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase的export 和import]]></title>
    <url>%2F2017%2F02%2F20%2Fhbase%E7%9A%84export-%E5%92%8Cimport%2F</url>
    <content type="text"><![CDATA[hbase org.apache.hadoop.hbase.mapreduce.Exporthbase org.apache.hadoop.hbase.mapreduce.Import Export是以表为单位导出数据的，若想完成整库的备份需要分别各个表都要执行本地hdfs转储 1、先将t表导出到hdfs的 /home/q/opdir的目录下1[hadoop@hostname /home/q/hbase/hbase-0.98.1-cdh5.1.0]$ ./bin/hbase org.apache.hadoop.hbase.mapreduce.Export t /home/q/opdir 2、查看hdfs上的文件是否导出1234[hadoop@hostname /home/q/hadoop/hadoop-2.3.0-cdh5.1.0]$ ./bin/hdfs dfs -ls /home/q/opdirFound 2 items-rw-r-r- 1 hadoop supergroup 0 2014-11-03 16:40 /home/q/opdir/_SUCCESS-rw-r-r- 1 hadoop supergroup 161 2014-11-03 16:40 /home/q/opdir/part-m-0000 3、创建一样结构的表t1 远程hdfs转储1[hadoop@hostname /home/q/hbase/hbase-0.98.1-cdh5.1.0]$ ./bin/hbase org.apache.hadoop.hbase.mapreduce.Export t hdfs://hostname:8020/home/opdir 4、将数据导入表t1[hadoop@hostname /home/q/hbase/hbase-0.98.1-cdh5.1.0]$ ./bin/hbase org.apache.hadoop.hbase.mapreduce.Import t /home/opdir]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[merge_region操作]]></title>
    <url>%2F2017%2F02%2F20%2Fmerge-region%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[通常只有相邻的region才能进行merge如果region的名称为TestTable,0094429456,1289497600452.527db22f95c8a9e0116f0cc13c680396. 那么region名称的encoded是527db22f95c8a9e0116f0cc13c680396 操作举例有2个region相邻12bookingInfo:booking_info_20150204,bi_2015-02-04~F5ZzQcv_-3QpLobklS2nPcc~EG,1423021249808.d5802b3cd2c50aaf332023d9f864d279. bookingInfo:booking_info_20150204,bi_2015-02-04~UppNqcpGtZInVxJNCXcpiq8~3,1423021284600.c4a2fc692d60fa803200de5a6cf6887d. 执行合并操作12hbase(main):006:0&gt; merge_region &apos;d5802b3cd2c50aaf332023d9f864d279&apos;,&apos;c4a2fc692d60fa803200de5a6cf6887d&apos;0 row(s) in 0.0540 seconds 合并成了1bookingInfo:booking_info_20150204,bi_2015-02-04~F5ZzQcv_-3QpLobklS2nPcc~EG,1437883624733.6197e75861aef012a52396e8cd9fc33b.]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase私有云]]></title>
    <url>%2F2017%2F02%2F20%2FHBase%E7%A7%81%E6%9C%89%E4%BA%91%2F</url>
    <content type="text"><![CDATA[这里讲给大家一步一步的介绍组成HBase私有云的各个组件，以方便大家理解每个组件的作用。 初始集群状态假设我们存在六台实体机，分为master和node两组，每组各三台机器。 其中master机器包含：Mesos主实例、Zookeeper实例、Marathon实例、Docker实例 而node机器包含：Mesos从实例、Docker实例 下面我们依次介绍每个组件，以及添加依次添加组件之后数据中心的状态。 ZookeeperZookeeper属于我们系统中的一大重要组成部分。它将帮助我们构建集群并允许来自Mesos生态系统中的其它应用程序，比如mesos-master启动需要指定zk地址、mesos-slave、marathon启动需要指定mesos-master的znode。 Zookeeper的安装过程非常简单，而且不需要什么技巧。 添加了zk之后，数据中心的状态更新为： MesosMesos是Apache下的开源分布式资源管理框架，为了充分利用数据中心（上述六台机器）的资源（譬如为不同的任务分配不同资源，按任务优先级分配资源等），我们就需要一个工具来进行整个数据中心资源的管理、分配等， 这个工具就是 Mesos，它被称为是分布式系统的内核。 我们在master节点启动mesos-master实例，在node节点启动mesos-slave实例。 在Mesos安装完成后，大家可以审视其Web UI并尝试点击其中的按钮：http://master-ip:5050。 需要注意的是，如果当前主机并非Mesos Master的集群主节点，那么UI会将大家重新定向至主节点主机。 添加了mesos之后，数据中心的状态更新为： Docker我觉得简单来说，Docker就是一个具有资源隔离功能的应用程序执行容器。 该步骤之后，数据中心的状态更新为： Marathon现在我们的私有云有了资源管理的功能，也有了执行应用程序的容器，但是至今为止我们还不知道如何在资源管理的基础上，在docker中运行一个应用程序，而这时就需要Marathon了，如果说mesos是分布式集群的内核，那么marathon就是分布式集群的init系统。 Marathon还拥有一套Web UI，大家可以通过URL: http://master-ip:8080;进行访问。 自动化部署现在整个HBase私有云架构已经搭建完成，接下来就是关于自动化部署的内容了，关于这部分内容我们写了一个脚本，详情请见子页面。]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase建表须知]]></title>
    <url>%2F2017%2F02%2F20%2Fhbase%E5%BB%BA%E8%A1%A8%E9%A1%BB%E7%9F%A5%2F</url>
    <content type="text"><![CDATA[建表示例：create ‘namespace:tablename’, {NUMREGIONS =&gt; 5, SPLITALGO =&gt; ‘HexStringSplit’}, {NAME =&gt; ‘f’, COMPRESSION=&gt;’SNAPPY’, VERSIONS =&gt; 1}提建表邮件时，建表语句请在测试环境测试成功后，再提交。 建表心得hbase号称支持上亿行，上百万列数据的实时随机读写。但是拥有这样功能的前提是要有良好的表结构设计，否则使用hbase反倒会适得其反。 下面是我们的一些hbase表设计心得，供大家参考。 rowkey的设计,尽量使用较短的family长度，最好一个字符hbase是一个key-value数据库，大家初次使用hbase时，往往会查一些博客等资料，逻辑上看它一行可以包含多个列，列数可以不固定，这是hbase的一个优势。有一点需要注意：hbase虽然一个rowkey可以对应很多列（column），但是由于key-value数据库的性质，每存一列，rowkey就同样也要再存一次，hbase也不例外。如下表： 下面的是简略表，真正存储比这个要复杂，大致意思明白就行 逻辑视图： 物理存储格式： row和family会多次存放，会有数据冗余，所以只要在能够区分family的情况下，family越小越好。而且这样的结构也表明：数据放在rowkey里面，或者放在value里面，在存储空间的占用上区别并不大，主要就是访问方式的区别，所以应该根据自己的业务需求，设计适合自己的rowkey。设计表时，尽量避免单行过大的情况，如果单行过大的话，访问该行有可能导致regionserver卡死的情况。 数据量超过100G时，进行预分区 pre-split为什么要预分区： hbase的rowkey是全局有序的，hbase将它的数据分散在各个regionserver中，每个regionserver的数据都存放一部分数据，而且有一个startKey和endKey，只有这个范围内的数据才会被这个regionserver存放。而且各个regionserver的startKey和endKey是没有重叠的。 最开始hbase的表是只有一个region的，首先数据不断的写入该region，当数据量到达一定程度后，region会进行split操作，分裂为2个region，然后各自负责各自的rowkey范围。 可以看到：假如要迁移到新集群，创建了一个表，默认只有一个region，往hbase中导数据时，开始的时候只会不断的往一个region里面写，这样这个region就成为一个单点问题。 希望最开始也能同时往多个机器中写数据：pre-split：可以预先定义region的个数，以及它们的starKey和rowKey，这样就可以均衡的将压力分散到各个机器上。pre-split算法： 目前hbase支持两种split算法：HexStringSplit 和 UniformSplit。 HexStringSplit：主要针对16进制开头的rowkey，可以将这类数据均匀分散，例如采用md5等hash算法作为前缀的row。 UniformSplit ：主要针对分布均匀的二进制数组。region个数： 目前公司的hbase单个region设置100G,如果预测表的数据量较大的话，预先分一部分region，保证能够将压力分散到各个节点，region初始设置为5个即可。 压缩hbase的压缩效果非常好，目前有两种压缩方案：compression和Data Block Encoding Compression：这种压缩针对数据块进行压缩，压缩方式一般采用SNAPPY，压缩比例可以达到70%，而且对解压缩开销很小，大大降低了磁盘使用。 Data Block Encoding：这种是针对rowkey的压缩，比如之前提到的，列数较多的话，row会多次存放，这种方案会将rowkey中相同的前缀进行压缩，类似字典树（trie-tree）。 虽然从压缩率、CPU利用率以及性能上来看，prefix_tree编码确实会比snappy压缩更加优秀。但线上遇到了很大的坑：compaction一直卡住，详见HBASE-12959，还可能造成scan miss，详见HBASE-12817。该功能目前还属于实验性质特性-experimental feature。鉴于安全考虑，prefix_tree功能建议不要设置,即禁止使用DATA_BLOCK_ENCODING =&gt; ‘PREFIX_TREE’，使用NONE。 不推荐使用多版本hbase虽然支持多版本，但是这个功能有些鸡肋，默认最大可以将版本数设置为Integer.MAX_VALUE，但是如果真的插入了接近该数量的版本，会有一些风险。比如： compaction的时候很可能会out of memory单个rowkey数据过多的时候，region不会进行split。]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase现有的备份方案介绍]]></title>
    <url>%2F2017%2F02%2F20%2Fhbase%E7%8E%B0%E6%9C%89%E7%9A%84%E5%A4%87%E4%BB%BD%E6%96%B9%E6%A1%88%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[原文连接：http://blog.cloudera.com/blog/2013/11/approaches-to-backup-and-disaster-recovery-in-hbase/ 下面是部分翻译： title: hbase的备份和容灾 purpose：了解hbase现有的备份方案，以及如何在不同的场景下恢复数据 随着hbase的广泛使用，越来越多的企业使用hbase作为其数据存储，但是能够保证PB级数据的及时恢复，hadoop和hbase提供了一些内在机制来满足这些功能。 以往都是用户都是在一个比较抽象，高层的角度来操作备份，读了这篇文章，你应该能够更专业的来为你的业务定制最合适的备份策略。本文帮助你了解各个备份的优缺点和它们的使用场景。 有以下几种备份方式： snapshots replication export copytable Htable API offline backup of HDFS data 下面是这几种方法的简单的比较： snapshotssnapshots功能齐全，而且不需要停掉集群。下面的文章有详细的解释：https://blog.cloudera.com/blog/2013/03/introduction-to-apache-hbase-snapshots/这里大致说一下：snapshot是通过创建HDFS文件系统上，某个时刻的linux硬连接来实现的。可以在秒级内完成，几乎对集群没有性能影响，占用非常小的数据空间。数据并没有存放多份， 而是仅仅编录在一些小的元文件里，这些元文件能够让你恢复数据到那个时刻。（这句翻译不大懂） 运行如下命令就可以创建snapshots：1hbase(main):001:0&gt; snapshot &apos;myTable&apos;, &apos;MySnapShot&apos; 之后你会发现在/hbase/.snapshot/mytable会有很多小文件，它们就是snapshots的文件，要执行恢复操作：123hbase(main):002:0&gt; disable &apos;myTable&apos;hbase(main):003:0&gt; restore_snapshot &apos;MySnapShot&apos;hbase(main):004:0&gt; enable &apos;myTable&apos; 执行snapshot需要disable表，执行之后，任何在这个时间点之后的操作都会清除。所以如果需要，请使用exportSnapshot来备份你的数据。 snapshot只是记录是个时间点表的状态，并不支持增量snapshot。 Replicationreplication也是一个并不占用多少资源的备份工具，详细请看：http://blog.cloudera.com/blog/2012/07/hbase-replication-overview-2/replication可以精细到family级别，在后台工作，并且保证主从集群之间的数据一致。 它有3种模式：主从、主主、循环（cyclic）。你可以灵活的从多个数据源来注入数据到你的数据中心，也可以确保你的数据在其他从集群上备份。 如果发生集群宕机，可以通过DNS工具来将访问重定位到从节点。 replication是强大的，保证容错的，并提供了最终一致性（eventual consistency），意味着：可能某一时刻数据并不一致，但最终会一致的。 note：对于已经存在的表，你应该首先采用上述的某一个方案来将源数据拷贝到从集群，replication只有在你启动之后才会生效。 Exportexport是hbase内置的一个工具，它会启动mapreduce程序，并且使用hbase的api来逐条的读取hbase中的数据，并将它们存到你指定的HDFS的文件中。这个工具比较占用资源，因为启动了MR，但是功能很齐全，因为可以使用hbase的api，可以指定版本和范围，因此可以支持增量备份。 简单的例子：1hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; export之后，你可以把导出的文件转移到任何地方，更方便的是你可以直接指定outputdir为一个远程路径，export将直接通过网络将数据导到那。 CopyTable详情：http://blog.cloudera.com/blog/2012/06/online-hbase-backups-with-copytable-2/copytable 和export很像，有一个关键的不同：copytable可以直接将主集群中的表数据直接导入到backup集群中的backup表中。 简单的例子：1hbase org.apache.hadoop.hbase.mapreduce.CopyTable --new.name=testCopy test 该指令将一个名为”test“表的数据复制到同一集群中的”testCopy“表中。（也可以远程复制） 注意一下几点：它是通过put来操作，会写从节点的momstore，会flush，也会出发compaction，GC，而且主集群也是运行mapreduce，也会占用资源， 所以当要拷贝大量数据时，这种方法不是很合适。 API方式这个不用多说了，自己写，自己调，性能好坏和程序员水准有关。 离线通过HDFS文件备份（严重不推荐）这是最野蛮，破坏性的备份，并且会占用大量的数据空间。关闭hbase集群，它是直接将/hbase下的所有文件拷贝出来。并且这样肯定不支持增量备份。而且在你恢复数据时，还要对META表进行修复，所以这个方法不推荐。 hbase的容灾主要有以下需求：1.数据中心挂了。 2.有误删操作，要恢复 3.恢复到某一个时间点，来进行审计 如果使用Replication时，采用DNS工具可以使自动切换到从集群，如果住集群好了，那么要保证在宕机之后从集群的更新数据同步到主集群， 主主方式和循环方式已经自动帮你做了，主从模式需要你自己手动来做。 若采用replication来保证备份，那么假如主节点一开始就有数据，在批量导出的过程中，如何保证这个过程的数据同步到从节点？]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YARN 作业执行流程]]></title>
    <url>%2F2017%2F02%2F19%2FYARN-%E4%BD%9C%E4%B8%9A%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[YARN 作业执行流程： 用户向 YARN 中提交应用程序，其中包括 MRAppMaster 程序，启动 MRAppMaster 的命令， 用户程序等。 ResourceManager 为该程序分配第一个 Container，并与对应的 NodeManager 通讯，要求 它在这个 Container 中启动应用程序 MRAppMaster。 MRAppMaster 首先向 ResourceManager 注册，这样用户可以直接通过 ResourceManager 查看应用程序的运行状态，然后将为各个任务申请资源，并监控它的运行状态，直到运行结束，重复 4 到 7 的步骤。 MRAppMaster 采用轮询的方式通过 RPC 协议向 ResourceManager 申请和领取资源。 一旦 MRAppMaster 申请到资源后，便与对应的 NodeManager 通讯，要求它启动任务。 NodeManager 为任务设置好运行环境（包括环境变量、JAR 包、二进制程序等）后，将 任务启动命令写到一个脚本中，并通过运行该脚本启动任务。 各个任务通过某个 RPC 协议向 MRAppMaster 汇报自己的状态和进度，以让 MRAppMaster 随时掌握各个任务的运行状态，从而可以在任务败的时候重新启动任务。 应用程序运行完成后，MRAppMaster 向 ResourceManager 注销并关闭自己。]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yarn资源优化]]></title>
    <url>%2F2017%2F02%2F19%2Fyarn%E8%B5%84%E6%BA%90%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[在hadoop 2.x中，引入了Yarn架构做资源管理，在每个节点上面运行NodeManager负责节点资源的分配，而slot也不再像1.x那样区分Map slot和Reduce slot。在Yarn上面Container是资源的分配的最小单元。yarn参数配置文件为yarn-site.xml 内存资源的调度和隔离12345678yarn.nodemanager.resource.memory-mb 设置每个节点的可用内存，单位MB。合理设置该参数，将影响到DataNode的运行情况。yarn.nodemanager.vmem-pmem-ratio 任务每使用1MB物理内存，最多可使用虚拟内存量，默认是2.1。 yarn.nodemanager.pmem-check-enabled 是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true。yarn.nodemanager.vmem-check-enable 是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true。yarn.scheduler.minimum-allocation-mb 分配给AM单个容器可申请的最小内存，默认1024MB，如果一个任务申请的物理内存量少于该值，则该对应的值改为这个数。yarn.scheduler.maximum-allocation-mb 分配给AM单个容器可申请的最大内存，默认8192MB 默认情况下，YARN采用了线程监控的方法判断任务是否超量使用内存，一旦发现超量，则直接将其杀死。由于Cgroups对内存的控制缺乏灵活性（即任务任何时刻不能超过内存上限，如果超过，则直接将其杀死或者报OOM），而Java进程在创建瞬间内存将翻倍，之后骤降到正常值，这种情况下，采用线程监控的方式更加灵活（当发现进程树内存瞬间翻倍超过设定值时，可认为是正常现象，不会将任务杀死），因此YARN未提供Cgroups内存隔离机制。 CPU资源的调度和隔离在YARN中，CPU资源的组织方式仍在探索中，目前（2.2.0版本）只是一个初步的，非常粗粒度的实现方式，更细粒度的CPU划分方式已经提出来了，正在完善和实现中。目前的CPU被划分成虚拟CPU（CPU virtual Core），这里的虚拟CPU是YARN自己引入的概念，初衷是，考虑到不同节点的CPU性能可能不同，每个CPU具有的计算能力也是不一样的，比如某个物理CPU的计算能力可能是另外一个物理CPU的2倍，这时候，你可以通过为第一个物理CPU多配置几个虚拟CPU弥补这种差异。用户提交作业时，可以指定每个任务需要的虚拟CPU个数。123yarn.nodemanager.resource.cpu-vcores 设置每个节点虚拟cpu内核数,注意，目前推荐将该值设值为与物理CPU核数数目相同。如果你的节点CPU核数不够8个，则需要调减小这个值，而YARN不会智能的探测节点的物理CPU总数。 yarn.scheduler.minimum-allocation-vcores 单个任务可申请的最小虚拟CPU个数，默认是1，如果一个任务申请的CPU个数少于该数，则该对应的值改为这个数。yarn.scheduler.maximum-allocation-vcores 单个任务可申请的最多虚拟CPU个数，默认是4。 默认情况下，YARN是不会对CPU资源进行调度的，你需要配置相应的资源调度器让你支持。默认情况下，NodeManager不会对CPU资源进行任何隔离，你可以通过启用Cgroups让你支持CPU隔离。1yarn.nodemanager.resource.detect-hardware-capabilities 是否自动检测节点的CPU和内存 默认为false]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive的桶]]></title>
    <url>%2F2017%2F02%2F19%2Fhive%E7%9A%84%E6%A1%B6%2F</url>
    <content type="text"><![CDATA[对于每一个表（table）或者分区， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。 把表（或者分区）组织成桶（Bucket）有两个理由： （1）获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。 （2）使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。 在建立桶之前，需要设置hive.enforce.bucketing属性为true，使得hive能识别桶。12345CREATE TABLE bucketed_user(id INT,name String)CLUSTERED BY (id) INTO 4 BUCKETS; 我们使用用户ID来确定如何划分桶(Hive使用对值进行哈希并将结果除 以桶的个数取余数).输出4个文件。 分区中的数据可以被进一步拆分成桶，bucket，不同于分区对列直接进行拆分，桶往往使用列的哈希值进行数据采样。 在分区数量过于庞大以至于可能导致文件系统崩溃时，建议使用桶。注意，hive使用对分桶所用的值进行hash，并用hash结果除以桶的个数做取余运算的方式来分桶，保证了每个桶中都有数据，但每个桶中的数据条数不一定相等。 分桶比分区，更高的查询效率对于map端连接的情况，两个表以相同方式划分桶。处理左边表内某个桶的 mapper知道右边表内相匹配的行在对应的桶内。因此，mapper只需要获取那个桶 (这只是右边表内存储数据的一小部分)即可进行连接。这一优化方法并不一定要求 两个表必须桶的个数相同，两个表的桶个数是倍数关系也可以。用HiveQL对两个划分了桶的表进行连接.桶中的数据可以根据一个或多个列另外进行排序(增加sort by)。由于这样对每个桶的连接变成了高效的归并排序(merge-sort), 因此可以进一步提升map端连接的效率。 12345678create table student(id int,age int,name string)partitioned by (stat_date string)clustered by (id) sorted by(age) into 2 bucketrow format delimited fields terminated by &apos;,&apos;; 如果不使用set hive.enforce.bucketing=true这项属性，我们需要显式地声明set mapred.reduce.tasks=100来设置Reducer的数量，主要结尾要加上clustered by (xxx)。 查看sampling数据12345678910hive&gt; select * from student1 tablesample(bucket 1 out of 2 on id);Total MapReduce jobs = 1Launching Job 1 out of 1.......OK4 18 mac 201208022 21 ljz 201208026 23 symbian 20120802Time taken: 20.608 seconds 注：tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y)y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了64份，当y=32时，抽取(64/32=)2个bucket的数据，当y=128时，抽取(64/128=)1/2个bucket的数据。x表示从哪个bucket开始抽取。例如，table总bucket数为32，tablesample(bucket 3 out of 16)，表示总共抽取（32/16=）2个bucket的数据，分别为第3个bucket和第（3+16=）19个bucket的数据。参考：https://www.cnblogs.com/xiohao/p/6429305.html]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hfile格式]]></title>
    <url>%2F2017%2F02%2F18%2FHfile%2F</url>
    <content type="text"><![CDATA[HBase的数据以KeyValue(Cell)的形式顺序的存储在HFile中，在MemStore的Flush过程中生成HFile，由于MemStore中存储的Cell遵循相同的排列顺序，因而Flush过程是顺序写，我们直到磁盘的顺序写性能很高，因为不需要不停的移动磁盘指针。 HFile参考BigTable的SSTable和Hadoop的TFile实现，从HBase开始到现在，HFile经历了三个版本，其中V2在0.92引入，V3在0.98引入。 V1V1的HFile由多个Data Block、Meta Block、FileInfo、Data Index、Meta Index、Trailer组成。 Data Block是HBase的最小存储单元，在前文中提到的BlockCache就是基于Data Block的缓存的。一个Data Block由一个魔数和一系列的KeyValue(Cell)组成，魔数是一个随机的数字，用于表示这是一个Data Block类型，以快速监测这个Data Block的格式，防止数据的破坏。Data Block的大小可以在创建Column Family时设置(HColumnDescriptor.setBlockSize())，默认值是64KB，大号的Block有利于顺序Scan，小号Block利于随机查询，因而需要权衡。 Meta块是可选的 FileInfo是固定长度的块，它纪录了文件的一些Meta信息，例如：AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY等。 Data Index和Meta Index纪录了每个Data块和Meta块的其实点、未压缩时大小、Key(起始RowKey？)等。 Trailer纪录了FileInfo、Data Index、Meta Index块的起始位置，Data Index和Meta Index索引的数量等。其中FileInfo和Trailer是固定长度的。 KeyValueHFile里面的每个KeyValue对就是一个简单的byte数组。但是这个byte数组里面包含了很多项，并且有固定的结构。我们来看看里面的具体结构： 开始是两个固定长度的数值，分别表示Key的长度和Value的长度。紧接着是Key，开始是固定长度的数值，表示RowKey的长度，紧接着是 RowKey，然后是固定长度的数值，表示Family的长度，然后是Family，接着是Qualifier，然后是两个固定长度的数值，表示Time Stamp和Key Type（Put/Delete）。Value部分没有这么复杂的结构，就是纯粹的二进制数据了。随着HFile版本迁移，KeyValue(Cell)的格式并未发生太多变化，只是在V3版本，尾部添加了一个可选的Tag数组。 HFileV1版本的在实际使用过程中发现它占用内存多，并且Bloom File和Block Index会变的很大，而引起启动时间变长。其中每个HFile的Bloom Filter可以增长到100MB，这在查询时会引起性能问题，因为每次查询时需要加载并查询Bloom Filter，100MB的Bloom Filer会引起很大的延迟；另一个，Block Index在一个HRegionServer可能会增长到总共6GB，HRegionServer在启动时需要先加载所有这些Block Index，因而增加了启动时间。为了解决这些问题，在0.92版本中引入HFileV2版本： V2在这个版本中，Block Index和Bloom Filter添加到了Data Block中间，而这种设计同时也减少了写的内存使用量；另外，为了提升启动速度，在这个版本中还引入了延迟读的功能，即在HFile真正被使用时才对其进行解析。 对HFileV2格式具体分析，它是一个多层的类B+树索引，采用这种设计，可以实现查找不需要读取整个文件： Data Block中的Cell都是升序排列，每个block都有它自己的Leaf-Index，每个Block的最后一个Key被放入Intermediate-Index中，Root-Index指向Intermediate-Index。在HFile的末尾还有Bloom Filter用于快速定位那么没有在某个Data Block中的Row；TimeRange信息用于给那些使用时间查询的参考。在HFile打开时，这些索引信息都被加载并保存在内存中，以增加以后的读取性能。https://blog.csdn.net/pun_c/article/details/46841625 V3FileV3版本基本和V2版本相比，并没有太大的改变，它在KeyValue(Cell)层面上添加了Tag数组的支持；并在FileInfo结构中添加了和Tag相关的两个字段。 参考：https://www.cnblogs.com/ios1988/p/6266767.html]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RegionServer的故障恢复]]></title>
    <url>%2F2017%2F02%2F18%2FRegionServer%E7%9A%84%E6%95%85%E9%9A%9C%E6%81%A2%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[我们知道，RegionServer的相关信息保存在ZK中，在RegionServer启动的时候，会在Zookeeper中创建对应的临时节点。RegionServer通过Socket和Zookeeper建立session会话，RegionServer会周期性地向Zookeeper发送ping消息包，以此说明自己还处于存活状态。而Zookeeper收到ping包后，则会更新对应session的超时时间。当Zookeeper超过session超时时间还未收到RegionServer的ping包，则Zookeeper会认为该RegionServer出现故障，ZK会将该RegionServer对应的临时节点删除，并通知Master，Master收到RegionServer挂掉的信息后就会启动数据恢复的流程。Master启动数据恢复流程后，其实主要的流程如下：RegionServer宕机—》ZK检测到RegionServer异常—》Master启动数据恢复—》Hlog切分—》Region重新分配—》Hlog重放—》恢复完成并提供服务故障恢复有3中模式，下面就一一来介绍。 Log Splitting在最开始的恢复流程中，Hlog的整个切分过程都由于Master来执行，如下图所示： a、将待切分的日志文件夹进行重命名，防止RegionServer未真的宕机而持续写入Hlogb、Master启动读取线程读取Hlog的数据，并将不同RegionServer的日志写入到不同的内存buffer中c、针对每个buffer，Master会启动对应的写线程将不同Region的buffer数据写入到HDFS中，对应的路径 为/hbase/table_name/region/recoverd.edits/.tmp。d、Master重新将宕机的RegionServer中的Rgion分配到正常的RegionServer中，对应的RegionServer读取Region的数据，会发现该region目录下的recoverd.edits目录以及相关的日志，然后RegionServer重放对应的Hlog日志，从而实现对应Region数据的恢复。从上面的步骤中，我们可以看出Hlog的切分一直都是master在干活，效率比较低。设想，如果集群中有多台RegionServer在同一时间宕机，会是什么情况？串行修复，肯定异常慢，因为只有master一个人在干Hlog切分的活。因此，为了提高效率，开发了Distributed Log Splitting架构。post-script:/hbase/table_name/region/为hbase表在hdfs上的实际目录 Distributed Log Splitting顾名思义，Distributed Log Splitting是LogSplitting的分布式实现，分布式就不是master一个人在干活了，而是充分使用各个RegionServer上的资源，利用多个RegionServer来并行切分Hlog，提高切分的效率。如下图所示： 上图的操作顺序如下：a、Master将要切分的日志发布到Zookeeper节点上（/hbase/splitWAL），每个Hlog日志一个任务，任务的初始状态为TASK_UNASSIGNEDb、在Master发布Hlog任务后，RegionServer会采用竞争方式认领对应的任务（先查看任务的状态，如果是TASK_UNASSIGNED，就将该任务状态修改为TASK_OWNED）c、RegionServer取得任务后会让对应的HLogSplitter线程处理Hlog的切分，切分的时候读取出Hlog的对，然后写入不同的Region buffer的内存中。d、RegionServer启动对应写线程，将Region buffer的数据写入到HDFS中，路径为/hbase/table/region/seqenceid.temp，seqenceid是一个日志中该Region对应的最大sequenceid，如果日志切分成功，而RegionServer会将对应的ZK节点的任务修改为TASK_DONE，如果切分失败，则会将任务修改为TASK_ERR。e、如果任务是TASK_ERR状态，则Master会重新发布该任务，继续由RegionServer竞争任务，并做切分处理。f、Master重新将宕机的RegionServer中的Rgion分配到正常的RegionServer中，对应的RegionServer读取Region的数据，将该region目录下的一系列的seqenceid.temp进行从小到大进行重放，从而实现对应Region数据的恢复。从上面的步骤中，我们可以看出Distributed Log Splitting采用分布式的方式，使用多台RegionServer做Hlog的切分工作，确实能提高效率。正常故障恢复可以降低到分钟级别。但是这种方式有个弊端是会产生很多小文件（切分的Hlog数 宕机的RegionServer上的Region数）。比如一个RegionServer有20个Region，有50个Hlog，那么产生的小文件数量为20*50=1000个。如果集群中有多台RegionServer宕机的情况，小文件更是会成倍增加，恢复的过程还是会比较慢。由次诞生了Distributed Log Replay模式。 Distributed Log ReplayDistributed Log Replay和Distributed Log Splitting的不同是先将宕机RegionServer上的Region分配给正常的RgionServer，并将该Region标记为recovering。再使用Distributed Log Splitting类似的方式进行Hlog切分，不同的是，RegionServer将Hlog切分到对应Region buffer后，并不写HDFS，而是直接进行重放。这样可以减少将大量的文件写入HDFS中，大大减少了HDFS的IO消耗。如下图所示：]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Region的拆分]]></title>
    <url>%2F2017%2F02%2F18%2FRegion%E7%9A%84%E6%8B%86%E5%88%86%2F</url>
    <content type="text"><![CDATA[Hbase Region的三种拆分策略Hbase Region的拆分策略有比较多，比如除了3种默认过的策略，还有DelimitedKeyPrefixRegionSplitPolicy、KeyPrefixRegionSplitPolicy、DisableSplitPolicy等策略，这里只介绍3种默认的策略。分别是ConstantSizeRegionSplitPolicy策略、IncreasingToUpperBoundRegionSplitPolicy策略和SteppingSplitPolicy策略。 ConstantSizeRegionSplitPolicyConstantSizeRegionSplitPolicy策略是0.94版本之前的默认拆分策略，这个策略的拆分规则是：当region大小达到hbase.hregion.max.filesize（默认10G）后拆分。这种拆分策略对于小表不太友好，按照默认的设置，如果1个表的Hfile小于10G就一直不会拆分。注意10G是压缩后的大小，如果使用了压缩的话。如果1个表一直不拆分，访问量小也不会有问题，但是如果这个表访问量比较大的话，就比较容易出现性能问题。这个时候只能手工进行拆分。还是很不方便。 IncreasingToUpperBoundRegionSplitPolicyIncreasingToUpperBoundRegionSplitPolicy策略是Hbase的0.94~2.0版本默认的拆分策略，这个策略相较于ConstantSizeRegionSplitPolicy策略做了一些优化，该策略的算法为：min(maxFileSize,r^2flushSize )，最大为maxFileSize 。从这个算是我们可以得出maxFileSize为10G,flushsize为128M的情况下，可以计算出Region的分裂情况如下：第一次拆分大小为：min(10G，11128M)=128M第二次拆分大小为：min(10G，33128M)=1152M第三次拆分大小为：min(10G，55128M)=3200M第四次拆分大小为：min(10G，77128M)=6272M第五次拆分大小为：min(10G，99128M)=10G第五次拆分大小为：min(10G，1111*128M)=10G从上面的计算我们可以看到这种策略能够自适应大表和小表，但是这种策略会导致小表产生比较多的小region，对于小表还是不是很完美。 SteppingSplitPolicySteppingSplitPolicy是在Hbase 2.0版本后的默认策略，，拆分规则为：If region=1 then: flush size 2 else: MaxRegionFileSize。还是以flushsize为128M、maxFileSize为10G场景为列，计算出Region的分裂情况如下：第一次拆分大小为：2128M=256M第二次拆分大小为：10G从上面的计算我们可以看出，这种策略兼顾了ConstantSizeRegionSplitPolicy策略和IncreasingToUpperBoundRegionSplitPolicy策略，对于小表也肯呢个比较好的适配。Hbase Region拆分的详细流程Hbase的详细拆分流程图如下： 从上图我们可以看出Region切分的详细流程如下：第1步、会在ZK的/hbase/region-in-transition/region-name下创建一个znode，并设置状态为SPLITTING第2步、master通过watch节点检测到Region状态的变化，并修改内存中Region状态的变化第3步、RegionServer在父Region的目录下创建一个名称 为.splits的子目录第4步、RegionServer关闭父Region，强制将数据刷新到磁盘，并这个Region标记为offline的状态。此时，落到这个Region的请求都会返回NotServingRegionException这个错误第5步、RegionServer在.splits创建daughterA和daughterB，并在文件夹中创建对应的reference文件，指向父Region的Region文件第6步、RegionServer在HDFS中创建daughterA和daughterB的Region目录，并将reference文件移动到对应的Region目录中第7步、在.META.表中设置父Region为offline状态，不再提供服务，并将父Region的daughterA和daughterB的Region添加到.META.表中，已表名父Region被拆分成了daughterA和daughterB两个Region第8步、RegionServer并行开启两个子Region，并正式提供对外写服务第9步、RegionSever将daughterA和daughterB添加到.META.表中，这样就可以从.META.找到子Region，并可以对子Region进行访问了第10步、RegionServr修改/hbase/region-in-transition/region-name的znode的状态为SPLIT transition(过渡，转变，变迁)Transaction(事务)备注：为了减少对业务的影响，Region的拆分并不涉及到数据迁移的操作，而只是创建了对父Region的指向。只有在做大合并的时候，才会将数据进行迁移。那么通过reference文件如何才能查找到对应的数据呢？如下图所示： 根据文件名来判断是否是reference文件由于reference文件的命名规则为前半部分为父Region对应的File的文件名，后半部分是父Region的名称，因此读取的时候也根据前半部分和后半部分来识别根据reference文件的内容来确定扫描的范围，reference的内容包含两部分，一部分是切分点splitkey，另一部分是boolean类型的变量（true或者false）。如果为true则扫描文件的上半部分，false则扫描文件的下半部分接下来确定了扫描的文件，以及文件的扫描范围，那就按照正常的文件检索了]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase读请求分析]]></title>
    <url>%2F2017%2F02%2F18%2FHBase%E8%AF%BB%E8%AF%B7%E6%B1%82%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文主要是基于HBase的0.98.8版本的实现。HBase能提供实时计算服务主要原因是由其架构和底层的数据结构决定的，即由LSM-Tree(Log-Structured Merge-Tree) + HTable(region分区) + Cache决定——客户端可以直接定位到要查数据所在的HRegion server服务器，然后直接在服务器的一个region上查找要匹配的数据，并且这些数据部分是经过cache缓存的。 客户端读请求HBase为客户端提供了的读请求API主要有两个，get和scan。其中，get是通过指定单个的rowKey，获取其对应的value值。而scan是指定startRow和stopRow两个边界rowKey来指定范围rowKey值，获取它们对应的value值。 首先我们知道HBase会根据算法把一个表划分为多个Region，然后HMaster会把这几个Region分配到不同的节点中去，因此，当我们查找一个表的数据时，有可能要把请求发送到不同的节点，再获取其查询结果。但是要如何根据get指定的rowKey到指定的节点上查询呢（scan的流程相差不大）？HBase依靠hbase:meta这个系统表实现，事实上hbase:meta表和普通的HBase表一样，都是要分配到某个节点上，如果这个节点出现故障就会转移到另外可用的节点上，这样就可以防止某个节点出现故障导致整个HBase服务不可用。 HBase要求客户端的读请求需要自行定位到所属的Region的节点上，然后把请求的rowKey以及Region一并发送到这个节点。 我们来看看客户端的读请求执行过程： get由于hbase:meta表在存放在某台可用的节点机器上，因此我们需要定位究竟是哪台节点。这个过程步骤如下： HBase使用zookeeper来存放hbase:meta表的位置，这样客户端就需要先到zookeeper查询hbase:meta表在哪个节点上，如图中的1。然后在根据读请求的rowKey发送请求到这个节点上，节点根据hbase:meta表查找到rowKey所属Region的节点，然后返回给客户端，即图中的2，3。客户端再真正把读请求发送到指定的HRegionServer上，如图中的4，然后HRegionServer再执行具体读请求的查询操作，最后把结果返回到客户端上 要注意的是，客户端会缓存每次hbase:meta表返回的结果缓存到本地上，这样就可以防止每次查询都要经过步骤1，2进行查询，导致过多的网络I/O操作。在上面的第2个过程中，节点是如何根据rowKey通过查询hbase:meta表得到所属的Region呢？hbase:meta表中保存了每个Region的startRow，这样可以查找小于rowKey里的最大startRow值即可确定Region，这个过程在HRegion.getClosestRowBefore()代码实现。 scan 对于scan来说，其实流程和get的查询相差不大，由于scan请求包含startRow和stopRow两个范围，因此可能需要查询多个Region。具体步骤如下： 会先发送startRow作为指定rowKey到hbase:meta表上请求Region信息，返回的Region信息会包含这个Region的startKey和endKey然后客户端再根据Regioin的范围判断。即如果stopRow&lt;=endKey，则只需要查询这个Region即可。否则，需要首先把startRow和endKey作为一次的请求发送到这个Region的节点上，然后再把endKey作为新的startRow去查询hbase:meta，然后重复步骤1 这样经过多次的迭代，即可把scan请求对应为多个Region的查询。 服务端读请求当客户端把读请求发送到对应的HRegionServer上时，服务端就会开始具体的查询了。实际上，客户端发送的读请求会包含Region信息，以及rowKey范围（如果是get，则只有一个rowKey），另外还有客户端指定的其它查询参数，包括columnFamily，timestamp，filter过滤器等。因此HRegionServer首先会简单地找出对应的HRegion对象来执行具体的读请求。另外，无论是get或者scan请求，HRegionServer都会转化成scan请求来对待，其实get请求就是一个特殊的scan请求，它的范围只有一个指定的rowKey。 对于HRegion是如何执行读请求的查询，首先要对HRegionServer内部有一个了解。 集中关注一下HRegionServer的内部结构。HRegion是HBase表经过Sharding后的一部分，每个HRegion里面可能有多个Store，每个Store就是一个Column Family的对象，同一Column Family允许有多个列，这些列的数据都必须存储到一个文件里（但有可能存在多个这样的文件），存储格式为HFile，因此每个Store都有多个StoreFile对象管理Column Family的列数据。另外注意到每个Store内部都有一个MemStore对象。MemStore是HBase用来缓存写请求数据的内存结构，由于HBase的写请求都会写MemStore，然后再写HLog（HBase的WriteAheadLog的实现，存储在HDFS里），这样写请求才算提交状态，才对读请求为可见。MemStore可以让我们从内存中读取最近写请求提交的数据，而不必从HLog中读取，因此避免了I/O操作。另外，图里没有画出的是BlockCache，这个是HRegionServer内部的单例，主要用于HFile读取时缓存HBlock，后面将会继续详述。 弄清楚HRegion的结构之后，可以知道，在某个存在读写请求的时刻里，HRegion包含表数据的地方有MemStore、HLog、HFile、BlockCache。由于MemStore的数据与HLog是等同的，因此对于每个读请求，HRegion需要查询MemStore、BlockCache和HFile这三个部分。 KeyValue 还有一个要注意的是，HBase对于数据KeyValue的定义，来看看下图KeyValue的二进制格式: 一个KeyValue只包含一个rowKey和一个对应的Value值，也就是说，如果一个rowKey在有多个列值，则会有多个KeyValue与其对应。上图的绿色部分中，第二个空格Row就是用于定义的rowKey，在HBase的内部里，对于整个KeyValue来说，包含Column Family、Column Qualifier、TimeStamp和KeyType的整个绿色Key部分才是KeyValue的key值。黄色的Value部分就是保存的列值。在查询流程里，可能会出现对比用于定义的rowKey，或者对比整个绿色部分的key值。 查询流程 为了提高查询速度，MemStore和HFile内部都是经过排序的数据，因此，在实际查询的过程中可以使用多路归并的方法进行查询。整个查询过程如下图所示： HRegion提供了Scanner接口方便解耦HRegion与MemStore、BlockCache和HFile这三个部分的查询过程。其中RegionScannerImpl负责HRegion内部查询的过程，StoreScanner则是负责HStore内部查询的过程，由于可能有多个HStore，则会有对应多个StoreScanner，MemStoreScanner则负责HStore内部的MemStore数据结构查询，StoreFileScanner则负责HStore内部多个HFile的查询。这几个部分分别用KeyValueHeap连接起来，KeyValueHeap实现了多路归并的查询逻辑。 多路归并的逻辑大致如下：首先初始化各个Scanner，这个过程需要Scanner根据要查询的KeyValue数据kv，进行一次seek查询，seek查询的结果是一个大于或等于查询值kv的peek值，然后KeyValueHeap使用优先级队列PriorityQueue保存各个Scanner，这个优先级队列会通过比较各个Scanner的peek值来进行排序，这个优先级队列的第一个元素就是这些scanner里查询kv的结果，如果这个值刚好等于kv，即成功找到，如果大于则表明不存在。如果需要查询多个值则重新进行一次seek查询，再获取优先级队列的第一个元素。可以类别优先级队列里的scanner就是多路归并的各个有序队列，这样就很容易理解算法。 另外，在StoreScanner的初始化过程中，为了减少进行多路归并的Scanner数，提高查询速度，会对Scanner进行一次简单的过滤。MemStoreScanner，会根据查询kv值的时间戳判断是否在TimeRange，以及TTL来判断。而StoreFileScanner则会利用TimeRange（包括TTL）、KeyRange、BloomFilter判断kv值是否在HFile中。BloomFilter特性通过计算kv的哈希值来快速判断是否存在，但有一定的失败几率。 从KeyValueHeap的多路归并算法逻辑可以看到，Scanner关键是如何通过seek查询更新对应的peek值。接下来主要分析一下MemStoreScanner和StoreFileScanner是如何通过seek查询来更新peek值的。 MemStoreScannerMemStoreScanner直接引用MemStore内部的两个主要的数据结构：kvset和snapshot。这两个数据结构都是保存KeyValue数据的KeyValueSkipListSet。KeyValueSkipListSet是HBase单独实现的一个专门保存KeyValue的SkipList，内部用ConcurrentNavigableMap封装实现了高效并发的有序队列。kvset是当HBase写入数据KeyValue的缓存，而snapshot则是指快照，这是MemStore大小超过阀值，需要把kvset数据写入HFile的时候生成的副本，这样当flush这份副本到HFile里的时候，减少对写请求的影响。 MemStoreScanner的seek过程比较简单，代码流程如下：123456789101112131415161718192021222324252627public synchronized KeyValue peek() &#123; //DebugPrint.println(&quot; MS@&quot; + hashCode() + &quot; peek = &quot; + getLowest()); return theNext;&#125; public synchronized boolean seek(KeyValue key) &#123; if (key == null) &#123; close(); return false; &#125; //tailSet返回的是大于等于key的集合 kvsetIt = kvsetAtCreation.tailSet(key).iterator(); snapshotIt = snapshotAtCreation.tailSet(key).iterator(); kvsetItRow = null; snapshotItRow = null; return seekInSubLists(key);&#125; private synchronized boolean seekInSubLists(KeyValue key)&#123; //getNext从迭代器获取符合的KeyValue值 kvsetNextRow = getNext(kvsetIt); snapshotNextRow = getNext(snapshotIt); // Calculate the next value theNext = getLowest(kvsetNextRow, snapshotNextRow); // has data return (theNext != null);&#125; 上面代码主要实现逻辑比较简单，kvsetAtCreation和snapshotAtCreation两个数据结构（引用MemStore的kvset和snapshot），通过比较KeyValue的绿色key部分，来查找大于等于的集合，然后getNext需要通过比较MVCC版本号以及rowKey部分找出集合里符合KeyValue的值，最后再从两个集合里面找出最小的那个KeyValue，同样也是类似多路合并的思想，这样的结果就是theNext的值，也就是peek返回的值。 这里的MVCC版本号是HBase提高读写并发事务，并维持一致性的实现。 StoreFileScannerStoreFileScanner由于涉及到BlockCache缓存，HFile读写，Bloom filter等特性，逻辑实现较为复杂。StoreFileScanner会首先利用HFile记录的时间戳范围、rowKey范围、Bloom filter来判断给定的KeyValue是否存在HFile中，如果不存在则直接跳过HFile的查找。然后会StoreFileScanner的seek操作就要涉及到HFile的查找过程。HFile采用HBlock作为文件操作的基本单位，有不同类型的HBlock，包括DataBlock（保存KeyValue），IndexBlock（保存索引以及对应的rowKey）、BloomMeta（Bloom filter相关）等。HFile采用类似B+树的多级索引算法，优化在数据量大的情况下查找一个指定的KeyValue的速度。在多级索引查找过程中，读取HBlock都会尝试从BlockCache缓存读取指定Block，尽量避免I/O操作；由于所有的Block的rowKey都是有序的，IndexBlock都是采用二分查找算法来得到下一个Block的offset，有助于提高查找速度。接下来是较为详细的实现解析。 HFile HBase的HFile目前有三个不同的版本，本文主要研究HFile v2的布局。首先来看看它的文件布局格式： HFile V2有四部分：”Scanned block”、”Non-scanned block”、”Load-on-open”和Trailer部分（详细请参考这里）。HFile的初始化记载过程中，默认把Trailer和”Load-on-open”部分加载到内存中。首先会加载位于文件末尾的Trailer，包含着HFile的版本、索引数、压缩/非压缩大小、”Load-on-open”部分的偏移等文件信息。从Trailer获得”Load-on-open”部分的偏移开始加载”Load-on-open”部分，这部分主要包括RootDataIndex、midkey、MetaIndex（V2起不再存储Bloom信息，但仍支持）、FileInfo以及Bloom filter metadata。RootDataIndex就是根索引Block，midKey是用于HFile在split的时候快速找到处于中间Key的值。FileInfo包括了LastKey、平均Key长度等信息，BloomFilterMetaData就是Bloom filter的rootIndexBlock。 对于多级索引的算法，是按照数据量大小来确定究竟需要查找多少个IndexBlock的。 在数据量较少的情况下，可以直接从RootDataIndex直接定位DataBlock，也就是RootDataIndex-&gt;DataBlock； 当数据量较大时，索引的数量超过RootDataIndex的大小，因此就要建立多级索引，索引的级数与数据量大小有关，其路径为Root-&gt;Intermediate(0…N)-&gt;Leaf(0…1)-&gt;DataBlock，除了Root和DataBlock必定会有之外，Intermediate会有0到N个（取决于数据量大小），Leaf会有0到1个； Bloom filter只采用一级索引，也就是只会有BloomFilterMetaData-&gt;BloomBlock这样的路径。 HFile保证在IndexBlock中的rowKey有序，这样在查找rowKey对应的索引中，采用的是二分查找算法，稍微有点不同的是，这里的二分查找并不是一定要找到对应的rowKey，而是要找到刚好小于等于这个rowKey的索引，这是由于这个rowKey都是IndexBlock或者DataBlock里最小值。 二分查找到最后确定DataBlock的时候，就需要顺序遍历DataBlock里的KeyValue值，由于每个DataBlock大小约为64kb（这里大约是由于DataBlock是按照KeyValue顺序写入，当超过64KB的时候才会停止写这个DataBlock，另外压缩也会有影响），因此遍历比较的时候不会有太大性能影响。 BlockCache在以上的HFile查找过程中，需要大量I/O读取HBlock，因此HBase提供了BlockCache缓存机制，专门优化读性能。BlockCache是以HBlock为基本单位进行缓存的内存结构，经过多次的改进与性能测试，HBase官方推荐的一种做法是按照HBlock的类型进行分类缓存。事实上，由于在读请求里，对于IndexBlock的请求命中率几乎可以达到100%，而DataBlock由于业务的不确定性，可能会有巨大的命中率差距，因此把这两种类型的Block进行分类缓存，其中IndexBlock会缓存到LruBlockCache中，而DataBlock则缓存到BucketCache中。 LruBlockCache是利用LRU算法+优先级比重对HBlock进行缓存。优先级比重是HBase对HBlock进行优先级划分，分为SINGLE、MULTI、MEMORY三个等级，刚开始缓存为SINGLE等级，如果后面有读请求命中则升级到MULTI等级，MEMORY等级是用户指定HBase表设置的。进行淘汰的时候，会尽可能地按照SINGLE、MULTI、MEMORY的优先级来进行淘汰。在我们的线上集群中，几台HRegionServer的LruBlockCache命中率几乎为100%。 BucketCache同样也是利用LRU算法对HBlock进行缓存。但不同的是，由于它是针对DataBlock进行缓存，如果在随机读较多的情景下，很容易造成大量GC，这个在我们集群中曾经造成过严重的问题，因此可以把它缓存的介质指定为offheap（也支持SSD的File方式），这样可以有效减轻GC带来的影响。BucketCache还有一个优点，就是它把缓存大小分为很多个尺寸，每个尺寸都有对应一个槽，这些槽可以互相转换，这对于经过压缩的DataBlock来说能够大大提高内存利用率。 多级索引+二分查找的做法不但有助于提高读性能，允许动态读取索引Block，减轻读取HFile内存初始化时的压力。BlockCache的使用也能够尽可能地利用好内存进行缓存，避免产生I/O。另外，由于多个HFile是并行查找的，如果对多个小的HFile进行compact操作，可以避免减少多个HFile的读操作。 总结 HBase使用了多种不同的内存结构、多级索引、二分查找等各种方法提高读请求性能，另外把HFile以HBlock为基本单位进行读写，减轻I/O的同时，也能够动态按照需要进行Block读取，减轻内存压力。 参考：https://blog.csdn.net/pun_c/article/details/46841625https://www.cnblogs.com/ulysses-you/p/10072883.html#_label2]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase写入逻辑]]></title>
    <url>%2F2017%2F02%2F18%2FHbase%E5%86%99%E5%85%A5%E9%80%BB%E8%BE%91%2F</url>
    <content type="text"><![CDATA[写入逻辑HBase仅仅支持行级别的事务一致性。本文主要探讨一下HBase的写请求流程。主要基于0.98.8版本号的实现Hbase的写逻辑涉及到写内存、写log、刷盘等操作从上图可以看出氛围3步骤：第1步：Client获取数据写入的Region所在的RegionServer第2步：请求写Hlog第3步：请求写MemStore只有当写Hlog和写MemStore都成功了才算请求写入完成。MemStore后续会逐渐刷到HDFS中。备注：Hlog存储在HDFS，当RegionServer出现异常，需要使用Hlog来恢复数据。 client写请求HBase提供的Java client API是以HTable为主要接口，相应当中的HBase表。写请求API主要为HTable.put（write和update）、HTable.delete等。 以HTable.put为样例，首先来看看客户端是怎么把请求发送到HRegionServer的。 每一个put请求表示一个KeyValue数据，考虑到client有大量的数据须要写入到HBase表，HTable.put默认是会把每一个put请求都放到本地缓存中去，当本地缓存大小超过阀值（默觉得2MB）的时候，就要请求刷新，即把这些put请求发送到指定的HRegionServer中去，这里是利用线程池并发发送多个put请求到不同的HRegionServer。 但假设多个请求都是同一个HRegionServer，甚至是同一个HRegion，则可能造成对服务端造成压力，为了避免发生这样的情况，clientAPI会对写请求做了并发数限制，主要是针对put请求须要发送到的HRegionServer和HRegion来进行限制。详细实如今AsyncProcess中。 主要參数设定为：123hbase.client.max.total.tasks 客户端最大并发写请求数。默觉得100hbase.client.max.perserver.tasks 客户端每一个HRegionServer的最大并发写请求数。默觉得2hbase.client.max.perregion.tasks 客户端每一个HRegion最大并发写请求数。默觉得1 为了提高I/O效率。AsyncProcess会合并同一个HRegion相应的put请求，然后再一次把这些同样HRegion的put请求发送到指定HRegionServer上去。另外AsyncProcess也提供了各种同步的方法，如waitUntilDone等，方便某些场景下必须对请求进行同步处理。每一个put和读请求一样。都是要通过訪问hbase:meta表来查找指定的HRegionServer和HRegion，这个流程和读请求一致。 服务端写请求当client把写请求发送到服务端时。服务端就要開始运行写请求操作。HRegionServer把写请求转发到指定的HRegion运行，HRegion每次操作都是以批量写请求为单位进行处理的。主要流程实如今HRegion.doMiniBatchMutation,大致例如以下： 应用了zookeeper的分布式事务锁的方式：1.获取region锁 有了这个锁的region，想要获取这个有锁的region的进程必须等待2.依次获取各行的锁：拿到每一行的锁3.先将拿到锁的一行数据，写入到memstore中，4.释放写入完毕的这一行数据的行锁5.写数据到wal中6.Memstore和wal都写入以后，释放region锁 为什么数据通过wal已经写入到hdfs中了，还要通过memstore再将数据往 内存中写一次呢？答：是为了数据的写入和读取,还有就是wal主要是为了回滚。为什么会有回滚操作机制？有可能写入到内存成功，但是没有成功的写入到hdfs中，比如hdfs的主机挂掉了，没有写入成功，就不能正常的释放锁，因此就会产生异常，然后启动回滚的操作，将内存的数据也删除回滚就会可能出现脏读的情况：刷新的速度比释放锁的速度块 ，刷新了就把内存中的数据，立马写入到hdfs中 这个可能大概是千万毫秒分子1先写memstore再写wal的优缺点？优点：这条数据没有写入到hdfs中，用户就可以访问了====habse速度快的原因缺点：脏读 Hbase的不同版本，menstore和wal写入的顺序是有差异的1.0以前的老版本：先写wal，写完了以后再写memstore新版本：memstore和wal是并行同时写数据，但是由于memstore’是将数据写入内存中，而wal是将数据写入hdfs中，因此memstore写数据的速度要块，因此memstore先于wal结束，释放行锁 先写memstore再写wal的优缺点？优点：这条数据没有写入到hdfs中，用户就可以访问了====habse速度快的原因缺点：脏读为什么会有回滚操作机制？有可能写入到内存成功，但是没有成功的写入到hdfs中，比如hdfs的主机挂掉了，没有写入成功，就不能正常的释放锁，因此就会产生异常，然后启动回滚的操作，将内存的数据也删除回滚就会可能出现脏读的情况：刷新的速度比释放锁的速度块 ，刷新了就把内存中的数据，立马写入到hdfs中 这个可能大概是千万毫秒分子1 MemStore刷盘为了提高Hbase的写入性能，当写请求写入MemStore后，不会立即刷盘生成storefile。而是会等到一定的时候进行刷盘的操作。具体是哪些场景会触发刷盘的操作呢？总结成如下的几个场景： 全局内存控制这个全局的参数是控制内存整体的使用情况，当所有memstore占整个heap的最大比例的时候，会触发刷盘的操作。这个参数是hbase.regionserver.global.memstore.upperLimit，默认为整个heap内存的40%。但这并不意味着全局内存触发的刷盘操作会将所有的MemStore都进行输盘，而是通过另外一个参数hbase.regionserver.global.memstore.lowerLimit来控制，默认是整个heap内存的35%。当flush到所有memstore占整个heap内存的比率为35%的时候，就停止刷盘。这么做主要是为了减少刷盘对业务带来的影响，实现平滑系统负载的目的。 MemStore达到上限当MemStore的大小达到hbase.hregion.memstore.flush.size大小的时候会触发刷盘，默认128M大小 RegionServer的Hlog数量达到上限前面说到Hlog为了保证Hbase数据的一致性，那么如果Hlog太多的话，会导致故障恢复的时间太长，因此Hbase会对Hlog的最大个数做限制。当达到Hlog的最大个数的时候，会强制刷盘。这个参数是hase.regionserver.max.logs，默认是32个。 手工触发可以通过hbase shell或者java api手工触发flush的操作。 关闭RegionServer触发在正常关闭RegionServer会触发刷盘的操作，全部数据刷盘后就不需要再使用Hlog恢复数据。 Region使用HLOG恢复完数据后触发当RegionServer出现故障的时候，其上面的Region会迁移到其他正常的RegionServer上，在恢复完Region的数据后，会触发刷盘，当刷盘完成后才会提供给业务访问。 每一次Memstore的flush，会为每一个CF创建一个新的storeFile。。 在读方面相对来说就会简单一些：HBase首先检查请求的数据是否在Memstore，不在的话就到HFile中查找，最终返回merged的一个结果给用户。 每次Memstore Flush，会为每个CF都创建一个新的HFile（storeFile）。这样，不同CF中数据量的不均衡将会导致产生过多HFile：当其中一个CF的Memstore达到阈值flush时，所有其他CF的也会被flush。如上所述，太频繁的flush以及过多的HFile将会影响集群性能。 Compaction MemStore频繁的Flush就会创建大量的storeFile(Hfile)。这样HBase在检索的时候，就不得不读取大量的storeFile，读性能会受很大影响。为预防打开过多HFile及避免读性能恶化，HBase有专门的HFile合并处理(HFile Compaction Process)。HBase会周期性的合并数个小HFile为一个大的HFile。明显的，有Memstore Flush产生的HFile越多，集群系统就要做更多的合并操作(额外负载)。更糟糕的是：Compaction处理是跟集群上的其他请求并行进行的。当HBase不能够跟上Compaction的时候(同样有阈值设置项)，会在RS上出现“写阻塞”。像上面说到的，这是最最不希望的。 提示：严重关切RS上Compaction Queue 的size。要在其引起问题前，阻止其持续增大。Compaction的主要目的，是为了减少同一个Region同一个ColumnFamily下面的小文件数目，从而提升读取的性能。 小合并（MinorCompaction）由前面的刷盘部分的介绍，我们知道MemStore会将数据刷到磁盘，生产StoreFile，因此势必产生很多的小问题，对于Hbase的读取，如果要扫描大量的小文件，会导致性能很差，因此需要将这些小文件合并成大一点的文件。因此所谓的小合并，就是把多个小的StoreFile组合在一起，形成一个较大的StoreFile，通常是累积到3个Store File后执行。通过参数hbase.hstore,compactionThreadhold配置。小合并的大致步骤为： 分别读取出待合并的StoreFile文件的KeyValues，并顺序地写入到位于./tmp目录下的临时文件中 将临时文件移动到对应的Region目录中 将合并的输入文件路径和输出路径封装成KeyValues写入WAL日志，并打上compaction标记，最后强制自行sync 将对应region数据目录下的合并的输入文件全部删除，合并完成这种小合并一般速度很快，对业务的影响也比较小。本质上，小合并就是使用短时间的IO消耗以及带宽消耗换取后续查询的低延迟。小范围的Compaction。有最少和最大文件数目限制。通常会选择一些连续时间范围的小文件进行合并====小合并,Minor Compaction选取文件时，遵循一定的算法。 大合并（MajorCompaction）所谓的大合并，就是将一个Region下的所有StoreFile合并成一个StoreFile文件，在大合并的过程中，之前删除的行和过期的版本都会被删除，拆分的母Region的数据也会迁移到拆分后的子Region上。大合并一般一周做一次，控制参数为hbase.hregion.majorcompaction。大合并的影响一般比较大，尽量避免统一时间多个Region进行合并，因此Hbase通过一些参数来进行控制，用于防止多个Region同时进行大合并。该参数为： hbase.hregion.majorcompaction.jitter具体算法为：123hbase.hregion.majorcompaction参数的值乘于一个随机分数，这个随机分数不能超过hbase.hregion.majorcompaction.jitter的值。hbase.hregion.majorcompaction.jitter的值默认为0.5。通过hbase.hregion.majorcompaction参数的值加上或减去hbase.hregion.majorcompaction参数的值乘于一个随机分数的值就确定下一次大合并的时间区间。用户如果想禁用major compaction，只需要将参数hbase.hregion.majorcompaction设为0。建议禁用。 涉及该Region该ColumnFamily下面的所有的HFile文件。删除的数据。===大合并 需要删除的数据包含哪些数据？1.ttl 带有有效期的数据，有效期过来之后，不能被访问，但是仍然存在2.Put数据：时间戳版本数据数量大于vsrsion可存在的数量3.需要删除的数据，将数据标记为删除 区别Major和minor合并的区别有哪些？1.合并范围不同小范围：在一定时间内，将多个小文件合并在一起，合并的是一部分发 hfile大范围：将所有的hfile合并2.功能不同小范围：只进行整理大范围：删除只会在大合并的时候执行操作， 一般情况下，不会做大合并！12Region是HBase中分布式存储和负载均衡的最小单元。不同Region分布到不同RegionServer上，但并不是存储的最小单元。Region由一个或者多个Store组成，每个store保存一个columns family，每个Strore又由一个memStore和0至多个StoreFile 组成。memStore存储在内存中， StoreFile存储在HDFS上。 防止误区：开启大合并，整个hbase的所有的hfile要合并，但是并不是所有的hfile合并成一个hfile，而是一个列族(store)当中的所有的storefile合并成一个storefile，所有的列族开始合并操作,major合并之后，一个store只有一个storeFile文件，会对store的所有数据进行重写，有较大的性能消耗 很少做大合并，为什么？工程量过大如果一定要大合并呢？对于某一张表的某些列族，手动执行大合并，不建议开启大合并！！！ 为什么不建议开启大合并？因为在执行大合并的时候，整个hbase的hfile都在参与合并，整个hbase会锁住，此时读数据和写数据都不行为什么可以做到不执行大合并操作来达到增加空间呢？因为hbase的底层是hdfs，如果空间不够，可以任意的增加硬盘，增加服务器 参考：https://www.cnblogs.com/liguangsunls/p/6792705.htmlhttps://blog.csdn.net/qq_41919284/article/details/81676636https://blog.csdn.net/xiao_jun_0820/article/details/26580247]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase中的zookeeper]]></title>
    <url>%2F2017%2F02%2F18%2Fhbase%E4%B8%AD%E7%9A%84zookeeper%2F</url>
    <content type="text"><![CDATA[ZooKeeper：协调者ZooKeeper为HBase集群提供协调服务 它管理着HMaster和HRegionServer的状态(available/alive等)，并且会在它们宕机时通知给HMaster，从而HMaster可以实现HMaster之间的failover 对宕机的HRegionServer中的HRegion集合的修复(将它们分配给其他的HRegionServer)。ZooKeeper集群本身使用一致性协议(PAXOS协议)保证每个节点状态的一致性。 通过Zoopkeeper存储元数据的统一入口地址 How The Components Work TogetherZooKeeper协调集群所有节点的共享信息，在HMaster和HRegionServer连接到ZooKeeper后创建Ephemeral节点，并使用Heartbeat机制维持这个节点的存活状态，如果某个Ephemeral节点实效，则HMaster会收到通知，并做相应的处理。 另外，HMaster通过监听ZooKeeper中的Ephemeral节点(默认：/hbase/rs/*)来监控HRegionServer的加入和宕机。在第一个HMaster连接到ZooKeeper时会创建Ephemeral节点(默认：/hbasae/master)来表示Active的HMaster，其后加进来的HMaster则监听该Ephemeral节点，如果当前Active的HMaster宕机，则该节点消失，因而其他HMaster得到通知，而将自身转换成Active的HMaster，在变为Active的HMaster之前，它会创建在/hbase/back-masters/下创建自己的Ephemeral节点。 HBase中的AssignmentManagerAssignmentManager模块是HBase中一个非常重要的模块，Assignment Manager（之后简称AM）负责了HBase中所有region的Assign，UnAssign，以及split/merge过程中region状态变化的管理等等。在HBase-0.90之前，AM的状态全部存在内存中，自从HBASE-2485之后，AM把状态持久化到了Zookeeper上。在此基础上，社区对AM又修复了大量的bug和优化（见此文章），最终形成了用在HBase-1.x版本上的这个AM。 老Assignment Mananger的问题相信深度使用过HBase的人一般都会被Region RIT的状态困扰过，长时间的region in transition状态简直令人抓狂。 除了一些确实是由于Region无法被RegionServer open的case，大部分的RIT，都是AM本身的问题引起的。总结一下HBase-1.x版本中AM的问题，主要有以下几点： region状态变化复杂这张图很好地展示了region在open过程中参与的组件和状态变化。可以看到，多达7个组件会参与region状态的变化。并且在region open的过程中多达20多个步骤！越复杂的逻辑意味着越容易出bug region状态多处缓存region的状态会缓存在多个地方，Master中RegionStates会保存Region的状态，Meta表中会保存region的状态，Zookeeper上也会保存region的状态，要保持这三者完全同步是一件很困难的事情。同时，Master和RegionServer都会修改Meta表的状态和Zookeeper的状态，非常容易导致状态的混乱。如果出现不一致，到底以哪里的状态为准？每一个region的transition流程都是各自为政，各自有各自的处理方法 重度依赖Zookeeper在老的AM中，region状态的通知完全通过Zookeeper。比如说RegionServer打开了一个region，它会在Zookeeper把这个region的RIT节点改成OPEN状态，而不去直接通知Master。Master会在Zookeeper上watch这个RIT节点，通过Zookeeper的通知机制来通知Master这个region已经发生变化。Master再根据Zookeeper上读取出来的新状态进行一定的操作。严重依赖Zookeeper的通知机制导致了region的上线/下线的速度存在了一定的瓶颈。特别是在region比较多的时候，Zookeeper的通知会出现严重的滞后现象。 正是这些问题的存在，导致AM的问题频发。 Assignment Mananger V2在这个设计中，它摒弃了Zookeeper这个持久化的存储，一些region transition过程中的中间状态无法被保存。因此，在此基础上，社区又更进了一步，提出了Assignment Mananger V2在这个方案。在这个方案中，仍然摒弃了Zookeeper参与Assignment的整个过程。但是，它引入了ProcedureV2这个持久化存储来保存Region transition中的各个状态，保证在master重启时，之前的assing/unassign，split等任务能够从中断点重新执行。具体的来说，AMv2方案中，主要的改进有以下几点： Procedure V2Master中会有许多复杂的管理工作，比如说建表，region的transition。这些工作往往涉及到非常多的步骤，如果master在做中间某个步骤的时候宕机了，这个任务就会永远停留在了中间状态（RIT因为之前有Zookeeper做持久化因此会继续从某个状态开始执行）。比如说在enable/disable table时，如果master宕机了，可能表就停留在了enabling/disabling状态。需要一些外部的手段进行恢复。那么从本质上来说，ProcedureV2提供了一个持久化的手段（通过ProcedureWAL，一种类似RegionServer中WAL的日志持久化到HDFS上），使master在宕机后能够继续之前未完成的任务继续完成。同时，ProcedureV2提供了非常丰富的状态转换并支持回滚执行，即使执行到某一个步骤出错，master也可以按照用户的逻辑对之前的步骤进行回滚。比如建表到某一个步骤失败了，而之前已经在HDFS中创建了一些新region的文件夹，那么ProcedureV2在rollback的时候，可以把这些残留删除掉。Procedure中提供了两种Procedure框架，顺序执行和状态机，同时支持在执行过程中插入subProcedure，从而能够支持非常丰富的执行流程。在AMv2中，所有的Assign，UnAssign，TableCreate等等流程，都是基于Procedure实现的。 去除Zookeeper依赖有了Procedure V2之后，所有的状态都可以持久化在Procedure中，Procedure中每次的状态变化，都能够持久化到ProcedureWAL中，因此数据不会丢失，宕机后也能恢复。同时，AMv2中region的状态扭转（OPENING，OPEN，CLOSING，CLOSE等）都会由Master记录在Meta表中，不需要Zookeeper做持久化。再者，之前的AM使用的Zookeeper watch机制通知master region状态的改变，而现在每当RegionServer Open或者close一个region后，都会直接发送RPC给master汇报，因此也不需要Zookeeper来做状态的通知。综合以上原因，Zookeeper已经在AMv2中没有了存在的必要。 减少状态冲突的可能性之前我说过，在之前的AM中，region的状态会同时存在于meta表，Zookeeper和master的内存状态。同时Master和regionserver都会去修改Zookeeper和meta表，维护状态统一的代价非常高，非常容易出bug。而在AMv2中，只有master才能去修改meta表。并在region整个transition中做为一个“权威”存在，如果regionserver汇报上来的region状态与master看到的不一致，则master会命令RegionServer abort。Region的状态，都以master内存中保存的RegionStates为准。 除了上述这些优化，AMv2中还有许多其他的优化。比如说AMv2依赖Procedure V2提供的一套locking机制，保证了对于一个实体，如一张表，一个region或者一个RegionServer同一时刻只有一个Procedure在执行。同时，在需要往RegionServer发送命令，如发送open，close等命令时，AMv2实现了一个RemoteProcedureDispatcher来对这些请求做batch，批量把对应服务器的指令一起发送等等。在代码结构上，之前处理相应region状态的代码散落在AssignmentManager这个类的各个地方，而在AMv2中，每个对应的操作，都有对应的Procedure实现，如AssignProcedure，DisableTableProcedure，SplitTableRegionProcedure等等。这样下来，使AssignmentManager这个之前杂乱的类变的清晰简单，代码量从之前的4000多行减到了2000行左右。 AssignProcedure讲解一下在AMv2中，一个region是怎么assign给一个RegionServer，并在对应的RS上Open的。 REGION_TRANSITION_QUEUEAssign开始时的状态。在这个状态时，Procedure会对region状态做一些改变和存储，并丢到AssignmentManager的assign queue中。对于单独region的assign，AssignmentManager会把他们group起来，再通过LoadBalancer分配相应的服务器。当这一步骤完成后，Procedure会把自己标为REGION_TRANSITION_DISPATCH，然后看是否已经分配服务器，如果还没有被分配服务器的话，则会停止继续执行，等待被唤醒。 REGION_TRANSITION_DISPATCH当AssignmentManager为这个region分配好服务器时，Procedure就会被唤醒。或者Procedure在执行完REGION_TRANSITION_QUEUE状态时master宕机，Procedure被恢复后，也会进入此步骤执行。所以在此步骤下，Procedure会先检查一下是否分配好了服务器，如果没有，则把状态转移回REGION_TRANSITION_QUEUE，否则的话，则把这个region交给RemoteProcedureDispatcher，发送RPC给对应的RegionServer来open这个region。同样的，RemoteProcedureDispatcher也会对相应的指令做一个batch，批量把一批region open的命令发送给某一台服务器。当命令发送完成之后，Procedure又会进入休眠状态，等待RegionServer成功OPen这个region后，唤醒这个Procedure REGION_TRANSITION_FINISH当有RegionServer汇报了此region被打开后，会把Procedure的状态置为此状态，并唤醒Procedure执行。此时，AssignProcedure会做一些状态改变的工作，并修改meta表，把meta表中这个region的位置指向对应的RegionServer。至此，region assign的工作全部完成。 AMv2中提供了一个Web页面（Master页面中的‘Procedures&amp;Locks’链接）来展示当前正在执行的Procedure和持有的锁。其实通过log，我们也可以看到Assign的整个过程。 总结Assignment Mananger V2依赖Procedure V2实现了一套清晰明了的region transition机制。去除了Zookeeper依赖，减少了region状态冲突的可能性。整体上来看，代码的可读性更强，出了问题也更好查错。对于解决之前AM中的一系列“顽疾”，AMv2做了很好的尝试，也是一个非常好的方向。AMv2之所以能保持简洁高效的一个重要原因就是重度依赖了Procedure V2，把一些复杂的逻辑都转移到了Procedure V2中。但是这样做的问题是：一旦ProcedureWAL出现了损坏，或者Procedure本身存在bug，这个后果就是灾难性的。 此时并不是说zookeep在hbase中已经没用了，原数据存储，Hmaster的failover还是需要zk的。 参考:https://yq.aliyun.com/articles/601096]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Region寻址]]></title>
    <url>%2F2017%2F02%2F18%2FRegion%E5%AF%BB%E5%9D%80%2F</url>
    <content type="text"><![CDATA[老的Region寻址方式在Hbase 0.96版本以前，Hbase有两个特殊的表，分别是-ROOT-表和.META.表，其中-ROOT-的位置存储在ZooKeeper中，-ROOT-本身存储了 .META. Table的RegionInfo信息，并且-ROOT-不会分裂，只有一个region。而.META.表可以被切分成多个region 。读取的流程如下图所示： 1234第1步：client请求ZK获得-ROOT-所在的RegionServer地址 第2步：client请求-ROOT-所在的RS地址，获取.META.表的地址，client会将-ROOT-的相关信息cache下来，以便下一次快速访问 第3步：client请求 .META.表的RS地址，获取访问数据所在RegionServer的地址，client会将.META.的相关信息cache下来，以便下一次快速访问 第4步：client请求访问数据所在RegionServer的地址，获取对应的数据 从上面的路径我们可以看出，用户需要3次请求才能直到用户Table真正的位置，然后第四次请求开始获取真正的数据,这在一定程序带来了性能的下降。在0.96之前使用3层设计的主要原因是考虑到元数据可能需要很大。但是真正集群运行，元数据的大小其实很容易计算出来。在BigTable的论文中，每行METADATA数据存储大小为1KB左右，如果按照一个Region为128M的计算，3层设计可以支持的Region个数为2^34个，采用2层设计可以支持2^17（131072）。那么2层设计的情况下一个 集群可以存储4P的数据。这仅仅是一个Region只有128M的情况下。如果是10G呢? 因此，通过计算，其实2层设计就可以满足集群的需求。因此在0.96版本以后就去掉了-ROOT-表了。 post-script:如果一个region的大小为128m,那么就说明.META.的大小也为128M。每行METADATA数据存储大小为1KB左右，那么.META.表就可以存储131072 KB = 128M =2^17个region. 此时按照每个region 128m大小，131072*128M = 16P，不知道为什么4P？应该是主本+三副本，16P/4=4P 3层则是-ROOT-表128M，其中每1kb的数据存储一张.META.表，总可以存128*128张.meta表即2^34张 新的Region寻址方式如上面的计算，2层结构其实完全能满足业务的需求，因此0.96版本以后将-ROOT-表去掉了。如下图所示： 访问路径变成了3步：123第1步：Client请求ZK获取.META.所在的RegionServer的地址。 第2步：Client请求.META.所在的RegionServer获取访问数据所在的RegionServer地址，client会将.META.的相关信息cache下来，以便下一次快速访问。 第3步：Client请求数据所在的RegionServer，获取所需要的数据。 这个Meta Table如以前的-ROOT- Table一样是不可split的.总结去掉-ROOT-的原因有如下2点：其一：提高性能其二：2层结构已经足以满足集群的需求这里还有一个问题需要说明，那就是Client会缓存.META.的数据，用来加快访问，既然有缓存，那它什么时候更新？如果.META.更新了，比如Region1不在RerverServer2上了，被转移到了RerverServer3上。client的缓存没有更新会有什么情况？其实，Client的元数据缓存不更新，当.META.的数据发生更新。如上面的例子，由于Region1的位置发生了变化，Client再次根据缓存去访问的时候，会出现错误，当出现异常达到重试次数后就会去.META.所在的RegionServer获取最新的数据，如果.META.所在的RegionServer也变了，Client就会去ZK上获取.META.所在的RegionServer的最新地址。 参考：https://blog.csdn.net/qq_36421826/article/details/82701677https://www.cnblogs.com/qcloud1001/p/7615526.htmlhttps://www.cnblogs.com/ios1988/p/6266767.htmlhttps://www.cnblogs.com/cenzhongman/p/7271761.html]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch]]></title>
    <url>%2F2017%2F02%2F16%2FElasticsearch%2F</url>
    <content type="text"><![CDATA[建立倒排索引 标准化规则(normalization)并不会单纯直接建立的倒排索引，会使用到标准化规则(normalization) dog、dogs fox foxes单复数问题只保留一个，有共同词根Quick、quick 不区分大小写问题jump和leap意思相近，只保留一个 分词器介绍以及内置分词器 配置中文分词器重新启动elasticsearch在日志中可以看到。 CRUD新建索引,默认配置 PUT lib2查看索引 GET _all/_settings指定文档id用put,不指定文档id用post 获取docGET /lib2/user/1GET /lib2/user/1?_source=age,about 修改文档修改文档全覆盖用PUT，先把原有文档标记为deleted(es会在合适的时间把它删除掉),然后会创建一个新的文档修改具体字段用post两种修改方式的区别是： post方式对并发问题处理，可以增加一个重试参数 删除文档DELETE /lib/user/1 DELETE /lib/user 先把文档标记为deleted，es会在合适的时间把它删除mget,bulk 版本控制外部的版本号需要大于内存的版本号才能成功，相等也不行 mapping当我们添加一个文档的时候，elasticsearch就会默认给我们创建一个mapping,指定字段的数据类型。 数值，日期类型必须是完全一样才能查出来，即精确查询，没有分词；但是text类型默认进行分词，只需要包含就可以查到自定义mapping除了规定字段的类型，最大作用就是可以指定字段的属性是否分词,是否存储，是否索引等 字段类型 支持的属性 查询term、terms只会从倒排索引中查询，如果倒排索引中没有就不会查到数据terms表示包含其中一个即可。 基本查询 中文查询 Filter建立倒排索引时，会默认转为小写的，所以查大写字符的时，查询条件要转为小写，termId为id100127 exists相当于数据库的is not null将query转化为filter进行优化 聚合查询 cardinality相当于数据库的distinct constant_score由于不查评分，效率会高一些 原理解析es分布式结构特点作为应用程序的使用方目前只需要关心怎么添加文档和搜索文档。这就是隐藏特性垂直扩容是原集群节点数不变，新采购的机器配置高于旧机器。 路由相关到有相关数据的其他节点，就是将请求转发到其他节点，该及诶单将结果再转发回原始请求节点，最终返回给客户端(即应用程序)。 分片和副本机制新增节点。副本可以处理查询请求 水平扩容的过程 容错机制 文档核心元数据从6开始一个index下只能有一个type GUID算法可以保证在分布式并发情况下生成的id不会冲突 基于grovvy脚本实现partial update 文档数据路由原理 文档增删改内部原理 写一致原理和qurom机制 文档查询内部原理 分页查询中的deep paging问题mysql也可能会出现 copyto 字符串排序问题 如何计算相关度分数 DocValues解析默认开启的es对非字符串类型(日期，数字等)除建立倒排索引外，还会建立一个正排索引 基于scroll技术滚动搜索大量数据 dynamic mapping策略 english分词器会把is,a,an等停用词去掉，这些词不会建立倒排索引。而standard会建立。 重建索引 索引不可变的原因]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KMP]]></title>
    <url>%2F2017%2F02%2F14%2FKMP%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637package bluebridgecup.array.kmp;public class Kmp &#123; public static void main(String[] args) &#123; int[] next = new int[14]; char[] p = &quot;abcdabdefgabca&quot;.toCharArray(); getNext(p,next); for (int n:next) &#123; System.out.println(n); &#125; &#125; static void getNext(char[] p,int next[])&#123; int nLen = p.length; next[0] = -1;//todo 初始化为-1，所以 int k = -1; int j = 0;// next[j]代表[0, j - 1]区段中最长相同真前后缀的长度 while (j &lt; nLen - 1)&#123; //j //针对数组p,p[k]表示前项,p[j]表示后项;因为k=-1,j=0 //注:k==-1表示未找到k前缀与k后缀相等,首次分析可先忽略// 细心的朋友会问if语句中k == -1存在的意义是何？第一，程序刚运行时，k是被初始为-1，直接进行P[i] == P[k]判断无疑会边界溢出；// 第二，else语句中k = next[k]，k是不断后退的，若k在后退中被赋值为-1（也就是j = next[0]），在P[i] == P[k]判断也会边界溢出。// 综上两点，其意义就是为了特殊边界判断。 if (k &gt; -1)System.out.println(p[j]+&quot;***&quot;+p[k]+&quot;$$$&quot;+j+&quot;,&quot;+k); if (k == -1 || p[j] == p[k])&#123;//匹配成功 ++j; ++k; next[j] = k;//todo 首次是next[1] = 0;next中存的是索引;next[0] = -1是初始化的，不用再计算 &#125; else &#123; //p[j]与p[k]失配,则继续递归计算前缀p[next[k]] k = next[k];//todo next[0]=-1是初始化的，该出逻辑是判断k的取值为next数组的前一项，前一项如果为-1，则k=-1,那么下一次循环会进入两一个逻辑 &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[递归]]></title>
    <url>%2F2017%2F02%2F14%2F%E8%93%9D%E6%A1%A5%E6%9D%AF-%E5%BE%AA%E7%8E%AF%2F</url>
    <content type="text"><![CDATA[任何循环都可以改为递归，关键是发现逻辑“相似性”和递归出口。有些语言没有循环只有递归，如lisp和clojure。拓展尾递归。 数组求和12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package bluebridgecup;public class A01_SumArray &#123; public static int f1(int[] arr) &#123; int sum = 0; for (int i = 0; i &lt; arr.length; i++) &#123; sum += arr[i]; &#125; return sum; &#125; //**************************线性递归************************** public static int f2(int[] arr,int start) &#123; if (start == arr.length) //递归出口 return 0; return arr[start] + f2(arr,start+1); &#125; public static int f3_2(int[] arr,int end) &#123; if (end &lt; 0) //递归出口 return 0; return f3_2(arr,end-1) + arr[end]; &#125; //**************************二分递归************************** public static int f4(int[] arr,int start,int end) &#123; if (start &gt; end) return 0;//递归出口 int mid = (start + end)/2; return f4(arr,start,mid-1) + arr[mid] + f4(arr,mid+1,end);//todo 不如下面的方式 &#125; public static int f5(int[] arr,int start,int end) &#123; if (start == end) return arr[start];//递归出口 int mid = (start + end)/2; return f5(arr,start,mid) + f5(arr,mid+1,end);//注意分为mid mid+1 &#125; public static void main(String[] args) &#123;// int[] arr = new int[]&#123;1,2,3,4,5,6,7,8,9,10&#125;;// System.out.println(f1(arr));// System.out.println(f2(arr,0));// System.out.println(f3_2(arr,arr.length - 1));// System.out.println(f4(arr,0,arr.length - 1));// System.out.println(f5(arr,0,arr.length - 1)); &#125;&#125; 打印数组123456789101112131415161718192021222324252627282930package bluebridgecup;public class A02_PrintArray &#123; private static void f1(int n) &#123; if (n &gt; 0) f1(n - 1); System.out.println(n); &#125; private static void f2(int start, int end) &#123; if (start &gt; end)return; System.out.println(start); f2(start+1,end); &#125; public static void f_arr(int[] arr,int start)&#123; if (start &gt; arr.length - 1)return; System.out.println(arr[start]); f_arr(arr,start+1); &#125; public static void main(String[] args)&#123;// f1(10);// f2(0,10);// int[] arr = new int[]&#123;1,2,3,4,5,6,7,8,9,10&#125;;// fa(arr,0); &#125;&#125; 字符串是否相同1234567891011121314151617181920212223242526package bluebridgecup;public class A03_IsSameString &#123; private static boolean isSameString2(String s1, String s2) &#123; if (s1.length() != s2.length()) return false;//******边界******** if (s1.length() == 0) return true;//******边界********// if (s1.charAt(0) != s2.charAt(0)) return false; if (s1.charAt(0) == s2.charAt(0)) return true; return isSameString2(s1.substring(1),s2.substring(1)); &#125; public static boolean isSameString1(String s1,String s2)&#123; return s1.equals(s2); &#125; public static void main(String[] args) &#123;// String s1 = &quot;abc&quot;;// String s2 = &quot;abcd&quot;;// System.out.println(isSameString1(s1,s2));// System.out.println(isSameString2(s1,s2)); &#125;&#125; 求阶乘123456789101112package bluebridgecup;public class A04_FactorialOfN &#123; public static void main(String[] args) &#123; System.out.println(helper(3)); &#125; private static int helper(int n) &#123; if (n &gt; 0)return n * helper(n - 1); return 1; &#125;&#125; 求n个不同元素的全排列*12345678910111213141516171819202122232425262728293031323334package bluebridgecup;/** * 求n个不同元素的全排列 * * 回溯问题，用于八皇后和迷宫问题 * 如果包含重复元素怎么办? * 罗列出每一种可能用什么方法?动态规划怎么实现 * * * R的全排列可归纳定义如下： * 当n=1时，perm(R)=(r)，其中r是集合R中唯一的元素； * 当n&gt;1时，perm(R)由(r1)perm(R1)，(r2)perm(R2)，…，(rn)perm(Rn)构成。 * 实现思想：将整组数中的所有的数分别与第一个数交换，这样就总是在处理后n-1个数的全排列。 */public class A05_FullPermutation &#123; public static void main(String[] args) &#123; char[] chars = &quot;ABC&quot;.toCharArray(); helper(chars,0); &#125; //k:当前的交换位置(关注点),与其后的元素交换 private static void helper(char[] chars,int k) &#123; if (k == chars.length)&#123; for (char c:chars) System.out.print(c +&quot; &quot;); System.out.println(); &#125; for (int i = k; i &lt; chars.length; i++) &#123; &#123;char t = chars[k];chars[k] = chars[i];chars[i] = t;&#125;//试探 helper(chars,k + 1); &#123;char t = chars[k];chars[k] = chars[i];chars[i] = t;&#125;//回溯 &#125; &#125;&#125; 在n个求中，任意取出m个，求有多少种不同取法12345678910111213141516171819package bluebridgecup;/** * 在n个求中，任意取出m个，求有多少种不同取法 */public class A06_GraspBall &#123; //从n个取m个 public static void main(String[] args) &#123; int k = helper(5,3); System.out.println(k); &#125; private static int helper(int n, int m) &#123; if (n &lt; m) return 0; if (n == m) return 1; if (m == 0) return 1; return helper(n - 1,m - 1) + helper(n - 1,m);//n个里有个特殊球x，取法划分，包不包含x &#125;&#125; 求最长公共子序列的长度12345678910111213141516171819package bluebridgecup;public class A07_MaxSequence &#123; public static void main(String[] args) &#123; String s1 = &quot;abc&quot;; String s2 = &quot;xbacd&quot;; int x = helper(s1,s2); System.out.println(x); &#125; private static int helper(String s1, String s2) &#123; if (s1.length() == 0 || s2.length() == 0)return 0; if (s1.charAt(0) == s2.charAt(0))&#123; return 1 + helper(s1.substring(1),s2.substring(1)); &#125; else &#123; return Math.max(helper(s1,s2.substring(1)),helper(s1.substring(1),s2)); &#125; &#125;&#125; 翻转字符串123456789101112package bluebridgecup;public class A08_Reverse &#123; public static void main(String[] args) &#123; System.out.println(helper_str(&quot;12345&quot;));; &#125; private static String helper_str(String s) &#123; if (s.length() == 0)return &quot;&quot;; return helper_str(s.substring(1))+s.charAt(0); &#125;&#125; 杨辉三角1234567891011121314151617181920212223242526272829package bluebridgecup;/** * 1 * 1 1 * 1 2 1 * 1 3 3 1 * 1 4 6 4 1 * 1 5 10 10 5 1 * * 计算第m层，第n个系数，m和n都从0开始 */public class A09_YangTriangle &#123; public static void main(String[] args) &#123; int level = 5; for (int i = 0; i &lt;= level; i++) &#123; System.out.print(helper(level,i) +&quot; &quot;); &#125; System.out.println(); &#125; //m层第n个元素 private static int helper(int m,int n)&#123; if (n == 0)return 1; if (m == n)return 1; return helper(m - 1,n) + helper(m - 1,n - 1); &#125;&#125; m个a,n个b 可以有多少种组合123456789101112package bluebridgecup;// m个a,n个b 可以有多少种组合public class A10_MN_sort &#123; public static void main(String[] args) &#123; System.out.println(helper(3,2)); &#125; private static int helper(int m, int n) &#123; if (m == 0 || n == 0)return 1; return helper(m - 1,n) + helper(m,n - 1); &#125;&#125; split 612345678910111213141516171819202122232425262728package bluebridgecup;public class A11_Split_6 &#123; public static void main(String[] args) &#123; int[] a = new int[100];//做缓冲 helper(3,a,0); &#125; //对n进行加法划分 // a:缓冲 // k:当前的位置 private static void helper(int n,int[] a,int k) &#123; if (n &lt;= 0)&#123; for(int i=0;i &lt; k;i++)System.out.printf(a[i] + &quot; &quot;); System.out.println(); return; &#125; //6 //5...f(1) //4...f(2) for (int i = n; i &gt; 0; i--) &#123; if (k &gt; 0 &amp;&amp; i &gt; a[k - 1])continue;//a[k - 1]表示前一项 ; 后一项 &gt; 前一项 a[k] = i; helper(n - i,a,k + 1); &#125; &#125;&#125; ErrorSum12345678910111213141516171819202122232425262728293031package bluebridgecup;public class A12_ErrorSum &#123; public static void main(String[] args) &#123; int sum = 6; int[] a = &#123;3,2,4,3,1&#125;; boolean[] b = new boolean[a.length];//表示a对应项是否选取 helper(sum,a,0,0,b); &#125; //error_sum:有错误的和 //a：明细 //k:当前处理的位置 //cur_sum:前边元素的累加和 //b:记录取舍 private static void helper(int error_sum, int[] a, int k, int cur_sum, boolean[] b) &#123; if (cur_sum &gt; error_sum)return; if (error_sum == cur_sum)&#123; for(int i = 0;i &lt; b.length;i++) if (b[i] == true)System.out.printf(a[i]+&quot; &quot;); System.out.println(); return; &#125; if (k &gt;= a.length)return; b[k] = false; helper(error_sum,a,k + 1,cur_sum,b); b[k] = true; cur_sum += a[k]; helper(error_sum,a,k + 1,cur_sum,b); b[k] = false;//回溯！！！！！ &#125;&#125; 打印二叉树从根节点到叶子节点的所有路径1234567891011121314151617181920212223242526272829303132333435363738394041424344package bluebridgecup;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;//打印二叉树从根节点到叶子节点的所有路径public class A13_BSTTreePath &#123; List&lt;String&gt; res = new ArrayList&lt;&gt;(); public List&lt;String&gt; binaryTreePaths(TreeNode root) &#123; if (root == null)return new ArrayList&lt;&gt;(); LinkedList&lt;Integer&gt; list = new LinkedList&lt;&gt;(); helper(root,list); return res; &#125; private void helper(TreeNode node,LinkedList&lt;Integer&gt; list) &#123; if (node == null) return; list.add(node.val); if (node.left == null &amp;&amp; node.right == null)&#123; String s = &quot;&quot;; for (Integer n:list) &#123; if (s.equals(&quot;&quot;))&#123; s += n+&quot;&quot;; &#125; else &#123; s = s + &quot;-&gt;&quot;+ n; &#125; &#125; res.add(s); list.removeLast();//TODO 最后一个节点是叶子节点,继续下一条路线,所以要剔除最后一个 return;//TODO 该次结束，返回到上一层 &#125; helper(node.left,list); helper(node.right,list); list.removeLast();//TODO 返回时一定要清除 最后一个节点是最后一个叉的根节点,一定是要排除的,因为这个节点的左右方向都走完了 &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125; 汉诺塔1234567891011121314151617package bluebridgecup;public class A14_Hanoi &#123; public static void main(String[] args) &#123; hanoi(2,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;); &#125; public static void hanoi(int n,String start,String middle,String end)&#123; if (n &lt;= 1)&#123; System.out.println(start+&quot;--&gt;&quot;+end); &#125; else &#123; hanoi(n -1,start,end,middle); System.out.println(start+&quot;--&gt;&quot;+end); hanoi(n - 1,middle,start,end); &#125; &#125;&#125; 遍历二叉树1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package bluebridgecup;import java.util.ArrayList;public class A15_TreeRec &#123; /** * 中序遍历递归解法 * （1）如果二叉树为空，空操作。 * （2）如果二叉树不为空，中序遍历左子树，访问根节点，中序遍历右子树 */ public void inOrderRec(TreeNode root)&#123; if (root == null)return; inOrderRec(root.left); System.out.println(root.val); inOrderRec(root.right); &#125; /** * 分层遍历二叉树（递归） * 很少有人会用递归去做level traversal * 基本思想是用一个大的ArrayList，里面包含了每一层的ArrayList。 * 大的ArrayList的size和level有关系 * */ public static void levelTraversalRec(TreeNode root) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; ret = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); dfs(root, 0, ret); System.out.println(ret); &#125; private static void dfs(TreeNode root, int level, ArrayList&lt;ArrayList&lt;Integer&gt;&gt; ret)&#123; if(root == null)&#123; return; &#125; // 添加一个新的ArrayList表示新的一层 if(level &gt;= ret.size())&#123; ret.add(new ArrayList&lt;Integer&gt;()); &#125; ret.get(level).add(root.val); // 把节点添加到表示那一层的ArrayList里 dfs(root.left, level+1, ret); // 递归处理下一层的左子树和右子树 dfs(root.right, level+1, ret); &#125; /** * 求二叉树中的节点个数递归解法： O(n) * （1）如果二叉树为空，节点个数为0 * （2）如果二叉树不为空，二叉树节点个数 = 左子树节点个数 + 右子树节点个数 + 1 */ public static int getNodeNumRec(TreeNode root) &#123; if (root == null) &#123; return 0; &#125; else &#123; return getNodeNumRec(root.left) + getNodeNumRec(root.right) + 1; //左 右节点加上主节点1为总数 &#125; &#125; /** * 求二叉树的深度（高度） 递归解法： O(n) * （1）如果二叉树为空，二叉树的深度为0 * （2）如果二叉树不为空，二叉树的深度 = max(左子树深度， 右子树深度) + 1 * * * maxDepth() 1. 如果树为空，那么返回0 2. 否则 (a) 递归得到左子树的最大高度 例如，调用maxDepth( tree-&gt; left-subtree ) (b) 递归得到右子树的最大高度 例如，调用maxDepth( tree-&gt; right-subtree ) (c) 对于当前节点，取左右子树高度的最大值并加1。 max_depth = max(左子树的最大高度, 右子树的最大高度) + 1 (d) 返回max_depth */ public static int getDepthRec(TreeNode root) &#123; if (root == null) &#123; return 0; &#125; int leftDepth = getDepthRec(root.left); int rightDepth = getDepthRec(root.right); return Math.max(leftDepth, rightDepth) + 1; // +1是因为根节点已经是一层了,否则root==null直接是0了 &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125; 翻转链表1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.lifeibigdata.offer;import java.util.Stack;/** * 从尾到头打印链表 * Created by lifei on 16/11/13. */public class PrintListReverse &#123; private static class Node &#123; int val; Node next; public Node(int val) &#123; this.val = val; &#125; &#125; public static void main(String[] args) &#123; Node n1 = new Node(1); Node n2 = new Node(2);Node n3 = new Node(3); Node n4 = new Node(4);Node n5 = new Node(5); n1.next = n2;n2.next = n3;n3.next = n4;n4.next = n5; printListReverse2(n1); &#125; static void printListReverse1(Node head)&#123; Stack&lt;Node&gt; stack = new Stack(); Node tmpNode = head; while (tmpNode != null)&#123; stack.push(tmpNode); tmpNode = tmpNode.next; &#125; while (!stack.isEmpty())&#123; System.out.println(stack.pop().val); &#125; &#125; /** * 栈的本质是递归,要实现反过来输出链表,我们访问到一个节点的时候,先递归输出后面的节点,再输入该节点本身,这样链表的输出结果就反过来了 * @param head */ static void printListReverse2(Node head)&#123; if (head != null)&#123; if (head.next != null)&#123; printListReverse2(head.next); &#125; System.out.println(head.val); &#125; &#125;&#125;]]></content>
      <categories>
        <category>recursion</category>
      </categories>
      <tags>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive窗口函数]]></title>
    <url>%2F2017%2F02%2F12%2Fhive%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Hive窗口函数可以计算一定范围内、一定值域内、或者一段时间内的累积和以及移动平均值等；可以结合聚集函数SUM() 、AVG()等使用；可以结合FIRST_VALUE() 和LAST_VALUE()，返回窗口的第一个和最后一个值。 如果只使用partition by子句,未指定order by的话,我们的聚合是分组内的聚合. 使用了order by子句,未使用window子句的情况下,默认从起点到当前行.window子句： PRECEDING：往前 FOLLOWING：往后 CURRENT ROW：当前行 UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING：表示到后面的终点 Ntile它把有序的数据集合 平均分配 到 指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。 语法 ntile (num) over ([partition_clause] order_by_clause) as your_bucket_num 可以根据桶号，选取前或后 n分之几的数据。 Rank，Dense_Rank, Row_NumberSQL很熟悉的3个组内排序函数 三者语法相同： R() over (partion by col1… order by col2… desc/asc) rank 会对相同数值，输出相同的序号，而且下一个序号不间断； dense_rank 会对相同数值，输出相同的序号，但下一个序号，间断 row_number 会对所有数值输出不同的序号，序号唯一连续； Lag, Lead, First_value,Last_valueLag, LeadLAG(col,n,DEFAULT) 用于统计窗口内往上第n行值 LEAD(col,n,DEFAULT) 用于统计窗口内往下第n行值, 与LAG相反 FIRST_VALUE, LAST_VALUEfirst_value: 取分组内排序后，截止到当前行，第一个值 last_value: 取分组内排序后，截止到当前行，最后一个值 参考: http://www.cnblogs.com/skyEva/p/5730531.html]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python fabric使用介绍]]></title>
    <url>%2F2016%2F02%2F21%2FPython-fabric%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[简介Fabric 是一个python的库，可以通过ssh批量管理服务器，将自动化部署或者多机操作的命令固化到一个脚本里，化繁为简 安装首先安装依赖包1yum install -y python-pip gcc python-devel 获取脚本(约1.4M)，执行脚本安装pip12wget https://bootstrap.pypa.io/get-pip.pypython get-pip.py 安装Fabric1pip install fabric 测试安装结果，输出当前Fabric版本号1python -c &quot;from fabric.api import * ; print env.version&quot; 使用方法Fabric以函数为单位12env.hosts ssh要用到的参数（账户、ip、端口）env.password ssh密码 调用命令123local() 本地执行操作run() 远程操作 fab命令参数 123456789-l 显示可用的task(命令)-f 指定入口文件-P 并发数，默认是串行-w warn_only,默认遇到异常直接退出-H 指定host,支持多个host，用逗号分隔 实例：12fab -f hbase_install.py mkdir installfab -H system1,system2,system3 -- uname -a]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql千万级分页]]></title>
    <url>%2F2016%2F02%2F17%2Fmysql%E5%8D%83%E4%B8%87%E7%BA%A7%E5%88%86%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[Mysql数据库最简单，是利用mysql的LIMIT函数,LIMIT [offset,] rows从数据库表中M条记录(不包含m条)开始检索N条记录的语句为：1SELECT * FROM 表名称 LIMIT M,N 其中limit为可选项，例如我们有个student表，我们选取前5条记录可以使用下面的sql语句1select * from student limit 5; 问题：千万级的表查询分页，怎么优化查询12select * from user limit 10000000,10select * from user where name=&quot;a&quot; limit 10000000,10 为了能更加优化查询，建立联合索引是一个好的方法，where 的条件 和主键id 作为索引 serach(name,id) 第一次建立索引时候 是id 在前 name在后，这样确实也解决这样的问题，都变成0.5s左右 ，但是还不是我想要的 ，最后上网搜索答案，发现建立联合索引顺序不同，性能也就大大提高，于是把建立索引顺序变成是id 在后 name在前。结果打出意料 ，速度变成了0.05秒 。性能大幅度提升. 开始的select id from collect order by id limit 90000,10;这么快就是因为走了索引，可是如果加了where 就不走索引了。抱着试试看的想法加了 search(vtype,id) 这样的索引。然后测试select id from collect where vtype=1 limit 90000,10; 非常快！0.04秒完成！再测试: select id ,title from collect where vtype=1 limit 90000,10; 非常遗憾，8-9秒，没走search索引！再测试：search(id,vtype)，还是select id 这个语句，也非常遗憾，0.5秒。综上：如果对于有where 条件，又想走索引用limit的，必须设计一个索引，将where放第一位，limit用到的主键放第2位，而且只能select 主键！即search(vtype,id) 这样的索引,如下查询select id from collect where vtype=1 limit 90000,10可以快速返回id，然后根据id in (xxxx)的方式去实现 参考:https://blog.csdn.net/m0_37922390/article/details/81976014https://blog.csdn.net/li772030428/article/details/52839987/]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[题目]]></title>
    <url>%2F2016%2F02%2F14%2F%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[简答说一下hadoop的map-reduce编程模型首先map task会从本地文件系统读取数据，转换成key-value形式的键值对集合 使用的是hadoop内置的数据类型，比如longwritable、text等 将键值对集合输入mapper进行业务处理过程，将其转换成需要的key-value在输出 之后会进行一个partition分区操作，默认使用的是hashpartitioner，可以通过重写hashpartitioner的getpartition方法来自定义分区规则 之后会对key进行进行sort排序，grouping分组操作将相同key的value合并分组输出，在这里可以使用自定义的数据类型，重写WritableComparator的Comparator方法来自定义排序规则，重写RawComparator的compara方法来自定义分组规则 之后进行一个combiner归约操作，其实就是一个本地段的reduce预处理，以减小后面shufle和reducer的工作量 reduce task会通过网络将各个数据收集进行reduce处理，最后将数据保存或者显示，结束整个job hadoop的TextInputFormat作用是什么，如何自定义实现InputFormat会在map操作之前对数据进行两方面的预处理1是getSplits，返回的是InputSplit数组，对数据进行split分片，每片交给map操作一次2是getRecordReader，返回的是RecordReader对象，对每个split分片进行转换为key-value键值对格式传递给map 常用的InputFormat是TextInputFormat，使用的是LineRecordReader对每个分片进行键值对的转换，以行偏移量作为键，行内容作为值 自定义类继承InputFormat接口，重写createRecordReader和isSplitable方法在createRecordReader中可以自定义分隔符 hadoop和spark的都是并行计算，那么他们有什么相同和区别两者都是用mr模型来进行并行计算，hadoop的一个作业称为job，job里面分为map task和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束 spark用户提交的任务成为application，一个application对应一个sparkcontext，一个application中存在多个job，每触发一次action操作就会产生一个job,这些job可以并行或串行执行，每个job中有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行，executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算 hadoop的job只有map和reduce操作，表达能力比较欠缺而且在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系 spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作如join，groupby等，而且通过DAG图可以实现良好的容错 为什么要用flume导入hdfs，hdfs的构架是怎样的flume可以实时的导入数据到hdfs中，当hdfs上的文件达到一个指定大小的时候会形成一个文件，或者超过指定时间的话也形成一个文件 文件都是存储在datanode上面的，namenode记录着datanode的元数据信息，而namenode的元数据信息是存在内存中的，所以当文件切片很小或者很多的时候会卡死 简单说一下hadoop和spark的shuffle过程hadoop：map端保存分片数据，通过网络收集到reduce端spark：spark的shuffle是在DAGSchedular划分Stage的时候产生的，TaskSchedule要分发Stage到各个worker的executor 减少shuffle可以提高性能 Hive中存放是什么？表。存的是和hdfs的映射关系，hive是逻辑上的dw，实际操作的都是hdfs上的文件，HQL就是用sql语法来写的mr程序。 Flume工作机制是什么？核心概念是agent，里面包括source、chanel和sink三个组件。source运行在日志收集节点进行日志采集，之后临时存储在chanel中，sink负责将chanel中的数据发送到目的地。只有成功发送之后chanel中的数据才会被删除。首先书写flume配置文件，定义agent、source、chanel和sink然后将其组装，执行flume-ng命令。 Sqoop工作原理是什么？hadoop生态圈上的数据传输工具。可以将关系型数据库的数据导入非结构化的hdfs、hive或者bbase中，也可以将hdfs中的数据导出到关系型数据库或者文本文件中。使用的是mr程序来执行任务，使用jdbc和关系型数据库进行交互。import原理：通过指定的分隔符进行数据切分，将分片传入各个map中，在map任务中在每行数据进行写入处理没有reduce。export原理：根据要操作的表名生成一个java类，并读取其元数据信息和分隔符对非结构化的数据进行匹配，多个map作业同时执行写入关系型数据库 Hbase行健列族的概念，物理模型，表的设计原则？行健：是hbase表自带的，每个行健对应一条数据。列族：是创建表时指定的，为列的集合，每个列族作为一个文件单独存储，存储的数据都是字节数组，其中的数据可以有很多，通过时间戳来区分。物理模型：整个hbase表会拆分为多个region，每个region记录着行健的起始点保存在不同的节点上，查询时就是对各个节点的并行查询，当region很大时使用.META表存储各个region的起始点，-ROOT又可以存储.META的起始点。rowkey的设计原则：各个列簇数据平衡，长度原则、相邻原则，创建表的时候设置表放入regionserver缓存中，避免自动增长和时间，使用字节数组代替string，最大长度64kb，最好16字节以内，按天分表，两个字节散列，四个字节存储时分毫秒。列族的设计原则：尽可能少（按照列族进行存储，按照region进行读取，不必要的io操作），经常和不经常使用的两类数据放入不同列族中，列族名字尽可能短。 Spark Streaming和Storm有何区别？一个实时毫秒一个准实时亚秒，不过storm的吞吐率比较低 mllib支持的算法？大体分为四大类，分类、聚类、回归、协同过滤。 Hadoop平台集群配置、环境变量设置？zookeeper：修改zoo.cfg文件，配置dataDir，和各个zk节点的server地址端口，tickTime心跳时间默认是2000ms，其他超时的时间都是以这个为基础的整数倍，之后再dataDir对应目录下写入myid文件和zoo.cfg中的server相对应。 hadoop：修改hadoop-env.sh配置java环境变量core-site.xml配置zk地址，临时目录等hdfs-site.xml配置nn信息，rpc和http通信地址，nn自动切换、zk连接超时时间等yarn-site.xml配置resourcemanager地址mapred-site.xml配置使用yarnslaves配置节点信息格式化nn和zk。 hbase：修改hbase-env.sh配置java环境变量和是否使用自带的zkhbase-site.xml配置hdfs上数据存放路径，zk地址和通讯超时时间、master节点regionservers配置各个region节点zoo.cfg拷贝到conf目录下 spark：安装Scala修改spark-env.sh配置环境变量和master和worker节点配置信息 环境变量的设置：直接在/etc/profile中配置安装的路径即可，或者在当前用户的宿主目录下，配置在.bashrc文件中，该文件不用source重新打开shell窗口即可，配置在.bash_profile的话只对当前用户有效。 Hadoop性能调优？调优可以通过系统配置、程序编写和作业调度算法来进行。hdfs的block.size可以调到128/256（网络很好的情况下，默认为64）调优的大头：mapred.map.tasks、mapred.reduce.tasks设置mr任务数（默认都是1）mapred.tasktracker.map.tasks.maximum每台机器上的最大map任务数mapred.tasktracker.reduce.tasks.maximum每台机器上的最大reduce任务数mapred.reduce.slowstart.completed.maps配置reduce任务在map任务完成到百分之几的时候开始进入这个几个参数要看实际节点的情况进行配置，reduce任务是在33%的时候完成copy，要在这之前完成map任务，（map可以提前完成）mapred.compress.map.output,mapred.output.compress配置压缩项，消耗cpu提升网络和磁盘io合理利用combiner注意重用writable对象 spark有哪些组件？（1）master：管理集群和节点，不参与计算。（2）worker：计算节点，进程本身不参与计算，和master汇报。（3）Driver：运行程序的main方法，创建spark context对象。（4）spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。（5）client：用户提交程序的入口。 spark工作机制？用户在client端提交作业后，会由Driver运行main方法并创建spark context上下文。执行add算子，形成dag图输入dagscheduler，按照add之间的依赖关系划分stage输入task scheduler。task scheduler会将stage划分为task set分发到各个节点的executor中执行。 spark的优化怎么做？通过spark-env文件、程序中sparkconf和set property设置。（1）计算量大，形成的lineage过大应该给已经缓存了的rdd添加checkpoint，以减少容错带来的开销。（2）小分区合并，过小的分区造成过多的切换任务开销，使用repartition。 kafka工作原理？producer向broker发送事件，consumer从broker消费事件。事件由topic区分开，每个consumer都会属于一个group。相同group中的consumer不能重复消费事件，而同一事件将会发送给每个不同group的consumer。 转：https://www.cnblogs.com/jchubby/p/5449379.html]]></content>
      <categories>
        <category>题目</category>
      </categories>
      <tags>
        <tag>题目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapreduce模型]]></title>
    <url>%2F2016%2F01%2F26%2Fmapreduce%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[graph TD A[Map] -->|partition,kvBuffer,sort,spill与combiner,merge| B{Shuffle} B -->|copy,cache_sort_merge,spill,merge,file,GroupingComparator| C[reduce] 从JVM的角度看Map和Reduce Map阶段包括：第一读数据：从HDFS读取数据1、问题:读取数据产生多少个Mapper？？ Mapper数据过大的话，会产生大量的小文件，由于Mapper是基于虚拟机的，过多的Mapper创建和初始化及关闭虚拟机都会消耗大量的硬件资源； Mapper数太小，并发度过小，Job执行时间过长，无法充分利用分布式硬件资源； 2、Mapper数量由什么决定？？ （1）输入文件数目 （2）输入文件的大小 （3）配置参数 所以说map数是可以控制的这三个因素决定的。 涉及参数：mapreduce.input.fileinputformat.split.minsize //启动map最小的split size大小，默认0 mapreduce.input.fileinputformat.split.maxsize //启动map最大的split size大小，默认256Mdfs.block.size//block块大小，默认64M计算公式：splitSize = Math.max(minSize, Math.min(maxSize, blockSize)); 例如默认情况下：例如一个文件800M，Block大小是128M，那么Mapper数目就是7个。6个Mapper处理的数据是128M，1个Mapper处理的数据是32M；再例如一个目录下有三个文件大小分别为：5M10M 150M 这个时候其实会产生四个Mapper处理的数据分别是5M，10M，128M，22M。 Mapper是基于文件自动产生的，如果想要自己控制Mapper的个数？？？ 就如上面，5M，10M的数据很快处理完了，128M要很长时间；这个就需要通过参数的控制来调节Mapper的个数。 减少Mapper的个数的话，就要合并小文件，这种小文件有可能是直接来自于数据源的小文件，也可能是Reduce产生的小文件。 设置合并器：（set都是在hive脚本，也可以配置Hadoop） 设置合并器本身：set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;set hive.merge.mapFiles=true;set hive.merge.mapredFiles=true;set hive.merge.size.per.task=256000000;//每个Mapper要处理的数据，就把上面的5M10M……合并成为一个 一般还要配合一个参数：set mapred.max.split.size=256000000 // mapred切分的大小set mapred.min.split.size.per.node=128000000//低于128M就算小文件，数据在一个节点会合并，在多个不同的节点会把数据抓过来进行合并。 Hadoop中的参数：可以通过控制文件的数量控制mapper数量mapreduce.input.fileinputformat.split.minsize（default：0），小于这个值会合并mapreduce.input.fileinputformat.split.maxsize 大于这个值会切分 第二处理数据：Partition说明对于map输出的每一个键值对，系统都会给定一个partition，partition值默认是通过计算key的hash值后对Reduce task的数量取模获得。如果一个键值对的partition值为1，意味着这个键值对会交给第一个Reducer处理。 自定义partitioner的情况：1、我们知道每一个Reduce的输出都是有序的，但是将所有Reduce的输出合并到一起却并非是全局有序的，如果要做到全局有序，我们该怎么做呢？最简单的方式，只设置一个Reduce task，但是这样完全发挥不出集群的优势，而且能应对的数据量也很受限。最佳的方式是自己定义一个Partitioner，用输入数据的最大值除以系统Reduce task数量的商作为分割边界，也就是说分割数据的边界为此商的1倍、2倍至numPartitions-1倍，这样就能保证执行partition后的数据是整体有序的。2、解决数据倾斜：另一种需要我们自己定义一个Partitioner的情况是各个Reduce task处理的键值对数量极不平衡。对于某些数据集，由于很多不同的key的hash值都一样，导致这些键值对都被分给同一个Reducer处理，而其他的Reducer处理的键值对很少，从而拖延整个任务的进度。当然，编写自己的Partitioner必须要保证具有相同key值的键值对分发到同一个Reducer。3、自定义的Key包含了好几个字段，比如自定义key是一个对象，包括type1，type2，type3,只需要根据type1去分发数据，其他字段用作二次排序。 环形缓冲区 Map的输出结果是由collector处理的，每个Map任务不断地将键值对输出到在内存中构造的一个环形数据结构中。使用环形数据结构是为了更有效地使用内存空间，在内存中放置尽可能多的数据。 这个数据结构其实就是个字节数组，叫Kvbuffer，名如其义，但是这里面不光放置了数据，还放置了一些索引数据，给放置索引数据的区域起了一个Kvmeta的别名，在Kvbuffer的一块区域上穿了一个IntBuffer（字节序采用的是平台自身的字节序）的马甲。数据区域和索引数据区域在Kvbuffer中是相邻不重叠的两个区域，用一个分界点来划分两者，分界点不是亘古不变的，而是每次Spill之后都会更新一次。初始的分界点是0，数据的存储方向是向上增长，索引数据的存储方向是向下增长Kvbuffer的存放指针bufindex是一直闷着头地向上增长，比如bufindex初始值为0，一个Int型的key写完之后，bufindex增长为4，一个Int型的value写完之后，bufindex增长为8。 索引是对在kvbuffer中的键值对的索引，是个四元组，包括：value的起始位置、key的起始位置、partition值、value的长度，占用四个Int长度，Kvmeta的存放指针Kvindex每次都是向下跳四个“格子”，然后再向上一个格子一个格子地填充四元组的数据。比如Kvindex初始位置是-4，当第一个键值对写完之后，(Kvindex+0)的位置存放value的起始位置、(Kvindex+1)的位置存放key的起始位置、(Kvindex+2)的位置存放partition的值、(Kvindex+3)的位置存放value的长度，然后Kvindex跳到-8位置，等第二个键值对和索引写完之后，Kvindex跳到-12位置。 第三写数据到磁盘Mapper中的Kvbuffer的大小默认100M，可以通过mapreduce.task.io.sort.mb（default：100）参数来调整。可以根据不同的硬件尤其是内存的大小来调整，调大的话，会减少磁盘spill的次数此时如果内存足够的话，一般都会显著提升性能。spill一般会在Buffer空间大小的80%开始进行spill（因为spill的时候还有可能别的线程在往里写数据，因为还预留空间，有可能有正在写到Buffer中的数据），可以通过mapreduce.map.sort.spill.percent（default：0.80）进行调整，Map Task在计算的时候会不断产生很多spill文件，在Map Task结束前会对这些spill文件进行合并，这个过程就是merge的过程。 mapreduce.task.io.sort.factor（default：10），代表进行merge的时候最多能同时merge多少spill，如果有100个spill个文件，此时就无法一次完成整个merge的过程，这个时候需要调大mapreduce.task.io.sort.factor（default：10）来减少merge的次数，从而减少磁盘的操作； Spill这个重要的过程是由Spill线程承担，Spill线程从Map任务接到“命令”之后就开始正式干活，干的活叫SortAndSpill，原来不仅仅是Spill，在Spill之前还有个颇具争议性的Sort。 Combiner存在的时候，此时会根据Combiner定义的函数对map的结果进行合并，什么时候进行Combiner操作呢？？？和Map在一个JVM中，是由min.num.spill.for.combine的参数决定的，默认是3，也就是说spill的文件数在默认情况下由三个的时候就要进行combine操作，最终减少磁盘数据； 减少磁盘IO和网络IO还可以进行：压缩，对spill，merge文件都可以进行压缩。中间结果非常的大，IO成为瓶颈的时候压缩就非常有用，可以通过mapreduce.map.output.compress（default：false）设置为true进行压缩，数据会被压缩写入磁盘，读数据读的是压缩数据需要解压，在实际经验中Hive在Hadoop的运行的瓶颈一般都是IO而不是CPU，压缩一般可以10倍的减少IO操作，压缩的方式Gzip，Lzo,BZip2,Lzma等，其中Lzo是一种比较平衡选择，mapreduce.map.output.compress.codec（default：org.apache.hadoop.io.compress.DefaultCodec）参数设置。但这个过程会消耗CPU，适合IO瓶颈比较大。 Shuffle和Reduce阶段包括：一、Copy1、由于job的每一个map都会根据reduce(n)数将数据分成map 输出结果分成n个partition，所以map的中间结果中是有可能包含每一个reduce需要处理的部分数据的。所以，为了优化reduce的执行时间，hadoop中是等job的第一个map结束后，所有的reduce就开始尝试从完成的map中下载该reduce对应的partition部分数据，因此map和reduce是交叉进行的，其实就是shuffle。Reduce任务通过HTTP向各个Map任务拖取（下载）它所需要的数据（网络传输），Reducer是如何知道要去哪些机器取数据呢？一旦map任务完成之后，就会通过常规心跳通知应用程序的Application Master。reduce的一个线程会周期性地向master询问，直到提取完所有数据（如何知道提取完？）数据被reduce提走之后，map机器不会立刻删除数据，这是为了预防reduce任务失败需要重做。因此map输出数据是在整个作业完成之后才被删除掉的。2、reduce进程启动数据copy线程(Fetcher)，通过HTTP方式请求maptask所在的TaskTracker获取maptask的输出文件。由于map通常有许多个，所以对一个reduce来说，下载也可以是并行的从多个map下载，那到底同时到多少个Mapper下载数据？？这个并行度是可以通过mapreduce.reduce.shuffle.parallelcopies(default5）调整。默认情况下，每个Reducer只会有5个map端并行的下载线程在从map下数据，如果一个时间段内job完成的map有100个或者更多，那么reduce也最多只能同时下载5个map的数据，所以这个参数比较适合map很多并且完成的比较快的job的情况下调大，有利于reduce更快的获取属于自己部分的数据。 在Reducer内存和网络都比较好的情况下，可以调大该参数；3、reduce的每一个下载线程在下载某个map数据的时候，有可能因为那个map中间结果所在机器发生错误，或者中间结果的文件丢失，或者网络瞬断等等情况，这样reduce的下载就有可能失败，所以reduce的下载线程并不会无休止的等待下去，当一定时间后下载仍然失败，那么下载线程就会放弃这次下载，并在随后尝试从另外的地方下载（因为这段时间map可能重跑）。reduce下载线程的这个最大的下载时间段是可以通过mapreduce.reduce.shuffle.read.timeout（default180000秒）调整的。如果集群环境的网络本身是瓶颈，那么用户可以通过调大这个参数来避免reduce下载线程被误判为失败的情况。一般情况下都会调大这个参数，这是企业级最佳实战。 二、MergeSort1、这里的merge和map端的merge动作类似，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，然后当使用内存达到一定量的时候才spill磁盘。这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置。这个内存大小的控制就不像map一样可以通过io.sort.mb来设定了，而是通过另外一个参数 mapreduce.reduce.shuffle.input.buffer.percent（default 0.7f 源码里面写死了） 来设置，这个参数其实是一个百分比，意思是说，shuffile在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of reduce task。JVM的heapsize的70%。内存到磁盘merge的启动门限可以通过mapreduce.reduce.shuffle.merge.percent（default0.66）配置。也就是说，如果该reduce task的最大heap使用量（通常通过mapreduce.admin.reduce.child.java.opts来设置，比如设置为-Xmx1024m）的一定比例用来缓存数据。默认情况下，reduce会使用其heapsize的70%来在内存中缓存数据。假设 mapreduce.reduce.shuffle.input.buffer.percent 为0.7，reducetask的max heapsize为1G，那么用来做下载数据缓存的内存就为大概700MB左右。这700M的内存，跟map端一样，也不是要等到全部写满才会往磁盘刷的，而是当这700M中被使用到了一定的限度（通常是一个百分比），就会开始往磁盘刷（刷磁盘前会先做sortMerge）。这个限度阈值也是可以通过参数 mapreduce.reduce.shuffle.merge.percent（default0.66）来设定。与map 端类似，这也是溢写的过程，这个过程中如果你设置有Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。这种merge方式一直在运行，直到没有map端的数据时才结束，然后启动磁盘到磁盘的merge方式生成最终的那个文件。 这里需要强调的是，merge有三种形式：1)内存到内存（memToMemMerger）2）内存中Merge（inMemoryMerger）3）磁盘上的Merge（onDiskMerger）具体包括两个：（一）Copy过程中磁盘合并（二）磁盘到磁盘。（1）内存到内存Merge（memToMemMerger） Hadoop定义了一种MemToMem合并，这种合并将内存中的map输出合并，然后再写入内存。这种合并默认关闭，可以通过mapreduce.reduce.merge.memtomem.enabled(default:false) 打开，当map输出文件达到mapreduce.reduce.merge.memtomem.threshold时，触发这种合并。（2）内存中Merge（inMemoryMerger）：当缓冲中数据达到配置的阈值时，这些数据在内存中被合并、写入机器磁盘。阈值有2种配置方式： 配置内存比例：前面提到reduceJVM堆内存的一部分用于存放来自map任务的输入，在这基础之上配置一个开始合并数据的比例。假设用于存放map输出的内存为500M，mapreduce.reduce.shuffle.merge.percent配置为0.66，则当内存中的数据达到330M的时候，会触发合并写入。配置map输出数量： 通过mapreduce.reduce.merge.inmem.threshold配置。在合并的过程中，会对被合并的文件做全局的排序。如果作业配置了Combiner，则会运行combine函数，减少写入磁盘的数据量。（3）磁盘上的Merge（onDiskMerger）： （3.1）Copy过程中磁盘Merge：在copy过来的数据不断写入磁盘的过程中，一个后台线程会把这些文件合并为更大的、有序的文件。如果map的输出结果进行了压缩，则在合并过程中，需要在内存中解压后才能给进行合并。这里的合并只是为了减少最终合并的工作量，也就是在map输出还在拷贝时，就开始进行一部分合并工作。合并的过程一样会进行全局排序。（3.2）最终磁盘中Merge：当所有map输出都拷贝完毕之后，所有数据被最后合并成一个整体有序的文件，作为reduce任务的输入。这个合并过程是一轮一轮进行的，最后一轮的合并结果直接推送给reduce作为输入，节省了磁盘操作的一个来回。最后（所以map输出都拷贝到reduce之后）进行合并的map输出可能来自合并后写入磁盘的文件，也可能来及内存缓冲，在最后写入内存的map输出可能没有达到阈值触发合并，所以还留在内存中。 每一轮合并不一定合并平均数量的文件数，指导原则是使用整个合并过程中写入磁盘的数据量最小，为了达到这个目的，则需要最终的一轮合并中合并尽可能多的数据，因为最后一轮的数据直接作为reduce的输入，无需写入磁盘再读出。因此我们让最终的一轮合并的文件数达到最大，即合并因子的值，通过mapreduce.task.io.sort.factor（default：10）来配置。 如上图：Reduce阶段中一个Reduce过程 可能的合并方式为：假设现在有20个map输出文件，合并因子配置为5，则需要4轮的合并。最终的一轮确保合并5个文件，其中包括2个来自前2轮的合并结果，因此原始的20个中，再留出3个给最终一轮。 三、Reduce函数调用（用户自定义业务逻辑）1、当reduce将所有的map上对应自己partition的数据下载完成后，就会开始真正的reduce计算阶段。reducetask真正进入reduce函数的计算阶段，由于reduce计算时肯定也是需要消耗内存的，而在读取reduce需要的数据时，同样是需要内存作为buffer，这个参数是控制，reducer需要多少的内存百分比来作为reduce读已经sort好的数据的buffer大小？？默认用多大内存呢？？默认情况下为0，也就是说，默认情况下，reduce是全部从磁盘开始读处理数据。可以用mapreduce.reduce.input.buffer.percent（default 0.0）(源代码MergeManagerImpl.java：674行)来设置reduce的缓存。如果这个参数大于0，那么就会有一定量的数据被缓存在内存并输送给reduce，当reduce计算逻辑消耗内存很小时，可以分一部分内存用来缓存数据，可以提升计算的速度。所以默认情况下都是从磁盘读取数据，如果内存足够大的话，务必设置该参数让reduce直接从缓存读数据，这样做就有点Spark Cache的感觉；2、Reduce在这个阶段，框架为已分组的输入数据中的每个 &lt;key, (list of values)&gt;对调用一次 reduce(WritableComparable,Iterator, OutputCollector, Reporter)方法。Reduce任务的输出通常是通过调用 OutputCollector.collect(WritableComparable,Writable)写入文件系统的。Reducer的输出是没有排序的,按照reduce的输出顺序，所以用户可以在reduce中自己实现排序。 性能调优如果能够根据情况对shuffle过程进行调优，对于提供MapReduce性能很有帮助。相关的参数配置列在后面的表格中。 一个通用的原则是给shuffle过程分配尽可能大的内存，当然你需要确保map和reduce有足够的内存来运行业务逻辑。因此在实现Mapper和Reducer时，应该尽量减少内存的使用，例如避免在Map中不断地叠加。 运行map和reduce任务的JVM，内存通过mapred.child.java.opts属性来设置，尽可能设大内存。容器的内存大小通过mapreduce.map.memory.mb和mapreduce.reduce.memory.mb来设置，默认都是1024M。 map优化 在map端，避免写入多个spill文件可能达到最好的性能，一个spill文件是最好的。通过估计map的输出大小，设置合理的mapreduce.task.io.sort.*属性，使得spill文件数量最小。例如尽可能调大mapreduce.task.io.sort.mb。 map端相关的属性如下表： reduce优化 在reduce端，如果能够让所有数据都保存在内存中，可以达到最佳的性能。通常情况下，内存都保留给reduce函数，但是如果reduce函数对内存需求不是很高，将mapreduce.reduce.merge.inmem.threshold（触发合并的map输出文件数）设为0，mapreduce.reduce.input.buffer.percent（用于保存map输出文件的堆内存比例）设为1.0，可以达到很好的性能提升。在2008年的TB级别数据排序性能测试中，Hadoop就是通过将reduce的中间数据都保存在内存中胜利的。 reduce端相关属性： 通用优化Hadoop默认使用4KB作为缓冲，这个算是很小的，可以通过io.file.buffer.size来调高缓冲池大小。]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L897_IncreasingBST]]></title>
    <url>%2F2016%2F01%2F26%2FL897-IncreasingBST%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package LeetCode.topic.tree;/** * 给定一个树，按中序遍历重新排列树，使树中最左边的结点现在是树的根，并且每个结点没有左子结点，只有一个右子结点。 * * * * 示例 ： * * 输入：[5,3,6,2,4,null,8,1,null,null,null,7,9] * * 5 * / \ * 3 6 * / \ \ * 2 4 8 * / / \ * 1 7 9 * * 输出：[1,null,2,null,3,null,4,null,5,null,6,null,7,null,8,null,9] * * 1 * \ * 2 * \ * 3 * \ * 4 * \ * 5 * \ * 6 * \ * 7 * \ * 8 * \ * 9 * * * 提示： * * 给定树中的结点数介于 1 和 100 之间。 * 每个结点都有一个从 0 到 1000 范围内的唯一整数值。 */public class L897_IncreasingBST &#123; TreeNode dummy = new TreeNode(-1); TreeNode res = dummy; public TreeNode increasingBST(TreeNode root) &#123; helper(root); return res.right; &#125; private void helper(TreeNode root) &#123;// if (root == null) return; if (root.left != null) helper(root.left); dummy.right = new TreeNode(root.val); dummy = dummy.right; if (root.right != null) helper(root.right); &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L965_IsUnivalTree]]></title>
    <url>%2F2016%2F01%2F26%2FL965-IsUnivalTree%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425package LeetCode.topic.tree;public class L965_IsUnivalTree &#123; public boolean isUnivalTree(TreeNode root) &#123; if(root == null)&#123; return true; &#125; return helper(root,root.val); &#125; public boolean helper(TreeNode node,int val)&#123; if(node == null)return true; if(node.val != val)return false; if(node.left != null &amp;&amp; node.left.val != val) return false; if(node.right != null &amp;&amp; node.right.val != val) return false; return helper(node.left,val) &amp;&amp; helper(node.right,val); &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L700_SearchBST]]></title>
    <url>%2F2016%2F01%2F26%2FL700-SearchBST%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425package LeetCode.topic.tree;public class L700_SearchBST &#123; public TreeNode searchBST(TreeNode root, int val) &#123; if(root == null) return null; if(root.val == val) return root; TreeNode left = searchBST(root.left,val); TreeNode right = searchBST(root.right,val); if(left != null)&#123; return left; &#125; if(right != null)&#123; return right; &#125; return null; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L687_LongestUnivaluePath]]></title>
    <url>%2F2016%2F01%2F26%2FL687-LongestUnivaluePath%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041package LeetCode.topic.tree;/** * * 解题思路类似于124题, 对于任意一个节点, 如果最长同值路径包含该节点, 那么只可能是两种情况: * 1. 其左右子树中加上该节点后所构成的同值路径中较长的那个继续向父节点回溯构成最长同值路径 * 2. 左右子树加上该节点都在最长同值路径中, 构成了最终的最长同值路径 * 需要注意因为要求同值, 所以在判断左右子树能构成的同值路径时要加入当前节点的值作为判断依据 */public class L687_LongestUnivaluePath &#123; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125; private int max = 0; public int longestUnivaluePath(TreeNode root) &#123; if (root == null)return 0; helper(root,root.val); return max; &#125; private int helper(TreeNode node, int val) &#123; if (node == null)return 0; int left = helper(node.left,node.val); int right = helper(node.right,node.val); max = Math.max(max,left + right);// 路径长度为节点数减1所以此处不加1 if (node.val == val) return Math.max(left,right) + 1; return 0; &#125;// 下面是自己作物的思路// if (node.val == val)&#123;// return Math.max(helper(node.left,node.val,count+1),helper(node.right,node.val,count+1));// &#125; else &#123;// return Math.max(helper(node.left,node.val,count),helper(node.right,node.val,count));// &#125;&#125;]]></content>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L671_BSTFindSecondMinimumValue]]></title>
    <url>%2F2016%2F01%2F26%2FL671-BSTFindSecondMinimumValue%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package LeetCode.topic.tree;/** * 给定一个非空特殊的二叉树，每个节点都是正数，并且每个节点的子节点数量只能为 2 或 0。如果一个节点有两个子节点的话，那么这个节点的值不大于它的子节点的值。 * * 给出这样的一个二叉树，你需要输出所有节点中的第二小的值。如果第二小的值不存在的话，输出 -1 。 * * 示例 1: * * 输入: * 2 * / \ * 2 5 * / \ * 5 7 * * 输出: 5 * 说明: 最小的值是 2 ，第二小的值是 5 。 * 示例 2: * * 输入: * 2 * / \ * 2 2 * * 输出: -1 * 说明: 最小的值是 2, 但是不存在第二小的值。 */public class L671_BSTFindSecondMinimumValue &#123; int second = -1; public int findSecondMinimumValue(TreeNode root) &#123; if (root == null) return -1; helper(root,root.val); return second; &#125; private void helper(TreeNode root, int min) &#123; if (root == null)return; if (root.val == min)&#123; helper(root.left,min); helper(root.right,min); return; &#125; if (second == -1 || root.val &lt; second)&#123; second = root.val; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L654_ConstructMaximumBinaryTree]]></title>
    <url>%2F2016%2F01%2F26%2FL654-ConstructMaximumBinaryTree%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package LeetCode.topic.tree;/** * 给定一个不含重复元素的整数数组。一个以此数组构建的最大二叉树定义如下： * * 二叉树的根是数组中的最大元素。 * 左子树是通过数组中最大值左边部分构造出的最大二叉树。 * 右子树是通过数组中最大值右边部分构造出的最大二叉树。 * 通过给定的数组构建最大二叉树，并且输出这个树的根节点。 * * Example 1: * * 输入: [3,2,1,6,0,5] * 输入: 返回下面这棵树的根节点： * * 6 * / \ * 3 5 * \ / * 2 0 * \ * 1 * 注意: * * 给定的数组的大小在 [1, 1000] 之间。 */public class L654_ConstructMaximumBinaryTree &#123; public TreeNode constructMaximumBinaryTree(int[] nums) &#123; return buildTree(nums,0,nums.length -1); &#125; public TreeNode buildTree(int[] nums, int start, int end)&#123; if (start &gt; end)&#123; return null; &#125; int maxIndex = maxIndex(nums, start, end); TreeNode root = new TreeNode(nums[maxIndex]); root.left = buildTree(nums,start,maxIndex - 1); root.right = buildTree(nums,maxIndex + 1,end); return root; &#125; public int maxIndex(int[] arr,int start,int end)&#123; int maxIndex = start; for(int i = start ;i &lt;= end; i++)&#123; if (arr[i] &gt; arr[maxIndex])&#123; maxIndex = i; &#125; &#125; return maxIndex; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L617_MergeTrees]]></title>
    <url>%2F2016%2F01%2F26%2FL617-MergeTrees%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243package LeetCode.topic.tree;/** * 给定两个二叉树，想象当你将它们中的一个覆盖到另一个上时，两个二叉树的一些节点便会重叠。 * * 你需要将他们合并为一个新的二叉树。合并的规则是如果两个节点重叠，那么将他们的值相加作为节点合并后的新值，否则不为 NULL 的节点将直接作为新二叉树的节点。 * * 示例 1: * * 输入: * Tree 1 Tree 2 * 1 2 * / \ / \ * 3 2 1 3 * / \ \ * 5 4 7 * 输出: * 合并后的树: * 3 * / \ * 4 5 * / \ \ * 5 4 7 * 注意: 合并必须从两个树的根节点开始。 */public class L617_MergeTrees &#123; public TreeNode mergeTrees(TreeNode t1, TreeNode t2) &#123; if (t1 == null) return t2; if (t2 == null) return t1; TreeNode cur = new TreeNode(t1.val + t2.val); cur.left = mergeTrees(t1.left,t2.left); cur.right = mergeTrees(t1.right,t2.right); return cur; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L572_IsSubtree]]></title>
    <url>%2F2016%2F01%2F26%2FL572-IsSubtree%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package LeetCode.topic.tree;/** * 给定两个非空二叉树 s 和 t，检验 s 中是否包含和 t 具有相同结构和节点值的子树。s 的一个子树包括 s 的一个节点和这个节点的所有子孙。s 也可以看做它自身的一棵子树。 * * 示例 1: * 给定的树 s: * * 3 * / \ * 4 5 * / \ * 1 2 * 给定的树 t： * * 4 * / \ * 1 2 * 返回 true，因为 t 与 s 的一个子树拥有相同的结构和节点值。 * * 示例 2: * 给定的树 s： * * 3 * / \ * 4 5 * / \ * 1 2 * / * 0 * 给定的树 t： * * 4 * / \ * 1 2 * 返回 false。 */public class L572_IsSubtree &#123; public boolean isSubtree(TreeNode s, TreeNode t) &#123; if (s == null &amp;&amp; t != null)return false; return isSame(s,t) || isSubtree(s.left,t) || isSubtree(s.right,t); &#125; private boolean isSame(TreeNode s, TreeNode t) &#123; if (s!= null &amp;&amp; t != null)&#123; return s.val == t.val &amp;&amp; isSame(s.left,t.left) &amp;&amp; isSame(s.right,t.right); &#125; else if (s == null &amp;&amp; t == null)&#123; return true; &#125; else &#123; return false; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L543_DiameterOfBinaryTree]]></title>
    <url>%2F2016%2F01%2F26%2FL543-DiameterOfBinaryTree%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041package LeetCode.topic.tree;/** * 给定一棵二叉树，你需要计算它的直径长度。一棵二叉树的直径长度是任意两个结点路径长度中的最大值。这条路径可能穿过根结点。 * * 示例 : * 给定二叉树 * * 1 * / \ * 2 3 * / \ * 4 5 * 返回 3, 它的长度是路径 [4,2,1,3] 或者 [5,2,1,3]。 * * 注意：两结点之间的路径长度是以它们之间边的数目表示。 */public class L543_DiameterOfBinaryTree &#123; private int rst = 0; public int diameterOfBinaryTree(TreeNode root) &#123; if (root == null) return 0; helper(root); return rst; &#125; private int helper(TreeNode root) &#123; if (root == null) return 0; int left = helper(root.left); int right = helper(root.right); if (rst &lt; left + right) rst = left + right; return Math.max(left,right) + 1; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L530_BSTgetMinimumDifference]]></title>
    <url>%2F2016%2F01%2F26%2FL530-BSTgetMinimumDifference%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637package LeetCode.topic.tree;public class L530_BSTgetMinimumDifference &#123; /** * //二叉查找树中，中间节点的值一定是其左右节点值的中间数，因此最小差别一定是在中间节点与左右节点之间 * //中序遍历二叉查找树，每次比较当前节点与前一节点差值的绝对值与目前result中保存的最小值的大小，将较小的保存在result中 * * @param root * @return */ private int result = Integer.MAX_VALUE; private TreeNode preNode = null; public int getMinimumDifference(TreeNode root) &#123; getMin(root); return result; &#125; private void getMin(TreeNode root) &#123; if(root == null)&#123; return; &#125; getMin(root.left); if(preNode != null)//todo &#123; result = Math.min(Math.abs(root.val - preNode.val), result); &#125; preNode = root; getMin(root.right); &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L501_BSTFindMode]]></title>
    <url>%2F2016%2F01%2F26%2FL501-BSTFindMode%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package LeetCode.topic.tree;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;/** * 给定一个有相同值的二叉搜索树（BST），找出 BST 中的所有众数（出现频率最高的元素）。 * * 假定 BST 有如下定义： * * 结点左子树中所含结点的值小于等于当前结点的值 * 结点右子树中所含结点的值大于等于当前结点的值 * 左子树和右子树都是二叉搜索树 * 例如： * 给定 BST [1,null,2,2], * * 1 * \ * 2 * / * 2 * 返回[2]. * * 提示：如果众数超过1个，不需考虑输出顺序 * * 进阶：你可以不使用额外的空间吗？（假设由递归产生的隐式调用栈的开销不被计算在内） * * 使用中序遍历 * https://blog.csdn.net/qq_38959715/article/details/82682383 * */public class L501_BSTFindMode &#123; TreeNode pre = null;//todo must全局 Integer curTimes = 1,maxTimes = 0;//todo must全局 public int[] findMode(TreeNode root) &#123; if (root == null) return new int[]&#123;&#125;; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); inOrder(root,list); int[] result = new int[list.size()]; for(int i = 0;i&lt;list.size();i++)&#123; result[i] = list.get(i); &#125; return result; &#125; private void inOrder(TreeNode root, List&lt;Integer&gt; list) &#123; if (root == null) return; inOrder(root.left,list); if (pre != null)&#123; curTimes = (root.val == pre.val) ? curTimes + 1 : 1; &#125; if (curTimes == maxTimes)&#123; list.add(root.val); &#125; else if (curTimes &gt; maxTimes)&#123; list.clear(); list.add(root.val); maxTimes = curTimes; &#125; pre = root; inOrder(root.right,list); &#125; public static void main(String[] args) &#123; TreeNode r = new TreeNode(1); TreeNode t1 = new TreeNode(2); TreeNode t2 = new TreeNode(2); r.right = t1; t1.left = t2; L501_BSTFindMode t = new L501_BSTFindMode(); int[] mode = t.findMode(r); for (int x:mode) &#123; System.out.println(x); &#125; &#125; public static class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L437_TreePathSum]]></title>
    <url>%2F2016%2F01%2F26%2FL437-TreePathSum%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package LeetCode.topic.tree;import java.util.Stack;/** * 给定一个二叉树，它的每个结点都存放着一个整数值。 * * 找出路径和等于给定数值的路径总数。 * * 路径不需要从根节点开始，也不需要在叶子节点结束，但是路径方向必须是向下的（只能从父节点到子节点）。 * * 二叉树不超过1000个节点，且节点数值范围是 [-1000000,1000000] 的整数。 * * 示例： * * root = [10,5,-3,3,2,null,11,3,-2,null,1], sum = 8 * * 10 * / \ * 5 -3 * / \ \ * 3 2 11 * / \ \ * 3 -2 1 * * 返回 3。和等于 8 的路径有: * * 1. 5 -&gt; 3 * 2. 5 -&gt; 2 -&gt; 1 * 3. -3 -&gt; 11 */public class L437_TreePathSum &#123; public int pathSum(TreeNode root, int sum) &#123; int count = 0; if(root == null) return 0; Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;(); stack.add(root); while(!stack.isEmpty())&#123; TreeNode node = stack.pop(); count += helper(node, sum); if(node.right != null) stack.push(node.right); if(node.left != null) stack.push(node.left); &#125; return count; &#125; public int helper(TreeNode root, int sum) &#123; int count = 0; if(root == null) return 0; if(sum == root.val)&#123; count++; &#125; count += helper(root.left, sum-root.val); count += helper(root.right, sum-root.val); return count; &#125;// public int pathSum(TreeNode root, int sum) &#123;// if(root == null) return 0;// int res = helper(root,sum);// res += pathSum(root.left,sum);// res += pathSum(root.right,sum);// return res;// &#125;// public int helper(TreeNode node,int num)&#123;// if(node == null)&#123;// return 0;// &#125;// int res = 0;// if(node.val == num)&#123;// res++;// &#125;// res += helper(node.left,num - node.val);// res += helper(node.right,num - node.val);// return res;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L429_TreeLevelOrder]]></title>
    <url>%2F2016%2F01%2F26%2FL429-TreeLevelOrder%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142package LeetCode.topic.tree;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;public class L429_TreeLevelOrder &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; levelOrder(Node root) &#123; if(root == null)return new ArrayList&lt;&gt;(); LinkedList&lt;Node&gt; queue = new LinkedList&lt;Node&gt;(); queue.add(root); while(!queue.isEmpty())&#123; int count = queue.size(); List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); while(count &gt; 0)&#123; Node cur = queue.pop(); list.add(cur.val); if(cur.children != null)&#123; for(Node n : cur.children)&#123; queue.add(n); &#125; &#125; count--; &#125; res.add(list); &#125; return res; &#125; class Node &#123; public int val; public List&lt;Node&gt; children; public Node() &#123;&#125; public Node(int _val,List&lt;Node&gt; _children) &#123; val = _val; children = _children; &#125; &#125;;&#125;]]></content>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L297_DeserializeSerializeTree]]></title>
    <url>%2F2016%2F01%2F26%2FL297-DeserializeSerializeTree%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package LeetCode.topic.tree;import java.util.ArrayDeque;import java.util.Arrays;import java.util.Queue;/** * 序列化是将一个数据结构或者对象转换为连续的比特位的操作，进而可以将转换后的数据存储在一个文件或者内存中，同时也可以通过网络传输到另一个计算机环境，采取相反方式重构得到原数据。 * * 请设计一个算法来实现二叉树的序列化与反序列化。这里不限定你的序列 / 反序列化算法执行逻辑，你只需要保证一个二叉树可以被序列化为一个字符串并且将这个字符串反序列化为原始的树结构。 * * 示例: * * 你可以将以下二叉树： * * 1 * / \ * 2 3 * / \ * 4 5 * * 序列化为 &quot;[1,2,3,null,null,4,5]&quot; * 提示: 这与 LeetCode 目前使用的方式一致，详情请参阅 LeetCode 序列化二叉树的格式。你并非必须采取这种方式，你也可以采用其他的方法解决这个问题。 * * 说明: 不要使用类的成员 / 全局 / 静态变量来存储状态，你的序列化和反序列化算法应该是无状态的。 */public class L297_DeserializeSerializeTree &#123; // Encodes a tree to a single string. public String serialize(TreeNode root) &#123; if (root == null) &#123; return &quot;$,&quot;; &#125; return root.val + &quot;,&quot; + serialize(root.left) + serialize(root.right); &#125; // Decodes your encoded data to tree. public TreeNode deserialize(String data) &#123; String[] strings = data.split(&quot;,&quot;); Queue&lt;String&gt; queue = new ArrayDeque&lt;&gt;(Arrays.asList(strings)); return func(queue); &#125; private TreeNode func(Queue&lt;String&gt; strings) &#123; String string = strings.remove(); if (&quot;$&quot;.equals(string)) &#123; return null; &#125; TreeNode newNode = new TreeNode(Integer.parseInt(string)); newNode.left = func(strings); newNode.right = func(strings); return newNode; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L404_TreeSumOfLeftLeaves]]></title>
    <url>%2F2016%2F01%2F26%2FL404-TreeSumOfLeftLeaves%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940package LeetCode.topic.tree;public class L404_TreeSumOfLeftLeaves &#123; /** * 本质还是二叉树的遍历 * * 1. 求所有左叶子节点的值的和，那就是求当前节点的左节点和右节点的所有左叶子节点的和 * * 2. 不是左叶子节点返回值为0，是左叶子节点放回叶子的值 * * 3. 递归求和即可 * * @param root * @return */ public int sumOfLeftLeaves(TreeNode root) &#123; return dfs(root, false); &#125; public int dfs(TreeNode node, Boolean isLeft) &#123; if (node == null) return 0; if (node.left == null &amp;&amp; node.right == null) &#123; if (isLeft == true) return node.val; else return 0; &#125; int leftVal = dfs(node.left, true); int rightVal = dfs(node.right, false); return leftVal + rightVal; &#125; class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L257_BinaryTreePaths]]></title>
    <url>%2F2016%2F01%2F26%2FL257-BinaryTreePaths%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;/** * 给定一个二叉树，返回所有从根节点到叶子节点的路径。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例: * * 输入: * * 1 * / \ * 2 3 * \ * 5 * * 输出: [&quot;1-&gt;2-&gt;5&quot;, &quot;1-&gt;3&quot;] * * 解释: 所有根节点到叶子节点的路径为: 1-&gt;2-&gt;5, 1-&gt;3 */public class L257_BinaryTreePaths &#123; List&lt;String&gt; res = new ArrayList&lt;&gt;(); public List&lt;String&gt; binaryTreePaths(TreeNode root) &#123; if (root == null)return new ArrayList&lt;&gt;(); ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); helper(root,list); return res; &#125; private void helper(TreeNode root, ArrayList&lt;Integer&gt; list) &#123; if (root == null) return; if (root.left == null &amp;&amp; root.right == null)&#123; list.add(root.val); String s = &quot;&quot;; for (Integer node:list) &#123; if (s.equals(&quot;&quot;))&#123; s += node+&quot;&quot;; &#125; else &#123; s = s + &quot;-&gt;&quot;+ node; &#125; &#125; res.add(s); list.remove(list.size() - 1); return; &#125; list.add(root.val); helper(root.right,list); helper(root.left,list); list.remove(list.size() - 1); &#125;// List&lt;String&gt; res = new ArrayList&lt;&gt;();// public List&lt;String&gt; binaryTreePaths(TreeNode root) &#123;// if (root == null)return new ArrayList&lt;&gt;();// ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;();// list.add(root.val);// helper(root,list);// return res;// &#125;//// private void helper(TreeNode root, ArrayList&lt;Integer&gt; list) &#123;// if (root.left == null &amp;&amp; root.right == null)&#123;// String s = &quot;&quot;;// for (Integer node:list) &#123;// if (s.equals(&quot;&quot;))&#123;// s += node+&quot;&quot;;// &#125; else &#123;// s = s + &quot;-&gt;&quot;+ node;// &#125;// &#125;// res.add(s);// &#125; else if (root.left == null)&#123;// list.add(root.right.val);// helper(root.right,list);// &#125; else if (root.right == null)&#123;// list.add(root.left.val);// helper(root.left,list);// &#125; else &#123;// ArrayList&lt;Integer&gt; mycopy=new ArrayList&lt;Integer&gt;();// mycopy=(ArrayList&lt;Integer&gt;) list.clone();// list.add(root.right.val);// helper(root.right,list);// mycopy.add(root.left.val);// helper(root.left,mycopy);// &#125;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L236_TreeLowestCommonAncestor]]></title>
    <url>%2F2016%2F01%2F26%2FL236-TreeLowestCommonAncestor%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package LeetCode.topic.tree;/** * 给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。 * * 百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。” * * 例如，给定如下二叉树: root = [3,5,1,6,2,0,8,null,null,7,4] * * * * * * 示例 1: * * 输入: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 1 * 输出: 3 * 解释: 节点 5 和节点 1 的最近公共祖先是节点 3。 * 示例 2: * * 输入: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 4 * 输出: 5 * 解释: 节点 5 和节点 4 的最近公共祖先是节点 5。因为根据定义最近公共祖先节点可以为节点本身。 * * * 说明: * * 所有节点的值都是唯一的。 * p、q 为不同节点且均存在于给定的二叉树中。 * * LCA 问题，查阅相关资料 */public class L236_TreeLowestCommonAncestor &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if (root == null) return null; if (root == q || root == p)&#123; return root;//找到待查询的值，一层一层向外传递 &#125; TreeNode left = lowestCommonAncestor(root.left,p,q); TreeNode right = lowestCommonAncestor(root.right,p,q); if (left != null &amp;&amp; right != null)&#123; return root; &#125; else if (left != null)&#123; return left; &#125; else if (right != null)&#123; return right; &#125; return null; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L235_BSTLowestCommonAncestor]]></title>
    <url>%2F2016%2F01%2F26%2FL235-BSTLowestCommonAncestor%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package LeetCode.topic.tree;/** * 给定一个二叉搜索树, 找到该树中两个指定节点的最近公共祖先。 * * 百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。” * * 例如，给定如下二叉搜索树: root = [6,2,8,0,4,7,9,null,null,3,5] * * * * * * 示例 1: * * 输入: root = [6,2,8,0,4,7,9,null,null,3,5], p = 2, q = 8 * 输出: 6 * 解释: 节点 2 和节点 8 的最近公共祖先是 6。 * 示例 2: * * 输入: root = [6,2,8,0,4,7,9,null,null,3,5], p = 2, q = 4 * 输出: 2 * 解释: 节点 2 和节点 4 的最近公共祖先是 2, 因为根据定义最近公共祖先节点可以为节点本身。 * * * 说明: * * 所有节点的值都是唯一的。 * p、q 为不同节点且均存在于给定的二叉搜索树中。 * * * //todo 可以利用二叉搜索树的特性 */public class L235_BSTLowestCommonAncestor &#123; TreeNode res = null; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if (root == null) return null; helper(root,p,q); return res; &#125; private void helper(TreeNode root, TreeNode p, TreeNode q) &#123; if(root == null)return; if ((p.val - root.val)*(q.val - root.val) &lt;= 0)&#123;//一定在最近公共祖先的两侧 res = root; &#125; else if (p.val &gt; root.val &amp;&amp; q.val &gt; root.val)&#123; helper(root.right,q,p); &#125; else &#123; helper(root.left,p,q); &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L230_TreeKthSmallest]]></title>
    <url>%2F2016%2F01%2F26%2FL230-TreeKthSmallest%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package LeetCode.topic.tree;import java.util.LinkedList;import java.util.Stack;/** * 给定一个二叉搜索树，编写一个函数 kthSmallest 来查找其中第 k 个最小的元素。 * * 说明： * 你可以假设 k 总是有效的，1 ≤ k ≤ 二叉搜索树元素个数。 * * 示例 1: * * 输入: root = [3,1,4,null,2], k = 1 * 3 * / \ * 1 4 * \ * 2 * 输出: 1 * 示例 2: * * 输入: root = [5,3,6,2,4,null,null,1], k = 3 * 5 * / \ * 3 6 * / \ * 2 4 * / * 1 * 输出: 3 * * 中序遍历，是升序的 */public class L230_TreeKthSmallest &#123; public int kthSmallest(TreeNode root, int k) &#123; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); int i = 0; TreeNode cur = root; while (true)&#123;//重要 while (cur != null)&#123; stack.push(cur); cur = cur.left; &#125; if (stack.isEmpty())&#123; break; &#125; TreeNode node = stack.pop(); if (i == k - 1)&#123; return node.val; &#125; i++; cur = node.right; &#125; return -1; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L226_InvertTree]]></title>
    <url>%2F2016%2F01%2F26%2FL226-InvertTree%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package LeetCode.topic.tree;/** * 翻转一棵二叉树。 * * 示例： * * 输入： * * 4 * / \ * 2 7 * / \ / \ * 1 3 6 9 * 输出： * * 4 * / \ * 7 2 * / \ / \ * 9 6 3 1 * 备注: * 这个问题是受到 Max Howell 的 原问题 启发的 ： * * 谷歌：我们90％的工程师使用您编写的软件(Homebrew)，但是您却无法在面试时在白板上写出翻转二叉树这道题，这太糟糕了。 */public class L226_InvertTree &#123; public TreeNode invertTree(TreeNode root) &#123; if (root == null)return null; helper(root); return root; &#125; private void helper(TreeNode root) &#123; if (root == null)return; // todo 必须在首位 helper(root.left); helper(root.right); //下面的代码，可在最前，也可以在最后 TreeNode tmp = root.left; root.left = root.right; root.right = tmp; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L222_TreeCountNodes]]></title>
    <url>%2F2016%2F01%2F26%2FL222-TreeCountNodes%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package LeetCode.topic.tree;/** * 给出一个完全二叉树，求出该树的节点个数。 * * 说明： * * 完全二叉树的定义如下：在完全二叉树中，除了最底层节点可能没填满外，其余每层节点数都达到最大值，并且最下面一层的节点都集中在该层最左边的若干位置。若最底层为第 h 层，则该层包含 1~ 2h 个节点。 * * 示例: * * 输入: * 1 * / \ * 2 3 * / \ / * 4 5 6 * * 输出: 6 */public class L222_TreeCountNodes &#123; public int countNodes(TreeNode root) &#123; if(root == null) return 0; return countNodes(root.left) + countNodes(root.right) + 1; &#125;// public int countNodes(TreeNode root) &#123;// /**// 完全二叉树的高度可以直接通过不断地访问左子树就可以获取// 判断左右子树的高度:// 如果相等说明左子树是满二叉树, 然后进一步判断右子树的节点数(最后一层最后出现的节点必然在右子树中)// 如果不等说明右子树是深度小于左子树的满二叉树, 然后进一步判断左子树的节点数(最后一层最后出现的节点必然在左子树中)// **/// if (root==null) return 0;// int ld = getDepth(root.left);// int rd = getDepth(root.right);// if(ld == rd)// return (1 &lt;&lt; ld) + countNodes(root.right); // 1(根节点) + (1 &lt;&lt; ld)-1(左完全左子树节点数) + 右子树节点数量// else// return (1 &lt;&lt; rd) + countNodes(root.left); // 1(根节点) + (1 &lt;&lt; rd)-1(右完全右子树节点数) + 左子树节点数量//// &#125;//// private int getDepth(TreeNode r) &#123;// int depth = 0;// while(r != null) &#123;// depth++;// r = r.left;// &#125;// return depth;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L199_TreeRightSideView]]></title>
    <url>%2F2016%2F01%2F26%2FL199-TreeRightSideView%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package LeetCode.topic.tree;import java.util.ArrayDeque;import java.util.ArrayList;import java.util.List;/** * 给定一棵二叉树，想象自己站在它的右侧，按照从顶部到底部的顺序，返回从右侧所能看到的节点值。 * * 示例: * * 输入: [1,2,3,null,5,null,4] * 输出: [1, 3, 4] * 解释: * * 1 &lt;--- * / \ * 2 3 &lt;--- * \ \ * 5 4 &lt;--- */public class L199_TreeRightSideView &#123; public List&lt;Integer&gt; rightSideView(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;Integer&gt;(); if(root == null) return res; ArrayDeque&lt;TreeNode&gt; queue = new ArrayDeque&lt;TreeNode&gt;(); queue.add(root); while(!queue.isEmpty())&#123; int cont = queue.size(); TreeNode node = queue.peek();//todo 队列是先进先出的，因为下面优先遍历的是右子树 res.add(node.val); while(cont &gt; 0)&#123; node = queue.poll(); if(node.right != null) queue.add(node.right);//todo here if(node.left != null) queue.add(node.left); cont--; &#125; &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L129_TreeSumNumbers]]></title>
    <url>%2F2016%2F01%2F26%2FL129-TreeSumNumbers%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109package LeetCode.topic.tree;import java.util.ArrayList;import java.util.Arrays;import java.util.List;/** * 给定一个二叉树，它的每个结点都存放一个 0-9 的数字，每条从根到叶子节点的路径都代表一个数字。 * * 例如，从根到叶子节点路径 1-&gt;2-&gt;3 代表数字 123。 * * 计算从根到叶子节点生成的所有数字之和。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例 1: * * 输入: [1,2,3] * 1 * / \ * 2 3 * 输出: 25 * 解释: * 从根到叶子节点路径 1-&gt;2 代表数字 12. * 从根到叶子节点路径 1-&gt;3 代表数字 13. * 因此，数字总和 = 12 + 13 = 25. * 示例 2: * * 输入: [4,9,0,5,1] * 4 * / \ * 9 0 * / \ * 5 1 * 输出: 1026 * 解释: * 从根到叶子节点路径 4-&gt;9-&gt;5 代表数字 495. * 从根到叶子节点路径 4-&gt;9-&gt;1 代表数字 491. * 从根到叶子节点路径 4-&gt;0 代表数字 40. * 因此，数字总和 = 495 + 491 + 40 = 1026. */public class L129_TreeSumNumbers &#123; private int total; public int sumNumbers(TreeNode root) &#123; if (root == null) return 0; ArrayList&lt;Integer&gt; sum = new ArrayList&lt;&gt;(); helper(root,sum); return total; &#125; private void helper(TreeNode cur,ArrayList&lt;Integer&gt; sum) &#123; if (cur == null) return; if (cur.left == null &amp;&amp; cur.right == null)&#123; sum.add(0,cur.val);//todo int s = 0; for (int i = sum.size() -1 ;i&gt;=0;i--)&#123; s += (int) (Math.pow(10,Double.parseDouble(i+&quot;&quot;))*sum.get(i).intValue()); &#125; total += s; sum.remove(0);//todo return; &#125; sum.add(0,cur.val); helper(cur.left,sum); helper(cur.right,sum); sum.remove(0);//todo &#125;// private int total;// public int sumNumbers(TreeNode root) &#123;// if (root == null) return 0;// ArrayList&lt;Integer&gt; sum = new ArrayList&lt;&gt;();// sum.add(root.val);// helper(root,sum);// return total;// &#125;//// private void helper(TreeNode cur,ArrayList&lt;Integer&gt; sum) &#123;// if (cur.left == null &amp;&amp; cur.right == null)&#123;// int s = 0;// for (int i = sum.size() -1 ;i&gt;=0;i--)&#123;// s += (int) (Math.pow(10,Double.parseDouble(i+&quot;&quot;))*sum.get(i).intValue());// &#125;// total += s;// &#125; else if (cur.left == null)&#123;// sum.add(0,cur.right.val);// helper(cur.right,sum);// &#125; else if (cur.right == null)&#123;// sum.add(0,cur.left.val);// helper(cur.left,sum);// &#125; else &#123;// ArrayList&lt;Integer&gt; mycopy=new ArrayList&lt;Integer&gt;();// mycopy=(ArrayList&lt;Integer&gt;) sum.clone();// sum.add(0,cur.right.val);// helper(cur.right,sum);// mycopy.add(0,cur.left.val);// helper(cur.left,mycopy);// &#125;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L144_PreorderTraversal]]></title>
    <url>%2F2016%2F01%2F26%2FL144-PreorderTraversal%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;import java.util.Stack;/** * 给定一个二叉树，返回它的 前序 遍历。 * * 示例: * * 输入: [1,null,2,3] * 1 * \ * 2 * / * 3 * * 输出: [1,2,3] * 进阶: 递归算法很简单，你可以通过迭代算法完成吗？ */public class L144_PreorderTraversal &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root); while (!stack.isEmpty())&#123; TreeNode cur = stack.pop(); res.add(cur.val); if (cur.right != null)&#123; stack.push(cur.right); &#125; if (cur.left != null)&#123; stack.push(cur.left); &#125; &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L124_TreeMaxPathSum]]></title>
    <url>%2F2016%2F01%2F26%2FL124-TreeMaxPathSum%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package LeetCode.topic.tree;/** * 给定一个非空二叉树，返回其最大路径和。 * * 本题中，路径被定义为一条从树中任意节点出发，达到任意节点的序列。该路径至少包含一个节点，且不一定经过根节点。 * * 示例 1: * * 输入: [1,2,3] * * 1 * / \ * 2 3 * * 输出: 6 * 示例 2: * * 输入: [-10,9,20,null,null,15,7] * * -10 * / \ * 9 20 * / \ * 15 7 * * 输出: 42 */public class L124_TreeMaxPathSum &#123; private int ret = Integer.MIN_VALUE; public int maxPathSum(TreeNode root) &#123; /** 对于任意一个节点, 如果最大和路径包含该节点, 那么只可能是两种情况: 1. 其左右子树中所构成的和路径值较大的那个加上该节点的值后向父节点回溯构成最大路径 2. 左右子树都在最大路径中, 加上该节点的值构成了最终的最大路径 **/ getMax(root); return ret; &#125; private int getMax(TreeNode r) &#123; if(r == null) return 0; int left = Math.max(0, getMax(r.left)); // 如果子树路径和为负则应当置0表示最大路径不包含子树 int right = Math.max(0, getMax(r.right)); ret = Math.max(ret, r.val + left + right); // 判断在该节点包含左右子树的路径和是否大于当前最大路径和 return Math.max(left, right) + r.val; &#125;// private int res = 0;// public int maxPathSum(TreeNode root) &#123;// res = root.val;// helper(root);// return res;// &#125;//// private int helper(TreeNode cur) &#123;// int sum;// if (cur.left != null &amp;&amp; cur.right != null)&#123;// sum = cur.val;// &#125; else if (cur.left == null)&#123;// int right = helper(cur.right);// sum = right &gt;0 ? right + cur.val : cur.val;// &#125; else if (cur.right == null)&#123;// int left = helper(cur.left);// sum = left &gt; 0 ? left + cur.val : cur.val;// &#125; else &#123;// int left = helper(cur.left);// int right = helper(cur.right);// res = Math.max(res,left + right + cur.val);//走当前节点//// int max = Math.max(left,right);//不走当前节点// sum = max &gt; 0 ? max + cur.val : cur.val;// &#125;// res = Math.max(res,sum);// return res;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L116_TreeConnectOne]]></title>
    <url>%2F2016%2F01%2F26%2FL116-TreeConnectOne%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package LeetCode.topic.tree;import java.util.LinkedList;/** * 给定一个二叉树 * * struct TreeLinkNode &#123; * TreeLinkNode *left; * TreeLinkNode *right; * TreeLinkNode *next; * &#125; * 填充它的每个 next 指针，让这个指针指向其下一个右侧节点。如果找不到下一个右侧节点，则将 next 指针设置为 NULL。 * * 初始状态下，所有 next 指针都被设置为 NULL。 * * 说明: * * 你只能使用额外常数空间。 * 使用递归解题也符合要求，本题中递归程序占用的栈空间不算做额外的空间复杂度。 * 你可以假设它是一个完美二叉树（即所有叶子节点都在同一层，每个父节点都有两个子节点）。 * 示例: * * 给定完美二叉树， * * 1 * / \ * 2 3 * / \ / \ * 4 5 6 7 * 调用你的函数后，该完美二叉树变为： * * 1 -&gt; NULL * / \ * 2 -&gt; 3 -&gt; NULL * / \ / \ * 4-&gt;5-&gt;6-&gt;7 -&gt; NULL */public class L116_TreeConnectOne &#123; public class TreeLinkNode &#123; int val; TreeLinkNode left, right, next; TreeLinkNode(int x) &#123; val = x; &#125; &#125; public void connect(TreeLinkNode root) &#123; if (root == null) return; LinkedList&lt;TreeLinkNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while (!queue.isEmpty())&#123; int count = queue.size(); TreeLinkNode last = null; while (count &gt; 0)&#123; TreeLinkNode cur = queue.pop(); if (cur.left != null)&#123; queue.add(cur.left); if (last == null)&#123; last = cur.left; &#125; else &#123; last.next = cur.left; last = last.next;//传递 &#125; &#125; if (cur.right != null)&#123; queue.add(cur.right); if (last == null)&#123; last = cur.right; &#125; else &#123; last.next = cur.right; last = last.next;//传递 &#125; &#125; count--; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L114_BstFlattenList]]></title>
    <url>%2F2016%2F01%2F26%2FL114-BstFlattenList%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package LeetCode.topic.tree;/** * 给定一个二叉树，**原地**将它展开为链表。 * * 例如，给定二叉树 * * 1 * / \ * 2 5 * / \ \ * 3 4 6 * 将其展开为： * * 1 * \ * 2 * \ * 3 * \ * 4 * \ * 5 * \ * 6 * 用了递归的思路，把左子树作为右子树，并把 原右子树(temp) 拼接在 现右子树 的最右端 * * * * 1 * / \ * 2 5 * \ \ * (3) 6 * \ * 4 * * * * 1 * \ * (2,3,4) * \ * 5 * \ * 6 */public class L114_BstFlattenList &#123; public void flatten(TreeNode root) &#123; if (root == null) return; flatten(root.left); flatten(root.right); if (root.left != null)&#123;//注意，一定要判断左子树是否为空 TreeNode right = root.right;//记录右节点 root.right = root.left; root.left = null;//将左节点置空 TreeNode cur = root.right;//到左节点的最后一个节点 while (cur.right != null)&#123;//把左子树作为右子树 cur = cur.right; &#125; cur.right = right; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L115_PostorderTraversal]]></title>
    <url>%2F2016%2F01%2F26%2FL115-PostorderTraversal%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;import java.util.Stack;/** * 给定一个二叉树，返回它的 后序 遍历。 * * 示例: * * 输入: [1,null,2,3] * 1 * \ * 2 * / * 3 * * 输出: [3,2,1] * 进阶: 递归算法很简单，你可以通过迭代算法完成吗？ */public class L115_PostorderTraversal &#123; public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); Stack&lt;TreeNode&gt; outputStack = new Stack&lt;&gt;(); TreeNode cur = root; stack.push(root); while (!stack.isEmpty())&#123; cur = stack.pop(); outputStack.push(cur); if (cur.left != null)&#123; stack.push(cur.left); &#125; if (cur.right != null)&#123; stack.push(cur.right); &#125; &#125; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); while (!outputStack.isEmpty())&#123; res.add(outputStack.pop().val); &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L113_PathSumTree]]></title>
    <url>%2F2016%2F01%2F26%2FL113-PathSumTree%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package LeetCode.topic.tree;import java.util.ArrayList;import java.util.Arrays;import java.util.List;/** * 给定一个二叉树和一个目标和，找到所有从根节点到叶子节点路径总和等于给定目标和的路径。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例: * 给定如下二叉树，以及目标和 sum = 22， * * 5 * / \ * 4 8 * / / \ * 11 13 4 * / \ / \ * 7 2 5 1 * 返回: * * [ * [5,4,11,2], * [5,8,4,5] * ] */public class L113_PathSumTree &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; pathSum(TreeNode root, int sum) &#123; if (root == null) return new ArrayList&lt;&gt;(); helper(root,sum,new ArrayList&lt;Integer&gt;()); return res; &#125; private void helper(TreeNode root, int sum, ArrayList&lt;Integer&gt; list) &#123; if (root == null) return; if (root.left == null &amp;&amp; root.right == null &amp;&amp; sum - root.val == 0)&#123; list.add(root.val);//todo 一定要添加 res.add(new ArrayList&lt;&gt;(list));//todo 重点，此处一定要copy一个list list.remove(list.size() - 1);//todo return; &#125; list.add(root.val); helper(root.left,sum - root.val,list); helper(root.right,sum - root.val,list); list.remove(list.size() - 1);//todo &#125;// List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;();// public List&lt;List&lt;Integer&gt;&gt; pathSum(TreeNode root, int sum) &#123;// if (root == null) return new ArrayList&lt;&gt;();// helper(root,sum,0,new ArrayList&lt;Integer&gt;());// return res;// &#125;//// private boolean helper(TreeNode root, int sum, int tempSum, ArrayList&lt;Integer&gt; list) &#123;// if (root.left == null &amp;&amp; root.right == null)&#123;// if (tempSum + root.val == sum)&#123;// list.add(root.val);// res.add(list);// return true;// &#125;// return false;// &#125; else &#123;// tempSum += root.val;// list.add(root.val);// if (root.left == null)&#123;// return helper(root.right,sum,tempSum,list);// &#125; else if (root.right == null)&#123;// return helper(root.left,sum,tempSum,list);// &#125; else &#123;// ArrayList&lt;Integer&gt; mycopy=new ArrayList&lt;Integer&gt;();// mycopy=(ArrayList&lt;Integer&gt;) list.clone();// helper(root.left,sum,tempSum,list);// helper(root.right,sum,tempSum,mycopy);// &#125;// &#125;// return false;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L112_HasPathSumTree]]></title>
    <url>%2F2016%2F01%2F26%2FL112-HasPathSumTree%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package LeetCode.topic.tree;/** * 给定一个二叉树和一个目标和，判断该树中是否存在根节点到叶子节点的路径，这条路径上所有节点值相加等于目标和。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例: * 给定如下二叉树，以及目标和 sum = 22， * * 5 * / \ * 4 8 * / / \ * 11 13 4 * / \ \ * 7 2 1 * 返回 true, 因为存在目标和为 22 的根节点到叶子节点的路径 5-&gt;4-&gt;11-&gt;2。 */public class L112_HasPathSumTree &#123; public boolean hasPathSum(TreeNode root, int sum) &#123; if (root == null) return false; return helper(root,sum,0); &#125; private boolean helper(TreeNode node, int sum, int tempSum) &#123; if (node.left == null &amp;&amp; node.right == null)&#123; return tempSum + node.val == sum; &#125; else &#123; tempSum += node.val; if (node.left == null)&#123; return helper(node.right,sum,tempSum); &#125; else if (node.right == null)&#123; return helper(node.left,sum,tempSum); &#125; else &#123; return helper(node.right,sum,tempSum) || helper(node.left,sum,tempSum); &#125; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L110_IsBalancedTree]]></title>
    <url>%2F2016%2F01%2F26%2FL110-IsBalancedTree%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package LeetCode.topic.tree;/** * 给定一个二叉树，判断它是否是高度平衡的二叉树。 * * 本题中，一棵高度平衡二叉树定义为： * * 一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过1。 * * 示例 1: * * 给定二叉树 [3,9,20,null,null,15,7] * * 3 * / \ * 9 20 * / \ * 15 7 * 返回 true 。 * * 示例 2: * * 给定二叉树 [1,2,2,3,3,null,null,4,4] * * 1 * / \ * 2 2 * / \ * 3 3 * / \ * 4 4 * 返回 false 。 */public class L110_IsBalancedTree &#123; public boolean isBalanced(TreeNode root) &#123; if (root == null)return true; int left = getDepth(root.left); int right = getDepth(root.right); if (Math.abs(left - right) &gt; 1)&#123; return false; &#125; return isBalanced(root.left) &amp;&amp; isBalanced(root.right); &#125; private int getDepth(TreeNode node) &#123; if (node == null)return 0; return Math.max(1 + getDepth(node.right),1 + getDepth(node.left)); &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L111_MinDepthTree]]></title>
    <url>%2F2016%2F01%2F26%2FL111-MinDepthTree%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738package LeetCode.topic.tree;/** * 给定一个二叉树，找出其最小深度。 * * 最小深度是从根节点到最近叶子节点的最短路径上的节点数量。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例: * * 给定二叉树 [3,9,20,null,null,15,7], * * 3 * / \ * 9 20 * / \ * 15 7 * 返回它的最小深度 2. */public class L111_MinDepthTree &#123;// 错误的写法；如果根节点的一侧为空，另一侧不为空；此时求的是不为空的一侧的最小深度 public int minDepth(TreeNode root) &#123; if (root == null) return 0; if (root.left == null) return 1 + minDepth(root.right); if (root.right == null) return 1 + minDepth(root.left); return Math.min(minDepth(root.left), minDepth(root.right)) + 1; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L108_SortedArrayToBST]]></title>
    <url>%2F2016%2F01%2F26%2FL108-SortedArrayToBST%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445package LeetCode.topic.tree;/** * 将一个按照升序排列的有序数组，转换为一棵高度平衡二叉搜索树。 * * 本题中，一个高度平衡二叉树是指一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过 1。 * * 示例: * * 给定有序数组: [-10,-3,0,5,9], * * 一个可能的答案是：[0,-3,9,-10,null,5]，它可以表示下面这个高度平衡二叉搜索树： * * 0 * / \ * -3 9 * / / * -10 5 * * 左右等分建立左右子树，中间节点作为子树根节点，递归该过程 */public class L108_SortedArrayToBST &#123; public TreeNode sortedArrayToBST(int[] nums) &#123; if (nums == null || nums.length == 0)return null; return helper(nums,0,nums.length - 1); &#125; private TreeNode helper(int[] nums, int low, int high) &#123; if (low &gt; high) return null; if (low == high) return new TreeNode(nums[low]);//low是序号，nums[low]才是其值 int mid = (low + high)&gt;&gt;1; TreeNode cur = new TreeNode(nums[mid]);//mid是序号，nums[low]才是其值 cur.left = helper(nums,low,mid-1); cur.right = helper(nums,mid+1,high); return cur; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L107_levelOrderBottomTraversal]]></title>
    <url>%2F2016%2F01%2F26%2FL107-levelOrderBottomTraversal%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package LeetCode.topic.tree;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;/** * 给定一个二叉树，返回其节点值自底向上的层次遍历。 （即按从叶子节点所在层到根节点所在的层，逐层从左向右遍历） * * 例如： * 给定二叉树 [3,9,20,null,null,15,7], * * 3 * / \ * 9 20 * / \ * 15 7 * 返回其自底向上的层次遍历为： * * [ * [15,7], * [9,20], * [3] * ] */public class L107_levelOrderBottomTraversal &#123; public List&lt;List&lt;Integer&gt;&gt; levelOrderBottom(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); LinkedList&lt;TreeNode&gt; quene = new LinkedList&lt;&gt;(); quene.add(root); while (!quene.isEmpty())&#123; int count = quene.size(); List&lt;Integer&gt; innerList = new ArrayList&lt;&gt;(); while (count &gt; 0)&#123; TreeNode cur = quene.pop(); innerList.add(cur.val); if (cur.left != null)&#123; quene.add(cur.left); &#125; if (cur.right != null)&#123; quene.add(cur.right); &#125; count--; &#125; res.add(0,innerList); &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L106_BuildTreeByMiddlePost]]></title>
    <url>%2F2016%2F01%2F26%2FL106-BuildTreeByMiddlePost%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;/** * 根据一棵树的中序遍历与后序遍历构造二叉树。 * * 注意: * 你可以假设树中没有重复的元素。 * * 例如，给出 * * 中序遍历 inorder = [9,3,15,20,7] * 后序遍历 postorder = [9,15,7,20,3] * 返回如下的二叉树： * * 3 * / \ * 9 20 * / \ * 15 7 * * * 后序遍历最后一个是根 */public class L106_BuildTreeByMiddlePost &#123; public TreeNode buildTree(int[] inorder, int[] postorder) &#123; TreeNode root = null; if (inorder.length != 0 &amp;&amp; postorder.length != 0)&#123; List&lt;Integer&gt; inorderList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; inorder.length; i++) &#123; inorderList.add(inorder[i]); &#125; List&lt;Integer&gt; postorderList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; postorder.length; i++) &#123; postorderList.add(postorder[i]); &#125; return helper(inorderList,postorderList); &#125; return root; &#125; private TreeNode helper(List&lt;Integer&gt; inorderList, List&lt;Integer&gt; postorderList) &#123; TreeNode root = null; List&lt;Integer&gt; leftInorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; rightInorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; leftPostorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; rightPostorderList = new ArrayList&lt;&gt;(); if (inorderList.size() != 0 &amp;&amp; postorderList.size() != 0)&#123; root = new TreeNode(postorderList.get(postorderList.size() - 1)); int rootInorderPos = inorderList.indexOf(root.val); leftInorderList = inorderList.subList(0,rootInorderPos); rightInorderList = inorderList.subList(rootInorderPos+1,inorderList.size()); int leftInorderSize =leftInorderList.size(); leftPostorderList = postorderList.subList(0,leftInorderSize); rightPostorderList = postorderList.subList(leftInorderSize,postorderList.size() - 1); root.left = helper(leftInorderList,leftPostorderList); root.right = helper(rightInorderList,rightPostorderList); &#125; return root; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L105_BuildTreeByPreMiddle]]></title>
    <url>%2F2016%2F01%2F26%2FL105-BuildTreeByPreMiddle%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package LeetCode.topic.tree;import java.util.ArrayList;import java.util.Arrays;import java.util.List;/** * 根据一棵树的前序遍历与中序遍历构造二叉树。 * * 注意: * 你可以假设树中没有重复的元素。 * * 例如，给出 * * 前序遍历 preorder = [3,9,20,15,7] * 中序遍历 inorder = [9,3,15,20,7] * 返回如下的二叉树： * * 3 * / \ * 9 20 * / \ * 15 7 */public class L105_BuildTreeByPreMiddle &#123; public TreeNode buildTree(int[] preorder, int[] inorder) &#123; TreeNode root = null; if (preorder.length != 0 &amp;&amp; inorder.length != 0)&#123; //todo 先转化为list处理 List&lt;Integer&gt; preorderList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; preorder.length; i++) &#123; preorderList.add(preorder[i]); &#125; List&lt;Integer&gt; inorderList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; inorder.length; i++) &#123; inorderList.add(inorder[i]); &#125; return helper(preorderList,inorderList); &#125; return root; &#125; private TreeNode helper(List&lt;Integer&gt; preorderList, List&lt;Integer&gt; inorderList) &#123; TreeNode root = null; List&lt;Integer&gt; leftPreorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; rightPreorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; leftInorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; rightInorderList = new ArrayList&lt;&gt;(); if (preorderList.size() != 0 &amp;&amp; inorderList.size() != 0)&#123; root = new TreeNode(preorderList.get(0)); int inOrderpos = inorderList.indexOf(root.val);//根节点 leftInorderList = inorderList.subList(0,inOrderpos); rightInorderList = inorderList.subList(inOrderpos+1,inorderList.size()); int leftInorderSize = leftInorderList.size(); leftPreorderList = preorderList.subList(1,leftInorderSize+1); rightPreorderList = preorderList.subList(leftInorderSize+1,preorderList.size()); //todo 重点 root.left = helper(leftPreorderList,leftInorderList); root.right = helper(rightPreorderList,rightInorderList); &#125; return root; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L104_TreeMaxDepth]]></title>
    <url>%2F2016%2F01%2F26%2FL104-TreeMaxDepth%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132package LeetCode.topic.tree;/** * 给定一个二叉树，找出其最大深度。 * * 二叉树的深度为根节点到最远叶子节点的最长路径上的节点数。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例： * 给定二叉树 [3,9,20,null,null,15,7]， * * 3 * / \ * 9 20 * / \ * 15 7 * 返回它的最大深度 3 。 */public class L104_TreeMaxDepth &#123; public int maxDepth(TreeNode root) &#123; if (root == null) return 0; return 1 + Math.max(maxDepth(root.left),maxDepth(root.right)); &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L103_ZigzagLevelOrder]]></title>
    <url>%2F2016%2F01%2F26%2FL103-ZigzagLevelOrder%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package LeetCode.topic.tree;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;/** * 给定一个二叉树，返回其节点值的锯齿形层次遍历。（即先从左往右，再从右往左进行下一层遍历，以此类推，层与层之间交替进行）。 * * 例如： * 给定二叉树 [3,9,20,null,null,15,7], * * 3 * / \ * 9 20 * / \ * 15 7 * 返回锯齿形层次遍历如下： * * [ * [3], * [20,9], * [15,7] * ] */public class L103_ZigzagLevelOrder &#123; public List&lt;List&lt;Integer&gt;&gt; zigzagLevelOrder(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); LinkedList&lt;TreeNode&gt; quene = new LinkedList&lt;&gt;(); quene.add(root); int depth = 0; while (!quene.isEmpty())&#123; int count = quene.size(); List&lt;Integer&gt; innerList = new ArrayList&lt;&gt;(); while (count &gt; 0)&#123; TreeNode cur = quene.poll(); if (depth % 2 == 1)&#123;//todo 判断奇偶层，从0开始计数 innerList.add(0,cur.val);//todo 如果是奇数层，就倒排 &#125; else &#123; innerList.add(cur.val); &#125; if (cur.left != null)&#123; quene.add(cur.left); &#125; if (cur.right != null)&#123; quene.add(cur.right);//.add 和 .push方法不同 &#125; count--; &#125; depth++; res.add(innerList); &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L102_LevelorderTraversal]]></title>
    <url>%2F2016%2F01%2F26%2FL102-LevelorderTraversal%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package LeetCode.topic.tree;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;import java.util.Queue;/** * 给定一个二叉树，返回其按层次遍历的节点值。 （即逐层地，从左到右访问所有节点）。 * * 例如: * 给定二叉树: [3,9,20,null,null,15,7], * * 3 * / \ * 9 20 * / \ * 15 7 * 返回其层次遍历结果： * * [ * [3], * [9,20], * [15,7] * ] */public class L102_LevelorderTraversal &#123; public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;();// if (root == null) return new ArrayList&lt;&gt;(); LinkedList&lt;TreeNode&gt; quene = new LinkedList&lt;&gt;(); quene.add(root); while (!quene.isEmpty())&#123; int count = quene.size();// List&lt;Integer&gt; innerList = new ArrayList&lt;&gt;();// while (count &gt; 0)&#123;// TreeNode cur = quene.pop(); innerList.add(cur.val);// System.out.println(cur.val); if (cur.left != null)&#123; quene.add(cur.left); &#125; if (cur.right != null)&#123; quene.add(cur.right); &#125; count--; &#125; res.add(innerList); &#125; return res; &#125;// public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123;//// LinkedList&lt;TreeNode&gt; quene = new LinkedList&lt;&gt;();// quene.add(root);//// while (!quene.isEmpty())&#123;// TreeNode cur = quene.pop();// System.out.println(cur.val);// if (cur.left != null)&#123;// quene.add(cur.left);// &#125;// if (cur.right != null)&#123;// quene.add(cur.right);// &#125;// &#125;// return null;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L101_IsSymmetric]]></title>
    <url>%2F2016%2F01%2F26%2FL101-IsSymmetric%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445package LeetCode.topic.tree;/** * 给定一个二叉树，检查它是否是镜像对称的。 * * 例如，二叉树 [1,2,2,3,4,4,3] 是对称的。 * * 1 * / \ * 2 2 * / \ / \ * 3 4 4 3 * 但是下面这个 [1,2,2,null,3,null,3] 则不是镜像对称的: * * 1 * / \ * 2 2 * \ \ * 3 3 * 说明: * * 如果你可以运用递归和迭代两种方法解决这个问题，会很加分。 */public class L101_IsSymmetric &#123; public boolean isSymmetric(TreeNode root) &#123; if (root == null)return true; return helper(root.left,root.right); &#125; private boolean helper(TreeNode leftNode, TreeNode rightNode) &#123; if (leftNode == null &amp;&amp; rightNode == null) return true; if ((leftNode != null &amp;&amp; rightNode != null &amp;&amp; leftNode.val == rightNode.val))&#123; return helper(leftNode.left,rightNode.right) &amp;&amp; helper(leftNode.right,rightNode.left); &#125; else &#123; return false; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L100_IsSameTree]]></title>
    <url>%2F2016%2F01%2F26%2FL100-IsSameTree%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package LeetCode.topic.tree;/** * 给定两个二叉树，编写一个函数来检验它们是否相同。 * * 如果两个树在结构上相同，并且节点具有相同的值，则认为它们是相同的。 * * 示例 1: * * 输入: 1 1 * / \ / \ * 2 3 2 3 * * [1,2,3], [1,2,3] * * 输出: true * 示例 2: * * 输入: 1 1 * / \ * 2 2 * * [1,2], [1,null,2] * * 输出: false * 示例 3: * * 输入: 1 1 * / \ / \ * 2 1 1 2 * * [1,2,1], [1,1,2] * * 输出: false */public class L100_IsSameTree &#123; public boolean isSameTree(TreeNode p, TreeNode q) &#123; if (p == null &amp;&amp; q == null)&#123; return true; &#125; else &#123; if (p == null || q == null)&#123; return false; &#125; else &#123; if (p.val == q.val)&#123; return isSameTree(p.left,q.left) &amp;&amp; isSameTree(p.right,q.right); &#125; else &#123; return false; &#125; &#125; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L99_RecoverTree]]></title>
    <url>%2F2016%2F01%2F26%2FL99-RecoverTree%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112package LeetCode.topic.tree;import java.util.Stack;/** * 二叉搜索树中的两个节点被错误地交换。 * * 请在不改变其结构的情况下，恢复这棵树。 * * 示例 1: * * 输入: [1,3,null,null,2] * * 1 * / * 3 * \ * 2 * * 输出: [3,1,null,null,2] * * 3 * / * 1 * \ * 2 * 示例 2: * * 输入: [3,1,4,null,null,2] * * 3 * / \ * 1 4 * / * 2 * * 输出: [2,1,4,null,null,3] * * 2 * / \ * 1 4 * / * 3 * 进阶: * * 使用 O(n) 空间复杂度的解法很容易实现。 * 你能想出一个只使用常数空间的解决方案吗？ * * 个人思路： * 中序遍历，是递增的，找到不一致的，然后进行交换 */public class L99_RecoverTree &#123; TreeNode first = null; TreeNode second = null; TreeNode prev = null; public void recoverTree(TreeNode root) &#123; if (root == null) return; helper(root); int tmp = first.val; first.val = second.val; second.val = tmp; &#125; private void helper(TreeNode root) &#123; if (root == null) return; helper(root.left); if (prev != null &amp;&amp; prev.val &gt;= root.val)&#123; if (first == null) first = prev; second = root;// &#125; prev =root;//如果正常继续轮转 helper(root.right); &#125;// public void recoverTree(TreeNode root) &#123;// if (root == null) return;// TreeNode first = null;// TreeNode second = null;// TreeNode prev = null;//// TreeNode cur = root;// Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();// while (!stack.isEmpty() || cur != null)&#123;// if (cur != null)&#123;// stack.push(cur);// cur = cur.left;// &#125; else &#123;// cur = stack.pop();// if (prev != null &amp;&amp; prev.val &gt;= cur.val)&#123;// if (first == null) first = prev;// second = cur;//// &#125;// prev = cur;// cur = cur.right;// &#125;// &#125;// int tmp = first.val;// first.val = second.val;// second.val = tmp;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L98_IsValidBST]]></title>
    <url>%2F2016%2F01%2F26%2FL98-IsValidBST%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package LeetCode.topic.tree;/** * 给定一个二叉树，判断其是否是一个有效的二叉搜索树。 * * 假设一个二叉搜索树具有如下特征： * * 节点的左子树只包含小于当前节点的数。 * 节点的右子树只包含大于当前节点的数。 * 所有左子树和右子树自身必须也是二叉搜索树。 * 示例 1: * * 输入: * 2 * / \ * 1 3 * 输出: true * 示例 2: * * 输入: * 5 * / \ * 1 4 * / \ * 3 6 * 输出: false * 解释: 输入为: [5,1,4,null,null,3,6]。 * 根节点的值为 5 ，但是其右子节点值为 4 。 * */public class L98_IsValidBST &#123; public boolean isValidBST(TreeNode root) &#123; if (root == null) return true; return helper(root,null,null); &#125; private boolean helper(TreeNode root, Integer min, Integer max) &#123; if (root == null) return true; if (min != null &amp;&amp; root.val &lt;= min) return false; if (max != null &amp;&amp; root.val &gt;= max) return false; return helper(root.left,min,root.val) &amp;&amp; helper(root.right,root.val,max); &#125;// double min = -Double.MAX_VALUE;// public boolean isValidBST(TreeNode root) &#123;// if (root == null) return true;// if (isValidBST(root.left))&#123;// if (root.val &gt; min)&#123;// min = root.val;// return isValidBST(root.right);// &#125;// &#125;// return false;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L96_NumTrees*]]></title>
    <url>%2F2016%2F01%2F26%2FL96-NumTrees%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839package LeetCode.topic.tree;/** * 给定一个整数 n，求以 1 ... n 为节点组成的二叉搜索树有多少种？ * * 示例: * * 输入: 3 * 输出: 5 * 解释: * 给定 n = 3, 一共有 5 种不同结构的二叉搜索树: * * 1 3 3 2 1 * \ / / / \ \ * 3 2 1 1 3 2 * / / \ \ * 2 1 2 3 * * n = 3 * root: 1 left:0 right:2 f(0)*f(2) * root: 2 left:1 right:1 f(1)*f(1) * root: 3 left:2 right:0 f(2)*f(0) * * f(n) = f(0)*f(n-1)+f(1)*(n-2)+...+ f(n-2)*f(1) + f(n-1)*f(0) * time:O(n) * space:O(n) */public class L96_NumTrees &#123; public int numTrees(int n) &#123; int[] res = new int[n + 1]; res[0] = 1; for (int i = 1; i &lt;= n; i++) &#123;//从1开始 for (int j = 0; j &lt; i; j++) &#123;//从0开始 res[i] += res[j] * res[i -j - 1];//res[j]是左子树 i是总个数，1是根节点，所以i - 1 -j是右子树 &#125; &#125; return res[n]; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L95_GenerateTrees*]]></title>
    <url>%2F2016%2F01%2F26%2FL95-GenerateTrees%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;/** * 给定一个整数 n，生成所有由 1 ... n 为节点所组成的二叉搜索树。 * * 示例: * * 输入: 3 * 输出: * [ * [1,null,3,2], * [3,2,null,1], * [3,1,null,null,2], * [2,1,3], * [1,null,2,null,3] * ] * 解释: * 以上的输出对应以下 5 种不同结构的二叉搜索树： * * 1 3 3 2 1 * \ / / / \ \ * 3 2 1 1 3 2 * / / \ \ * 2 1 2 3 */public class L95_GenerateTrees &#123; public List&lt;TreeNode&gt; generateTrees(int n) &#123; if (n ==0) return new ArrayList&lt;&gt;(); return generateTrees(1,n); &#125; private List&lt;TreeNode&gt; generateTrees(int start, int end) &#123; List&lt;TreeNode&gt; res = new ArrayList&lt;&gt;(); if (start &gt; end)&#123; res.add(null); return res; &#125; // 每一个i作为根 // start～～i-1为左子树 // i+1～～end为右子树 for (int i = start;i &lt;= end;i++)&#123; List&lt;TreeNode&gt; subLeftTree = generateTrees(start,i - 1);//todo i-1 List&lt;TreeNode&gt; subRightTree = generateTrees(i + 1,end);//todo i+1 for (TreeNode left : subLeftTree)&#123; for (TreeNode right: subRightTree) &#123; TreeNode node = new TreeNode(i);//todo i node.left = left; node.right = right; res.add(node); &#125; &#125; &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L94_InorderTraversal]]></title>
    <url>%2F2016%2F01%2F26%2FL94-InorderTraversal%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;import java.util.Stack;/** * 给定一个二叉树，返回它的中序 遍历。 * * 示例: * * 输入: [1,null,2,3] * 1 * \ * 2 * / * 3 * * 输出: [1,3,2] */public class L94_InorderTraversal &#123; public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode cur = root; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); while (cur != null || !stack.isEmpty())&#123;//变形 while (cur != null)&#123; stack.push(cur); cur = cur.left; &#125;// if (stack.isEmpty())&#123;// break;// &#125; cur = stack.pop(); list.add(cur.val); cur = cur.right; &#125; return list; &#125;// public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123;// if (root == null) return new ArrayList&lt;&gt;();// Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();// TreeNode cur = root;// List&lt;Integer&gt; list = new ArrayList&lt;&gt;();// while (true)&#123;// while (cur != null)&#123;// stack.push(cur);// cur = cur.left;// &#125;// if (stack.isEmpty())&#123;// break;// &#125;// cur = stack.pop();// list.add(cur.val);// cur = cur.right;// &#125;// return list;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GroupComparator原理]]></title>
    <url>%2F2016%2F01%2F25%2Fmr-GroupComparator%2F</url>
    <content type="text"><![CDATA[分析最近看dadoop中关于辅助排序（SecondarySort）的实现，说到了三个东西要设置：1. partioner；2. Key Comparator；3. Group Comparator。前两个都比较容易理解，但是关于group的概念我一直理解不了：一，有了partioner，所有的key已经放到一个分区了，每个分区对应一个reducer，而且key也可以排序了，那么不是实现了整个数据集的全排序了吗？第二，mapper产生的中间结果经过shuffle和sort后，每个key整合成一个记录(集合)，每次reduce方法调用处理这个记录(集合)，但是group的目的是让一次reduce调用处理多条记录(将该集合进行内部分组)，这不是矛盾吗，找了好久一直都没找到这个问题的清晰解释。 后来找到一本书，《Pro Hadoop》，里面有一部分内容详细解释了这个问题，看后终于明白了，和大家分享一下。reduce方法每次是读一条记录(集合)，读到相应的key，但是处理value集合时，处理完当前记录的value后，还会判断下一条记录是不是和当前的key是不是同一个组，如果是的话，会继续读取这些记录的值，而这个记录也会被认为已经处理了，直到记录不是当前组，这次reduce调用才结束，这样一次reduce调用就会处理掉一个组中的所有记录，而不仅仅是一条完整的记录(集合)了。 这个有什么用呢？如果不用分组，那么同一组的记录就要在多次reduce方法中独立处理(所有的数据都在同一组中)，那么有些状态数据就要传递了，就会增加复杂度，在一次调用中处理的话，这些状态只要用方法内的变量就可以的。比如查找最大值，只要读第一个值就可以了。 参考：https://blog.csdn.net/qq_20641565/article/details/52770872 源码分析目标：弄明白，我们配置的GroupComparator是如何对进入reduce函数中的key Iterable 进行影响。如下是一个配置了GroupComparator 的reduce 函数。具体影响是我们可以在自定义的GroupComparator 中确定哪儿些value组成一组，进入一个reduce函数 123456789101112131415161718192021public static class DividendGrowthReducer extends Reducer&lt;Stock, DoubleWritable, NullWritable, DividendChange&gt; &#123; private NullWritable outputKey = NullWritable.get(); private DividendChange outputValue = new DividendChange(); @Override protected void reduce(Stock key, Iterable&lt;DoubleWritable&gt; values, Context context) throws IOException, InterruptedException &#123; double previousDividend = 0.0; for(DoubleWritable dividend : values) &#123; double currentDividend = dividend.get(); double growth = currentDividend - previousDividend; if(Math.abs(growth) &gt; 0.000001) &#123; outputValue.setSymbol(key.getSymbol()); outputValue.setDate(key.getDate()); outputValue.setChange(growth); context.write(outputKey, outputValue); previousDividend = currentDividend; &#125; &#125; &#125; &#125; 着先我们找到向上找，是谁调用了我们写的这个reduce函数。 Reducer类的run 方法。通过如下代码，可以看到是在run方法中，对于每个key，调用一次reduce函数。此处传入reduce函数的都是对象引用。1234567891011121314/** * Advanced application writers can use the * &#123;@link #run(org.apache.hadoop.mapreduce.Reducer.Context)&#125; method to * control how the reduce task works. */ public void run(Context context) throws IOException, InterruptedException &#123; ..... while (context.nextKey()) &#123; reduce(context.getCurrentKey(), context.getValues(), context); ..... &#125; ..... &#125;&#125; 结合我们写的reduce函数，key是在遍历value的时候会对应变化。那我们继续跟踪context.getValues 得到的迭代器的next方法。context 此处是ReduceContext.java （接口）. 对应的实现类为ReduceContextImpl.java123456789101112131415161718protected class ValueIterable implements Iterable&lt;VALUEIN&gt; &#123; private ValueIterator iterator = new ValueIterator(); @Override public Iterator&lt;VALUEIN&gt; iterator() &#123; return iterator; &#125; &#125; /** * Iterate through the values for the current key, reusing the same value * object, which is stored in the context. * @return the series of values associated with the current key. All of the * objects returned directly and indirectly from this method are reused. */ public Iterable&lt;VALUEIN&gt; getValues() throws IOException, InterruptedException &#123; return iterable; &#125; 直接返回了一个iterable。继续跟踪ValueIterable 类型的iterable。那明白了，在reduce 函数中进行Iterable的遍历，其实调用的是ValueIterable的next方法。下面看一下next的实现。 1234567@Override public VALUEIN next() &#123; ……………… nextKeyValue(); return value; ……………… &#125; 再继续跟踪nextKeyValue()方法。终于找了一个comparator。 这个就是我们配置的GroupingComparator.1234567891011121314151617@Overridepublic boolean nextKeyValue() throws IOException, InterruptedException &#123; …………………………………… if (hasMore) &#123; nextKey = input.getKey(); nextKeyIsSame = comparator.compare(currentRawKey.getBytes(), 0, currentRawKey.getLength(), nextKey.getData(), nextKey.getPosition(), nextKey.getLength() - nextKey.getPosition() ) == 0; &#125; else &#123; nextKeyIsSame = false; &#125; inputValueCounter.increment(1); return true;&#125; 为了证明这个就是我们配置的GroupingComparator。 跟踪ReduceContextImpl的构造调用者。 ReduceTask的run方法。12345678@Override @SuppressWarnings(&quot;unchecked&quot;) public void run(JobConf job, final TaskUmbilicalProtocol umbilical)&#123; ……………………………… RawComparator comparator = job.getOutputValueGroupingComparator(); runNewReducer(job, umbilical, reporter, rIter, comparator, keyClass, valueClass); &#125; 下面把runNewReducer 的代码也贴出来。1234567891011121314151617void runNewReducer(JobConf job, final TaskUmbilicalProtocol umbilical, final TaskReporter reporter, RawKeyValueIterator rIter, RawComparator&lt;INKEY&gt; comparator, Class&lt;INKEY&gt; keyClass, Class&lt;INVALUE&gt; valueClass ) &#123; org.apache.hadoop.mapreduce.Reducer.Context reducerContext = createReduceContext(reducer, job, getTaskID(), rIter, reduceInputKeyCounter, reduceInputValueCounter, trackedRW, committer, reporter, comparator, keyClass, valueClass); 好吧，关于自定义GroupingComparator如何起做用的代码分析，就到此吧。参考：http://blog.itpub.net/30066956/viewspace-2095520/]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L1_twosum]]></title>
    <url>%2F2016%2F01%2F23%2Ftwosum%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829/** * 给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。 * * 你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。 * * 示例: * * 给定 nums = [2, 7, 11, 15], target = 9 * * 因为 nums[0] + nums[1] = 2 + 7 = 9 * 所以返回 [0, 1] */public class L1_twoSum &#123; public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); int[] rst = new int[2]; for (int i = 0; i &lt; nums.length; i++) &#123; int diff = target - nums[i]; if (map.containsKey(diff))&#123; rst[0] = map.get(diff); rst[1] = i; break; &#125; map.put(nums[i],i); &#125; return rst; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索 ConcurrentHashMap 高并发性的实现机制]]></title>
    <url>%2F2015%2F02%2F23%2F%E6%8E%A2%E7%B4%A2-ConcurrentHashMap-%E9%AB%98%E5%B9%B6%E5%8F%91%E6%80%A7%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[简介ConcurrentHashMap 是 util.concurrent 包的重要成员。本文将结合 Java 内存模型，分析 JDK 源代码，探索 ConcurrentHashMap 高并发的具体实现机制。 由于 ConcurrentHashMap 的源代码实现依赖于 Java 内存模型，所以阅读本文需要读者了解 Java 内存模型。同时，ConcurrentHashMap 的源代码会涉及到散列算法和链表数据结构，所以，读者需要对散列算法和基于链表的数据结构有所了解。 Java 内存模型由于 ConcurrentHashMap 是建立在 Java 内存模型基础上的，为了更好的理解 ConcurrentHashMap，让我们首先来了解一下 Java 的内存模型。 Java 语言的内存模型由一些规则组成，这些规则确定线程对内存的访问如何排序以及何时可以确保它们对线程是可见的。下面我们将分别介绍 Java 内存模型的重排序，内存可见性和 happens-before 关系。 重排序内存模型描述了程序的可能行为。具体的编译器实现可以产生任意它喜欢的代码 – 只要所有执行这些代码产生的结果，能够和内存模型预测的结果保持一致。这为编译器实现者提供了很大的自由，包括操作的重排序。 编译器生成指令的次序，可以不同于源代码所暗示的“显然”版本。重排序后的指令，对于优化执行以及成熟的全局寄存器分配算法的使用，都是大有脾益的，它使得程序在计算性能上有了很大的提升。 重排序类型包括： 编译器生成指令的次序，可以不同于源代码所暗示的“显然”版本。 处理器可以乱序或者并行的执行指令。 缓存会改变写入提交到主内存的变量的次序。内存可见性由于现代可共享内存的多处理器架构可能导致一个线程无法马上（甚至永远）看到另一个线程操作产生的结果。所以 Java 内存模型规定了 JVM 的一种最小保证：什么时候写入一个变量对其他线程可见。 在现代可共享内存的多处理器体系结构中每个处理器都有自己的缓存，并周期性的与主内存协调一致。假设线程 A 写入一个变量值 V，随后另一个线程 B 读取变量 V 的值，在下列情况下，线程 B 读取的值可能不是线程 A 写入的最新值： 执行线程 A 的处理器把变量 V 缓存到寄存器中。 执行线程 A 的处理器把变量 V 缓存到自己的缓存中，但还没有同步刷新到主内存中去。 执行线程 B 的处理器的缓存中有变量 V 的旧值。Happens-before 关系happens-before 关系保证：如果线程 A 与线程 B 满足 happens-before 关系，则线程 A 执行动作的结果对于线程 B 是可见的。如果两个操作未按 happens-before 排序，JVM 将可以对他们任意重排序。 下面介绍几个与理解 ConcurrentHashMap 有关的 happens-before 关系法则： 程序次序法则：如果在程序中，所有动作 A 出现在动作 B 之前，则线程中的每动作 A 都 happens-before 于该线程中的每一个动作 B。 监视器锁法则：对一个监视器的解锁 happens-before 于每个后续对同一监视器的加锁。 Volatile 变量法则：对 Volatile 域的写入操作 happens-before 于每个后续对同一 Volatile 的读操作。 传递性：如果 A happens-before 于 B，且 B happens-before C，则 A happens-before C。 ConcurrentHashMap 的结构分析为了更好的理解 ConcurrentHashMap 高并发的具体实现，让我们先探索它的结构模型。 ConcurrentHashMap 类中包含两个静态内部类 HashEntry 和 Segment。HashEntry 用来封装映射表的键 / 值对；Segment 用来充当锁的角色，每个 Segment 对象守护整个散列映射表的若干个桶。每个桶是由若干个 HashEntry 对象链接起来的链表。一个 ConcurrentHashMap 实例中包含由若干个 Segment 对象组成的数组。 HashEntry 类HashEntry 用来封装散列映射表中的键值对。在 HashEntry 类中，key，hash 和 next 域都被声明为 final 型，value 域被声明为 volatile 型。清单 1.HashEntry 类的定义12345678910111213static final class HashEntry&lt;K,V&gt; &#123; final K key; // 声明 key 为 final 型 final int hash; // 声明 hash 值为 final 型 volatile V value; // 声明 value 为 volatile 型 final HashEntry&lt;K,V&gt; next; // 声明 next 为 final 型 HashEntry(K key, int hash, HashEntry&lt;K,V&gt; next, V value) &#123; this.key = key; this.hash = hash; this.next = next; this.value = value; &#125; &#125; 在 ConcurrentHashMap 中，在散列时如果产生“碰撞”，将采用“分离链接法”来处理“碰撞”：把“碰撞”的 HashEntry 对象链接成一个链表。由于 HashEntry 的 next 域为 final 型，所以新节点只能在链表的表头处插入。 下图是在一个空桶中依次插入 A，B，C 三个 HashEntry 对象后的结构图： 图 1. 插入三个节点后桶的结构示意图： 注意：由于只能在表头插入，所以链表中节点的顺序和插入的顺序相反。 Segment 类Segment 类继承于 ReentrantLock 类，从而使得 Segment 对象能充当锁的角色。每个 Segment 对象用来守护其（成员对象 table 中）包含的若干个桶。 table 是一个由 HashEntry 对象组成的数组。table 数组的每一个数组成员就是散列映射表的一个桶。 count 变量是一个计数器，它表示每个 Segment 对象管理的 table 数组（若干个 HashEntry 组成的链表）包含的 HashEntry 对象的个数。每一个 Segment 对象都有一个 count 对象来表示本 Segment 中包含的 HashEntry 对象的总数。注意，之所以在每个 Segment 对象中包含一个计数器，而不是在 ConcurrentHashMap 中使用全局的计数器，是为了避免出现“热点域”而影响 ConcurrentHashMap 的并发性。 避免热点域在 ConcurrentHashMap中，每一个 Segment 对象都有一个 count 对象来表示本 Segment 中包含的 HashEntry 对象的个数。这样当需要更新计数器时，不用锁定整个 ConcurrentHashMap。 清单 2.Segment 类的定义123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; /** * 在本 segment 范围内，包含的 HashEntry 元素的个数 * 该变量被声明为 volatile 型 */ transient volatile int count; /** * table 被更新的次数 */ transient int modCount; /** * 当 table 中包含的 HashEntry 元素的个数超过本变量值时，触发 table 的再散列 */ transient int threshold; /** * table 是由 HashEntry 对象组成的数组 * 如果散列时发生碰撞，碰撞的 HashEntry 对象就以链表的形式链接成一个链表 * table 数组的数组成员代表散列映射表的一个桶 * 每个 table 守护整个 ConcurrentHashMap 包含桶总数的一部分 * 如果并发级别为 16，table 则守护 ConcurrentHashMap 包含的桶总数的 1/16 */ transient volatile HashEntry&lt;K,V&gt;[] table; /** * 装载因子 */ final float loadFactor; Segment(int initialCapacity, float lf) &#123; loadFactor = lf; setTable(HashEntry.&lt;K,V&gt;newArray(initialCapacity)); &#125; /** * 设置 table 引用到这个新生成的 HashEntry 数组 * 只能在持有锁或构造函数中调用本方法 */ void setTable(HashEntry&lt;K,V&gt;[] newTable) &#123; // 计算临界阀值为新数组的长度与装载因子的乘积 threshold = (int)(newTable.length * loadFactor); table = newTable; &#125; /** * 根据 key 的散列值，找到 table 中对应的那个桶（table 数组的某个数组成员） */ HashEntry&lt;K,V&gt; getFirst(int hash) &#123; HashEntry&lt;K,V&gt;[] tab = table; // 把散列值与 table 数组长度减 1 的值相“与”，// 得到散列值对应的 table 数组的下标 // 然后返回 table 数组中此下标对应的 HashEntry 元素 return tab[hash &amp; (tab.length - 1)]; &#125; &#125; 下图是依次插入 ABC 三个 HashEntry 节点后，Segment 的结构示意图。 图 2. 插入三个节点后 Segment 的结构示意图： ConcurrentHashMap 类ConcurrentHashMap 在默认并发级别会创建包含 16 个 Segment 对象的数组。每个 Segment 的成员对象 table 包含若干个散列表的桶。每个桶是由 HashEntry 链接起来的一个链表。如果键能均匀散列，每个 Segment 大约守护整个散列表中桶总数的 1/16。 清单 3.ConcurrentHashMap 类的定义123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public class ConcurrentHashMap&lt;K, V&gt; extends AbstractMap&lt;K, V&gt; implements ConcurrentMap&lt;K, V&gt;, Serializable &#123; /** * 散列映射表的默认初始容量为 16，即初始默认为 16 个桶 * 在构造函数中没有指定这个参数时，使用本参数 */ static final int DEFAULT_INITIAL_CAPACITY= 16; /** * 散列映射表的默认装载因子为 0.75，该值是 table 中包含的 HashEntry 元素的个数与* table 数组长度的比值 * 当 table 中包含的 HashEntry 元素的个数超过了 table 数组的长度与装载因子的乘积时，* 将触发 再散列 * 在构造函数中没有指定这个参数时，使用本参数 */ static final float DEFAULT_LOAD_FACTOR= 0.75f; /** * 散列表的默认并发级别为 16。该值表示当前更新线程的估计数 * 在构造函数中没有指定这个参数时，使用本参数 */ static final int DEFAULT_CONCURRENCY_LEVEL= 16; /** * segments 的掩码值 * key 的散列码的高位用来选择具体的 segment */ final int segmentMask; /** * 偏移量 */ final int segmentShift; /** * 由 Segment 对象组成的数组 */ final Segment&lt;K,V&gt;[] segments; /** * 创建一个带有指定初始容量、加载因子和并发级别的新的空映射。 */ public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; if(!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if(concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; // 寻找最佳匹配参数（不小于给定参数的最接近的 2 次幂） int sshift = 0; int ssize = 1; while(ssize &lt; concurrencyLevel) &#123; ++sshift; ssize &lt;&lt;= 1; &#125; segmentShift = 32 - sshift; // 偏移量值 segmentMask = ssize - 1; // 掩码值 this.segments = Segment.newArray(ssize); // 创建数组 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; int c = initialCapacity / ssize; if(c * ssize &lt; initialCapacity) ++c; int cap = 1; while(cap &lt; c) cap &lt;&lt;= 1; // 依次遍历每个数组元素 for(int i = 0; i &lt; this.segments.length; ++i) // 初始化每个数组元素引用的 Segment 对象this.segments[i] = new Segment&lt;K,V&gt;(cap, loadFactor); &#125; /** * 创建一个带有默认初始容量 (16)、默认加载因子 (0.75) 和 默认并发级别 (16) * 的空散列映射表。 */ public ConcurrentHashMap() &#123; // 使用三个默认参数，调用上面重载的构造函数来创建空散列映射表this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL); &#125;&#125; 下面是 ConcurrentHashMap 的结构示意图。 图 3.ConcurrentHashMap 的结构示意图： 用分离锁实现多个线程间的并发写操作在 ConcurrentHashMap 中，线程对映射表做读操作时，一般情况下不需要加锁就可以完成，对容器做结构性修改的操作才需要加锁。下面以 put 操作为例说明对 ConcurrentHashMap 做结构性修改的过程。 首先，根据 key 计算出对应的 hash 值： 清单 4.Put 方法的实现1234567public V put(K key, V value) &#123; if (value == null) //ConcurrentHashMap 中不允许用 null 作为映射值 throw new NullPointerException(); int hash = hash(key.hashCode()); // 计算键对应的散列码 // 根据散列码找到对应的 Segment return segmentFor(hash).put(key, hash, value, false); &#125; 然后，根据 hash 值找到对应的Segment 对象： 清单 5.根据 hash 值找到对应的 Segment12345678910/** * 使用 key 的散列码来得到 segments 数组中对应的 Segment */ final Segment&lt;K,V&gt; segmentFor(int hash) &#123; // 将散列值右移 segmentShift 个位，并在高位填充 0 // 然后把得到的值与 segmentMask 相“与”// 从而得到 hash 值对应的 segments 数组的下标值// 最后根据下标值返回散列码对应的 Segment 对象 return segments[(hash &gt;&gt;&gt; segmentShift) &amp; segmentMask]; &#125; 最后，在这个 Segment 中执行具体的 put 操作： 清单 6.在 Segment 中执行具体的 put 操作12345678910111213141516171819202122232425262728293031323334353637V put(K key, int hash, V value, boolean onlyIfAbsent) &#123; lock(); // 加锁，这里是锁定某个 Segment 对象而非整个 ConcurrentHashMap try &#123; int c = count; if (c++ &gt; threshold) // 如果超过再散列的阈值 rehash(); // 执行再散列，table 数组的长度将扩充一倍 HashEntry&lt;K,V&gt;[] tab = table; // 把散列码值与 table 数组的长度减 1 的值相“与” // 得到该散列码对应的 table 数组的下标值 int index = hash &amp; (tab.length - 1); // 找到散列码对应的具体的那个桶 HashEntry&lt;K,V&gt; first = tab[index]; HashEntry&lt;K,V&gt; e = first; while (e != null &amp;&amp; (e.hash != hash || !key.equals(e.key))) e = e.next; V oldValue; if (e != null) &#123; // 如果键 / 值对以经存在 oldValue = e.value; if (!onlyIfAbsent) e.value = value; // 设置 value 值 &#125; else &#123; // 键 / 值对不存在 oldValue = null; ++modCount; // 要添加新节点到链表中，所以 modCont 要加 1 // 创建新节点，并添加到链表的头部 tab[index] = new HashEntry&lt;K,V&gt;(key, hash, first, value); count = c; // 写 count 变量 &#125; return oldValue; &#125; finally &#123; unlock(); // 解锁 &#125; &#125; 注意：这里的加锁操作是针对（键的 hash 值对应的）某个具体的 Segment，锁定的是该 Segment 而不是整个 ConcurrentHashMap。因为插入键 / 值对操作只是在这个 Segment 包含的某个桶中完成，不需要锁定整个ConcurrentHashMap。此时，其他写线程对另外 15 个Segment 的加锁并不会因为当前线程对这个 Segment 的加锁而阻塞。同时，所有读线程几乎不会因本线程的加锁而阻塞（除非读线程刚好读到这个 Segment 中某个 HashEntry 的 value 域的值为 null，此时需要加锁后重新读取该值）。 相比较于 HashTable 和由同步包装器包装的 HashMap每次只能有一个线程执行读或写操作，ConcurrentHashMap 在并发访问性能上有了质的提高。在理想状态下，ConcurrentHashMap 可以支持 16 个线程执行并发写操作（如果并发级别设置为 16），及任意数量线程的读操作。 用 HashEntery 对象的不变性来降低读操作对加锁的需求在代码清单“HashEntry 类的定义”中我们可以看到，HashEntry 中的 key，hash，next 都声明为 final 型。这意味着，不能把节点添加到链接的中间和尾部，也不能在链接的中间和尾部删除节点。这个特性可以保证：在访问某个节点时，这个节点之后的链接不会被改变。这个特性可以大大降低处理链表时的复杂性。 同时，HashEntry 类的 value 域被声明为 Volatile 型，Java 的内存模型可以保证：某个写线程对 value 域的写入马上可以被后续的某个读线程“看”到。在 ConcurrentHashMap 中，不允许用 unll 作为键和值，当读线程读到某个 HashEntry 的 value 域的值为 null 时，便知道产生了冲突——发生了重排序现象，需要加锁后重新读入这个 value 值。这些特性互相配合，使得读线程即使在不加锁状态下，也能正确访问 ConcurrentHashMap。 下面我们分别来分析线程写入的两种情形：对散列表做非结构性修改的操作和对散列表做结构性修改的操作。 非结构性修改操作只是更改某个 HashEntry 的 value 域的值。由于对 Volatile 变量的写入操作将与随后对这个变量的读操作进行同步。当一个写线程修改了某个 HashEntry 的 value 域后，另一个读线程读这个值域，Java 内存模型能够保证读线程读取的一定是更新后的值。所以，写线程对链表的非结构性修改能够被后续不加锁的读线程“看到”。 对 ConcurrentHashMap 做结构性修改，实质上是对某个桶指向的链表做结构性修改。如果能够确保：在读线程遍历一个链表期间，写线程对这个链表所做的结构性修改不影响读线程继续正常遍历这个链表。那么读 / 写线程之间就可以安全并发访问这个 ConcurrentHashMap。 结构性修改操作包括 put，remove，clear。下面我们分别分析这三个操作。 clear 操作只是把 ConcurrentHashMap 中所有的桶“置空”，每个桶之前引用的链表依然存在，只是桶不再引用到这些链表（所有链表的结构并没有被修改）。正在遍历某个链表的读线程依然可以正常执行对该链表的遍历。 从上面的代码清单“在 Segment 中执行具体的 put 操作”中，我们可以看出：put 操作如果需要插入一个新节点到链表中时 , 会在链表头部插入这个新节点。此时，链表中的原有节点的链接并没有被修改。也就是说：插入新健 / 值对到链表中的操作不会影响读线程正常遍历这个链表。 下面来分析 remove 操作，先让我们来看看 remove 操作的源代码实现。 清单 7.remove 操作123456789101112131415161718192021222324252627282930313233343536V remove(Object key, int hash, Object value) &#123; lock(); // 加锁 try&#123; int c = count - 1; HashEntry&lt;K,V&gt;[] tab = table; // 根据散列码找到 table 的下标值 int index = hash &amp; (tab.length - 1); // 找到散列码对应的那个桶 HashEntry&lt;K,V&gt; first = tab[index]; HashEntry&lt;K,V&gt; e = first; while(e != null&amp;&amp; (e.hash != hash || !key.equals(e.key))) e = e.next; V oldValue = null; if(e != null) &#123; V v = e.value; if(value == null|| value.equals(v)) &#123; // 找到要删除的节点 oldValue = v; ++modCount; // 所有处于待删除节点之后的节点原样保留在链表中 // 所有处于待删除节点之前的节点被克隆到新链表中 HashEntry&lt;K,V&gt; newFirst = e.next;// 待删节点的后继结点 for(HashEntry&lt;K,V&gt; p = first; p != e; p = p.next) newFirst = new HashEntry&lt;K,V&gt;(p.key, p.hash, newFirst, p.value); // 把桶链接到新的头结点 // 新的头结点是原链表中，删除节点之前的那个节点 tab[index] = newFirst; count = c; // 写 count 变量 &#125; &#125; return oldValue; &#125; finally&#123; unlock(); // 解锁 &#125; &#125; 和 get 操作一样，首先根据散列码找到具体的链表；然后遍历这个链表找到要删除的节点；最后把待删除节点之后的所有节点原样保留在新链表中，把待删除节点之前的每个节点克隆到新链表中。下面通过图例来说明 remove 操作。假设写线程执行 remove 操作，要删除链表的 C 节点，另一个读线程同时正在遍历这个链表。 图 4. 执行删除之前的原链表： 图 5. 执行删除之后的新链表 从上图可以看出，删除节点 C 之后的所有节点原样保留到新链表中；删除节点 C 之前的每个节点被克隆到新链表中，注意：它们在新链表中的链接顺序被反转了。 在执行 remove 操作时，原始链表并没有被修改，也就是说：读线程不会受同时执行 remove 操作的并发写线程的干扰。 综合上面的分析我们可以看出，写线程对某个链表的结构性修改不会影响其他的并发读线程对这个链表的遍历访问。 用 Volatile 变量协调读写线程间的内存可见性由于内存可见性问题，未正确同步的情况下，写线程写入的值可能并不为后续的读线程可见。 下面以写线程 M 和读线程 N 来说明 ConcurrentHashMap 如何协调读 / 写线程间的内存可见性问题。 图 6. 协调读 - 写线程间的内存可见性的示意图： 假设线程 M 在写入了 volatile 型变量 count 后，线程 N 读取了这个 volatile 型变量 count。 根据 happens-before 关系法则中的程序次序法则，A appens-before 于 B，C happens-before D。 根据 Volatile 变量法则，B happens-before C。 根据传递性，连接上面三个 happens-before 关系得到：A appens-before 于 B； B appens-before C；C happens-before D。也就是说：写线程 M 对链表做的结构性修改，在读线程 N 读取了同一个 volatile 变量后，对线程 N 也是可见的了。 虽然线程 N 是在未加锁的情况下访问链表。Java 的内存模型可以保证：只要之前对链表做结构性修改操作的写线程 M 在退出写方法前写 volatile 型变量 count，读线程 N 在读取这个 volatile 型变量 count 后，就一定能“看到”这些修改。 ConcurrentHashMap 中，每个 Segment 都有一个变量 count。它用来统计 Segment 中的 HashEntry 的个数。这个变量被声明为 volatile。 清单 8.Count 变量的声明1transient volatile int count; 所有不加锁读方法，在进入读方法时，首先都会去读这个 count 变量。比如下面的 get 方法： 清单 9.get 操作12345678910111213141516V get(Object key, int hash) &#123; if(count != 0) &#123; // 首先读 count 变量 HashEntry&lt;K,V&gt; e = getFirst(hash); while(e != null) &#123; if(e.hash == hash &amp;&amp; key.equals(e.key)) &#123; V v = e.value; if(v != null) return v; // 如果读到 value 域为 null，说明发生了重排序，加锁后重新读取 return readValueUnderLock(e); &#125; e = e.next; &#125; &#125; return null; &#125; 在 ConcurrentHashMap 中，所有执行写操作的方法（put, remove, clear），在对链表做结构性修改之后，在退出写方法前都会去写这个 count 变量。所有未加锁的读操作（get, contains, containsKey）在读方法中，都会首先去读取这个 count 变量。 根据 Java 内存模型，对 同一个 volatile 变量的写 / 读操作可以确保：写线程写入的值，能够被之后未加锁的读线程“看到”。 这个特性和前面介绍的 HashEntry 对象的不变性相结合，使得在 ConcurrentHashMap 中，读线程在读取散列表时，基本不需要加锁就能成功获得需要的值。这两个特性相配合，不仅减少了请求同一个锁的频率（读操作一般不需要加锁就能够成功获得值），也减少了持有同一个锁的时间（只有读到 value 域的值为 null 时 , 读线程才需要加锁后重读）。 ConcurrentHashMap 实现高并发的总结基于通常情形而优化在实际的应用中，散列表一般的应用场景是：除了少数插入操作和删除操作外，绝大多数都是读取操作，而且读操作在大多数时候都是成功的。正是基于这个前提，ConcurrentHashMap 针对读操作做了大量的优化。通过 HashEntry 对象的不变性和用 volatile 型变量协调线程间的内存可见性，使得 大多数时候，读操作不需要加锁就可以正确获得值。这个特性使得 ConcurrentHashMap 的并发性能在分离锁的基础上又有了近一步的提高。 总结ConcurrentHashMap 是一个并发散列映射表的实现，它允许完全并发的读取，并且支持给定数量的并发更新。相比于 HashTable 和用同步包装器包装的 HashMap（Collections.synchronizedMap(new HashMap())），ConcurrentHashMap 拥有更高的并发性。在 HashTable 和由同步包装器包装的 HashMap 中，使用一个全局的锁来同步不同线程间的并发访问。同一时间点，只能有一个线程持有锁，也就是说在同一时间点，只能有一个线程能访问容器。这虽然保证多线程间的安全并发访问，但同时也导致对容器的访问变成串行化的了。 在使用锁来协调多线程间并发访问的模式下，减小对锁的竞争可以有效提高并发性。有两种方式可以减小对锁的竞争： 减小请求 同一个锁的 频率。 减少持有锁的 时间。 ConcurrentHashMap 的高并发性主要来自于三个方面： 用分离锁实现多个线程间的更深层次的共享访问。用 HashEntery 对象的不变性来降低执行读操作的线程在遍历链表期间对加锁的需求。 通过对同一个 Volatile 变量的写 / 读访问，协调不同线程间读 / 写操作的内存可见性。 使用分离锁，减小了请求 同一个锁的频率。 通过 HashEntery 对象的不变性及对同一个 Volatile 变量的读 / 写来协调内存可见性，使得 读操作大多数时候不需要加锁就能成功获取到需要的值。由于散列映射表在实际应用中大多数操作都是成功的 读操作，所以 2 和 3 既可以减少请求同一个锁的频率，也可以有效减少持有锁的时间。 通过减小请求同一个锁的频率和尽量减少持有锁的时间 ，使得 ConcurrentHashMap 的并发性相对于 HashTable 和用同步包装器包装的 HashMap有了质的提高。 转：https://www.ibm.com/developerworks/cn/java/java-lo-concurrenthashmap/index.html?ca=drs-http://www.ibm.com/developerworks/cn/java/j-concurrent/http://www.ibm.com/developerworks/cn/java/http://www.ibm.com/developerworks/cn/topics/]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群间主机批量建立互信]]></title>
    <url>%2F2015%2F02%2F21%2FHBase%E9%9B%86%E7%BE%A4%E9%97%B4%E4%B8%BB%E6%9C%BA%E6%89%B9%E9%87%8F%E5%BB%BA%E7%AB%8B%E4%BA%92%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[步骤登录服务器hostname进入目录，将集群全部机器列表写入文件，每行一个机器名12cd /home/q/fabric/hbase/vim fab_hbase_hostfile.txt fab_hbase_hostfile.txt是hostname列表 删除上次执行脚本遗留的文件1rm /home/q/fabric/hbase/auth_test.txt 执行以下命令行1fab -f fabfile_hbase_hostfile.py set_hosts del_user mk_user host_make_idrsa get_idrsapub put_authfile delete_authfile 脚本内容123456789101112131415161718192021222324252627282930313233343536373839404142from fabric.api import *from fabric.contrib.console import confirmfrom fabric.contrib.files import exists env.user=&quot;qnrdba&quot;env.key_filename=&quot;/home/qnrdba/.ssh/id_rsa&quot; def set_hosts(): env.hosts=open(&apos;/home/q/fabric/hbase/fab_hbase_hostfile.txt&apos;,&apos;r&apos;).readlines() def del_user(): if exists(&apos;/home/hadoop&apos;,use_sudo=True): sudo(&apos;userdel hadoop&apos;) sudo(&apos;rm -r /home/hadoop/&apos;) def mk_user(): sudo(&apos;useradd hadoop&apos;) def host_make_idrsa(): sudo(&apos;ssh-keygen -d&apos;,user=&apos;hadoop&apos;) def get_idrsapub(): sudo(&apos;cp /home/hadoop/.ssh/id_dsa.pub /tmp/&apos;) get(&apos;/tmp/id_dsa.pub&apos;,&apos;/home/q/fabric/hbase/&apos;) local(&apos;mv /home/q/fabric/hbase/id_dsa.pub /home/q/fabric/hbase/id_dsa_test.pub&apos;) local(&apos;cat /home/q/fabric/hbase/id_dsa_test.pub &gt;&gt; /home/q/fabric/hbase/auth_test.txt&apos;) local(&apos;rm /home/q/fabric/hbase/id_dsa_test.pub&apos;) def put_authfile(): put(&apos;/home/q/fabric/hbase/auth_test.txt&apos;,&apos;/tmp/&apos;) sudo(&apos;cp /tmp/auth_test.txt /home/hadoop/.ssh/authorized_keys&apos;) sudo(&apos;chmod 600 /home/hadoop/.ssh/authorized_keys&apos;) sudo(&apos;chown hadoop.hadoop /home/hadoop/.ssh/authorized_keys&apos;) def delete_authfile(): sudo(&apos;rm /tmp/auth_test.txt&apos;) sudo(&apos;rm /tmp/id_dsa.pub&apos;) 使用说明12345678910111213del_user 判断：若已存在hadoop账户，删除该账户、家目录delete_authfile 删除集群各服务器临时文件get_idrsapub 获取公钥到本地，追加生成auth汇总文件host_make_idrsa 使用hadoop账户生成公钥文件mk_user 创建hadoop账户put_authfile 将本地anth汇总文件分发至集群各服务器，修改权限set_hosts 配置集群服务器主机列表]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iptables的使用]]></title>
    <url>%2F2015%2F02%2F20%2Fiptables%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1、iptables的启停1service iptables stop|start 2、查看iptables的状态1service iptables status 3、查看当前服务器上的iptables配置规则1iptables --list 4、设置默认策略12345678iptables -P INPUT (DROP|ACCEPT) 默认是关的/默认是开的-A：指定链名-p：指定协议类型-d：指定目标地址--dport：指定目标端口（destination port 目的端口）--sport：指定源端口（source port 源端口）-j：指定动作类型 保存设置规则1[root@hostname ~]# /etc/rc.d/init.d/iptables save 这样就可以写到/etc/sysconfig/iptables配置文件里了.写入后记得把防火墙重起一下,才能起作用. 重启iptables1[root@hostname ~]# service iptables restart INPUT：主要与想要进入我们 Linux 本机的数据包有关OUTPUT：主要与我们 Linux 本机所要送出的数据包有关FORWARD：与 Linux 本机比较没有关系， 他可以传递数据包到后台的计算机中，nat table相关性较高只允许xxx.xxx.xxx.xxx的机器进行SSH连接1[root@hostname ~]# iptables -A INPUT -s xxx.xxx.xxx.xxx -p tcp --dport 22 -j ACCEPT 设置预定规则，不允许其他机器访问1[root@hostname ~]# iptables -P INPUT DROP 允许所有机器访问1[root@hostname ~]# iptables -P INPUT ACCEPT 删除某条规则1iptables -D INPUT -s xxx.xxx.xxx.xxx -p tcp --dport 22 -j ACCEPT]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用备份恢复namenode数据]]></title>
    <url>%2F2015%2F02%2F20%2F%E5%88%A9%E7%94%A8%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8Dnamenode%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[1、将备份的namenode数据拷贝到NN1启动配置的目录下（只是拷贝fsimage文件） 2、启动各个journalnode1./sbin/hadoop-daemon.sh start journalnode 3、将本地Namenode中的edits初始化Journalnodes1./bin/hdfs namenode -initializeSharedEdits 4、NN1启动1./sbin/hadoop-daemon.sh start namenode 5、NN2拷贝元数据1./bin/hdfs namenode -bootstrapStandby 6、启动NN21./sbin/hadoop-daemon.sh start namenode 7、启动各个数据节点的DataNode1./sbin/hadoop-daemons.sh start datanode 8、启动zkfc在NN1、NN2上分别执行./sbin/hadoop-daemon.sh start zkfc 9、重启hbase10、使用hbck检查hbase数据是否一致，需要修复]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[减少/增加regionserver节点]]></title>
    <url>%2F2015%2F02%2F20%2F%E5%87%8F%E5%B0%91regionserver%E8%8A%82%E7%82%B9%2F</url>
    <content type="text"><![CDATA[hbase减少regionserver节点可以在指定节点的hbase目录下执行以下命令来停止集群中一个regionserver1./bin/hbase-daemon.sh stop regionserver regionserver会将它上面所有的region关闭，然后再把自己的进程停止。regionserver在zookeeper中对应的临时节点将会过期，master会注意到regionserver停止服务并将其按崩溃服务器处理：master会将服务器上的region重新分配到其他机器上。这方法的坏处是，region会下线一段时间，时间长度由zookeeper超时时间决定，这样在该进程关闭的过程中，部署在该区域服务器上的所有region都将脱机一段时间。 hbase0.90.2引入了一种可以让regionserver逐渐减少其负载并停止服务的办法 1、1[hadoop@hostname /home/q/hbase/hbase-0.98.1-cdh5.1.0]$ ./bin/graceful_stop.sh hostname 2、关闭完成后，打开负载均衡由于会关闭hbase的balancer，因此需要在其他regionserver节点上打开hbase shell，检查hbase状态同时重新设置：12hbase(main):005:0&gt; balance_switch truefalse 3、在regionservers配置文件中去掉关闭的服务器 hbase增加regionserver节点1&gt; 执行以下命令启动regionserver1hbase-daemon.sh start regionserver 2&gt; 在新启动的节点上打开hbase shell，如下设置：1balance_switch true]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[namenode的HA由手动改成自动切换]]></title>
    <url>%2F2015%2F02%2F20%2Fnamenode%E7%9A%84HA%E7%94%B1%E6%89%8B%E5%8A%A8%E6%94%B9%E6%88%90%E8%87%AA%E5%8A%A8%E5%88%87%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[1、修改配置文件 在core-site.xml配置文件中加入12345678&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hostname:2181,hostname:2181,hostname:2181&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt; &lt;value&gt;20000&lt;/value&gt;&lt;/property&gt; 修改hdfs-site.xml配置文件12345678910&lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt;&lt;/property&gt; 将更改后的配置文件scp到各节点 2、格式化zkfc1./bin/hdfs zkfc -formatZK 3、重启standby namenode节点(直到safemode变成off)12./sbin/hadoop-daemon.sh stop namenode./sbin/hadoop-daemon.sh start namenode 4、关闭active namenode1./sbin/hadoop-daemon.sh stop namenode 5、启动standby namenode节点上的zkfc1./sbin/hadoop-daemon.sh start zkfc 6、 启动原active namenode(启动之后变成standby) 1./sbin/hadoop-daemon.sh start namenode 7、最后启动原active namenode节点zkfc]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase move region]]></title>
    <url>%2F2015%2F02%2F20%2Fhbase-move-region%2F</url>
    <content type="text"><![CDATA[语法：move ‘encodeRegionName’, ‘ServerName’encodeRegionName指的regioName后面的编码ServerName指的是master-status的Region Servers列表例如要移动tablename,602CFEBBEA72-5D39-42A4-5EBA-44C175D6|2015-09-10|01:26:03|1057122,1442138043465.4d47dcb1e96426f4b1cfbe02908d5fd1.到hostname上使用如下命令1move &apos;4d47dcb1e96426f4b1cfbe02908d5fd1&apos;,&apos;hostname,60020,1441770084156&apos;]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[phoenix映射HBase数据表]]></title>
    <url>%2F2015%2F02%2F20%2Fphoenix%E6%98%A0%E5%B0%84HBase%E6%95%B0%E6%8D%AE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[近期开发有一需求，主要是想使用phoenix映射Hbase的数据表 1、创建HBase表1create &apos;cong&apos;,&apos;cf&apos; 2、插入数据12345hbase(main):003:0&gt; scan &apos;cong&apos;ROW COLUMN+CELLrow1 column=cf:a, timestamp=1447227186580, value=agagagrow2 column=cf:b, timestamp=1447227198719, value=woietw2 row(s) in 0.0910 seconds 注意：HBase数据表默认主键列名是ROW 3、创建phoenix表120: jdbc:phoenix:hostname&gt; create table &quot;cong&quot;(row varchar primary key,&quot;cf&quot;.&quot;a&quot; varchar,&quot;cf&quot;.&quot;b&quot; varchar);2 rows affected (5.285 seconds) 注意：这里一定要注意的是表名和列族以及列名需要用双引号括起来，因为HBase是区分大小写的，如果不用双引号括起来的话Phoenix在创建表的时候会自动将小写转换为大写字母，这样HBase中会创建另外一张表PHOENIX。 4、验证 查看hbase表1234567hbase(main):003:0&gt; scan &apos;cong&apos;ROW COLUMN+CELLrow1 column=cf:_0, timestamp=1447227186580, value=row1 column=cf:a, timestamp=1447227186580, value=agagagrow2 column=cf:_0, timestamp=1447227198719, value=row2 column=cf:b, timestamp=1447227198719, value=woietw2 row(s) in 0.0910 seconds 参考连接http://blog.csdn.net/maomaosi2009/article/details/45598985]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM内存组成和分配]]></title>
    <url>%2F2015%2F02%2F20%2FJVM%E5%86%85%E5%AD%98%E7%BB%84%E6%88%90%E5%92%8C%E5%88%86%E9%85%8D%2F</url>
    <content type="text"><![CDATA[java内存组成介绍：堆(Heap)和非堆(Non-heap)内存按照官方的说法：“Java 虚拟机具有一个堆，堆是运行时数据区域，所有类实例和数组的内存均从此处分配。堆是在 Java 虚拟机启动时创建的。”“在JVM中堆之外的内存称为非堆内存(Non-heap memory)”。可以看出JVM主要管理两种类型的内存：堆和非堆。简单来说堆就是Java代码可及的内存，是留给开发人员使用的；非堆就是JVM留给 自己用的，所以方法区、JVM内部处理或优化所需的内存(如JIT编译后的代码缓存)、每个类结构(如运行时常数池、字段和方法数据)以及方法和构造方法 的代码都在非堆内存中。 堆内存分配JVM初始分配的内存由-Xms指定，默认是物理内存的1/64；JVM最大分配的内存由-Xmx指 定，默认是物理内存的1/4。默认空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制；空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制。因此服务器一般设置-Xms、-Xmx相等以避免在每次GC 后调整堆的大小。对象的堆内存由称为垃圾回收器的自动内存管理系统回收。 123[hadoop@hostname ~]$ /home/q/java/default/bin/jstat -gc 36041 S0C S1C S0U S1U EC EU OC OU PC PU YGC YGCT FGC FGCT GCT 5242880.0 5242880.0 0.0 0.0 41943040.0 6432353.7 14680064.0 11131537.5 262144.0 38704.8 96 38.396 438 898.519 936.915]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM介绍]]></title>
    <url>%2F2015%2F02%2F20%2FJVM%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[堆内存：JVM运行时数据区域，它为类实例和数组分配的内存。堆可以是固定大小的也可以是可变大小的。其中 Heap = (Old + NEW =(Eden , from, to)) 程序中运行的各种类实例称之为对象，每个对象都有不同的生命周期，有的存活时间长点，有的存活时间短点。JVM内存机制的设置就是为了要满足这种不同生命周期的对象对内存的需求，并使之能达到最大的性能表现。JVM为了对各种不同生命周期的对象进行有效管理也划分了各种不同的区域，这就是“代”的概念，分别叫作：“年轻代”、“老年代”、“持久代” 年轻代年青代由一个Eden Space和两个Survivor Spaces组成，虚拟机初始时分配所有的对象到Eden Space，许多对象也是在这里死去。当它执行一个“minor GC”的时候，虚拟机将从Eden Space中移动一些残余的对象到其中的一个Survivor Spaces中。 Eden Space: 这个内存池在对象初始化时被分配； Survivor Space: 这个内存池中包含着Eden Space 经过GC之后幸存下来的对象； 新对象在Eden区分配内存,GC时将Eden和有对象的Survivor区(From Space)的所有对象复制到另外一个Survior(To Space)，然后清空Eden和原Survior 当一个对象在from和to之间复制次数超过一定阈值(-XX:MaxTenuringThreshold)后，进入到年老代。如果是CMS GC，这个阈值默认为0，也就是经过一次copy后就进入年老代 老年代虚拟机将在Survivor Spaces中生存足够长时间的对象移动到老年代的Tenured Spaces中。当Tenured Generation被填满，则将执行一个完全GC，这个完全GC非常的慢，因为它要处理所有存活着的对象，用的是串行标记收集的方式，并发收集可以减少对于应用的影响。 JVM垃圾回收JVM所管理的有限内存也要实现最优化利用，Garbage Collection（GC）就是用来释放没有被引用的对象所占领的内存,目的在于清除不再使用的对象。GC通过算法和参数的配置可以对性能产生效果显著的影响。JVM调优的目的在于如何能是系统表现出更好的响应时间、更大的吞吐量。MinorCollections（局部垃圾回收）：当通用内存消耗完被分配的内存时，JVM会在内存池上执行一个局部的GC（总是调用minor collection）去释放被dead的对象所占用的内存。这个局部的GC通常比完全GC要快许多。青年代中的垃圾回收就是采用局部垃圾回收机制，因此，青年代中内存分配和管理效率也是最高。通常情况下，对于内存的申请优先在年轻代中申请，当内存不够时会整理年轻代，当整理以后还是不能满足申请的内存，就会向老年代移动一些生命周期较长的对象。这种整理和移动会消耗资源，同时降低系统运行响应能力，因此如果青年代设置的过小，就会频繁的整理和移动，对性能造成影响。那是否把年青代设置的越大越好，其实不然，青年代采用的是复制搜集算法，这种算法必须停止所有应用程序线程，服务器线程切换时间就会成为应用响应的瓶颈。 Major Collections（完全垃圾回收）：当老年代需要被回收，这就是一个major collection ，它的运行常常非常慢，因为它要涉及所有存活着的类。 JVM参数可参考链接地址http://www.cnblogs.com/redcreen/archive/2011/05/04/2037057.html]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM GC介绍]]></title>
    <url>%2F2015%2F02%2F20%2FJVM-GC%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[JVM Heap在实现中被切分成了不同的generation，比如生命周期短的对象会放在young generation，而生命周期长的对象放在tenured generation中，如下图 当GC只发生在young generation中，回收young generation中的对象，称为Minor GC；当GC发生在tenured generation时则称为Major GC或者Full GC。一般的，Minor GC的发生频率要比Major GC高很多。 负责Young Generation的collector有三种Serial最简单的collector，只有一个thread负责GC，并且，在执行GC的时候，会暂停整个程序（所谓的“stop-the-world”），如下图所示 Parallel Scavenge和Serial相比，它的特点在于使用multi-thread来处理GC，当然，在执行的时候，仍然会“stop-the-world”，好处在于，暂停的时间也许更短，如下图所示 ParNew它基本上和Parallel Scavenge非常相似，唯一的区别，在于它做了强化能够和CMS一起使用 负责Tenured Generation的collector也有三种：Serial Old单线程，采用的是mark-sweep-compact回收方法，相当于单线程，Serial类似； Parallel Old同理，多线程的GC collector CMS全称“concurrent-mark-sweep”，它是最并发，暂停时间最低的collector，之所以称为concurrent，是因为它在执行GC任务的时候，GC thread是和application thread一起工作的，基本上不需要暂停application thread，如下图所示 组合方案6种collector介绍完了。不过，在设定JVM参数的时候，很少有人去分别制定young generation和tenured generation的collector，而是提供了几套选择方案： -XX:+UseSerialGC相当于”Serial” + “SerialOld”，这个方案直观上就应该是性能最差的，我的实验证明也确实如此； -XX:+UseParallelGC相当于” Parallel Scavenge” + “SerialOld”，也就是说，在young generation中是多线程处理，但是在tenured generation中则是单线程； -XX:+UseParallelOldGC相当于” Parallel Scavenge” + “ParallelOld”，都是多线程并行处理； -XX:+UseConcMarkSweepGC相当于”ParNew” + “CMS” + “Serial Old”，即在young generation中采用ParNew，多线程处理；在tenured generation中使用CMS，以求得到最低的暂停时间，但是，采用CMS有可能出现”Concurrent Mode Failure”，如果出现了，就只能采用”SerialOld”模式了。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jmap、jstat命令说明]]></title>
    <url>%2F2015%2F02%2F20%2Fjmap%E3%80%81jstat%E5%91%BD%E4%BB%A4%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[jmap使用G1收集器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253 /home/q/java/default/bin/jmap -heap 29591Attaching to process ID 29591, please wait...Debugger attached successfully.Server compiler detected.JVM version is 24.45-b08 //显示JVM版本号using thread-local object allocation.Garbage-First (G1) GC with 10 thread(s) //使用了G1垃圾收集器，启用10个线程Heap Configuration:MinHeapFreeRatio = 40MaxHeapFreeRatio = 70 //堆内存的使用比例在30%~60%之间MaxHeapSize = 8589934592 (8192.0MB) //最大堆内存大小为8192.0MBNewSize = 5368709120 (5120.0MB) MaxNewSize = 17592186044415 MBOldSize = 5452592 (5.1999969482421875MB)NewRatio = 2SurvivorRatio = 8PermSize = 268435456 (256.0MB) //持久代大小为256MBMaxPermSize = 268435456 (256.0MB) //持久代最大大小为256MBG1HeapRegionSize = 4194304 (4.0MB)Heap Usage:G1 Heap:regions = 2048capacity = 8589934592 (8192.0MB)used = 2063597568 (1968.0MB)free = 6526337024 (6224.0MB)24.0234375% usedG1 Young Generation: //年轻代信息Eden Space:regions = 492capacity = 5637144576 (5376.0MB)used = 2063597568 (1968.0MB)free = 3573547008 (3408.0MB)36.607142857142854% usedSurvivor Space:regions = 0capacity = 0 (0.0MB)used = 0 (0.0MB)free = 0 (0.0MB)0.0% usedG1 Old Generation: //年老代信息regions = 0capacity = 2952790016 (2816.0MB)used = 0 (0.0MB)free = 2952790016 (2816.0MB)0.0% usedPerm Generation: //持久代信息capacity = 268435456 (256.0MB)used = 37596920 (35.85521697998047MB)free = 230838536 (220.14478302001953MB)14.00594413280487% used 使用cms收集器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/home/q/java/default/bin/jmap -heap 20396Attaching to process ID 20396, please wait...Debugger attached successfully.Server compiler detected.JVM version is 24.45-b08using parallel threads in the new generation.using thread-local object allocation.Concurrent Mark-Sweep GCHeap Configuration:MinHeapFreeRatio = 40MaxHeapFreeRatio = 70MaxHeapSize = 34359738368 (32768.0MB)NewSize = 21474836480 (20480.0MB)MaxNewSize = 21474836480 (20480.0MB)OldSize = 12884901888 (12288.0MB)NewRatio = 2SurvivorRatio = 3PermSize = 268435456 (256.0MB)MaxPermSize = 268435456 (256.0MB)G1HeapRegionSize = 0 (0.0MB)Heap Usage:New Generation (Eden + 1 Survivor Space):capacity = 17179869184 (16384.0MB)used = 7197689648 (6864.251754760742MB)free = 9982179536 (9519.748245239258MB)41.89606783911586% usedEden Space:capacity = 12884901888 (12288.0MB)used = 7183594472 (6850.809547424316MB)free = 5701307416 (5437.190452575684MB)55.75203082213799% usedFrom Space:capacity = 4294967296 (4096.0MB)used = 14095176 (13.442207336425781MB)free = 4280872120 (4082.557792663574MB)0.32817889004945755% usedTo Space:capacity = 4294967296 (4096.0MB)used = 0 (0.0MB)free = 4294967296 (4096.0MB)0.0% usedconcurrent mark-sweep generation:capacity = 12884901888 (12288.0MB)used = 0 (0.0MB)free = 12884901888 (12288.0MB)0.0% usedPerm Generation:capacity = 268435456 (256.0MB)used = 21351800 (20.36266326904297MB)free = 247083656 (235.63733673095703MB)7.95416533946991% used jstat123456789jstat -gcutil 54863 1000S0 S1 E O P YGC YGCT FGC FGCT GCT0.00 100.00 100.00 100.00 14.82 8 50.530 0 0.000 50.5300.00 100.00 100.00 100.00 14.82 8 50.530 0 0.000 50.5300.00 100.00 100.00 100.00 14.82 8 50.530 0 0.000 50.5300.00 100.00 100.00 100.00 14.82 8 50.530 0 0.000 50.5300.00 100.00 100.00 100.00 14.82 8 50.530 0 0.000 50.5300.00 100.00 100.00 100.00 14.82 8 50.530 0 0.000 50.5300.00 0.00 0.00 100.00 14.82 9 68.780 0 0.000 68.780 12345678910S0：Heap上的 Survivor space 0 段已使用空间的百分比S1：Heap上的 Survivor space 1 段已使用空间的百分比E： Heap上的 Eden space 段已使用空间的百分比O： Heap上的 Old space 段已使用空间的百分比P： Perm space 已使用空间的百分比YGC：从程序启动到采样时发生Young GC的次数YGCT：Young GC所用的时间(单位秒)FGC：从程序启动到采样时发生Full GC的次数FGCT：Full GC所用的时间(单位秒)GCT：用于垃圾回收的总时间(单位秒)]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS HA与QJM-官网整理]]></title>
    <url>%2F2015%2F02%2F20%2FHDFS-HA%E4%B8%8EQJM-%E5%AE%98%E7%BD%91%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[默认的”sbin/start-dfs.sh”脚本会根据”dfs.namenode.shared.edits.dir”配置，在相应的Datanode上启动journalNodes。当然我们可以使用:：”bin/hdfs start journalnode”分别在相应的机器上启动。 一旦JournalNodes启动成功，它们将会从Namenode上同步metadata。 1、如果你的HDFS集群是新建的，那么需要在每个Namenode上执行”hdfs namenode -format”指令。 2、如果你的namenodes已经format了，或者是将non-ha转换成ha架构，你应该在将其中一个namenode上的metadata复制到另一台上(dfs.namenode.name.dir目录下的数据)，然后在那个没有format的新加入的namenode上执行”hdfs namenode -bootstrapStandby”。运行这个指令需要确保JournalNodes中持有足够多的edits。 3、如果你将一个non-ha的Namenode(比如backup，其已经formated)切换成HA，你需要首先运行”hdfs -initializeSharedEdits”，这个指令将本地Namenode中的edits初始化Journalnodes。 此后，你就可以启动HA Namenodes。可以通过配置指定的HTTP地址(dfs.namenode.https-address)来查看各个Namenode的状态，Active or Standby。 参考网址http://www.tuicool.com/articles/jameeqm]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase配置参考]]></title>
    <url>%2F2015%2F02%2F20%2Fhbase%E9%85%8D%E7%BD%AE%E5%8F%82%E8%80%83%2F</url>
    <content type="text"><![CDATA[hbase内存配置修改/home/q/hbase/hbase-0.98.1-cdh5.1.0/conf/hbase-env.sh 守护进程8G1export HBASE_HEAPSIZE=8192 Hmaster 8G12345export HBASE_OPTS=&quot;Xms8g -Xmx8g -XX:NewSize=5g -XX:PermSize=256m -server -XX:SurvivorRatio=3 -XX:+DisableExplicitGC -XX:+UseConcMarkSweepGC -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=75 -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+PrintClassHistogram -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -Xloggc:/home/q/hbase/hbase-0.98.1-cdh5.1.0/logs/gc$(hostname)-hbase.log -XX:ParallelGCThreads=10&quot; export HBASE_MASTER_OPTS=&quot;-Xms8g -Xmx8g -XX:NewSize=5g -XX:PermSize=256m -server -XX:SurvivorRatio=3 -XX:+DisableExplicitGC -XX:+UseConcMarkSweepGC -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=75 -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+PrintClassHistogram -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -Xloggc:/home/q/hbase/hbase-0.98.1-cdh5.1.0/logs/gc-$(hostname)-hbase.log -XX:ParallelGCThreads=10&quot;HRegion 64GBexport HBASE_REGIONSERVER_OPTS=&quot;-Xms64g -Xmx64g -XX:NewSize=50g -XX:PermSize=256m -server -XX:SurvivorRatio=3 -XX:+DisableExplicitGC -XX:+UseConcMarkSweepGC -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=75 -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+PrintClassHistogram -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -Xloggc:/home/q/hbase/hbase-0.98.1-cdh5.1.0/logs/gc-$(hostname)-hbase.log -XX:ParallelGCThreads=10&quot; hbase-site.xml修改/home/q/hbase/hbase-0.98.1-cdh5.1.0/conf/hbase-site.xml 123456789101112131415zookeeper.session.timeout=60000hbase.regionserver.handler.count=150hbase.regionserver.global.memstore.upperLimit=0.4hbase.regionserver.global.memstore.lowerLimit=0.38hfile.block.cache.size=0.2hbase.hregion.memstore.block.multiplier=4hbase.hstore.blockingStoreFiles=30hbase.hregion.majorcompaction=0 1234567891011121314151617181920212223zookeeper.session.timeout这个参数的意义是regionserver在zookeeper的会话过期时间，默认是9000ms，如果regionserver 在zookeeper.session.timeout这个配置的时间没有去连zookeeper的话，zookeeper会将该 regionserver在zookeeper摘除，不让该regionserver向提供服务hbase.regionserver.handler.countregionserver的工作线程数量，默认是30，官方默认值太小，通常都调到100~200之间，提高regionserver性能hbase.regionserver.global.memstore.lowerLimit指定了何时对MemStores进行强制刷盘，系统会一直进行写盘直到MemStore所占用的总内存大小低于该属性的值为止hbase.regionserver.global.memstore.upperLimit该属性控制了一台区域服务器中所有MemStore的总大小的最大值，超过该值后，新的更新就会被阻塞，并且强制进行刷盘hfile.block.cache.sizeregionserver cache的大小，默认是0.2，是整个堆内存的多少比例作为regionserver的cache，适当调大该值会提升查询性能hbase.hregion.majorcompaction表示majorcompaction的周期，默认是1 天，majorcompaction与普通的compaction的区别是majorcompaction会清除过期的历史版本数据，同时合并 storefile，而普通的compaction只做合并，通常都是majorcompaction，调为0，然后手工定期的去执行一下 majorcompaction]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase的timestamp相关操作]]></title>
    <url>%2F2015%2F02%2F20%2Fhbase%E7%9A%84timestamp%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[timestamp转换为可读的方式1234hbase(main):010:0&gt;import java.util.Date=&gt; Java::JavaUtil::Datehbase(main):011:0&gt; Date.new(1446520588410).toString()=&gt; &quot;Tue Nov 03 11:16:28 CST 2015&quot; 时间转换为timestamp格式123456hbase(main):013:0&gt; import java.text.SimpleDateFormat=&gt; Java::JavaText::SimpleDateFormathbase(main):014:0&gt; import java.text.ParsePosition=&gt; Java::JavaText::ParsePositionhbase(main):016:0&gt; SimpleDateFormat.new(&quot;yyyy-MM-dd hh:mm:ss&quot;).parse(&quot;2015-11-03 11:16:37&quot;,ParsePosition.new(0)).getTime()=&gt; 1446520597000 根据时间戳范围查询1234hbase(main):021:0&gt; scan &apos;cong&apos;, &#123;TIMERANGE =&gt; [1446520588410, 1446520598000] &#125;ROW COLUMN+CELLrow1 column=cf:a, timestamp=1446520588410, value=agoangrow2 column=cf:a, timestamp=1446520597026, value=ljoo]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase存储架构深入]]></title>
    <url>%2F2015%2F02%2F20%2Fhbase%E5%AD%98%E5%82%A8%E6%9E%B6%E6%9E%84%E6%B7%B1%E5%85%A5%2F</url>
    <content type="text"><![CDATA[HBase的存储架构图 HBase操控两种基本类型的文件，一种用于存储WAL的log，另一种用于存储具体的数据。 这两种文件主要由HRegionServer来管理，但是在有的情况下HMaster会跳过HRegionServer，直接操作这两种文件。这些文件都被存储在HDFS上面，并且每个文件包含了多个数据块。 数据如何被写到实际存储中Client发起了一个HTable.put(Put)请求给HRegionServer，HRegionServer会将请求匹配到某个具体的HRegion上面。紧接着的操作时决定是否写WAL log。是否写WAL log由Client传递的一个标志决定，你可以设置这个标志：Put.setWriteToWAL(boolean write)。WAL log文件是一个标准的Hadoop SequenceFile（现在还在讨论是否应该把文件格式改成一个更适合HBase的格式）。在文件中存储了HLogKey，这些Keys包含了和实际数据对应的序列号，用途是当RegionServer崩溃以后能将WAL log中的数据同步到永久存储中去。做完这一步以后，Put数据会被保存到MemStore中，同时会检查MemStore是否已经满了，如果已经满了，则会触发一个Flush to Disk的请求。HRegionServer有一个独立的线程来处理Flush to Disk的请求，它负责将数据写成HFile文件并存到HDFS上。它也会存储最后写入的数据序列号，这样就可以知道哪些数据已经存入了永久存储的HDFS中。 数据的大致流程。假设你需要通过某个特定的RowKey查询一行记录，首先Client端会连接Zookeeper Qurom，通过Zookeeper，Client能获知哪个Server管理-ROOT- Region。接着Client访问管理-ROOT-的Server，进而获知哪个Server管理.META.表。这两个信息Client只会获取一次 并缓存起来。在后续的操作中Client会直接访问管理.META.表的Server，并获取Region分布的信息。一旦Client获取了这一行的位 置信息，比如这一行属于哪个Region，Client将会缓存这个信息并直接访问HRegionServer。久而久之Client缓存的信息渐渐增 多，即使不访问.META.表也能知道去访问哪个HRegionServer。 HRegionServer打开这个Region并创建一个HRegion对象。当HRegion打开以后，它给每个table的每个 HColumnFamily创建一个Store实例。每个Store实例拥有一个或者多个StoreFile实例。StoreFile对HFile做了轻 量级的包装。除了Store实例以外，每个HRegion还拥有一个MemStore实例和一个HLog实例。 存储文件HBase在HDFS上面的所有文件有一个可配置的根目录(由zookeeper.znode.parent参数决定目录名称)，默认根目录是/hbase。通过使用hadoop的DFS工具就可以看到这些文件夹的结构。12345[hadoop@hostname /home/q/hadoop/hadoop-2.3.0-cdh5.1.0]$ ./bin/hdfs dfs -ls /Found 3 itemsdrwxr-xr-x - hadoop supergroup 0 2014-08-04 16:34 /hbase_test1drwxr-xr-x - hadoop supergroup 0 2014-08-28 16:37 /hbase_test2drwxr-xr-x - hadoop supergroup 0 2014-08-05 10:02 /user 在根目录下有个WALs文件夹，这里存了所有由HLog管理的WAL log文件。在WALs目录下的每个文件夹对应一个HRegionServer，每个HRegionServer下面的每个log文件对应一个Region123456789[hadoop@hostname /home/q/hadoop/hadoop-2.3.0-cdh5.1.0]$ ./bin/hdfs dfs -ls /hbase_test2/WALsFound 3 itemsdrwxr-xr-x - hadoop supergroup 0 2014-09-10 11:33 /hbase_test2/WALs/hostname,60020,1409216059863drwxr-xr-x - hadoop supergroup 0 2014-09-10 11:33 /hbase_test2/WALs/hostname,60020,1409220327233drwxr-xr-x - hadoop supergroup 0 2014-09-10 11:33 /hbase_test2/WALs/hostname,60020,1409216096540 HBase的每个Table在根目录下面用一个文件夹来存储，文件夹的名字就是Table的名字。123456789[hadoop@hostname /home/q/hadoop/hadoop-2.3.0-cdh5.1.0]$ ./bin/hdfs dfs -ls /hbase_test2/data/defaultdrwxr-xr-x - hadoop supergroup 0 2014-09-04 10:32 /hbase_test2/data/default/TestTabledrwxr-xr-x - hadoop supergroup 0 2014-08-05 18:37 /hbase_test2/data/default/airfare_onewaydrwxr-xr-x - hadoop supergroup 0 2014-08-05 18:38 /hbase_test2/data/default/airfare_returndrwxr-xr-x - hadoop supergroup 0 2014-08-05 18:29 /hbase_test2/data/default/blacklist 在Table文件夹下面每个Region也用一个文件夹来存储，但是文件夹的名字并不是Region的名字，而是Region的名字通过Jenkins Hash计算所得到的字符串。这样做的原因是Region的名字里面可能包含了不能在HDFS里面作为路径名的字符。1234567[hadoop@hostname /home/q/hadoop/hadoop-2.3.0-cdh5.1.0]$ ./bin/hdfs dfs -ls /hbase_test2/data/default/TestTableFound 5 itemsdrwxr-xr-x - hadoop supergroup 0 2014-08-05 10:02 /hbase_test2/data/default/TestTable/.tabledescdrwxr-xr-x - hadoop supergroup 0 2014-08-05 10:02 /hbase_test2/data/default/TestTable/.tmpdrwxr-xr-x - hadoop supergroup 0 2014-09-04 10:31 /hbase_test2/data/default/TestTable/3a5ac9e6882c9204bd24f6adac4b1af8drwxr-xr-x - hadoop supergroup 0 2014-09-04 10:31 /hbase_test2/data/default/TestTable/4ce3e4160a0bba40f8784f17d38f6dcbdrwxr-xr-x - hadoop supergroup 0 2014-09-04 10:32 /hbase_test2/data/default/TestTable/736bab73a6d47458f63d084e84b5415d 在每个Region文件夹下面每个ColumnFamily也有自己的文件夹，在每个ColumnFamily文件夹下面就是一个个HFile文件了12345[hadoop@hostname /home/q/hadoop/hadoop-2.3.0-cdh5.1.0]$ ./bin/hdfs dfs -ls /hbase_test2/data/default/TestTable/3a5ac9e6882c9204bd24f6adac4b1af8Found 3 items-rwxr-xr-x 3 hadoop supergroup 68 2014-09-04 10:31 /hbase_test2/data/default/TestTable/3a5ac9e6882c9204bd24f6adac4b1af8/.regioninfodrwxr-xr-x - hadoop supergroup 0 2014-09-04 11:13 /hbase_test2/data/default/TestTable/3a5ac9e6882c9204bd24f6adac4b1af8/.tmpdrwxr-xr-x - hadoop supergroup 0 2014-09-04 11:13 /hbase_test2/data/default/TestTable/3a5ac9e6882c9204bd24f6adac4b1af8/info 所以整个文件夹结构看起来应该是这个样子的： //data/default//// 在每个Region文件夹下面你会发现一个.regioninfo文件，这个文件用来存储这个Region的Meta Data。通过这些Meta Data我们可以重建被破坏的.META.表 HFile 如上图所示，HFile这个文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。正如图中所示的，Trailer中有指针指向其他数据块的起始点。Index数据块记录了每个Data块和Meta块的起始点。Data块和Meta块都是可有可无的，但是对于大部分的HFile，你都可以看到Data块。 关于文件块的大小：默认块大小64KB，在创建表时可以通过HColumnDescriptor设定每个Family的块大小。大数据块：适合顺序查找，不适合随机查找。小数据块，适合随机查找，需要更多内存保存Data Index，创建文件慢，更多的flush操作 HFile.main()本身就提供了一个用来dump HFile的工具，类是org.apache.hadoop.hbase.io.hfile.HFile1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465[hadoop@hostname /home/q/hbase/hbase-0.98.1-cdh5.1.0]$ ./bin/hbase org.apache.hadoop.hbase.io.hfile.HFile -v -p -m -f hdfs://mycluster:8020/hbase_test2/data/default/t2/a01c6b2e3bd8b7b2d7c7da725805552b/f1/73cfd18ddfc24b29a3699df4cf7ba88c2014-09-10 16:45:18,077 INFO [main] Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available2014-09-10 16:45:18,207 INFO [main] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc322014-09-10 16:45:18,208 INFO [main] util.ChecksumType: Checksum can use org.apache.hadoop.util.PureJavaCrc32CSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:[file:/home/q/prj/mvn/repo/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class|file:/home/q/prj/mvn/repo/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]]SLF4J: Found binding in [jar:[file:/home/q/hbase/hbase-0.98.1-cdh5.1.0/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class|file:/home/q/hbase/hbase-0.98.1-cdh5.1.0/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]]SLF4J: Found binding in [jar:[file:/usr/lib/hadoop-0.20/lib/slf4j-log4j12-1.4.3.jar!/org/slf4j/impl/StaticLoggerBinder.class|file:/usr/lib/hadoop-0.20/lib/slf4j-log4j12-1.4.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]2014-09-10 16:45:18,919 INFO [main] Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFSScanning -&gt; hdfs://mycluster:8020/hbase_test2/data/default/t2/a01c6b2e3bd8b7b2d7c7da725805552b/f1/73cfd18ddfc24b29a3699df4cf7ba88c2014-09-10 16:45:19,035 INFO [main] bucket.FileIOEngine: Allocating 1 GB, on the path:/home/q/hbase/bucketcache/cache.data2014-09-10 16:45:19,042 INFO [main] bucket.BucketCache: Started bucket cache2014-09-10 16:45:19,045 INFO [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 1.4 GK: 1409554876558|row/f1:/1409624450156/Put/vlen=5/mvcc=0 V: iugsa(这一部分是存储具体数据的KeyValue对，每个数据块除了开头的Magic以外就是一个个KeyValue对拼接而成)K: 1409560567750|row/f1:/1409624421019/Put/vlen=7/mvcc=0 V: agoajgaK: 1409562897135|row/f1:/1409624602499/Put/vlen=6/mvcc=0 V: sdaojgBlock index size as per heapsize: 400reader=hdfs://mycluster:8020/hbase_test2/data/default/t2/a01c6b2e3bd8b7b2d7c7da725805552b/f1/73cfd18ddfc24b29a3699df4cf7ba88c, compression=none, cacheConf=CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheCompressed=false], firstKey=1409554876558|row/f1:/1409624450156/Put, lastKey=1409562897135|row/f1:/1409624602499/Put, avgKeyLen=31, avgValueLen=6, entries=3, length=1082Trailer:(这一部分是Tailer块的具体内容) fileinfoOffset=335, loadOnOpenDataOffset=217, dataIndexCount=1, metaIndexCount=0, totalUncomressedBytes=981, entryCount=3, compressionCodec=NONE, uncompressedDataIndexSize=44, numDataIndexLevels=1, firstDataBlockOffset=0, lastDataBlockOffset=0, comparatorClassName=org.apache.hadoop.hbase.KeyValue$KeyComparator, majorVersion=2, minorVersion=3Fileinfo:(这一部分是FileInfo块的具体内容) BLOOM_FILTER_TYPE = ROW DELETE_FAMILY_COUNT = \x00\x00\x00\x00\x00\x00\x00\x00 EARLIEST_PUT_TS = \x00\x00\x01H4)\xB2\x9B LAST_BLOOM_KEY = 1409562897135|row MAJOR_COMPACTION_KEY = \xFF MAX_SEQ_ID_KEY = 5 TIMERANGE = 1409624421019....1409624602499 hfile.AVG_KEY_LEN = 31 hfile.AVG_VALUE_LEN = 6 hfile.LASTKEY = \x00\x111409562897135|row\x02f1\x00\x00\x01H4,w\x83\x04Mid-key: \x00\x111409554876558|row\x02f1\x00\x00\x01H4*$l\x04Bloom filter: BloomSize: 8 No of Keys in bloom: 3 Max Keys for bloom: 6 Percentage filled: 50% Number of chunks: 1 Comparator: RawBytesComparatorDelete Family Bloom filter: Not presentScanned kv count -&gt; 3 org.apache.hadoop.hbase.KeyValue]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase协处理器]]></title>
    <url>%2F2015%2F02%2F20%2Fhbase%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Hbase自0.92之后开始支持Coprocessor（协处理器），旨在使用户可以将自己的代码放在regionserver上来运行，即将计算程序移动到数据所在的位置进行运算。这一点与MapReduce的思想一致。协处理器框架已经提供了一些类，用户可以通过继承这些类来扩展自己的功能，Hbase的Coprocess分为observer和endpoint两大类。简单说，observer相当于关系型数据库中的触发器，而endpoint则相当于关系型数据库中的存储过程。 observer这类协处理器与触发器类似：回调函数（也被称做钩子函数，hook）在一些特定事件发生时会被执行主要有3种接口RegionObserver：用户可以用这种的处理器处理数据修改事件，它们与表的region联系紧密MasterObserver：可以被用作管理或DDL类型的操作，这些是集群级事件WALObserver：提供控制WAL的钩子函数 编写回调函数，具体代码如附件RegionObserverEpl.java ，打包成jar123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package hbaseCoprocessor;import java.io.IOException; import java.util.List;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.KeyValue; import org.apache.hadoop.hbase.client.Get; import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.coprocessor.BaseRegionObserver; import org.apache.hadoop.hbase.coprocessor.ObserverContext; import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment; import org.apache.hadoop.hbase.util.Bytes; public class RegionObserverEpl extends BaseRegionObserver &#123; public static final byte[] FIXED_ROW = Bytes.toBytes(&quot;@@@GETTIME@@@&quot;); public static String tablename = &quot;table&quot;; public static String rowkey = &quot;rowkey&quot;; @Override public void preGet( final ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, final Get get, final List&lt;KeyValue&gt; results) throws IOException &#123; //if (Bytes.equals(get.getRow(), FIXED_ROW)) &#123; //书中原来的功能是如果查询的row为FIXED_ROW时，在结果返回系统时间 KeyValue kv = new KeyValue(get.getRow(), FIXED_ROW, FIXED_ROW, Bytes.toBytes(System.currentTimeMillis())); results.add(kv); //&#125; &#125; public static void selectRow(String tablename, String rowKey) throws IOException &#123; Configuration config = HBaseConfiguration.create(); HTable table =new HTable(config, tablename); Get g =new Get(rowKey.getBytes()); Result rs = table.get(g); for (KeyValue kv : rs.raw()) &#123; System.out.print(new String(kv.getRow()) +&quot; &quot;); System.out.print(new String(kv.getFamily()) +&quot;:&quot;); System.out.print(new String(kv.getQualifier()) +&quot; &quot;); System.out.println(new String(kv.getValue())); &#125; table.close(); &#125; public static void main(String args[])&#123; try &#123; selectRow( tablename, rowkey); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;sucess!&quot;); &#125; &#125; 运行方式1将jar放到hdfs中123[]$ ./bin/hdfs dfs -mkdir /jars[]$ ./bin/hdfs dfs -put /home/q/opdir/test.jar /jars 拷贝jar包到hbase的lib目录下 登录hbase1234567891011hbase(main):004:0&gt; disable &apos;t&apos;0 row(s) in 1.3360 secondshbase(main):005:0&gt; alter &apos;t&apos;,METHOD =&gt; &apos;table_att&apos;,&apos;coprocessor&apos;=&gt;&apos;hdfs://mycluster:8020/jars/test.jar|hbaseCoprocessor.RegionObserverEpl|1001|&apos;Updating all regions with the new schema...1/1 regions updated.Done.0 row(s) in 2.3120 secondshbase(main):006:0&gt; enable &apos;t&apos;0 row(s) in 0.4810 seconds 运行 将列值转化为uninx 时间12hbase(main):010:0&gt; Time.at(Bytes.toLong(&quot;\x00\x00\x01JQ:\x9D\xD2&quot;.to_java_bytes)/ 1000)=&gt; Tue Dec 16 11:53:23 +0800 2014 运行方式2（这个对所有表都生效）拷贝jar包到hbase的lib目录下 在hbase-site.xml增加如下配置1234&lt;property&gt;&lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;&lt;value&gt;hbaseCoprocessor.RegionObserverEpl&lt;/value&gt;&lt;/property&gt; 重启hbase]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase zabbix监控项]]></title>
    <url>%2F2015%2F02%2F20%2FHBase-zabbix%E7%9B%91%E6%8E%A7%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[jvm监控“name” : “Hadoop:service=HBase,name=JvmMetrics”12345678910111213141516171819202122232425262728293031323334GcCount JVM进行GC的次数 GcTimeMillis GC花费的时间 LogError Log中输出Error 的次数 LogFatal Log中输出Fatal的次数 LogInfo Log中输出Info的次数 LogWarn Log中输出Warn的次数 MemHeapCommittedM JVM分配的堆大小 MemHeapUsedM JVM已经使用的堆大小 MemMaxM JVM最大的可用内存大小 MemNonHeapCommittedM JVM分配给非堆的最大大小 MemNonHeapUsedM JVM已使用的非堆的大小 ThreadsBlocked 处于Blocked状态的线程数量 ThreadsNew 处于New状态的线程数量 ThreadsRunnable 处于Runnable状态的线程数量 ThreadsTerminated 处于Terminated状态的线程数量 ThreadsTimedWaiting 处于TimedWaiting状态的线程数量 ThreadsWaiting 处于Waiting状态的线程数量 regionserver监控“name” : “Hadoop:service=HBase,name=RegionServer,sub=Server”123456789101112131415161718192021222324252627282930blockCacheCount BC数量 blockCacheSize BC内存大小 blockCacheFreeSize BC空闲内存 blockCacheHitCount BC命中次数 blockCacheMissCount BC缺失次数 blockCacheEvictionCount BC换出次数 blockCountHitPercent BC命中率 regionCount RS上region数量 storeCount store数量 storeFileCount storefile数量 storeFileSize storefile大小 memStoreSize memStore大小 storeFileIndexSize Storefileindex占用内存大小 compactionQueueLength compaction队列长度 flushQueueLength flush队列长度 IPC监控“name” : “Hadoop:service=HBase,name=IPC,sub=IPC”1234ProcessCallTime_num_ops 实际处理IPC的数量 QueueCallTime_num_ops IPC在call队列中等待的数量 master监控（只针对master服务器）“name” : “Hadoop:service=HBase,name=Master,sub=Server”123456clusterRequests 集群请求总数，累加所有region服务器统计的值 numRegionServers regionserver服务器数量 numDeadRegionServers dead的regionserver服务器数量]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop配置参考]]></title>
    <url>%2F2015%2F02%2F20%2Fhadoop%E9%85%8D%E7%BD%AE%E5%8F%82%E8%80%83%2F</url>
    <content type="text"><![CDATA[hadoop内存配置修改/home/q/hadoop/hadoop-2.3.0-cdh5.1.0/etc/hadoop/hadoop-env.sh hadoop守护进程和namenode堆进程12export HADOOP_HEAPSIZE=8192export HADOOP_NAMENODE_INIT_HEAPSIZE=8192 NameNode 64G1export HADOOP_NAMENODE_OPTS=&quot;-Xms64g -Xmx64g -XX:NewSize=50g -XX:PermSize=256m -server -XX:SurvivorRatio=3 -XX:+DisableExplicitGC -XX:+UseConcMarkSweepGC -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=75 -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+PrintClassHistogram -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -Xloggc:/home/q/hadoop/hadoop-2.3.0-cdh5.1.0/logs/gc.log -Dhadoop.security.logger=ERROR,RFAS &quot; DataNode 32G1export HADOOP_DATANODE_OPTS=&quot;-Xms32g -Xmx32g -XX:NewSize=20g -XX:PermSize=256m -server -XX:SurvivorRatio=3 -XX:+DisableExplicitGC -XX:+UseConcMarkSweepGC -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=75 -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+PrintClassHistogram -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -Xloggc:/home/q/hadoop/hadoop-2.3.0-cdh5.1.0/logs/gc.log -Dhadoop.security.logger=$&#123;HADOOP_SECURITY_LOGGER:-INFO,RFAS&#125; -Dhdfs.audit.logger=$&#123;HDFS_AUDIT_LOGGER:-INFO,NullAppender&#125;&quot; core-site.xml修改/home/q/hadoop/hadoop-2.3.0-cdh5.1.0/etc/hadoop/core-site.xml12io.file.buffer.size=4194304fs.trash.interval=1440 hdfs-site.xml修改/home/q/hadoop/hadoop-2.3.0-cdh5.1.0/etc/hadoop/hdfs-site.xml12345678dfs.ha.fencing.ssh.connect-timeout=60000ipc.client.connect.timeout=60000dfs.image.transfer.bandwidthPerSec=1048576dfs.namenode.handler.count=60dfs.datanode.handler.count=40dfs.datanode.balance.bandwidthPerSec=5242880dfs.datanode.du.reserved=400000000000dfs.datanode.max.xcievers=8192]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Garbage-First(G1)收集器]]></title>
    <url>%2F2015%2F02%2F20%2FGarbage-First-G1-%E6%94%B6%E9%9B%86%E5%99%A8%2F</url>
    <content type="text"><![CDATA[垃圾回收的瓶颈传统分代垃圾回收方式，已经在一定程度上把垃圾回收给应用带来的负担降到了最小，把应用的吞吐量推到了一个极限。但是他无法解决的一个问题，就是Full GC所带来的应用暂停。在一些对实时性要求很高的应用场景下，GC暂停所带来的请求堆积和请求失败是无法接受的。这类应用可能要求请求的返回时间在几百甚至几十毫秒以内，如果分代垃圾回收方式要达到这个指标，只能把最大堆的设置限制在一个相对较小范围内，但是这样有限制了应用本身的处理能力，同样也是不可接收的。 分代垃圾回收方式确实也考虑了实时性要求而提供了并发回收器，支持最大暂停时间的设置，但是受限于分代垃圾回收的内存划分模型，其效果也不是很理想。 为了达到实时性的要求（其实Java语言最初的设计也是在嵌入式系统上的），一种新垃圾回收方式呼之欲出，它既支持短的暂停时间，又支持大的内存空间分配。可以很好的解决传统分代方式带来的问题。 增量收集的演进增量收集的方式在理论上可以解决传统分代方式带来的问题。增量收集把对堆空间划分成一系列内存块，使用时，先使用其中一部分（不会全部用完），垃圾收集时把之前用掉的部分中的存活对象再放到后面没有用的空间中，这样可以实现一直边使用边收集的效果，避免了传统分代方式整个使用完了再暂停的回收的情况。 当然，传统分代收集方式也提供了并发收集，但是他有一个很致命的地方，就是把整个堆做为一个内存块，这样一方面会造成碎片（无法压缩），另一方面他的每次收集都是对整个堆的收集，无法进行选择，在暂停时间的控制上还是很弱。而增量方式，通过内存空间的分块，恰恰可以解决上面问题。 Garbage Firest（G1）Garbage First简称G1，它的目标是要做到尽量减少GC所导致的应用暂停的时间，让应用达到准实时的效果，同时保持JVM堆空间的利用率，将作为CMS的替代者在JDK 7中闪亮登场，其最大的特色在于允许指定在某个时间段内GC所导致的应用暂停的时间最大为多少，例如在100秒内最多允许GC导致的应用暂停时间为1秒，这个特性对于准实时响应的系统而言非常的吸引人，这样就再也不用担心系统突然会暂停个两三秒了。 G1要做到这样的效果，也是有前提的，一方面是硬件环境的要求，必须是多核的CPU以及较大的内存（从规范来看，512M以上就满足条件了），另外一方面是需要接受吞吐量的稍微降低，对于实时性要求高的系统而言，这点应该是可以接受的。G1对大内存管理的效果很好]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fsck检查hadoop文件系统的一致性]]></title>
    <url>%2F2015%2F02%2F20%2Ffsck%E6%A3%80%E6%9F%A5hadoop%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122[hadoop@hostname /home/q/hadoop/hadoop-2.3.0-cdh5.1.0]$ ./bin/hdfs fsck / 14/10/23 11:17:31 WARN ssl.FileBasedKeyStoresFactory: The property &apos;ssl.client.truststore.location&apos; has not been set, no TrustStore will be loadedConnecting to namenode via http://hostname:50070FSCK started by hadoop (auth:SIMPLE) from /192.168.28.11 for path / at Thu Oct 23 11:17:32 CST 2014............................................................................................Status: HEALTHY Total size: 91509994 B (Total open files size: 99 B) Total dirs: 162 Total files: 92 Total symlinks: 0 (Files currently being written: 11) Total blocks (validated): 92 (avg. block size 994673 B) (Total open file blocks (not validated): 11) Minimally replicated blocks: 92 (100.0 %) 满足最小备份要求的块数量 Over-replicated blocks: 0 (0.0 %) Under-replicated blocks: 0 (0.0 %) 不满足备份数量要求的块叫备份不足块，HDFS会自动修复丢失的备份，用现存的可用备份生成新的备份 Mis-replicated blocks: 0 (0.0 %) Default replication factor: 3 备份因子 Average block replication: 3.0 Corrupt blocks: 0 损坏块数量，是指没有任何可用备份的块 Missing replicas: 0 (0.0 %) 丢失块的总数 Number of data-nodes: 10 datanode的数量 Number of racks: 1FSCK ended at Thu Oct 23 11:17:32 CST 2014 in 26 millisecondsThe filesystem under path &apos;/&apos; is HEALTHY]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper常用管理命令]]></title>
    <url>%2F2015%2F02%2F20%2Fzookeeper%E5%B8%B8%E7%94%A8%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[zookeeper服务命令 命令为./bin/zkServer.sh [start|start-foreground|stop|restart|status|upgrade|print-cmd]以下命令可以知道节点的角色1234[hadoop@hostname /home/q/hbase_zookeeper/zookeeper-3.4.5-cdh5.1.0]$ ./bin/zkServer.sh statusJMX enabled by defaultUsing config: /home/q/hbase_zookeeper/zookeeper-3.4.5-cdh5.1.0/bin/../conf/zoo.cfgMode: leader 命令行工具1、ls 使用 ls 命令来查看当前 ZooKeeper 中所包含的内容[zk: localhost:2181(CONNECTED) 0] ls / [zookeeper]2、ls2 查看当前节点数据并能看到更新次数等数据12345678910111213[zk: localhost:2181(CONNECTED) 4] ls2 / [zookeeper]cZxid = 0x0ctime = Thu Jan 01 07:00:00 CST 1970mZxid = 0x0mtime = Thu Jan 01 07:00:00 CST 1970pZxid = 0x0cversion = -1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 0numChildren = 1 3、create 创建一个新的 znode节点“/pucong ”以及与它关联的字符串12[zk: localhost:2181(CONNECTED) 6] create /pucong mydataCreated /pucong 创建临时节点12[zk: localhost:2181(CONNECTED) 1] create -e /pucong/temp testdataCreated /pucong/temp 4、get 获取znode节点文件内容12345678910111213[zk: localhost:2181(CONNECTED) 10] get /pucongmydatacZxid = 0x100000008ctime = Wed Oct 15 11:20:47 CST 2014mZxid = 0x100000008mtime = Wed Oct 15 11:20:47 CST 2014pZxid = 0x100000008cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 6numChildren = 0 5、set 修改znode节点关联的数据123456789101112[zk: localhost:2181(CONNECTED) 16] set /pucong testdatacZxid = 0x100000008ctime = Wed Oct 15 11:20:47 CST 2014mZxid = 0x100000009mtime = Wed Oct 15 11:27:55 CST 2014pZxid = 0x100000008cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 10numChildren = 0 6、delete 删除某个znode节点 [zk: localhost:2181(CONNECTED) 23] delete /pucong/test1 假如有子目录，需要使用rmr来进行递归删除 [zk: localhost:2181(CONNECTED) 27] rmr /pucong 常用四字命令123456789101112echo stat|nc 127.0.0.1 2181 来查看哪个节点被选择作为follower或者leaderecho ruok|nc 127.0.0.1 2181 测试是否启动了该Server，若回复imok表示已经启动echo dump| nc 127.0.0.1 2181 ,列出未经处理的会话和临时节点echo kill | nc 127.0.0.1 2181 ,关掉serverecho conf | nc 127.0.0.1 2181 ,输出相关服务配置的详细信息echo cons | nc 127.0.0.1 2181 ,列出所有连接到服务器的客户端的完全的连接 / 会话的详细信息echo envi |nc 127.0.0.1 2181 ,输出关于服务环境的详细信息（区别于 conf 命令）echo wchs | nc 127.0.0.1 2181 ,列出服务器 watch 的详细信息echo wchc | nc 127.0.0.1 2181 ,通过 session 列出服务器 watch 的详细信息，它的输出是一个与 watch 相关的会话的列表。echo wchp | nc 127.0.0.1 2181 ,通过路径列出服务器 watch 的详细信息。它输出一个与 session 相关的路径echo mntr | nc 127.0.0.1 2181，这个命令可以列出变量列表，这些变量可以用作监控集群的健康状态echo srvr | nc 127.0.0.1 2181，获取服务器的详细信息]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper原理]]></title>
    <url>%2F2015%2F02%2F20%2Fzookeeper%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[zookeeper数据模型zookeeper拥有一个层次的命名空间，命名空间中的每个节点可以和它自身或它的子节点相关联的数据，这就好像一个文件系统，只不过文件系统中的文件还可以具有目录功能。 Znodezookeeper目录树中每一个节点对应着一个Znode。每个Znode维护着一个属性结构，包含数据的版本号(dataVersion)、时间戳(ctime、mtime)等状态信息。 Znode是客户端访问的zookeeper的主要实体，包含以下几个主要特征： (1)watchs 客户端可以在节点上设置watch（可以称之为监视器）。当节点状态发生改变时将会触发watch对应的操作。当watch被触发时，zookeeper将会向客户端发送且仅发送一个通知，因为watch只能被触发一次。 (2)数据访问 zookeeper中每个节点上存储的数据需要被原子性的操作，也就是说读操作将获取与节点相关的所有数据，写操作也将替换掉节点的所有数据。 (3)临时节点 zookeeper中节点有两种，分别为临时节点和永久节点。节点的类型在创建时即被确定，并且不能改变。zookeeper临时节点的生命周期依赖于创建它们的会话，一旦会话结束，临时节点将会被自动删除，也可以手动删除，临时节点不允许拥有子节点。永久节点的生命周期不依赖于会话，并且只有在客户端显示执行删除操作的时候，它们才会被删除。 (4)顺序节点（唯一性保证） 当创建Znode的时候，用户可以请求在zookeeper的路径结尾添加一个递增的计数。这个计数对于此节点的父节点来说是唯一的，它的格式为 zookeeper中的时间zookeeper中有多种记录时间的方式(1)Zxid对每个节点的每一个操作都将使节点接收到一个zxid格式的时间戳，并且这个时间戳是全局有效的，也就是说每一个对节点的改变都将产生一个唯一的zxid。zookeeper的每个节点都维护着3个zxid值，分别为:cZxid(创建时间戳)、mZxid(修改时间戳)、pZxid12345678910111213[zk: localhost:2181(CONNECTED) 3] ls2 /pucong[a1]cZxid = 0x100000014ctime = Wed Oct 15 12:19:38 CST 2014mZxid = 0x100000017mtime = Wed Oct 15 12:34:58 CST 2014pZxid = 0x100000031cversion = 11dataVersion = 2aclVersion = 0ephemeralOwner = 0x0dataLength = 7numChildren = 1 (2)版本号对每个节点的每一个操作都将使得这个节点的版本号增加，每个节点维护着3个版本号，它们分别为dataVersion(节点数据版本号)、cversion(子节点版本号)、aclVersion(节点拥有的ACL的版本号)12345678910111213[zk: localhost:2181(CONNECTED) 2] ls2 /pucong[a1]cZxid = 0x100000014ctime = Wed Oct 15 12:19:38 CST 2014mZxid = 0x100000017mtime = Wed Oct 15 12:34:58 CST 2014pZxid = 0x100000031cversion = 11dataVersion = 2aclVersion = 0ephemeralOwner = 0x0dataLength = 7numChildren = 1 zookeeper watchszookeeper可以为所有的读操作设置watch，这些读操作包括:exists()、getChildren()、getData()。watch事件是一次性的触发器，当watch的对象状态发生改变时，将会触发此对象上所设置的watch对应的事件。 zookeeper所管理的watch可以分为2类，一类是数据watch(data watchs)，一类是子watch(child watchs)。 getData()和exists()负责设置数据watch，getChildren()负责孩子watch。因此，setData()将触发设置了数据watch的对应事件，一个成功的create()操作将触发Znode的数据watch以及孩子watch。一个成功的delete()操作将触发数据watch和孩子watch，因为Znode被删除的时候，它的child watch也将被删除。]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase过滤器]]></title>
    <url>%2F2015%2F02%2F20%2Fhbase%E8%BF%87%E6%BB%A4%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1、值过滤器(ValueFilter) 谁的值=bbbb hbase(main):008:0&gt; scan ‘t2’,FILTER=&gt;”ValueFilter(=,’binary:bbbb’)” 谁的值包含bbb hbase(main):010:0&gt; scan ‘t2’,FILTER=&gt;”ValueFilter(=,’substring:bbb’)” 2、列过滤器(ColumnPrefixFilter) column为b值包含ac hbase(main):017:0&gt; scan ‘t2’,FILTER=&gt;”ColumnPrefixFilter(‘b’) AND ValueFilter(=,’substring:ac’)” column为b值包含aa或cchbase(main):003:0&gt;scan ‘t2’,FILTER=&gt;”ColumnPrefixFilter(‘b’) AND (ValueFilter(=,’substring:aa’) OR ValueFilter(=,’substring:cc’))” 3、行过滤器 row key以cn开头 hbase(main):014:0&gt; scan ‘t2’,FILTER=&gt;”PrefixFilter(‘cn’)” 从user1|ts2开始,找到所有的rowkey以user1开头的 hbase(main):025:0&gt; scan ‘t2’,STARTROW=&gt;’user1|ts3’,FILTER=&gt;”PrefixFilter(‘user1’)” 从user1|ts2开始，找到所有rowkey以user2|ts4结束的 hbase(main):031:0&gt; scan ‘t2’,STARTROW=&gt;’user1|ts3’,STOPROW=&gt;’user2|ts4’ 查询rowkey中包含ts的 import org.apache.hadoop.hbase.filter.CompareFilterimport org.apache.hadoop.hbase.filter.SubstringComparatorimport org.apache.hadoop.hbase.filter.RowFilterhbase(main):039:0&gt; scan ‘t2’,FILTER=&gt;RowFilter.new(CompareFilter::CompareOp.valueOf(‘EQUAL’),SubstringComparator.new(‘ts’)) 查询rowkey里面以user开头的，符合正则表达式的结果 import org.apache.hadoop.hbase.filter.RegexStringComparatorimport org.apache.hadoop.hbase.filter.CompareFilterimport org.apache.hadoop.hbase.filter.SubstringComparatorimport org.apache.hadoop.hbase.filter.RowFilterhbase(main):010:0&gt; scan ‘t2’,FILTER=&gt;RowFilter.new(CompareFilter::CompareOp.valueOf(‘EQUAL’),RegexStringComparator.new(‘^user\d+|ts\d+$’)) 4、时间戳过滤器 hbase(main):014:0&gt; scan ‘t2’,FILTER=&gt;”TimestampsFilter(1409559455702,1409562905630,1407982676242)”]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查看hfile的内容]]></title>
    <url>%2F2015%2F02%2F20%2F%E6%9F%A5%E7%9C%8Bhfile%E7%9A%84%E5%86%85%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[12345678910111213关键字： HFile Tool 采用org.apache.hadoop.hbase.io.hfile.HFile工具 命令：$&#123;HBASE_HOME&#125;/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile -v -f hdfs://10.81.47.41:8020/hbase/TEST/1418428042/DSMP/4759508618286845475 格式：类名 -v -f 文件位置和名称 备注：如果去掉-v 则将会只看到一个统计信息]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper服务对hbase的影响]]></title>
    <url>%2F2015%2F02%2F20%2Fzookeeper%E6%9C%8D%E5%8A%A1%E5%AF%B9hbase%E7%9A%84%E5%BD%B1%E5%93%8D%2F</url>
    <content type="text"><![CDATA[在hbase运行期间，删除所有的znode，hbase会自动停掉所有进程，服务停止。 但是这些znode信息可以通过重启hbase来恢复。 所以假如误删了znode，hbase无法提供服务的时候，立马重启hbase即可解决问题。]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS权限配置]]></title>
    <url>%2F2015%2F02%2F20%2FHDFS%E6%9D%83%E9%99%90%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1.开启权限配置修改hdfs-site.xml配置文件，添加：1234&lt;property&gt;&lt;name&gt;dfs.namenode.acls.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 重启hdfs 2.查看目录权限1bin/hdfs dfs -getfacl [-R] /path [-R]可选 3.设置权限12345678910111213hdfs dfs -setfacl [-R] [-b|-k -m|-x &lt;acl_spec&gt; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;] &lt;!-- COMMAND OPTIONS&lt;path&gt;: Path to the file or directory for which ACLs should be set.-R: Use this option to recursively list ACLs for all files and directories.-b: Revoke all permissions except the base ACLs for user, groups and others.-k: Remove the default ACL.-m: Add new permissions to the ACL with this option. Does not affect existing permissions.-x: Remove only the ACL specified.&lt;acl_spec&gt;: Comma-separated list of ACL permissions.--set: Use this option to completely replace the existing ACL for the path specified. Previous ACL entries will no longer apply.--&gt; 例子： 在指定路径下，给用户ben赋予rw权限1hdfs dfs -setfacl -m user:ben:rw- /user/hdfs/file 取消用户alice权限1hdfs dfs -setfacl -x user:alice /user/hdfs/file 给用户hadoop赋予rw权限，给目录所属组，以及其他用户赋予r只读权限1hdfs dfs -setfacl --set user::rw-,user:hadoop:rw-,group::r--,other::r-- /user/hdfs/file 注：mask的作用是做一层过滤，用户真正的权限是：用户显示的权限 与 mask权限的并集。 mask权限可以由chmod来进行修改。默认值随着setfacl指令而扩展。]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase权限控制]]></title>
    <url>%2F2015%2F02%2F20%2FHBase%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[周末线上HBase集群全部机器报警，紧急上线重启，然后查找原因，通过log看到是人为shutdown， 这是因为线上HBase应用服务器都配置了hbase-site.xml，直接就能连到线上进行操作。这对我们 运维人员来说是个漏洞，需要立刻补上这个漏洞。 相比较MySQL等关系型数据库，HBase的权限控制是在新版本中才加入的，也不如关系型数据库 的权限控制细致，但是足以满足我们的需求。 HBase权限列表Read (R) - can read data at the given scopeWrite (W) - can write data at the given scopeExecute (X) - can execute coprocessor endpoints at the given scopeCreate (C) - can create tables or drop tables (even those they did not create) at the given scopeAdmin (A) - can perform cluster operations such as balancing the cluster or assigning regions at the given scopeHBase权限范围HBase权限范围包括Super，Global，Namespace，Table，Cell。对于我们运维来说，DBA用户拥有Super权限， 应用拥有读写权限即可 开启HBase需要修改hbase-site.xml，添加如下配置：12345678910111213141516171819&lt;property&gt;&lt;name&gt;hbase.superuser&lt;/name&gt;&lt;value&gt;****&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt; &lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.rpc.engine&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hbase.ipc.SecureRpcEngine&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.security.authorization&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 修改完配置文件后，需要重启整个集群 授权我们主要针对用户进行授权：1grant &apos;username&apos;, &apos;RW&apos; 参考资料http://www.cloudera.com/content/cloudera/en/documentation/core/v5-2-x/topics/cdh_sg_hbase_authorization.htmlhttps://issues.apache.org/jira/browse/HBASE-8409]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BlockCache]]></title>
    <url>%2F2015%2F02%2F20%2FBlockCache%2F</url>
    <content type="text"><![CDATA[默认配置HBase使用single on-heap cache(堆内)。 如果配置了off-heap BucketCache，则on-heap缓存将用于布隆过滤器和索引，而off-heap BucketCache则用于缓存数据块,称为Combined Blockcache。 Combined BlockCache允许您使用更大的内存缓存，同时减少垃圾回收在堆中的负面影响，因为HBase管理BucketCache时不需要依赖于垃圾回收器。 为了解决LRUBlockCache方案中因为JVM垃圾回收导致的服务中断，SlabCache方案使用Java NIO DirectByteBuffer技术实现了堆外内存存储，不再由JVM管理数据内存。 堆外内存使用率低。 BlockCache推荐适用LRUBLockCache+BucketCache]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase GC原理和优化]]></title>
    <url>%2F2015%2F02%2F20%2FHBase-GC%E5%8E%9F%E7%90%86%E5%92%8C%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[学习来源：HBase官网和http://hbasefly.com/ Java GC GC的出现是来源于在内存管理时大多数内存对象要么生存周期特别短，可以快速进行内存回收。要么生存周期比较长。 JVM将整个堆内存分为两部分：新生代(young generation)和老生代(tenured generation),同时对于常驻内存的对象JVM设有一个非堆内存区-Perm区(主要是存放静态的类信息和方法信息),class信息和meta元数据信息就存放在此区。 其中Young区为了方便垃圾回收细分为Eden区和两个Survivor区:S0和S1。因此JVM内存结构如下: 一个内存对象创建后首先会进入新生代，如果内存对象存活时间很长后会迁移到老生代。 在大多数对延迟敏感的业务场景下（比如HBase），建议使用如下JVM参数，-XX:+UseParNewGC和XX:+UseConcMarkSweepGC，其中前者表示对新生代执行并行的垃圾回收机制，而后者表示对老生代执行并行标记－清除垃圾回收机制。可见，JVM允许针对不同内存区执行不同的GC策略。 GC信息查看jstat -gcutil 3600 S0 S1 E O P YGC YGCT FGC FGCT GCT 0.00 53.12 28.15 93.10 99.17 455569 3210.991 3470 196.670 3407.661 S0 — Heap上的 Survivor space 0 区已使用空间的百分比S1 — Heap上的 Survivor space 1 区已使用空间的百分比E — Heap上的 Eden space 区已使用空间的百分比O — Heap上的 Old space 区已使用空间的百分比P — Perm space 区已使用空间的百分比YGC — 从应用程序启动到采样时发生 Young GC 的次数YGCT– 从应用程序启动到采样时 Young GC 所用的时间(单位秒)FGC — 从应用程序启动到采样时发生 Full GC 的次数FGCT– 从应用程序启动到采样时 Full GC 所用的时间(单位秒)GCT — 从应用程序启动到采样时用于垃圾回收的总时间(单位秒) 如果gc没有问题，继续查看thriftserver的连接数是否异常,如果连接数打满也会有问题:netstat -apn|grep 15046|grep 9090|wc -l 新生代GC策略-Parallel New Collector新生代为了方便进行内存回收，分为S0、S1、Eden三部分。新建的内存对象是存放在Eden区，当Eden区满后会触发一次小GC(Minor GC)：GC算法会检查所有对象的引用情况，如果对象还被引用则表示存活并进行标记，检查完Eden区后会将标记为存活的对象迁移到S0区，然后回收整个Eden区。 当过段时间Eden区再次满之后，GC算法会检查Eden区和S0区存活的对象并将所有存活的对象迁移到S1区，然后回收整个Eden区和S0区的内存空间。因此S0、S1总会有一个区是空闲预留给下一次存放迁移的存活对象的。 新生代GC算法为复制算法，一般情况影响会很小。需要注意的是:1.算法会执行 stop-the-world暂停，但是时间非常短。因为Young区通常会设置的比较小(不建议超过512M)，同时JVM会启动大量线程并发执行，一次Minor GC通常会在毫秒级完成。2.新生代GC不会出现内存碎片，每次GC之后都会将存活对象迁移到整个空闲的S0或S1空间,然后回收整个非存活对象存储的区(Eden+ S1/S0) 新生代的对象什么时候迁移到老年代？ 内存中所有对象都会维护一个计数器，每次Minor GC迁移一个内存对象后，该内存对象所属的计数器＋1，当内存对象的计数达到一定阀值则会将该内存对象迁移到老生代。阀值通过JVM参数XX:MaxTenuringThreshold指定。 老生代GC策略-Concurrent Mark-Sweep每次执行Minor GC之后都会有部分生命周期长的对象迁移到老生代，久而久之老生代空间也会占满，同样此时需要堆老生代进行GC操作。常见的算法有CMS、G1,先介绍CMS(Concurrent Mark-Sweep)算法。 CMS算法整个流程分为6个阶段，其中部分阶段会执行 ‘stop-the-world’ 暂停，部分阶段会和应用线程一起并发执行： initial-mark：这个阶段虚拟机会暂停所有正在执行的任务。这一过程虚拟机会标记所有 ‘根对象’(ROOT)， 所谓‘根对象’，一般是指一个运行线程直接引用到的对象。虽然会暂停整个JVM，但因为’根对象’相对较少，这个过程通常很快。 concurrent mark：垃圾回收器会从‘根节点’开始，将所有引用到的对象都打上标记。 这个阶段应用程序的线程和标记线程并发执行，因此用户并不会感到停顿。 concurrent precleaning：并发预清理阶段仍然是并发的。在这个阶段，虚拟机查找在执行mark阶段新进入老年代的对象(可能会有一些对象从新生代晋升到老年代， 或者有一些对象被分配到老年代)。 remark：在阶段3的基础上对查找到的对象进行重新标记，这一阶段会暂停整个JVM，但是因为阶段3已经欲检查出了所有新进入的对象，因此这个过程也会很快。 concurrent sweep：上述3阶段完成了引用对象的标记，此阶段会将所有没有标记的对象作为垃圾回收掉。这个阶段应用程序的线程和标记线程并发执行。 concurrent reset：重置CMS收集器的数据结构，等待下一次垃圾回收。 相应的，对于CMS算法，也需要关注两点： ‘stop－the－world’暂停时间也很短暂，耗时较长的标记和清理都是并发执行的。 CMS算法在标记清理之后并没有重新压缩分配存活对象，因此整个老生代会产生很多的内存碎片。 CMS算法缺点理想情况下CMS的stop-thr-world时间会很短，ms级别。但是真实业务场景通常并非如此，在读写压力很大的情况下会出现长时间的卡顿，甚至严重阻塞读写。HBase相关会导致RS和ZK超时，RS异常离线后shutdown。 CMS在两种场景下会出现严重的Full GC: Concurrent Failure &amp;&amp; Promotion Failure。 Concurrent Failure根据CMS GC算法，当老生代正在进行GC时，由于读写压力特别大导致新生代一批内存对象需要进入老生代，老生代由于还没有GC完成导致没有足够的空间容纳需要新迁移的新生代对象。这种场景下CMS回收器会stop-the-world，并且回收算法退化到单线程复制算法，重新分配整个堆内存的存活对象到S0中并释放所有其他空间。整个过程会持续很长时间，影响较大。 JVM提供参数XX:CMSInitiatingOccupancyFraction=N来设置CMS回收的时机，其中N表示当前老生代已使用内存占总内存的比例，该值默认为68，可以将该值修改的更小使得回收更早进行。 CMS回收器更早回收就可以避免该类FULL GC的发生。 Promotion Failure 假设此时设置XX:CMSInitiatingOccupancyFraction＝60，但是在已使用内存还没有达到总内存60%的时候，已经没有空间容纳从新生代迁移的对象了。罪魁祸首就是内存碎片，上文中提到CMS算法会产生大量碎片，当碎片容量积累到一定大小之后就会造成上面的场景。这种场景下，CMS回收器一样会停止工作，进入漫长的 ’stop-the-world’ 模式。 JVM也提供了参数 -XX: UseCMSCompactAtFullCollection来减少碎片的产生，这个参数表示会在每次CMS回收垃圾之后执行一次碎片整理，很显然，这个参数会对性能有比较大的影响，对HBase这种对延迟敏感的业务来说并不是一个完美解决方案。 注意：CMS GC会不断产生内存碎片，当碎片小到一定程度之后就会基本维持不变，如果此时业务写入一些单条数据量很大的KeyValue，就有可能触发Promotion Failure模式Full GC。在实际线上环境中，很少出现Concurrent Failure模式的Full GC，大多数Full GC场景都是Promotion Failure。 CMS GC调优GC优化的思想：避免长时间Full GC 和 减少GC时间，这样可以避免影响用户的读写请求和提高读写性能。 CMS默认JVM参数常见的JVM推荐配置参数模版: 123-Xmx -Xms -Xmn -Xss -XX:MaxPermSize= M -XX:SurvivorRatio=S -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:MaxTenuringThreshold=N -XX:+UseCMSCompactAtFullCollection -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=C -XX:-DisableExplicitGC 1234567891011121314Xmx 分配给JVM的最大堆内存Xms 分给给JVM的初始化内存，一般和Xmx设置相同Xmn 分配给Young区的内存大小，建议不要设置过大Xss 分配给每个线程的堆栈大小，建议设置1M左右。MaxPermSize 分配给持久代的内存大小SurvivorRatio 表示Young区中Eden区和Survivor区的内存大小比例，默认为8，该值设置对系统影响很大。UseConcMarkSweepGC 表示回收器使用CMS GC策略UseParNewGC 表示Young区采用并行回收机制，推荐使用CMSParallelRemarkEnabled 表示CMS的Remark阶段采用并行发誓，推荐使用MaxTenuringThreshold 表示Young区对象晋升到Tenured区的阀值，该值的设置对系统影响很大。UseCMSCompactAtFullCollection 表示每次执行完CMS GC之后执行一次碎片合并，推荐使用UseCMSInitiatingOccupancyOnly 表示CMS GC只基于参数CMSInitiatingOccupancyFraction触发CMSInitiatingOccupancyFraction 表示当Tenured区内存使用量超过Tenured总大小的百分比超过该阀值之后会触发CMS GC，建议设置为70%~80%DisableExplicitGC 表示禁止使用命令System.gc(),该命令用于触发整个JVM的垃圾回收，禁用。 通过上文对各个GC参数的说明，可以轻松得出第一阶段推荐的参数设置如下，这样的设置基本适用于所有的场景：12-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=75% -XX:-DisableExplicitGC CMS GC JVM参数调优主要的参数在于 Xmn(Young区的大小)、SurvivorRatio(Eden区和Survivor区内存大小比例)。 Xmn -Young区大小设置JVM调优跟应用的内存对象类型紧密相关，需要区分应用的内存对象是短生命周期居多还是长生命周期居多。HBase应用分析,主要的内存对象有三类：RPC请求、Memstore对象、BlockCache对象。 RPC请求对象:生命周期短，Request对象和Reponse对象在ms～s级别。 Memstore对象:生命周期长，持续到整个Memstore flush到HDFS生成HFile。通常内存对象为2M。 BlockCache对象: 读缓存，生命周期长。 因此HBase应用是生命周期长的内存对象较多，GC时避免大对象进行GC，也就是减少老生代的GC次数。而 新生代的大小直接决定了老生代GC的速度。具体情况就是： Young区大小决定了minor gc的频率，minor gc频率决定了对象晋升老年代对象的速度(计数器到达即可),从而老年代GC的评率。 Young区设置过大，则minor gc频率降低，单次gc时间会比较长从而stop-the-world时间较长，业务读写操作延迟大。 Young区设置过小，则minor gc频率增大，晋升老年代对象的速度加快，增加了老年代GC的评率和风险。 因此Young区大小设置要合理，在大内存场景RS配置64G内存时 Young区大小建议设置为1~3g：即-Xmn2g。 SurvivorRatio(Eden区和Survivor区内存大小比例) 每次Minor GC都会将存活对象从Eden区复制到Survivor区，因此增大Survivor区可以容纳更多的存活对象。Survivor区的合理设置可以避免因为Survivor区太小导致存活对象没有达到MaxTenuringThreshold阀值就直接进入老生代，减少老生代GC的评率。但是如果设置过大会导致存活对象来回的在Young区进行复制，增加了Minor GC的开销。 SurvivorRatio越大，Survivor区大小越小，建议设置为2。即新生代中Eden区20%，S区80%。 MaxTenuringThreshold阀值不能设置过小，该值直接决定了新生代存活对象进入老生代的概率.默认设置为15即可。 CMS GC 调优结论 缓存模式采用BucketCache策略Offheap模式 对于大内存（大于64G），采用如下配置：123-Xmx64g -Xms64g -Xmn2g -Xss256k -XX:MaxPermSize=256m -XX:SurvivorRatio=2 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:MaxTenuringThreshold=15 -XX:+UseCMSCompactAtFullCollection -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=75 -XX:-DisableExplicitGC 其中Xmn可以随着Java分配堆内存增大而适度增大，但是不能大于4g，取值范围在1~3g范围；SurvivorRatio一般建议选择为2；MaxTenuringThreshold默认设置15即可； 对于小内存（小于64G），只需要将上述配置中Xmn改为512m-1g即可不同于CMS等其他的分代回收算法、收集的范围都是整个新生代或者老年代，而G1不再是这样。G1将堆空间划分成了互相独立的heap区块（Region）。每块区域既有可能属于O区、也有可能是Y区，且每类区域空间可以是不连续的（对比CMS的O区和Y区都必须是连续的）。G1在全局标记阶段(global marking phase)并发执行, 以确定堆内存中哪些对象是存活的。标记阶段完成后,G1就可以知道哪些heap区的empty空间最大。它会首先回收这些区,通常会得到大量的自由空间. 这也是为什么这种垃圾收集方法叫做Garbage-First(垃圾优先)的原因。 现有参数 123456789101112131415-Xms64g -Xmx64g -XX:PermSize=256m -XX:MaxPermSize=256m -XX:+UseG1GC -server -XX:+DisableExplicitGC -XX:+UseFastAccessorMethods -XX:SoftRefLRUPolicyMSPerMB=0 -XX:G1ReservePercent=15 -XX:InitiatingHeapOccupancyPercent=40 -XX:ConcGCThreads=18 -XX:+ParallelRefProcEnabled -XX:-ResizePLAB -XX:ParallelGCThreads=18 优化 1234567891011121314151617181920-Xms64g -Xmx64g -XX:PermSize=256m -XX:MaxPermSize=256m -XX:+UseG1GC -server -XX:+DisableExplicitGC -XX:+UseFastAccessorMethods -XX:SoftRefLRUPolicyMSPerMB=0 -XX:G1ReservePercent=15 -XX:InitiatingHeapOccupancyPercent=50 -XX:ConcGCThreads=6 -XX:+ParallelRefProcEnabled -XX:-ResizePLAB -XX:ParallelGCThreads=18-XX:+UnlockExperimentalVMOptions-XX:MaxGCPauseMillis=90-XX:G1NewSizePercent=5-XX:MaxTenuringThreshold=1-XX:G1HeapRegionSize=32m 参考：http://blog.csdn.net/u013980127/article/details/53913994http://blog.csdn.net/renfufei/article/details/41897113http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/G1GettingStarted/index.htmlhttp://www.oracle.com/technetwork/articles/java/g1gc-1984535.htmlhttp://ifeve.com/深入理解g1垃圾收集器/https://blogs.apache.org/hbase/entry/tuning_g1gc_for_your_hbasehttp://hbase-help.com/?/question/19http://www.cnblogs.com/ityouknow/p/5614961.htmlhttps://cantellow.iteye.com/blog/1472072]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase HBCK]]></title>
    <url>%2F2015%2F02%2F20%2FHBase-HBCK%2F</url>
    <content type="text"><![CDATA[hbase hbck检查和修复数据一致性。 HBase提供了hbck命令来检查各种不一致问题。检查hbase集群数据一致性1[hadoop@hostname /home/q/hbase/hbase-0.98.1-cdh5.1.0]$ ./bin/hbase hbck Status: OK表示集群是一致的Status：INCONSISTENT表示集是不一致的 修复区域分配1[hadoop@hostname /home/q/pucong1/hbase]$ ./bin/hbase hbck -fixAssignments 修复meta表问题1[hadoop@hostname /home/q/pucong1/hbase]$ ./bin/hbase hbck -fixMeta 假如hdfs的数据空洞已经删除了，需要以下命令来进行修复 [hadoop@hostname /home/q/pucong1/hbase]$ ./bin/hbase hbck -fixAssignments -fixMeta -fixHdfsHoles 或者使用 ./bin/hbase hbck -repairHoles Shortcut for -fixAssignments -fixMeta -fixHdfsHoles]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Non DFS Used计算]]></title>
    <url>%2F2015%2F02%2F20%2FHBase-Non-DFS-Used%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[100G的磁盘，dfs.datanode.du.reserved配置是30G，系统文件和其它一些文件一共40G，dfs使用了10个G，那么NonDFSUsed使用了多少？ Non DFS used = 100GB(Total) - 30 GB( Reserved) - 10 GB (DFS used) - 50GB(Remaining) = 10 GB。 在 Linux ext2/ext3/ext4 文件系统上通常默认预留5％的硬盘空间，如果硬盘是 4TB 的话就意味着有 200GB 的空间就这样浪费了，我们可以通过 tune2fs 来改变5％的默认设置，比如只预留1％的空间。xfs文件系统不存在预留的问题。 du查看的文件大小是和df -h查看的used匹配的，used包含了 Non DFS Used + Used 。 如果有磁盘使用率监控报警，可以适当调整预留的百分比。 调整为1% : for i in df -h | grep /data | awk &#39;{ print $1 }&#39;; do echo tune2fs -m 1 $i ;done http://blog.csdn.net/levy_cui/article/details/53199360 http://www.vpsee.com/2012/09/release-linux-system-disk-space-using-tune2fs/ 调整预留值 http://blog.csdn.net/u014297175/article/details/48679321 http://stackoverflow.com/questions/18477983/what-exactly-non-dfs-used-means]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase备份恢复实战篇]]></title>
    <url>%2F2015%2F02%2F20%2FHBase%E5%A4%87%E4%BB%BD%E6%81%A2%E5%A4%8D%E5%AE%9E%E6%88%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[模拟线上需求：数据不断有写入，需要搭建一个该表的slave供离线分析。12create &apos;testtable&apos;, &#123;NAME =&gt; &apos;c1&apos;&#125;for i in &apos;1&apos;..&apos;10000&apos; do for j in &apos;1&apos;..&apos;10000&apos; do for k in &apos;0&apos;..&apos;9&apos; do put &apos;testtable&apos;,&quot;row#&#123;i&#125;&quot;,&quot;c1:#&#123;j&#125;&#123;k&#125;&quot;,&quot;#&#123;j&#125;#&#123;k&#125;&quot; end end end 数据量 10000* 10000,保证执行的时间足够长。 步骤: 使用snapshotExport将线上的数据全量导入新集群 记录时间点T1. 设置replication，来同步之后的数据。 记录时间点T2. 使用copyTable将 &lt;T1,T2&gt;之间的数据同步。 (startTime可以记录为T1之前的值) 演练： 事先配置yarn并hbase-site.xml 开启复制参数和快照,滚动重启RS生效配置。12345678&lt;property&gt; &lt;name&gt;hbase.security.authorization&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.snapshot.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 步骤1 ExportSnapshot源集群： 创建snapshot123hbase(main):003:0&gt; snapshot &apos;testtable&apos;,&apos;testtable_snapshot&apos;0 row(s) in 0.6990 secondshbase(main):004:0&gt; exit 执行ExportSnapshot12[hadoop@hostname /home/q/hbase/q_hbase/bin]$ ./hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot &apos;testtable_snapshot&apos; -copy-to hdfs://hostname:8020/hbase/haas_test -mappers 16 -bandwidth 200 执行list_snapshots查看时间,记录为T1date -d ‘2017-03-17 17:24:04’ +%s 方式转化为时间戳。 1[hadoop@hostname /home/q/hbase/q_hbase/bin]$ date -d &apos;2017-03-21 15:00:58&apos; +%s 1490079658目的端hostname 进行表恢复。12hbase(main):001:0&gt; restore_snapshot &apos;testtable_snapshot&apos;0 row(s) in 1.8920 seconds hbase(main):002:0&gt; list_snapshots步骤2 Replication 源端创建复制通道并开启复制.12345678910 hbase(main):026:0&gt; disable &apos;testtable&apos;hbase(main):028:0* alter &apos;testtable&apos;, &#123;NAME =&gt; &apos;c1&apos;,REPLICATION_SCOPE =&gt; &apos;1&apos;&#125;hbase(main):029:0&gt; enable &apos;testtable&apos;hbase(main):032:0&gt;add_peer &apos;2&apos;,&quot;hostname:2181:/hbase/haas_test&quot;0 row(s) in 0.0120 secondshbase(main):033:0&gt; set_peer_tableCFs &apos;2&apos;,&apos;testtable&apos; 开启复制0 row(s) in 0.0060 seconds 开启复制后，记录时间为T2. 1234[hadoop@hostname /home/q/hbase/q_hbase/bin]$ dateTue Mar 21 15:07:42 CST 2017[hadoop@hostname /home/q/hbase/q_hbase/bin]$ date -d &apos;2017-03-21 15:07:42&apos; +%s1490080062 步骤3 CopyTable，拷贝&lt;T1,T2&gt;时间段内的变量数据。123[hadoop@hostname /home/q/hbase/q_hbase/bin]$ ./hbase org.apache.hadoop.hbase.mapreduce.CopyTable --peer.adr=hostname:2181:/hbase/haas_test --starttime=1489718120 --endtime=1489718187 --new.name=testtable testtable 完成配置，等待脚本执行完成验证源端和目的端数据是否一致。两边执行count ，行数一致。123hbase(main):006:0&gt; count &apos;testtable&apos;23 row(s) in 0.0360 seconds=&gt; 23 问题问题1：复制不会同步ddl,开发新建表如果需要复制则该表都得重新搭建复制。源端执行truncate12345678910hbase(main):003:0&gt; truncate_preserve &apos;testtable&apos;Truncating &apos;testtable&apos; table (it may take a while): - Disabling table... - Dropping table... - Creating table with region boundaries...0 row(s) in 2.4070 secondshbase(main):004:0&gt; scan &apos;testtable&apos;ROW COLUMN+CELL0 row(s) in 0.0590 seconds 目的端scan数据依旧存在 12 row1 column=c1:9999&#123;k&#125;, timestamp=1490081458186, value=999991001 row(s) in 6.2230 seconds]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP网卡绑定的问题]]></title>
    <url>%2F2015%2F02%2F20%2FTCP%E7%BD%91%E5%8D%A1%E7%BB%91%E5%AE%9A%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[现象haas中regionserver频繁报警killed、running。分析hbase regionserver日志发现异常： 12345678910111213141516171819202122232425262728293031322017-08-29 11:14:33,703 WARN [regionserver/hostname/172.42.0.1:60023] regionserver.HRegionServer: error telling master we are upcom.google.protobuf.ServiceException: org.apache.hadoop.net.ConnectTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending local=/172.42.0.1:41015 remote=hostname/10.95.26.153:60003] at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:231) at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:300) at org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$BlockingStub.regionServerStartup(RegionServerStatusProtos.java:8277) at org.apache.hadoop.hbase.regionserver.HRegionServer.reportForDuty(HRegionServer.java:2167) at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:826) at java.lang.Thread.run(Thread.java:744)Caused by: org.apache.hadoop.net.ConnectTimeoutException: 10000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending local=/172.42.0.1:41015 remote=hostname/10.95.26.153:60003] at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:532) at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493) at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.setupConnection(RpcClientImpl.java:408) at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.setupIOstreams(RpcClientImpl.java:714) at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.writeRequest(RpcClientImpl.java:894) at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.tracedWriteRequest(RpcClientImpl.java:863) at org.apache.hadoop.hbase.ipc.RpcClientImpl.call(RpcClientImpl.java:1214) at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:216) ... 5 more2017-08-29 11:14:33,703 WARN [regionserver/hostname/172.42.0.1:60023] regionserver.HRegionServer: reportForDuty failed; sleeping and then retrying.2017-08-29 11:14:36,704 INFO [regionserver/hostname/172.42.0.1:60023] regionserver.HRegionServer: reportForDuty to master=hostname,60003,1502412914707 with port=60023, startcode=15039759941422017-08-29 11:14:36,704 DEBUG [regionserver/hostname/172.42.0.1:60023] ipc.AbstractRpcClient: Use SIMPLE authentication for service RegionServerStatusService, sasl=false2017-08-29 11:14:36,704 DEBUG [regionserver/hostname/172.42.0.1:60023] ipc.AbstractRpcClient: Connecting to hostname/10.95.26.153:60003 发现regionserver的IP地址是docker0 而master对应的是bond0的IP地址。 从regionserver到master的TCP会话都停留在SYN_SENT。 12[hadoop@hostname logs]$ netstat -an | egrep 6000tcp 0 1 172.42.0.1:43184 10.95.26.153:60003 SYN_SENT 原因从错误日志看到，hbase regionserver的本地子网是172.x，但是远程master子网是10.x1[connection-pending local=/172.42.0.1:41015 remote=hostname/10.95.26.153:60003] 当服务器分配有多层网络接口时，可能会发生这种情况。 默认情况下，region server通过查找标识的主要主机名和性能DNS来确定它应该绑定到哪个接口。 基于网络路由表之后，region server确定它需要进行docker0接口，即使主服务器正在bond0上进行通信。123456789101112[hadoop@hostname logs]$ netstat -rnKernel IP routing tableDestination Gateway Genmask Flags MSS Window irtt Iface0.0.0.0 10.90.18.4 0.0.0.0 UG 0 0 0 bond010.0.0.0 10.90.18.1 255.0.0.0 UG 0 0 0 bond010.90.18.0 0.0.0.0 255.255.254.0 U 0 0 0 bond010.232.0.0 10.90.18.4 255.255.224.0 UG 0 0 0 bond0100.64.0.0 10.90.18.1 255.192.0.0 UG 0 0 0 bond0169.254.0.0 0.0.0.0 255.255.0.0 U 0 0 0 bond0172.16.0.0 10.90.18.1 255.240.0.0 UG 0 0 0 bond0172.42.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0192.168.0.0 10.90.18.1 255.255.0.0 UG 0 0 0 bond0 解决在hbase-site.xml中设置param hbase.regionserver.dns.interface，以强制区域服务器在启动时使用bond0，以便它可以在正确的网络接口上与hbase master进行通信。 &lt;property&gt; &lt;name&gt;hbase.regionserver.dns.interface&lt;/name&gt; &lt;value&gt;bond0&lt;/value&gt; &lt;/property&gt; 重启regionserver 即可。]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS增加/减少Hadoop节点]]></title>
    <url>%2F2015%2F02%2F20%2FHDFS%E9%80%80%E5%BD%B9Hadoop%E8%8A%82%E7%82%B9%2F</url>
    <content type="text"><![CDATA[last conact值一直增大是cdh5 的bug：http://heylinux.com/archives/3490.html 集群故障：操作失误，导致业务受影响时间加长(10分30秒)。1) 手动stop datanode(问题根源：不用手动操作，namenode自己会kill datanode,否则namenode会一直尝试去进行kill datanode)这样下线会导致namenode只是发现datanode连接不上而不是死亡，并且开始不停的尝试获取心跳。此期间业务请求还会往此datanode进行发送，从而影响业务响应时间。2) 修改slave文件去掉下线datanode节点，namenode进行 /home/q/hadoop/q_hadoop/bin/hadoop dfsadmin -refreshNodes datanode停机后汇报到namenode的时间差问题 默认10分30秒，该操作时间也是维护窗口(网络抖动、下线多久才hdfs balance)，设置太小不适合运维。1234567HDFS默认的超时时间为10分钟+30秒。这里暂且定义超时时间为timeout计算公式为：timeout = 2 * dfs.namenode.heartbeat.recheck.interval + 10 * dfs.heartbeat.interval而默认的dfs.namenode.heartbeat.recheck.interval 大小为5分钟，dfs.heartbeat.interval默认的大小为3秒。需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒 改小参数值会导致namenode负载较高。http://www.firefoxbug.com/index.php/archives/2429/如下操作可以让namenode迅速知晓datanode死亡并下线服务(正确操作)。 修改slave文件去掉下线datanode节点，namenode执行 /home/q/hadoop/q_hadoop/bin/hadoop dfsadmin -refreshNodes该操作会kill掉节点上datanode服务，namenode感知节点dead并下线服务。 问题排查及改进 datanode 下线操作很常见，业务受影响严重意味着业务强依赖hbase，客户端没有设置超时时间并且没有重试机制。( tc 异步客户端默认不设置超时时间，会一直等待响应 ） 在Hadoop集群中，当一个datanode发生故障（宕机，进程被kill，网络不通等）时，namenode在一定时间内（默认10分30秒）无法收到该datanode的心跳信息，就会将该datanode从集群中下线设置为dead。在感知到下线之前这段时间，认为该节点还在提供服务，访问该节点的数据块就会失败，为了尽快让namenode感知到该节点已经下线，我们可以手动进行一些操作，具体如下： 1.规范化datanode下线操作,有两种方法：方法一: 修改salve文件后namenode执行refresh。让namenode去kill datanode并下线服务。 方法二:引入参数 dfs.hosts.exclude, 类似于对应slave文件的参数dfs.hosts。在Namenode上把需要Decommission的Datanode的机器名加入到dfs.hosts.exclude(该配置项在hdfs-site.xml)所指定文件中，也就是告诉Namenode哪些Datanode要被Decommission。 在hdfs-site.xml文件中增加配置12345678&lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/home/q/hadoop/hadoop-2.5.0-cdh5.2.0/etc/hadoop/slaves&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/home/q/hadoop/hadoop-2.5.0-cdh5.2.0/etc/hadoop/excludes&lt;/value&gt;&lt;/property&gt; 1)下线datanode2)执行命令./bin/hdfs dfsadmin -refreshNodes 下线datanode后该节点的last contact一直增大，Admin State为In Service状态。执行refresh操作后节点状态Admin State为Decommisson In Progress,last contact值依旧增大,该节点数据都balance完成后状态变为退役完成Decommissioned 。该方法会导致last contact一直增大。 2.客户端设置超时时间和重试机制1234hbase.rpc.timeout rpc超时时间hbase.client.operation.timeout 写超时的参数hbase.client.scanner.timeout.period 查询scan超时的参数client设置参数，请求超时后client发起重试。 3.datanode参数优化 datanode 相关优化功能点主要游: 1)开启Short Circuit Local Read 功能 2）开启Hedged Read功能。 以下内容为整理和实际实践笔记： 在Hadoop集群中，当一个datanode发生故障（宕机，进程被kill，网络不通等）时，namenode在一定时间内（默认10分30秒）无法收到该datanode的心跳信息，就会将该datanode从集群中下线设置为dead。在感知到下线之前这段时间，认为该节点还在提供服务，访问该节点的数据块就会失败，为了尽快让namenode感知到该节点已经下线，我们可以手动进行一些操作，具体如下： 添加文件excludes，在该文件中添加想要退役节点的机器名 在hdfs-site.xml文件中增加配置1234&lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/home/q/hadoop/hadoop-2.5.0-cdh5.2.0/etc/hadoop/excludes&lt;/value&gt;&lt;/property&gt; 最后执行命令./bin/hdfs dfsadmin -refreshNodes 下面我执行操作之后web ui的情况可以通过观察，首先我直接关闭了一台datanode，节点还是处于in Service中，只是last contact在不断增大 接着根据上面的操作执行了./bin/hdfs dfsadmin -refreshNodes之后节点在退役过程中 最后等节点数据都转移完成后，退役完成Decommissioned 验证成功访问web UI查看集群信息访问http://master:50070/可以看到&quot;Live Nodes”的数量，如果增加了新节点数，说明添加成功。 执行命令查看123yarn rmadmin -refreshNodes hdfs dfsadmin -refreshNodes $ bin/hadoop dfsadmin -report 集群负载均衡在master主机上面，运行start-balancer.sh进行数据负载均衡。目的是将其他节点的数据分担一些到新节点上来，看集群原来数据的多少需要一定的时间才能完成。设置带宽，配置均衡器balancer，一般不在主节点上运行，以避免影响业务，可以有专门的balancer节点12345hdfs dfsadmin -setBalancerBandwidth 1048576# 如果某个datanode的磁盘里用率比平均水平高出5%，Blocks向其他低于平均水平的datanode中传送 hadoop balancer或者start-balancer.sh -threshold 5 负载均衡作用：当节点出现敀障，或新增加节点时，数据块可能分布不均匀，负载均衡可重新平衡各个datanode上数据块的分布，使得所有的节点数据和负载能处于一个相对平均的状态，从而避免由于新节点的加入而效率降低(如果不进行balance,新数据一般会被插入到新节点中) 随时时间推移，各个datanode上的块分布来越来越不均衡，这将降低MR的本地性，导致部分datanode相对更加繁忙。balancer是一个hadoop守护进程，它将块从忙碌的datanode移动相对空闲的datanode，同时坚持块复本放置策略，将复本分散到不同的机器、机架。balancer会促使每个datanode的使用率与整个集群的使用率接近，这个“接近”是通过-threashold参数指定的，默认是10%。不同节点之间复制数据的带宽是受限的，默认是1MB/s，可以通过hdfs-site.xml文件中的dfs.balance.bandwithPerSec属性指定（单位是字节）。建议定期执行均衡器，如每天或者每周。 新增节点是需要同时启动nodemanager12hadoop-daemon.sh start datanodeyarn-daemon.sh start nodemanager 参考：https://www.cnblogs.com/fefjay/p/6048269.htmlhttps://www.cnblogs.com/learn21cn/p/6196183.html]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[equal和hashcode]]></title>
    <url>%2F2015%2F02%2F15%2Fequal%E5%92%8Chashcode%2F</url>
    <content type="text"><![CDATA[1、 为什么要重载equal方法？答案：因为Object的equal方法默认是两个对象的引用的比较，意思就是指向同一内存地址则相等，否则不相等；如果你现在需要利用对象里面的值来判断是否相等，则重载equal方法。 2、 为什么重载hashCode方法？答案：一般的地方不需要重载hashCode，只有当类需要放在HashTable、HashMap、HashSet等等hash结构的集合时才会 重载hashCode，那么为什么要重载hashCode呢？就HashMap来说，好比HashMap就是一个大内存块，里面有很多小内存块，小内存块 里面是一系列的对象，可以利用hashCode来查找小内存块hashCode%size(小内存块数量-即容量)，所以当equal相等时，hashCode必须相等，而且如果是object对象，必须重载hashCode和equal方法。 3、 为什么equals()相等，hashCode就一定要相等，而hashCode相等，却不要求equals相等?答案：1、因为是按照hashCode来访问小内存块，所以hashCode必须相等。2、HashMap获取一个对象是比较key的hashCode相等和equal为true。之所以hashCode相等，却可以equal不等，就比如ObjectA和ObjectB他们都有属性name，那么hashCode都以name计算，所以hashCode一样，但是两个对象属于不同类型，所以equal为false。 4、 为什么需要hashCode?1、 通过hashCode可以很快的查到小内存块。2、 通过hashCode比较比equal方法快，当get时先比较hashCode，如果hashCode不同，直接返回false。 ==比较符只会比较地址,如果地址不同就返回falsejava中任何类都可以重写equals()方法来实现自己的比较方式,String类重写了equals()方法. String类的equals()方法不仅会比较两个对象的地址,还会比较他们的字符串的内容.如果被比较的两个引用指向不同的地址,但是两个地址中的字符串的内容是相同的String的equals()方法仍然会返回true. 哈希码(HashCode)哈希码产生的依据：哈希码并不是完全唯一的，它是一种算法，让同一个类的对象按照自己不同的特征尽量的有不同的哈希码，但不表示不同的对象哈希码完全不同。也有相同的情况，看程序员如何写哈希码的算法。 什么是哈希码(HashCode)在Java中，哈希码代表对象的特征。例如对象String str1 = “aa”, str1.hashCode= 3104String str2 = “bb”, str2.hashCode= 3106String str3 = “aa”, str3.hashCode= 3104根据HashCode由此可得出str1!=str2,str1==str3下面给出几个常用的哈希码的算法。1：Object类的hashCode.返回对象的内存地址经过处理后的结构，由于每个对象的内存地址都不一样，所以哈希码也不一样。2：String类的hashCode.根据String类包含的字符串的内容，根据一种特殊算法返回哈希码，只要字符串所在的堆空间相同，返回的哈希码也相同。3：Integer类，返回的哈希码就是Integer对象里所包含的那个整数的数值，例如Integer i1=new Integer(100),i1.hashCode的值就是100 。由此可见，2个一样大小的Integer对象，返回的哈希码也一样。 案例分析https://www.cnblogs.com/keyi/p/7119825.html 总结1、equals方法用于比较对象的内容是否相等（覆盖以后）2、hashcode方法只有在集合中用到3、当覆盖了equals方法时，比较对象是否相等将通过覆盖后的equals方法进行比较（判断对象的内容是否相等）。4、将对象放入到集合中时，首先判断要放入对象的hashcode值与集合中的任意一个元素的hashcode值是否相等，如果不相等直接将该对象放入集合中。如果hashcode值相等，然后再通过equals方法判断要放入对象与集合中的任意一个对象是否相等，如果equals判断不相等，直接将该元素放入到集合中，否则不放入。5、将元素放入集合的流程图：]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二进制与16进制]]></title>
    <url>%2F2015%2F02%2F15%2F%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%8E16%E8%BF%9B%E5%88%B6%2F</url>
    <content type="text"><![CDATA[首先呢，先要看看十六位数的表示方法 再来掌握二进制数与十六进制数之间的对应关系表 二进制转换成十六进制的方法是，取四合一法，即从二进制的小数点为分界点，向左（或向右）每四位取成一位举例:组分好以后，对照二进制与十六进制数的对应表（如图2中所示），将四位二进制按权相加，得到的数就是一位十六进制数，然后按顺序排列，小数点的位置不变哦，最后得到的就是十六进制数哦]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hashmap]]></title>
    <url>%2F2015%2F02%2F14%2FHashmap%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[哈希(hash)Hash，一般翻译做“散列”，也有直接音译为“哈希”的，就是把任意长度的输入，通过散列算法，变换成固定长度的输出，该输出就是散列值。这种转换是一种压缩映射，也就是，散列值的空间通常远小于输入的空间，不同的输入可能会散列成相同的输出，所以不可能从散列值来唯一的确定输入值。简单的说就是一种将任意长度的消息压缩到某一固定长度的消息摘要的函数。所有散列函数都有如下一个基本特性：根据同一散列函数计算出的散列值如果不同，那么输入值肯定也不同。但是，根据同一散列函数计算出的散列值如果相同，输入值不一定相同。两个不同的输入值，根据同一散列函数计算出的散列值相同的现象叫做碰撞。常见的Hash函数有以下几个：1234567891011121314151617直接定址法：直接以关键字k或者k加上某个常数（k+c）作为哈希地址。数字分析法：提取关键字中取值比较均匀的数字作为哈希地址。除留余数法：用关键字k除以某个不大于哈希表长度m的数p，将所得余数作为哈希表地址。分段叠加法：按照哈希表地址位数将关键字分成位数相等的几部分，其中最后一部分可以比较短。然后将这几部分相加，舍弃最高进位后的结果就是该关键字的哈希地址。平方取中法：如果关键字各个部分分布都不均匀的话，可以先求出它的平方值，然后按照需求取中间的几位作为哈希地址。伪随机数法：采用一个伪随机数当作哈希函数。 上面介绍过碰撞。衡量一个哈希函数的好坏的重要指标就是发生碰撞的概率以及发生碰撞的解决方案。任何哈希函数基本都无法彻底避免碰撞，常见的解决碰撞的方法有以下几种：1234567891011开放定址法：开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。链地址法将哈希表的每个单元作为链表的头结点，所有哈希地址为i的元素构成一个同义词链表。即发生冲突时就把该关键字链在以该单元为头结点的链表的尾部。再哈希法当哈希地址发生冲突用其他的函数计算另一个哈希函数地址，直到冲突不再产生为止。建立公共溢出区将哈希表分为基本表和溢出表两部分，发生冲突的元素都放入溢出表中。 HashMapHashMap是Array和Linkedlist的结合体 我们可以从上图看到，左边很明显是个数组，数组的每个成员是一个链表。该数据结构所容纳的所有元素均包含一个指针，用于元素间的链接。我们根据元素的自身特征把元素分配到不同的链表中去，反过来我们也正是通过这些特征找到正确的链表，再从链表中找出正确的元素。其中，根据元素特征计算元素数组下标的方法就是哈希算法，即本文的主角hash()函数（当然，还包括indexOf()函数）。 源码解析首先，在同一个版本的Jdk中，HashMap、HashTable以及ConcurrentHashMap里面的hash方法的实现是不同的。再不同的版本的JDK中（Java7 和 Java8）中也是有区别的。我会尽量全部介绍到。相信，看文这篇文章，你会彻底理解hash方法。 在上代码之前，我们先来做个简单分析。我们知道，hash方法的功能是根据Key来定位这个K-V在链表数组中的位置的。也就是hash方法的输入应该是个Object类型的Key，输出应该是个int类型的数组下标。如果让你设计这个方法，你会怎么做？ 其实简单，我们只要调用Object对象的hashCode()方法，该方法会返回一个整数，然后用这个数对HashMap或者HashTable的容量进行取模就行了。没错，其实基本原理就是这个，只不过，在具体实现上，由两个方法int hash(Object k)和int indexFor(int h, int length)来实现。但是考虑到效率等问题，HashMap的实现会稍微复杂一点。 12hash ：该方法主要是将Object转换成一个整型。indexFor ：该方法主要是将hash生成的整型转换成链表数组中的下标。 HashMap In Java 71234567891011121314final int hash(Object k) &#123; int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; indexFor方法前面我说过，indexFor方法其实主要是将hash生成的整型转换成链表数组中的下标。那么return h &amp; (length-1);是什么意思呢？其实，他就是取模。Java之所有使用位运算(&amp;)来代替取模运算(%)，最主要的考虑就是效率。位运算(&amp;)效率要比代替取模运算(%)高很多，主要原因是位运算直接对内存数据进行操作，不需要转成十进制，因此处理速度非常快。 那么，为什么可以使用位运算(&amp;)来实现取模运算(%)呢？这实现的原理如下：123456789X % 2^n = X &amp; (2^n – 1)2^n表示2的n次方，也就是说，一个数对2^n取模 == 一个数和(2^n – 1)做按位与运算 。假设n为3，则2^3 = 8，表示成2进制就是1000。2^3 -1 = 7 ，即0111。此时X &amp; (2^3 – 1) 就相当于取X的2进制的最后三位数。从2进制角度来看，X / 8相当于 X &gt;&gt; 3，即把X右移3位，此时得到了X / 8的商，而被移掉的部分(后三位)，则是X % 8，也就是余数。 上面的解释不知道你有没有看懂，没看懂的话其实也没关系，你只需要记住这个技巧就可以了。或者你可以找几个例子试一下。126 % 8 = 6 ，6 &amp; 7 = 610 &amp; 8 = 2 ，10 &amp; 7 = 2 所以，return h &amp; (length-1);只要保证length的长度是2^n的话，就可以实现取模运算了。而HashMap中的length也确实是2的倍数，初始值是16，之后每次扩充为原来的2倍。扩容因子是0.75。 为了推断HashMap的默认长度为什么是1612长度16或者其他2的幂,length - 1的值是所有二进制位全为1,这种情况下,index的结果等同于hashcode后几位的值只要输入的hashcode本身分布均匀,hash算法的结果就是均匀的 所以,HashMap的默认长度为16,是为了降低hash碰撞的几率https://blog.csdn.net/zjcjava/article/details/78495416 分析完indexFor方法后，我们接下来准备分析hash方法的具体原理和实现。在深入分析之前，至此，先做个总结。 HashMap的数据是存储在链表数组里面的。在对HashMap进行插入/删除等操作时，都需要根据K-V对的键值定位到他应该保存在数组的哪个下标中。而这个通过键值求取下标的操作就叫做哈希。HashMap的数组是有长度的，Java中规定这个长度只能是2的倍数，初始值为16。简单的做法是先求取出键值的hashcode，然后在将hashcode得到的int值对数组长度进行取模。为了考虑性能，Java总采用按位与操作实现取模操作。 hash方法接下来我们会发现，无论是用取模运算还是位运算都无法直接解决冲突较大的问题。比如：CA11 0000和0001 0000在对0000 1111进行按位与运算后的值是相等的。 两个不同的键值，在对数组长度进行按位与运算后得到的结果相同，这不就发生了冲突吗。那么如何解决这种冲突呢，来看下Java是如何做的。 其中的主要代码部分如下：1234h ^= k.hashCode();h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12);return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); 这段代码是为了对key的hashCode进行扰动计算，防止不同hashCode的高位不同但低位相同导致的hash冲突。简单点说，就是为了把高位的特征和低位的特征组合起来，降低哈希冲突的概率，也就是说，尽量做到任何一位的变化都能对最终得到的结果产生影响。 举个例子来说，我们现在想向一个HashMap中put一个K-V对，Key的值为“hollischuang”，经过简单的获取hashcode后，得到的值为“1011000110101110011111010011011”，如果当前HashTable的大小为16，即在不进行扰动计算的情况下，他最终得到的index结果值为11。由于15的二进制扩展到32位为“00000000000000000000000000001111”，所以，一个数字在和他进行按位与操作的时候，前28位无论是什么，计算结果都一样（因为0和任何数做与，结果都为0）。如下图所示。 可以看到，后面的两个hashcode经过位运算之后得到的值也是11 ，虽然我们不知道哪个key的hashcode是上面例子中的那两个，但是肯定存在这样的key，这就产生了冲突。 那么，接下来，我看看一下经过扰动的算法最终的计算结果会如何。 从上面图中可以看到，之前会产生冲突的两个hashcode，经过扰动计算之后，最终得到的index的值不一样了，这就很好的避免了冲突。 其实，使用位运算代替取模运算，除了性能之外，还有一个好处就是可以很好的解决负数的问题。1因为我们知道，hashcode的结果是int类型，而int的取值范围是-2^31 ~ 2^31 – 1，即[ -2147483648, 2147483647]；这里面是包含负数的，我们知道，对于一个负数取模还是有些麻烦的。如果使用二进制的位运算的话就可以很好的避免这个问题。首先，不管hashcode的值是正数还是负数。length-1这个值一定是个正数。那么，他的二进制的第一位一定是0（有符号数用最高位作为符号位，“0”代表“+”，“1”代表“-”），这样里两个数做按位与运算之后，第一位一定是个0，也就是，得到的结果一定是个正数。 HashTable In Java 7上面是Java 7中HashMap的hash方法以及indexOf方法的实现，那么接下来我们要看下，线程安全的HashTable是如何实现的，和HashMap有何不同，并试着分析下不同的原因。以下是Java 7中HashTable的hash方法的实现。1234private int hash(Object k) &#123; // hashSeed will be zero if alternative hashing is disabled. return hashSeed ^ k.hashCode();&#125; 我们可以发现，很简单，(1)相当于只是对k做了个简单的hash，取了一下其hashCode。(1)而HashTable中也没有indexOf方法，取而代之的是这段代码：int index = (hash &amp; 0x7FFFFFFF) % tab.length;。也就是说，HashMap和HashTable对于计算数组下标这件事，采用了两种方法。HashMap采用的是位运算，而HashTable采用的是直接取模。12为啥要把hash值和0x7FFFFFFF做一次按位与操作呢，主要是为了保证得到的index的第一位为0，也就是为了得到一个正数。因为有符号数第一位0代表正数，1代表负数。 0x7FFFFFFF 是long int的最大值 我们前面说过，HashMap之所以不用取模的原因是为了提高效率。有人认为，因为HashTable是个线程安全的类，本来就慢，所以Java并没有考虑效率问题，就直接使用取模算法了呢？但是其实并不完全是，Java这样设计还是有一定的考虑在的，虽然这样效率确实是会比HashMap慢一些。 其实，HashTable采用简单的取模是有一定的考虑在的。这就要涉及到HashTable的构造函数和扩容函数了。由于篇幅有限，这里就不贴代码了，直接给出结论：12345HashTable默认的初始大小为11，之后每次扩充为原来的2n+1。也就是说，HashTable的链表数组的默认大小是一个素数、奇数。之后的每次扩充结果也都是奇数。由于HashTable会尽量使用素数、奇数作为容量的大小。当哈希表的大小为素数时，简单的取模哈希的结果会更加均匀。（这个是可以证明出来的，由于不是本文重点，暂不详细介绍，可参考：http://zhaox.github.io/algorithm/2015/06/29/hash） 至此，我们看完了Java 7中HashMap和HashTable中对于hash的实现，我们来做个简单的总结。12345HashMap默认的初始化大小为16，之后每次扩充为原来的2倍。HashTable默认的初始大小为11，之后每次扩充为原来的2n+1。当哈希表的大小为素数时，简单的取模哈希的结果会更加均匀，所以单从这一点上看，HashTable的哈希表大小选择，似乎更高明些。因为hash结果越分散效果越好。在取模计算时，如果模数是2的幂，那么我们可以直接使用位运算来得到结果，效率要大大高于做除法。所以从hash计算的效率上，又是HashMap更胜一筹。但是，HashMap为了提高效率使用位运算代替哈希，这又引入了哈希分布不均匀的问题，所以HashMap为解决这问题，又对hash算法做了一些改进，进行了扰动计算。 ConcurrentHashMap In Java 71234567891011121314151617181920private int hash(Object k) &#123; int h = hashSeed; if ((0 != h) &amp;&amp; (k instanceof String)) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); // Spread bits to regularize both segment and index locations, // using variant of single-word Wang/Jenkins hash. h += (h &lt;&lt; 15) ^ 0xffffcd7d; h ^= (h &gt;&gt;&gt; 10); h += (h &lt;&lt; 3); h ^= (h &gt;&gt;&gt; 6); h += (h &lt;&lt; 2) + (h &lt;&lt; 14); return h ^ (h &gt;&gt;&gt; 16);&#125; int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; 上面这段关于ConcurrentHashMap的hash实现其实和HashMap如出一辙。都是通过位运算代替取模，然后再对hashcode进行扰动。区别在于，ConcurrentHashMap 使用了一种变种的Wang/Jenkins 哈希算法，其主要母的也是为了把高位和低位组合在一起，避免发生冲突。至于为啥不和HashMap采用同样的算法进行扰动，我猜这只是程序员自由意志的选择吧。至少我目前没有办法证明哪个更优。 HashMap In Java 8在Java 8 之前，HashMap和其他基于map的类都是通过链地址法解决冲突，它们使用单向链表来存储相同索引值的元素。在最坏的情况下，这种方式会将HashMap的get方法的性能从O(1)降低到O(n)。为了解决在频繁冲突时hashmap性能降低的问题，Java 8中使用平衡树来替代链表存储冲突的元素。这意味着我们可以将最坏情况下的性能从O(n)提高到O(logn)。关于HashMap在Java 8中的优化，我后面会有文章继续深入介绍。 如果恶意程序知道我们用的是Hash算法，则在纯链表情况下，它能够发送大量请求导致哈希碰撞，然后不停访问这些key导致HashMap忙于进行线性查找，最终陷入瘫痪，即形成了拒绝服务攻击（DoS）。 关于Java 8中的hash函数，原理和Java 7中基本类似。Java 8中这一步做了优化，只做一次16位右位移异或混合，而不是四次，但原理是不变的。1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的。以上方法得到的int的hash值，然后再通过h &amp; (table.length -1)来得到该对象在数据中保存的位置。 HashTable In Java 8在Java 8的HashTable中，已经不在有hash方法了。但是哈希的操作还是在的，比如在put方法中就有如下实现：12int hash = key.hashCode();int index = (hash &amp; 0x7FFFFFFF) % tab.length; 这其实和Java 7中的实现几乎无差别，就不做过多的介绍了 ConcurrentHashMap In Java 8Java 8 里面的求hash的方法从hash改为了spread。实现方式如下：123static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; Java 8的ConcurrentHashMap同样是通过Key的哈希值与数组长度取模确定该Key在数组中的索引。同样为了避免不太好的Key的hashCode设计，它通过如下方法计算得到Key的最终哈希值。不同的是，Java 8的ConcurrentHashMap作者认为引入红黑树后，即使哈希冲突比较严重，寻址效率也足够高，所以作者并未在哈希值的计算上做过多设计，只是将Key的hashCode值与其高16位作异或并保证最高位为0（从而保证最终结果为正整数）。 总结至此，我们已经分析完了HashMap、HashTable以及ConcurrentHashMap分别在Jdk 1.7 和 Jdk 1.8中的实现。我们可以发现，为了保证哈希的结果可以分散、为了提高哈希的效率，JDK在一个小小的hash方法上就有很多考虑，做了很多事情。当然，我希望我们不仅可以深入了解背后的原理，还要学会这种对代码精益求精的态度。 非线程安全问题解决方案HashMap为什么线程不安全1、在两个线程同时尝试扩容HashMap时，可能将一个链表形成环形的链表，所有的next都不为空，进入死循环2、在两个线程同时进行put时可能造成一个线程数据的丢失 我们可以使用CocurrentHashMap来代替HashTable吗？这是另外一个很热门的面试题，因为ConcurrentHashMap越来越多人用了。我们知道HashTable是synchronized的，但是ConcurrentHashMap同步性能更好，因为它仅仅根据同步级别对map的一部分进行上锁。ConcurrentHashMap当然可以代替HashTable，但是HashTable提供更强的线程安全性。 先来些简单的问题你用过HashMap吗？什么是HashMap？你为什么用到它？ 几乎每个人都会回答“是的”，然后回答HashMap的一些特性，譬如HashMap可以接受null键值和值，而HashTable则不能；HashMap是非synchronized;HashMap很快；以及HashMap储存的是键值对等等。这显示出你已经用过HashMap，而且对它相当的熟悉。但是面试官来个急转直下，从此刻开始问出一些刁钻的问题，关于HashMap的更多基础的细节。面试官可能会问出下面的问题： 你知道HashMap的工作原理吗？你知道HashMap的get()方法的工作原理吗？ 你也许会回答“我没有详查标准的Java API，你可以看看Java源代码或者Open JDK。”“我可以用Google找到答案。” 但一些面试者可能可以给出答案，“HashMap是基于hashing的原理，我们使用put(key, value)存储对象到HashMap中，使用get(key)从HashMap中获取对象。当我们给put()方法传递键和值时，我们先对键调用hashCode()方法，返回的hashCode用于找到bucket位置来储存Entry对象。”这里关键点在于指出，HashMap是在bucket中储存键对象和值对象，作为Map.Entry。这一点有助于理解获取对象的逻辑。如果你没有意识到这一点，或者错误的认为仅仅只在bucket中存储值的话，你将不会回答如何从HashMap中获取对象的逻辑。这个答案相当的正确，也显示出面试者确实知道hashing以及HashMap的工作原理。但是这仅仅是故事的开始，当面试官加入一些Java程序员每天要碰到的实际场景的时候，错误的答案频现。下个问题可能是关于HashMap中的碰撞探测(collision detection)以及碰撞的解决方法： 当两个对象的hashcode相同会发生什么？hashcode相同只会说明存在相同的bucket下。从这里开始，真正的困惑开始了，一些面试者会回答因为hashcode相同，所以两个对象是相等的，HashMap将会抛出异常，或者不会存储它们。然后面试官可能会提醒他们有equals()和hashCode()两个方法，并告诉他们两个对象就算hashcode相同，但是它们可能并不相等。一些面试者可能就此放弃，而另外一些还能继续挺进，他们回答“因为hashcode相同，所以它们的bucket位置相同，‘碰撞’会发生。因为HashMap使用LinkedList存储对象，这个Entry(包含有键值对的Map.Entry对象)会存储在LinkedList中 。”这个答案非常的合理，虽然有很多种处理碰撞的方法，这种方法是最简单的，也正是HashMap的处理方法。但故事还没有完结，面试官会继续问： 如果两个键的hashcode相同，你如何获取值对象？ 面试者会回答：当我们调用get()方法，HashMap会使用键对象的hashcode找到bucket位置，然后获取值对象。面试官提醒他如果有两个值对象储存在同一个bucket，他给出答案:将会遍历LinkedList直到找到值对象。面试官会问因为你并没有值对象去比较，你是如何确定确定找到值对象的？除非面试者直到HashMap在LinkedList中存储的是键值对，否则他们不可能回答出这一题 。 其中一些记得这个重要知识点的面试者会说，找到bucket位置之后，会调用keys.equals()方法去找到LinkedList中正确的节点，最终找到要找的值对象。完美的答案！ 许多情况下，面试者会在这个环节中出错，因为他们混淆了hashCode()和equals()方法。因为在此之前hashCode()屡屡出现，而equals()方法仅仅在获取值对象的时候才出现。一些优秀的开发者会指出使用不可变的、声明作final的对象，并且采用合适的equals()和hashCode()方法的话，将会减少碰撞的发生，提高效率。不可变性使得能够缓存不同键的hashcode，这将提高整个获取对象的速度，使用String，Interger这样的wrapper类作为键是非常好的选择。 如果你认为到这里已经完结了，那么听到下面这个问题的时候，你会大吃一惊。 如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？除非你真正知道HashMap的工作原理，否则你将回答不出这道题。默认的负载因子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其它集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程叫作rehashing，因为它调用hash方法找到新的bucket位置。 如果你能够回答这道问题，下面的问题来了： 你了解重新调整HashMap大小存在什么问题吗？你可能回答不上来，这时面试官会提醒你当多线程的情况下，可能产生条件竞争(race condition) 。 当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在LinkedList中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在LinkedList的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。这个时候，你可以质问面试官，为什么这么奇怪，要在多线程的环境下使用HashMap呢？ 热心的读者贡献了更多的关于HashMap的问题： 为什么String, Interger这样的wrapper类适合作为键？String, Interger这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的，也是final的，而且已经重写了equals()和hashCode()方法了。其他的wrapper类也有这个特点。不可变性是必要的，因为为了要计算hashCode()，就要防止键值改变，如果键值在放入时和获取时返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性还有其他的优点如线程安全。如果你可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法是非常重要的。如果两个不相等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能。 我们可以使用自定义的对象作为键吗？这是前一个问题的延伸。当然你可能使用任何对象作为键，只要它遵守了equals()和hashCode()方法的定义规则，并且当对象插入到Map中之后将不会再改变了。如果这个自定义对象时不可变的，那么它已经满足了作为键的条件，因为当它创建之后就已经不能改变了。我们可以使用CocurrentHashMap来代替HashTable吗？这是另外一个很热门的面试题，因为ConcurrentHashMap越来越多人用了。我们知道HashTable是synchronized的，但是ConcurrentHashMap同步性能更好，因为它仅仅根据同步级别对map的一部分进行上锁。ConcurrentHashMap当然可以代替HashTable，但是HashTable提供更强的线程安全性。 总结 HashMap的工作原理 HashMap基于hashing原理，我们通过put()和get()方法储存和获取对象。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，然后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用LinkedList来解决碰撞问题，当发生碰撞了，对象将会储存在LinkedList的下一个节点中。 HashMap在每个LinkedList节点中储存键值对对象。 当两个不同的键对象的hashcode相同时会发生什么？ 它们会储存在同一个bucket位置的LinkedList中。键对象的equals()方法用来找到键值对。 Jdk的源代码，每一行都很有意思，都值得花时间去钻研、推敲。参考:https://blog.csdn.net/skiof007/article/details/80253587]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[static、final关键字及参数加载顺序]]></title>
    <url>%2F2014%2F02%2F21%2Fstatic%E3%80%81final%E5%85%B3%E9%94%AE%E5%AD%97%E5%8F%8A%E5%8F%82%E6%95%B0%E5%8A%A0%E8%BD%BD%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[static1、静态变量被所有的对象所共享，在内存中只有一个副本，它当且仅当在类初次加载时会被初始化2、虽然在静态方法中不能访问非静态成员方法和非静态成员变量，但是在非静态成员方法中是可以访问静态成员方法/变量的 final1、当用final修饰一个类时，表明这个类不能被继承2、只有在想明确禁止 该方法在子类中被覆盖的情况下才将方法设置为final的。注：类的private方法会隐式地被指定为final方法。3、对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。它指向的对象的内容是可变的 静态块、静态变量加载顺序详解1234567891011121314151617181920212223242526package com.qnr.test;public class TestLoadClass &#123; //1.第一步，准备加载类 public static void main(String[] args) &#123; new TestLoadClass(); //4.第四步，new一个类，但在new之前要处理匿名代码块 &#125; static int num = 4; //2.第二步，静态变量和静态代码块的加载顺序由编写先后决定 &#123; num += 3; System.out.println(&quot;b&quot;); //5.第五步，按照顺序加载匿名代码块，代码块中有打印 &#125; int a = 5; //6.第六步，按照顺序加载变量 &#123; // 成员变量第三个 System.out.println(&quot;c&quot;); //7.第七步，按照顺序打印c &#125; TestLoadClass() &#123; // 类的构造函数，第四个加载 System.out.println(&quot;d&quot;); //8.第八步，最后加载构造函数，完成对象的建立 &#125; static &#123; // 3.第三步，静态块，然后执行静态代码块，因为有输出，故打印a System.out.println(&quot;a&quot;); &#125; static void run() // 静态方法，调用的时候才加载// 注意看，e没有加载 &#123; System.out.println(&quot;e&quot;); &#125;&#125; 第一步，准备加载类 第二步，静态变量和静态代码块的加载顺序由编写先后决定 第三步，静态块，然后执行静态代码块，因为有输出，故打印a 第四步，new一个类，但在new之前要处理匿名代码块 第五步，按照顺序加载匿名代码块，代码块中有打印 第六步，按照顺序加载变量 第七步，按照顺序打印c 第八步，最后加载构造函数，完成对象的建立 静态方法，调用的时候才加载// 注意看，e没有加载]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hdfs映射本地磁盘webdav]]></title>
    <url>%2F2014%2F02%2F20%2Fhdfs%E6%98%A0%E5%B0%84%E6%9C%AC%E5%9C%B0%E7%A3%81%E7%9B%98webdav%2F</url>
    <content type="text"><![CDATA[安装tomcat1、下载wget http://apache.dataguru.cn/tomcat/tomcat-8/v8.0.14/bin/apache-tomcat-8.0.14.tar.gz 2、解压tar zxvf apache-tomcat-8.0.14.tar.gz 3、在.bashrc中增加export TOMCAT_HOME=/home/q/opdir/apache-tomcat-8.0.14 4、启动tomcat服务器1234567[hadoop@hostname /home/q/opdir/apache-tomcat-8.0.14]$ ./bin/startup.sh Using CATALINA_BASE: /home/q/opdir/apache-tomcat-8.0.14Using CATALINA_HOME: /home/q/opdir/apache-tomcat-8.0.14Using CATALINA_TMPDIR: /home/q/opdir/apache-tomcat-8.0.14/tempUsing JRE_HOME: /home/q/java/defaultUsing CLASSPATH: /home/q/opdir/apache-tomcat-8.0.14/bin/bootstrap.jar:/home/q/opdir/apache-tomcat-8.0.14/bin/tomcat-juli.jarTomcat started. 5、测试是否启动成功http://hostname:8080/123456789101112131415或者 ps -ef |grep tomcatWebdav部署WebDAV（Web-based Distributed Authoring and Versioning）是基于 HTTP 1.1 的一个通信协议。它为 HTTP 1.1 添加了一些扩展（就是在 GET、POST、HEAD 等几个 HTTP 标准方法以外添加了一些新的方法），使得应用程序可以直接将文件写到 Web Server 上，并且在写文件时候可以对文件加锁，写完后对文件解锁，还可以支持对文件所做的版本控制1、下载https://code.google.com/p/hdfs-webdav/downloads/detail?name=hdfs-webdav.war2、解压将hdfs-webdav.war拷贝到webapps目录下，重启tomcat，会将这个文件自动解压为hdfs-webdav目录，然后删除hdfs-webdav.war文件3、修改配置修改hdfs-webdav.war里面的WEB-INF/classes/hadoop-site.xml fs.default.name hdfs://10.86.36.218:8020/ namenode`]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解hadoop的恢复处理]]></title>
    <url>%2F2014%2F02%2F20%2F%E7%90%86%E8%A7%A3hadoop%E7%9A%84%E6%81%A2%E5%A4%8D%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[原文链接：http://blog.cloudera.com/blog/2015/02/understanding-hdfs-recovery-processes-part-1/ 理解HDFS的恢复机制对使用hadoop的非常有帮助！ HDFS中一个重要的设计原则就是保证生产环境下能够持续正确的执行一些操作，一个重要的点就是保证HDFS在不稳定和网络下，或者节点失败的情况下也能正确写入。HDFS有lease recovery，block recovery，pipeline recovery这几种恢复机制，理解什么情况下将会调用哪种恢复机制也能够帮我们更好的理解我们的集群的工作情况。 本文将带你深入了解恢复机制。首先简单介绍HDFS的write pipr和恢复处理，解释两个重要概念：block/replica stats和generation stamps，然后逐步介绍恢复过程，最后我们总结一些相关的问题。 本文分两部分：part1将会详细介绍lease recovery和block recovery，part2将介绍pipeline recovery。 背景 在HDFS中，文件是切分为一块一块的，多个reader来读文件，一个writer写文件。为了满足数据容错，不同的datanode上会保留文件的备份，备份数量被称为：replication factor。当新的文件块创建，或者要对一个已经存在的文件进行追加时，HDFS会创建一个写管道，管道里面就是存放数据和备份的datanode。如下图写过程：对于读操作，客户端选择含有数据的其中一个datanode，然后发送请求。 下面是两个特别需要容错的场景： hbase用WAL机制来保证数据安全，WAL是是存放在HDFS上的文件，如果regionserver挂了，就指望WAL来恢复数据了，所以一定要保证写pipeline的正确性。 当一个Flume客户端流式的写入HDFS中时，即使一些datanode挂了，也必须要保证写连续。 既然有了这些需求，下面就该隆重介绍 lease recovery，block recovery，pipeline recovery这三个的作用了。 在client往HDFS中写数据之前，它必须获取一个租约，我们可以理解是一把锁，这样就保证了单点写。这个租约要在预定的时间段内刷新，否则就会超时，HDFS认为这个写操作超时，这时HDFS将会关闭这个文件，释放这个租约，以方便其他client写文件。这个过程叫做lease recovery。 如果正在写入的文件块还没有在所有的datanode中同步时，如果这时候发生lease recovery，那这几个datanode中这个数据块的数据就会不一致，因此在lease recovery关闭这个文件块之前，有必要同步一下各个datanode之间的数据。这个过程叫做block recovery。它只会被lease recovery触发，并且只有不是COMPLETE状态的文件块才会被lease recovery触发block recovery。 在管道写的时候，假如一些datanode挂掉了，这时候HDFS并不会就直接停止写操作，它会试图恢复错误，来保证继续写文件，这个过程叫pipeline recovery。 下面我们详细解释一下：为了区分namenode上下文中的block和datanode中的blocks，这里我们指定前者为blocks，后者为replicas。 在datanode的上下文中，一个replica的状态有以下几种（可以在org.apache.hadoop.hdfs.server.common.HdfsServerConstats.java这个类中找到ReplicaState枚举类）： FNALIZED：当replica是这个状态时，表明数据已经写完了，这个replica将不会再发生变化，除非这个replica再次因为追加数据被打开。一个block中的所有Finalized的replica都有一个仙童的generation stamp（GS），GS也有可能因为一次recovery而变大。 RBW（replica being written）：表明一个replica正在被写入，一般处于RBW的replica都是文件的最后面的block。RBW的replica的数据对cilent是可见的，如果期间发生错误，HDFS将会尝试保护这些没有Finalized的数据。 RWR（replica waiting to be recovered）:如果datanode挂了并且重启了，它所有的RBW的replica将会变成RWR状态，RWR状态下的replica要么因为过时被丢弃，要么参与lease recovery（这里是什么意思）。 RUR（replica under recovery）：一个non-TEMPRORARY的replica将会变成RUR状态，当它参与lease recovery的时候。 TEMPORARY：为了block replica而临时创建的replica。它和RBW replica很类似，只不过它的数据对client是不可见的。如果replication失败，它将会被删除。 namenode中的block可能有下面集中状态（可以在org.apache.hadoop.hdfs.server.common.HdfsServerConstats.java这个类中找到BlockUCState枚举类）： UNDER_CONSTRUCTION:这个状态发生在正在写入时，UNDER_CONSTRUCTION通常是一个打开的文件的最后一个block，它的茶馆难度和genaration stamp都是可变的，数据对用户也是可见的，这个UNDER_CONSTRUCTION block是namenode是用来追踪写管道，和定位RWR replicas的位置。 UNDER_RECOVERY:如果处于UNDER_CONSTRUCTION的block发生了lease expire，它将变成UNDER_RECOVERY。 COMMITED：这个状态表示block的数据和GS不再改变，但是（还是贴原文吧，这句翻译不好，and there are fewer than the minimal-replication number of DataNodes that have reported FINALIZED replicas of same GS/length，大概意思是：已经FINALIZED的replicas比datanode最小的复制数要少，），这时候为了安全提供读服务，COMMITTEDblock必须追踪RBW replicas的位置，GS，和FINALIZED replicas的数量。当client要求namenode添加一个新的文件block，或者关闭一个文件时，UNDER_CONSTRUCTION block将会变成COMITTED状态。但是如果（这个文件）之前的blocks是COMMITED状态时，文件不能被关闭，client必须部队重试。 COMPLETE：当namenode看到FINALIZED的replica的最小数量和GS/length（疑问）匹配时，对应的COMMITTED block变成COMPLETE状态。只有当文件所有的block都是COMPLETED状态时才可以被关闭。有时block也有可能被强制变成COMPLETE状态，即使replicas的数量并没有满足最低要求，例如：当client要求一个新的block，并且周期i安的block还没有COMPLETE。 datanode持久化replicas的状态到硬盘上，但是namenode并不持久化block的状态，当namenode重启时，它将任何之前打开的文件的最后block变成UNDER_CONSTRUCTION状态，其他的block变成COMPLETE状态。 下面是replica和block的状态转换图：replica： block： generation stampGS是一个单调递增的8byte的数字，namenode傻姑娘的每一个block都各自维护一个。GS为了以下目的设计（参考链接：HDFS Append and Truncates ）： 发现block中陈旧的replicas：意思是，replica的GS比block的GS要older，例如当一个append操作不知怎么的跳过了replica。 发现datanode上过期时间很长的replica，并且重新加入集群。 下面情况发生时会产生新的GS： 创建一个新文件 client打开一个存在的文件，为了追加或者删除。 当在写数据到datanode上时发生了错误。 namenode为一个文件初始化一个lease recovery lease recovery和block recoverylease Manager租约是namenode上的租约管理器来管理，namenode追踪每一个被client打开写的文件。client不会单独为每一个它所打开的文件刷新租约，它是每隔一段时间发送一个请求来刷新所有的租约。（org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos.RenewLeaseResponseProto类，HDFS client和namenode的RPC协议） 每一个namenode管理一个HDFS namespaces,每个namespaces通过一个lease 管理器来管理所有的文件。（Federater HDFS集群可能有多个namespaces。） lease管理器维护一个软限制（1分钟）和一个硬限制（1小时）超时时间，在软限制没有过期之前，client拥有对文件的排它写权限，，如果软限制超时，或者文件被关闭，另一个client可以强制领走该文件的租约。如果硬限制超时，HDFS就会认为client已经退出，并且关闭该client在该文件上的所有行为，并且开始恢复租约。 如果一个client已经获取了一个文件的租约，但这并不影响其他client读这个文件，一个client可以对文件写的时候，同时可以有多个client对文件读。 lease管理器内部支持的操作有; 1.为一个client和路径添加租约（如果client已经拥有了租约，它添加路径到这个租约，否则它将会创建一个新的租约，并且绑定路径） 2.移除租约和路径（如果这是租约中的最后一个路径，那么租约也一起移除） 3.检查是否有软限制或者硬限制超时 4.刷新client的租约 lease 管理器每隔2妙会进行一次检查，看是否硬限制超时，如果发现有，它将会触发这个租约上的所有文件的lease recovery。 HDFS client通过org.apache.hadoop.hdfs.LeaseRenewer.LeaseRenewer类来刷新租约，这个类包含了一个用户列表，并且client为每个namenode上的每个用户运行一个线程。这线程定时检查namenode并刷新租约，当lease period过半的时候。 （注意：一个HDFS client只链接一个namenode） lease recovery process 有2种情况会触发namenode对一个client进行lease recovery：1.监控线程发现硬限制超时，2.当软限制发生时，一个client尝试拿走另一个client上面的租约。它将会检查该client打开的所有文件，如果文件最后的block不是COMPLETE状态，就进行block recovery，并且关闭文件。block recovery只有在恢复文件的租约的时候才会被触发。 下面是lease recovery的算法，针对一个文件f： 1.得到含有f的最后block的datanodes。 2.指定其中的一个datanode为主datanode p。 3.p从namenode上获取一个新的GS 4.p获取每一个datanode上的block信息 5.p计算最小的block长度 6.p更新所有的datanodes，包括GS和最小block长度 7.p通知namenode更新结果 8.namenode更新blockinfo 9.namenode移除f的租约（现在其他的client可以获取这个文件的租约了） 10.namnode提交日志 步骤3到7是block recovery算法，如果一个文件需要block recovery，namenode选择一个主datanode，这个datanode哟国有文件最后block的replica，然后namenode告知这个datanode负责其他datanode的block recovery。然后datanode向namenode汇报结果，namenode更新它自己的block状态，移除租约，提交日志。 有时，管理员需要自己强制执行recovery，在硬限制超时之前，可以通过命令行来执行：12hdfs debug recoverLease [-path&lt;path&gt;] [-retries &lt;num-retries&gt;] 总结：Lease recovery, block recovery, and pipeline recovery对HDFS的容错是必须的。它们保证即使有网络不稳定，或者节点失败，仍然能够保证数据持续写和一致性。 下篇我们介绍 pipe recoverythe write pipeline:当HDFS client写文件时，数据作为block块依次写进去，HDFS将block切分为packets，然后发送给datanode管道，如下图： wite pipeline有3个阶段： 1.pipeline setup：client向pipeline发送一个写请求，然后最后一个datanode节点返回一个确认信息，收到确认信息之后，pipeline开始准备写数据。 2.Data streaming：数据是以packet的方式送到pipeline中的，client端缓存数据，当buffer的大小等于packet的大小时，发送出去。client可以选择使用hflush（）来强制发送，但是下一个packet必须要等到上一个调用hflush()的pakcet的确认信息后才能发送。 3.close：所有的packet都返回确认信息后，client发送一个关闭请求，所有的datanode将对应的replica变成FINALIZED状态，并且报告给namenode。当FINALIZED的datanode满足配置的replica个数时，然后namenode将block的状态变成COMPLETE。 pipeline recovery： 当datanode在文件正在写入时，以下三种情况下发生错误，都会触发pipeline恢复。 1.recovery from pipeline setup failure 2.Recovery from Data Streaming Failure 3.Recovery from Close Failure]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode 22]]></title>
    <url>%2F2013%2F02%2F22%2FLeetcode-22%2F</url>
    <content type="text"><![CDATA[生成所有的括号组合求两个有序数组的中位数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108package com.lifeibigdata.algorithms.lcd;import com.sun.org.apache.bcel.internal.generic.IF_ACMPEQ;import java.util.ArrayDeque;import java.util.ArrayList;import java.util.List;import java.util.Queue;import java.util.concurrent.LinkedBlockingDeque;/** * Created by lifei on 16/5/30. * * 递归思想 */public class GenerateParenthesis &#123; public static void main(String[] args) &#123; GenerateParenthesis gp = new GenerateParenthesis(); List&lt;String&gt; list = gp.generateParenthesis(2); for (String str:list) &#123; System.out.println(str); &#125; &#125; public List&lt;String&gt; generateParenthesis(int n) &#123; ArrayList&lt;String&gt; result = new ArrayList&lt;String&gt;();// dfs(result, "", n, n);// help(n,0,0,"",result); help(n,result); return result; &#125; //BFS //c++ http://blog.csdn.net/zhy_cheng/article/details/8090346 //java http://www.cnblogs.com/end/archive/2012/10/25/2738493.html private void help(int n, ArrayList&lt;String&gt; answer) &#123; if (n == 0)&#123; answer.add(""); return; &#125; node temp = new node(); Queue&lt;node&gt; q = new ArrayDeque&lt;&gt;(); for (q.add(temp);!q.isEmpty();q.poll()) &#123; temp = q.peek(); node other; if (temp.x &lt; n)&#123; other = new node();//TODO other.x = temp.x + 1; other.y = temp.y; other.now = temp.now + "("; q.offer(other); &#125; if (temp.x &gt; temp.y)&#123; other = new node();//TODO other.x = temp.x; other.y = temp.y + 1; other.now = temp.now + ")"; if (other.y == n)&#123; answer.add(other.now); &#125; else &#123; q.offer(other); &#125; &#125; &#125; &#125; //DFS private void help(int n, int x, int y, String now, ArrayList&lt;String&gt; answer) &#123; if (y == n)&#123; answer.add(now); return; &#125; if (x &lt; n)&#123; help(n,x + 1,y,now+"(",answer); &#125; if (x &gt; y)&#123; help(n,x,y + 1,now+")",answer); &#125; &#125; /* left and right represents the remaining number of ( and ) that need to be added. When left &gt; right, there are more ")" placed than "(". Such cases are wrong and the method stops. */ public void dfs(ArrayList&lt;String&gt; result, String s, int left, int right)&#123; if(left &gt; right) //阻止)开始的情况,必须是left减一开始 return; if(left==0&amp;&amp;right==0)&#123; result.add(s); return; &#125; if(left&gt;0)&#123; dfs(result, s+"(", left-1, right); // &#125; if(right&gt;0)&#123; dfs(result, s+")", left, right-1); &#125; &#125;&#125;class node&#123; int x,y; String now = "";&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode 39/40]]></title>
    <url>%2F2013%2F02%2F22%2FLeetcode-39-40%2F</url>
    <content type="text"><![CDATA[等于target的所有组合 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package com.lifeibigdata.algorithms.lcd;import java.util.ArrayList;import java.util.Arrays;import java.util.List;/** * Created by lifei on 16/7/4. */public class CombinationSum &#123; public static void main(String[] args) &#123; int[] can = new int[]&#123;5,3,2,1&#125;; CombinationSum cs = new CombinationSum();// cs.combinationSum(can,6);// for (List&lt;Integer&gt; list:ans) &#123;// for (int i:list) &#123;// System.out.print(i+",");// &#125;// System.out.println();// &#125; cs.combinationSum2(can,6); for (List&lt;Integer&gt; list:sums) &#123; for (int i:list) &#123; System.out.print(i+","); &#125; System.out.println(); &#125; &#125; static List&lt;List&lt;Integer&gt;&gt; ans = new ArrayList&lt;List&lt;Integer&gt;&gt;();//声明全局变量 int[] cans = &#123;&#125;; public List&lt;List&lt;Integer&gt;&gt; combinationSum(int[] candidates, int target) &#123; this.cans = candidates; Arrays.sort(cans); backTracking(new ArrayList(), 0, target); return ans; &#125; public void backTracking(List&lt;Integer&gt; cur, int from, int target) &#123;//初次cur为空 if (target == 0) &#123; List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(cur); ans.add(list); &#125; else &#123; for (int i = from; i &lt; cans.length &amp;&amp; cans[i] &lt;= target; i++) &#123;//界限条件 cur.add(cans[i]); backTracking(cur, i, target - cans[i]); //递归调用 cur.remove(new Integer(cans[i])); &#125; &#125; &#125; /** * * * * 1,1,1,1,1,1, 1,1,1,1,2, 1,1,1,3, 1,1,2,2, 1,2,3, 1,5, 2,2,2, 3,3, */ //时间复杂度O(n!) 空间复杂度O(n) public static List&lt;List&lt;Integer&gt;&gt; sums = new ArrayList&lt;List&lt;Integer&gt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; combinationSum2(int[] candidates, int target) &#123; Arrays.sort(candidates); combinationSum2(candidates, 0, new ArrayList&lt;Integer&gt;(), target); return sums; &#125; public void combinationSum2(int[] candidates, int begin, List&lt;Integer&gt; sum, int target) &#123; if (target == 0) &#123; sums.add(sum); return; &#125; int pre = -1; for (int i = begin; i &lt; candidates.length &amp;&amp; candidates[i] &lt;= target; i++) &#123; //如果当前数和前一个数相同， 则此次循环直接跳过 if(pre == candidates[i]) &#123; continue; &#125; List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(sum); pre = candidates[i]; list.add(candidates[i]); combinationSum2(candidates, i + 1, list, target - candidates[i]); &#125; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>dfs</tag>
        <tag>bfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode 139]]></title>
    <url>%2F2013%2F02%2F22%2Fword-break%2F</url>
    <content type="text"><![CDATA[word break12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package com.lifeibigdata.algorithms.lcd;import java.util.ArrayDeque;import java.util.HashSet;import java.util.Queue;import java.util.Set;/** * Created by lifei on 16/10/17. */public class WordBreak &#123; public static void main(String[] args) &#123; Set&lt;String&gt; set = new HashSet&lt;&gt;(); set.add("cat"); set.add("cats"); set.add("and"); set.add("sand"); set.add("an"); set.add("dog"); System.out.println(wordBreak("catsandog",set)); &#125; static boolean wordBreak(String s, Set&lt;String&gt; wordDict) &#123; boolean[] have = new boolean[s.length()];// return help(s,0,wordDict,have);// return help2(s,wordDict,have); return help3(s,wordDict,have); &#125; static boolean help(String s, int now, Set&lt;String&gt; d, boolean[] have) &#123; if (now &gt;= s.length()) return true; if (have[now]) return false; have[now] = true; for (int i = now; i &lt; s.length(); ++i) &#123; System.out.println("---"+s.substring(now,i+1)); if (d.contains(s.substring(now,i+1))//TODO &amp;&amp; help(s,i + 1,d,have))&#123; return true; &#125; &#125; return false; &#125; static boolean help2(String s, Set&lt;String&gt; d, boolean[] have) &#123; if (s.length() == 0) return true; have[0] = true; Queue&lt;Integer&gt; q = new ArrayDeque&lt;&gt;(); for (q.add(0);!q.isEmpty();q.poll()) &#123; int now = q.peek(); for (int i = now; i &lt; s.length(); ++i) &#123; if (d.contains(s.substring(now,i + 1)))&#123; if (i + 1 &gt;= s.length()) return true; if (!have[i + 1])&#123; have[i + 1] = true; q.offer(i + 1); &#125; &#125; &#125; &#125; return false; &#125; static boolean help3(String s, Set&lt;String&gt; d, boolean[] dp) &#123; if (s.length() == 0) return true; dp[0] = true; for (int now = 0;now &lt; s.length();++now) &#123; if (!dp[now])&#123; continue; &#125; for (int i = now; i &lt; s.length(); ++i) &#123; if (d.contains(s.substring(now,i + 1)))&#123; if (i + 1 &gt;= s.length()) return true; dp[i + 1] = true; &#125; &#125; &#125; return false; &#125;&#125;]]></content>
      <categories>
        <category>lcd</category>
      </categories>
      <tags>
        <tag>lcd</tag>
        <tag>dfs</tag>
        <tag>bfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小岛的数量]]></title>
    <url>%2F2013%2F02%2F22%2F%E5%B0%8F%E5%B2%9B%E7%9A%84%E6%95%B0%E9%87%8F%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package com.lifeibigdata.algorithms.lcd;import java.util.ArrayDeque;import java.util.Queue;/** * Created by lifei on 16/10/17. */public class NumIslands &#123; public static void main(String[] args) &#123; System.out.println(numIslands(new char[][]&#123; &#123; '1','0','0','1','1'&#125;, &#123; '1','0','0','0','0'&#125;, &#123; '1','0','0','1','1'&#125;, &#123; '1','0','0','1','1'&#125; &#125;)); &#125; static int numIslands(char[][] grid)&#123; int answer = 0; for (int i = 0; i &lt; grid.length; i++) &#123; for (int j = 0; j &lt; grid[0].length; j++) &#123; if (grid[i][j] == '1')&#123; helper2(grid,i,j); ++answer; &#125; &#125; &#125; return answer; &#125; static void helper(char[][] a, int x, int y) &#123; if (x &lt; 0 || x &gt;= a.length || y &lt; 0 || y &gt;= a[x].length || a[x][y] != '1')&#123; return; &#125; a[x][y] = '0'; helper(a,x - 1,y); helper(a,x + 1,y); helper(a,x,y - 1); helper(a,x,y + 1); &#125; static void helper2(char[][] a, int x, int y) &#123; Queue&lt;pair&gt; q = new ArrayDeque&lt;&gt;(); int dx[] = &#123;-1,1,0,0&#125;; int dy[] = &#123;0,0,-1,1&#125;; a[x][y] = '0'; for (q.add(new pair(x,y));!q.isEmpty();q.poll()) &#123; x = q.peek().x; y = q.peek().y; for (int i = 0; i &lt; 4; i++) &#123; int nx = x + dx[i]; int ny = y + dy[i]; if (nx &gt;= 0 &amp;&amp; nx &lt; a.length &amp;&amp; ny &gt;=0 &amp;&amp; ny &lt; a[nx].length &amp;&amp; a[nx][ny] == '1')&#123; a[nx][ny] = '0'; q.add(new pair(nx,ny)); &#125; &#125; &#125; &#125; static class pair&#123; int x,y; public pair(int x, int y) &#123; this.x = x; this.y = y; &#125; &#125;&#125;]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百数问题]]></title>
    <url>%2F2013%2F02%2F22%2F%E7%99%BE%E6%95%B0%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package com.lifeibigdata.algorithms.tree;import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map;/** * Created by lifei on 16/9/21. */public class Sum100 &#123; static int counts; public static void main(String[] args) &#123; int a[] = &#123;1,2,3,4,5,6,7,8,9&#125;; List&lt;Pair&gt; op = new ArrayList&lt;&gt;();//某个位置存放的符号 int count = 0; calc(a,0,0,0,op,100);// System.out.println(count); &#125; //考察第cur个空位,当前表达式的值是n,最后一个数是last,操作符放置于op private static boolean calc(int[] a, int cur, int n, int last, List&lt;Pair&gt; op,int sum) &#123; if (cur == a.length - 1)&#123;//递归结束 last = 10 * last + a[a.length - 1]; if ((lastOperator(op,cur - 1)?(n + last) : (n - last)) == sum)&#123; counts++;// System.out.println(lastOperator(op,cur - 1)); print(counts,a,op,a.length); return true; &#125; return false; &#125; last = 10 * last + a[cur]; calc(a,cur + 1,n,last,op,sum);//空 boolean bAdd = lastOperator(op,cur -1); op.add(new Pair(cur,'+')); calc(a,cur + 1,bAdd ? n + last : n - last,0,op,sum);//'+' op.get(op.size() - 1).ch = '-';//TODO ??? calc(a,cur + 1,bAdd ? n + last : n - last,0,op,sum);//'-' op.remove(op.size() - 1);//回溯 return false;// return count != 0; &#125; private static void print(int count, int[] a, List&lt;Pair&gt; op, int size) &#123; Map&lt;Integer,Character&gt; map = new HashMap&lt;&gt;(); for (Pair p:op) &#123; map.put(p.value,p.ch); &#125; String s = ""; for (int i = 0; i &lt; a.length; i++) &#123; if (map.get(i) != null)&#123; s += (a[i]+" "+String.valueOf(map.get(i))+" "); &#125; else &#123; s += a[i]; &#125; &#125; System.out.println(count+"===&gt;"+s); &#125; private static boolean lastOperator(List&lt;Pair&gt; op, int i) &#123; if (op.size() == 0) return true;//TODO 同下 for (Pair p:op) &#123; if (p.value == i)&#123; if (p.ch == '+')&#123; return true; &#125; else if (p.ch == '-')&#123; return false; &#125; &#125; &#125; return op.get(op.size() - 1).ch == '+'?true:false; //TODO 如果找不到i,则返回最后一个符号 &#125; static class Pair&#123; int value; char ch; public Pair(int value, char ch) &#123; this.value = value; this.ch = ch; &#125; &#125;&#125;]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[老鼠吃奶酪]]></title>
    <url>%2F2013%2F02%2F22%2F%E8%80%81%E9%BC%A0%E5%90%83%E5%A5%B6%E9%85%AA%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798package com.lifeibigdata.algorithms.tree;import java.util.ArrayList;import java.util.List;/** * Created by lifei on 16/9/21. */public class MousePath &#123; static List&lt;List&lt;Pair&gt;&gt; all = new ArrayList&lt;&gt;(); public static void main(String[] args) &#123; int[][] chess = &#123; &#123;1,1,0,0,0,0,0,1&#125;, &#123;1,1,1,1,1,1,1,1&#125;, &#123;1,0,0,0,1,0,0,1&#125;, &#123;1,1,1,0,1,0,0,1&#125;, &#123;0,1,0,0,1,1,1,1&#125;, &#123;0,1,0,0,0,0,0,1&#125;, &#123;0,1,0,9,1,1,1,1&#125;, &#123;0,1,1,1,0,0,1,0&#125; &#125;; mouthPath(chess); for (List&lt;Pair&gt; path:all) &#123; System.out.println("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"); for (Pair pair:path) &#123; System.out.println(pair.from+"-"+pair.to); &#125; System.out.println("~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"); &#125; &#125; private static void mouthPath(int[][] chess) &#123; List&lt;Pair&gt; path = new ArrayList&lt;&gt;(); boolean[][] visit = new boolean[chess.length][chess[0].length]; //开始搜索路径 path.add(new Pair(0,0)); visit[0][0] = true; boolean b = search(chess,0,0,path,visit); System.out.println(b); &#125; private static boolean search(int[][] chess, int i, int j, List&lt;Pair&gt; path, boolean[][] visit) &#123; if (chess[i][j] == 9)&#123;// print(path); return true; &#125; int iNext[] = &#123;0,0,-1,1&#125;; int jNext[] = &#123;-1,1,0,0&#125;; int iCur,jCur; int m = chess.length; int n = chess[0].length; for (int k = 0; k &lt; 4; k++) &#123; iCur = i + iNext[k]; jCur = j + jNext[k]; if (iCur &lt; 0 || iCur &gt;= m || jCur &lt;0 || jCur &gt;= n) continue; if (!visit[iCur][jCur] &amp;&amp; chess[iCur][jCur] != 0)&#123; path.add(new Pair(iCur,jCur)); visit[iCur][jCur] = true; if (search(chess,iCur,jCur,path,visit))&#123; //如果求所有路径 List&lt;Pair&gt; d = new ArrayList&lt;&gt;();//TODO 要用到对象拷贝,因为path在后面的代码中会变化,这里需要记录path的中间状态,所以需要使用到对象拷贝 for (Pair temPair:path) &#123; d.add(new Pair(temPair.from,temPair.to)); &#125; all.add(d);// System.out.println("======================");// for (Pair pair:path) &#123;// System.out.println(pair.from+"-"+pair.to);// &#125;// System.out.println("======================");// return true;//仅仅判断是否可以吃到 &#125; path.remove(path.size() -1); visit[iCur][jCur] = false; &#125; &#125; return false; &#125;// private static void print(List&lt;Pair&gt; path) &#123;// &#125; static class Pair&#123; int from; int to; public Pair(int from, int to) &#123; this.from = from; this.to = to; &#125; &#125;&#125;]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并查集求连通分量]]></title>
    <url>%2F2013%2F02%2F22%2F%E5%B9%B6%E6%9F%A5%E9%9B%86%E6%B1%82%E8%BF%9E%E9%80%9A%E5%88%86%E9%87%8F%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class CUnionFindSet &#123; static int m_mN; static int []m_pParent; void Union(int i,int j)&#123; if(i &lt; 0 || i &gt;= m_mN || j &lt; 0 || j&gt; m_mN)&#123; return; &#125; int ri = Find(i); int rj = Find(j); if(ri != rj)&#123; m_pParent[ri] = rj; &#125; &#125; int Find(int i)&#123; if(i &lt; 0 || i &gt; m_mN)&#123; return -1; &#125; int root = i; while(root != m_pParent[root])&#123;//尚未到达根节点 root = m_pParent[root]; &#125; int t = i; int p; while (t != root)&#123; p = m_pParent[t];//t2的原父节点 m_pParent[t] = root;//t2的父节点直接赋值给根t t = p;//沿着原来的父节点继续向上查找 &#125; return root; &#125; public CUnionFindSet(int n) &#123; m_mN = n; m_pParent = new int[m_mN]; for (int i = 0; i &lt; m_mN; i++)&#123; m_pParent[i] = i; &#125; &#125; public static void main(String[] args) &#123; int N = 10;//岛屿数量 CUnionFindSet ufs = new CUnionFindSet(N); ufs.Union(2,6);//根据边初始化并查集 ufs.Union(5,6); ufs.Union(1,8); ufs.Union(2,9); ufs.Union(5,3); ufs.Union(4,8); ufs.Union(4,0); int []component = new int[N]; for (int i = 0; i &lt; N; i++)&#123;//计算每个岛屿的“首府” component[ufs.Find(i)]++; &#125; int nComponent = 0;//统计“首府”的数目 for (int i = 0; i &lt; N; i++)&#123; if(component[i] != 0)&#123; nComponent++; &#125; &#125; System.out.println(nComponent); &#125;&#125;]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最小生成树]]></title>
    <url>%2F2013%2F02%2F22%2F%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%2F</url>
    <content type="text"><![CDATA[最小生成树的方法Kruskal算法和Prim算法。二者都是基于贪心思想的，使用贪心是需要证明的。 Prim（普里姆算法）从一个顶点出发，逐个找到各个顶点上权值最小的边来构成最小生成树，针对顶点http://blog.csdn.net/pigli/article/details/5776587 Kruskal(克鲁斯卡尔)找到各个节点最小权值的边,关键判断是否有回路，针对边http://www.icoolxue.com/play/3164]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最短路径]]></title>
    <url>%2F2013%2F02%2F22%2F%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%2F</url>
    <content type="text"><![CDATA[Dijkstra（迪杰斯特拉 权值都是正的）&nbsp;&nbsp;&nbsp;&nbsp;是典型的单源最短路径算法，用于计算一个节点到其他所有节点的最短路径。主要特点是以起始点为中心向外层层扩展，直到扩展到终点为止算法的输入是：&nbsp;&nbsp;&nbsp;有权（有向）图&nbsp;&nbsp;&nbsp;起点（源）&nbsp;&nbsp;&nbsp;所有边的权非负算法的输出是：&nbsp;&nbsp;&nbsp;起点（源）到其他各点的最短路径 Floyd（弗洛伊德 可以带负权值）是解决任意两点间的最短路径的一种算法，可以正确处理有向图或负权的最短路径问题，同时也被用于计算有向图的传递闭包。Floyd-Warshall算法的时间复杂度为O(N3)，空间复杂度为O(N2) Bellman-Ford（伯尔曼 单源最短路径可以带负权值,比Dijkstra要广）&nbsp;&nbsp;&nbsp;&nbsp;首先假设源点到所有点的距离为无穷大，然后从任一顶点u出发，遍历其它所有顶点vi，计算从源点到其它顶点vi的距离与从vi到u的距离的和，如果比原来距离小，则更新，遍历完所有的顶点为止，即可求得源点到所有顶点的最短距离。 有向无权图的最短路径算法(广搜)http://blog.csdn.net/zhutulang/article/details/7886747 无向无权图的最短路径算法 Dijkstra1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public class Dijkstra &#123; static int max = Integer.MAX_VALUE; static int dist[] = new int[6]; static int prev[] = new int[6]; static int a[][] = &#123; &#123;0,max,10,max,30,100&#125;, &#123;max,0,5,max,max,max&#125;, &#123;max,max,0,50,max,max&#125;, &#123;max,max,max,0,max,10&#125;, &#123;max,max,max,20,0,60&#125;, &#123;max,max,max,max,max,0&#125; &#125;; void dijkstra(int v,int a[][], int dist[], int prev[])&#123; int n = dist.length - 1; boolean s[] = new boolean[n+1]; for (int i = 1; i&lt;= n;i++)&#123; dist[i] = a[v][i]; s[i] = false; if (dist[i] &lt; Integer.MAX_VALUE) prev[i] = v; else prev[i] = -1; &#125; dist[v] = 0; s[v] = true; for (int i=1;i&lt;=n;i++)&#123; int temp = Integer.MAX_VALUE; int u = v; for (int j =1; j&lt;= n;j++)&#123; if (!s[j] &amp;&amp; dist[j] &lt;temp)&#123; u = j; temp = dist[j]; &#125; &#125; s[u] = true; for (int j = 0;j &lt;= n; j++)&#123; if(!s[j] &amp;&amp; a[u][j] &lt; Integer.MAX_VALUE)&#123; int newDist = dist[u] + a[u][j]; if (newDist &lt; dist[j])&#123; dist[j] = newDist; prev[j] = u; &#125; &#125; &#125; &#125; &#125; void outPath(int m, int p[],int []d)&#123; for (int i = 0; i&lt; dist.length; i++)&#123; if (d[i] &lt; Integer.MAX_VALUE &amp;&amp; i != m)&#123; System.out.print(&quot;v&quot;+i+&quot;&lt;--&quot;); int next = p[i]; while (next != m)&#123; System.out.print(&quot;v&quot;+next+&quot;&lt;--&quot;); next = p[next]; &#125; System.out.println(&quot;v&quot;+m+&quot;:&quot;+d[i]); &#125; else if( i != m) System.out.println(&quot;v&quot;+i+&quot;&lt;--&quot;+&quot;v&quot;+m+&quot;:no path&quot;); &#125; &#125; public static void main(String[] args) &#123; Dijkstra d = new Dijkstra(); d.dijkstra(0,a,dist,prev); d.outPath(0,prev,dist); &#125;&#125; Floyd12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import java.util.ArrayList;public class Floyd &#123; /* * 给出一个含有n个顶点的带权有向图，要求其每一对顶点之间的最短路径。 * 这里采用佛洛依德(Floyd)最短路径算法： */ private static int max=Integer.MAX_VALUE; private static int [][]dist=new int[6][6]; //存储最短路径 private static int [][]path=new int[6][6]; //存储最短路径的长度 private static ArrayList list=new ArrayList&lt;Integer&gt;(); private static int [][]Arcs=&#123; &#123;max,max,10,max,30,100&#125;, &#123;max,max,5,max,max,max&#125;, &#123;max,max,max,50,max,max&#125;, &#123;max,max,max,max,20,10&#125;, &#123;max,max,max,max,max,60&#125;, &#123;max,max,max,max,max,max&#125; &#125;; public void findCheapestPath(int begin,int end,int Arcs[][])&#123; floyd(Arcs); list.clear(); list.add(begin); findPath(begin,end); list.add(end); &#125; public void findPath(int i,int j)&#123; int k=path[i][j]; if(k==-1) return ; findPath(i,k); list.add(k); findPath(k,j); &#125; public void floyd(int [][] Arcs)&#123; int n=Arcs.length; for(int i=0;i&lt;n;i++) for(int j=0;j&lt;n;j++)&#123; path[i][j]=-1; //初始化当前的路径长度表 dist[i][j]=Arcs[i][j]; //初始化当前的路径表 &#125; for(int k=0;k&lt;n;k++) for(int i=0;i&lt;n;i++) for(int j=0;j&lt;n;j++)&#123; if(dist[i][k]!=max&amp;&amp;dist[k][j]!=max&amp;&amp;dist[i][k]+dist[k][j]&lt;dist[i][j])&#123; dist[i][j]=dist[i][k]+dist[k][j]; path[i][j]=k; &#125; &#125; &#125; public static void main(String[] args) &#123; // TODO Auto-generated method stub Floyd f=new Floyd(); for(int i=0;i&lt;Arcs.length;i++) for(int j=0;j&lt;Arcs.length;j++)&#123; f.findCheapestPath(i, j, Arcs); ArrayList&lt;Integer&gt;L=f.list; System.out.print(i+&quot;--&gt;&quot;+j+&quot;:&quot;); if(f.dist[i][j]==max)&#123; System.out.println(&quot;之间没有最短路径&quot;); System.out.println(); &#125; else&#123; System.out.println(&quot;的最短路径是：&quot;); System.out.print(L.toString()+&quot; &quot;); System.out.println(&quot;路径长度:&quot;+f.dist[i][j]); System.out.println(); &#125; &#125; &#125;&#125; bellmanford12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import java.io.*;import java.util.*;public class bellmanford &#123; final public static int MAX = 1000000000; // Short driver program to test the Bellman Ford&apos;s method. public static void main(String[] args) &#123; // Read in graph. int[][] adj = new int[5][5]; Scanner fin = new Scanner(System.in); int numEdges = 0; for (int i = 0; i&lt;25; i++) &#123; adj[i/5][i%5] = fin.nextInt(); if (adj[i/5][i%5] != 0) numEdges++; &#125; // Form edge list. edge[] eList = new edge[numEdges]; int eCnt = 0; for (int i = 0; i&lt;25; i++) if (adj[i/5][i%5] != 0) eList[eCnt++] = new edge(i/5, i%5, adj[i/5][i%5]); // Run algorithm and print out shortest distances. int[] answers = bellmanford(eList, 5, 0); for (int i=0; i&lt;5; i++) System.out.print(answers[i]+&quot; &quot;); System.out.println(); &#125; // Returns the shortest paths from vertex source to the rest of the // vertices in the graph via Bellman Ford&apos;s algorithm. public static int[] bellmanford(edge[] eList, int numVertices, int source) &#123; // This array will store our estimates of shortest distances. int[] estimates = new int[numVertices]; // Set these to a very large number, larger than any path in our // graph could possibly be. for (int i=0; i&lt;estimates.length; i++) estimates[i] = MAX; // We are already at our source vertex. estimates[source] = 0; // Runs v-1 times since the max number of edges on any shortest path is v-1, if // there are no negative weight cycles. for (int i=0; i&lt;numVertices-1; i++) &#123; // Update all estimates based on this particular edge only. for (edge e: eList) &#123; if (estimates[e.v1]+e.w &lt; estimates[e.v2]) estimates[e.v2] = estimates[e.v1] + e.w; &#125; &#125; return estimates; &#125;&#125;class edge &#123; public int v1; public int v2; public int w; public edge(int a, int b, int c) &#123; v1 = a; v2 = b; w = c; &#125; public void negate() &#123; w = -w; &#125;&#125;]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创建线程的四种方法以及区别]]></title>
    <url>%2F2013%2F02%2F22%2F%E5%88%9B%E5%BB%BA%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%9B%9B%E7%A7%8D%E6%96%B9%E6%B3%95%E4%BB%A5%E5%8F%8A%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[Java使用Thread类代表线程，所有的线程对象都必须是Thread类或其子类的实例。Java可以用四种方式来创建线程，如下所示：1）继承Thread类创建线程2）实现Runnable接口创建线程3）使用Callable和Future创建线程4）使用线程池例如用Executor框架]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[原子变量(AtomicLong, AtomicInteger, AtomicReference)]]></title>
    <url>%2F2013%2F02%2F22%2F%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[关于Atomic的几个方法getAndSet() : 设置新值，返回旧值.compareAndSet(expectedValue, newValue) : 如果当前值(current value)等于期待的值(expectedValue), 则原子地更新指定值为新值(newValue), 如果更新成功，返回true, 否则返回false, 换句话可以这样说: 将原子变量设置为新的值, 但是如果从我上次看到的这个变量之后到现在被其他线程修改了(和我期望看到的值不符), 那么更新失败 原理是原子变量+CAS（硬件cpu提供的原子操作）来实现的 public final boolean compareAndSet(V expect, V update) { return unsafe.compareAndSwapObject(this, valueOffset, expect, update); }CAScompare and set]]></content>
      <categories>
        <category>juc</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
</search>
