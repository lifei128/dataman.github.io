<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[KMP]]></title>
    <url>%2F2019%2F02%2F14%2FKMP%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637package bluebridgecup.array.kmp;public class Kmp &#123; public static void main(String[] args) &#123; int[] next = new int[14]; char[] p = &quot;abcdabdefgabca&quot;.toCharArray(); getNext(p,next); for (int n:next) &#123; System.out.println(n); &#125; &#125; static void getNext(char[] p,int next[])&#123; int nLen = p.length; next[0] = -1;//todo 初始化为-1，所以 int k = -1; int j = 0;// next[j]代表[0, j - 1]区段中最长相同真前后缀的长度 while (j &lt; nLen - 1)&#123; //j //针对数组p,p[k]表示前项,p[j]表示后项;因为k=-1,j=0 //注:k==-1表示未找到k前缀与k后缀相等,首次分析可先忽略// 细心的朋友会问if语句中k == -1存在的意义是何？第一，程序刚运行时，k是被初始为-1，直接进行P[i] == P[k]判断无疑会边界溢出；// 第二，else语句中k = next[k]，k是不断后退的，若k在后退中被赋值为-1（也就是j = next[0]），在P[i] == P[k]判断也会边界溢出。// 综上两点，其意义就是为了特殊边界判断。 if (k &gt; -1)System.out.println(p[j]+&quot;***&quot;+p[k]+&quot;$$$&quot;+j+&quot;,&quot;+k); if (k == -1 || p[j] == p[k])&#123;//匹配成功 ++j; ++k; next[j] = k;//todo 首次是next[1] = 0;next中存的是索引;next[0] = -1是初始化的，不用再计算 &#125; else &#123; //p[j]与p[k]失配,则继续递归计算前缀p[next[k]] k = next[k];//todo next[0]=-1是初始化的，该出逻辑是判断k的取值为next数组的前一项，前一项如果为-1，则k=-1,那么下一次循环会进入两一个逻辑 &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[递归]]></title>
    <url>%2F2019%2F02%2F14%2F%E8%93%9D%E6%A1%A5%E6%9D%AF-%E5%BE%AA%E7%8E%AF%2F</url>
    <content type="text"><![CDATA[任何循环都可以改为递归，关键是发现逻辑“相似性”和递归出口。有些语言没有循环只有递归，如lisp和clojure。拓展尾递归。 数组求和12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package bluebridgecup;public class A01_SumArray &#123; public static int f1(int[] arr) &#123; int sum = 0; for (int i = 0; i &lt; arr.length; i++) &#123; sum += arr[i]; &#125; return sum; &#125; //**************************线性递归************************** public static int f2(int[] arr,int start) &#123; if (start == arr.length) //递归出口 return 0; return arr[start] + f2(arr,start+1); &#125; public static int f3_2(int[] arr,int end) &#123; if (end &lt; 0) //递归出口 return 0; return f3_2(arr,end-1) + arr[end]; &#125; //**************************二分递归************************** public static int f4(int[] arr,int start,int end) &#123; if (start &gt; end) return 0;//递归出口 int mid = (start + end)/2; return f4(arr,start,mid-1) + arr[mid] + f4(arr,mid+1,end);//todo 不如下面的方式 &#125; public static int f5(int[] arr,int start,int end) &#123; if (start == end) return arr[start];//递归出口 int mid = (start + end)/2; return f5(arr,start,mid) + f5(arr,mid+1,end);//注意分为mid mid+1 &#125; public static void main(String[] args) &#123;// int[] arr = new int[]&#123;1,2,3,4,5,6,7,8,9,10&#125;;// System.out.println(f1(arr));// System.out.println(f2(arr,0));// System.out.println(f3_2(arr,arr.length - 1));// System.out.println(f4(arr,0,arr.length - 1));// System.out.println(f5(arr,0,arr.length - 1)); &#125;&#125; 打印数组123456789101112131415161718192021222324252627282930package bluebridgecup;public class A02_PrintArray &#123; private static void f1(int n) &#123; if (n &gt; 0) f1(n - 1); System.out.println(n); &#125; private static void f2(int start, int end) &#123; if (start &gt; end)return; System.out.println(start); f2(start+1,end); &#125; public static void f_arr(int[] arr,int start)&#123; if (start &gt; arr.length - 1)return; System.out.println(arr[start]); f_arr(arr,start+1); &#125; public static void main(String[] args)&#123;// f1(10);// f2(0,10);// int[] arr = new int[]&#123;1,2,3,4,5,6,7,8,9,10&#125;;// fa(arr,0); &#125;&#125; 字符串是否相同1234567891011121314151617181920212223242526package bluebridgecup;public class A03_IsSameString &#123; private static boolean isSameString2(String s1, String s2) &#123; if (s1.length() != s2.length()) return false;//******边界******** if (s1.length() == 0) return true;//******边界********// if (s1.charAt(0) != s2.charAt(0)) return false; if (s1.charAt(0) == s2.charAt(0)) return true; return isSameString2(s1.substring(1),s2.substring(1)); &#125; public static boolean isSameString1(String s1,String s2)&#123; return s1.equals(s2); &#125; public static void main(String[] args) &#123;// String s1 = &quot;abc&quot;;// String s2 = &quot;abcd&quot;;// System.out.println(isSameString1(s1,s2));// System.out.println(isSameString2(s1,s2)); &#125;&#125; 求阶乘123456789101112package bluebridgecup;public class A04_FactorialOfN &#123; public static void main(String[] args) &#123; System.out.println(helper(3)); &#125; private static int helper(int n) &#123; if (n &gt; 0)return n * helper(n - 1); return 1; &#125;&#125; 求n个不同元素的全排列*12345678910111213141516171819202122232425262728293031323334package bluebridgecup;/** * 求n个不同元素的全排列 * * 回溯问题，用于八皇后和迷宫问题 * 如果包含重复元素怎么办? * 罗列出每一种可能用什么方法?动态规划怎么实现 * * * R的全排列可归纳定义如下： * 当n=1时，perm(R)=(r)，其中r是集合R中唯一的元素； * 当n&gt;1时，perm(R)由(r1)perm(R1)，(r2)perm(R2)，…，(rn)perm(Rn)构成。 * 实现思想：将整组数中的所有的数分别与第一个数交换，这样就总是在处理后n-1个数的全排列。 */public class A05_FullPermutation &#123; public static void main(String[] args) &#123; char[] chars = &quot;ABC&quot;.toCharArray(); helper(chars,0); &#125; //k:当前的交换位置(关注点),与其后的元素交换 private static void helper(char[] chars,int k) &#123; if (k == chars.length)&#123; for (char c:chars) System.out.print(c +&quot; &quot;); System.out.println(); &#125; for (int i = k; i &lt; chars.length; i++) &#123; &#123;char t = chars[k];chars[k] = chars[i];chars[i] = t;&#125;//试探 helper(chars,k + 1); &#123;char t = chars[k];chars[k] = chars[i];chars[i] = t;&#125;//回溯 &#125; &#125;&#125; 在n个求中，任意取出m个，求有多少种不同取法12345678910111213141516171819package bluebridgecup;/** * 在n个求中，任意取出m个，求有多少种不同取法 */public class A06_GraspBall &#123; //从n个取m个 public static void main(String[] args) &#123; int k = helper(5,3); System.out.println(k); &#125; private static int helper(int n, int m) &#123; if (n &lt; m) return 0; if (n == m) return 1; if (m == 0) return 1; return helper(n - 1,m - 1) + helper(n - 1,m);//n个里有个特殊球x，取法划分，包不包含x &#125;&#125; 求最长公共子序列的长度12345678910111213141516171819package bluebridgecup;public class A07_MaxSequence &#123; public static void main(String[] args) &#123; String s1 = &quot;abc&quot;; String s2 = &quot;xbacd&quot;; int x = helper(s1,s2); System.out.println(x); &#125; private static int helper(String s1, String s2) &#123; if (s1.length() == 0 || s2.length() == 0)return 0; if (s1.charAt(0) == s2.charAt(0))&#123; return 1 + helper(s1.substring(1),s2.substring(1)); &#125; else &#123; return Math.max(helper(s1,s2.substring(1)),helper(s1.substring(1),s2)); &#125; &#125;&#125; 翻转字符串123456789101112package bluebridgecup;public class A08_Reverse &#123; public static void main(String[] args) &#123; System.out.println(helper_str(&quot;12345&quot;));; &#125; private static String helper_str(String s) &#123; if (s.length() == 0)return &quot;&quot;; return helper_str(s.substring(1))+s.charAt(0); &#125;&#125; 杨辉三角1234567891011121314151617181920212223242526272829package bluebridgecup;/** * 1 * 1 1 * 1 2 1 * 1 3 3 1 * 1 4 6 4 1 * 1 5 10 10 5 1 * * 计算第m层，第n个系数，m和n都从0开始 */public class A09_YangTriangle &#123; public static void main(String[] args) &#123; int level = 5; for (int i = 0; i &lt;= level; i++) &#123; System.out.print(helper(level,i) +&quot; &quot;); &#125; System.out.println(); &#125; //m层第n个元素 private static int helper(int m,int n)&#123; if (n == 0)return 1; if (m == n)return 1; return helper(m - 1,n) + helper(m - 1,n - 1); &#125;&#125; m个a,n个b 可以有多少种组合123456789101112package bluebridgecup;// m个a,n个b 可以有多少种组合public class A10_MN_sort &#123; public static void main(String[] args) &#123; System.out.println(helper(3,2)); &#125; private static int helper(int m, int n) &#123; if (m == 0 || n == 0)return 1; return helper(m - 1,n) + helper(m,n - 1); &#125;&#125; split 612345678910111213141516171819202122232425262728package bluebridgecup;public class A11_Split_6 &#123; public static void main(String[] args) &#123; int[] a = new int[100];//做缓冲 helper(3,a,0); &#125; //对n进行加法划分 // a:缓冲 // k:当前的位置 private static void helper(int n,int[] a,int k) &#123; if (n &lt;= 0)&#123; for(int i=0;i &lt; k;i++)System.out.printf(a[i] + &quot; &quot;); System.out.println(); return; &#125; //6 //5...f(1) //4...f(2) for (int i = n; i &gt; 0; i--) &#123; if (k &gt; 0 &amp;&amp; i &gt; a[k - 1])continue;//a[k - 1]表示前一项 ; 后一项 &gt; 前一项 a[k] = i; helper(n - i,a,k + 1); &#125; &#125;&#125; ErrorSum12345678910111213141516171819202122232425262728293031package bluebridgecup;public class A12_ErrorSum &#123; public static void main(String[] args) &#123; int sum = 6; int[] a = &#123;3,2,4,3,1&#125;; boolean[] b = new boolean[a.length];//表示a对应项是否选取 helper(sum,a,0,0,b); &#125; //error_sum:有错误的和 //a：明细 //k:当前处理的位置 //cur_sum:前边元素的累加和 //b:记录取舍 private static void helper(int error_sum, int[] a, int k, int cur_sum, boolean[] b) &#123; if (cur_sum &gt; error_sum)return; if (error_sum == cur_sum)&#123; for(int i = 0;i &lt; b.length;i++) if (b[i] == true)System.out.printf(a[i]+&quot; &quot;); System.out.println(); return; &#125; if (k &gt;= a.length)return; b[k] = false; helper(error_sum,a,k + 1,cur_sum,b); b[k] = true; cur_sum += a[k]; helper(error_sum,a,k + 1,cur_sum,b); b[k] = false;//回溯！！！！！ &#125;&#125; 打印二叉树从根节点到叶子节点的所有路径1234567891011121314151617181920212223242526272829303132333435363738394041424344package bluebridgecup;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;//打印二叉树从根节点到叶子节点的所有路径public class A13_BSTTreePath &#123; List&lt;String&gt; res = new ArrayList&lt;&gt;(); public List&lt;String&gt; binaryTreePaths(TreeNode root) &#123; if (root == null)return new ArrayList&lt;&gt;(); LinkedList&lt;Integer&gt; list = new LinkedList&lt;&gt;(); helper(root,list); return res; &#125; private void helper(TreeNode node,LinkedList&lt;Integer&gt; list) &#123; if (node == null) return; list.add(node.val); if (node.left == null &amp;&amp; node.right == null)&#123; String s = &quot;&quot;; for (Integer n:list) &#123; if (s.equals(&quot;&quot;))&#123; s += n+&quot;&quot;; &#125; else &#123; s = s + &quot;-&gt;&quot;+ n; &#125; &#125; res.add(s); list.removeLast();//TODO 最后一个节点是叶子节点,继续下一条路线,所以要剔除最后一个 return;//TODO 该次结束，返回到上一层 &#125; helper(node.left,list); helper(node.right,list); list.removeLast();//TODO 返回时一定要清除 最后一个节点是最后一个叉的根节点,一定是要排除的,因为这个节点的左右方向都走完了 &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125; 汉诺塔1234567891011121314151617package bluebridgecup;public class A14_Hanoi &#123; public static void main(String[] args) &#123; hanoi(2,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;); &#125; public static void hanoi(int n,String start,String middle,String end)&#123; if (n &lt;= 1)&#123; System.out.println(start+&quot;--&gt;&quot;+end); &#125; else &#123; hanoi(n -1,start,end,middle); System.out.println(start+&quot;--&gt;&quot;+end); hanoi(n - 1,middle,start,end); &#125; &#125;&#125; 遍历二叉树1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package bluebridgecup;import java.util.ArrayList;public class A15_TreeRec &#123; /** * 中序遍历递归解法 * （1）如果二叉树为空，空操作。 * （2）如果二叉树不为空，中序遍历左子树，访问根节点，中序遍历右子树 */ public void inOrderRec(TreeNode root)&#123; if (root == null)return; inOrderRec(root.left); System.out.println(root.val); inOrderRec(root.right); &#125; /** * 分层遍历二叉树（递归） * 很少有人会用递归去做level traversal * 基本思想是用一个大的ArrayList，里面包含了每一层的ArrayList。 * 大的ArrayList的size和level有关系 * */ public static void levelTraversalRec(TreeNode root) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; ret = new ArrayList&lt;ArrayList&lt;Integer&gt;&gt;(); dfs(root, 0, ret); System.out.println(ret); &#125; private static void dfs(TreeNode root, int level, ArrayList&lt;ArrayList&lt;Integer&gt;&gt; ret)&#123; if(root == null)&#123; return; &#125; // 添加一个新的ArrayList表示新的一层 if(level &gt;= ret.size())&#123; ret.add(new ArrayList&lt;Integer&gt;()); &#125; ret.get(level).add(root.val); // 把节点添加到表示那一层的ArrayList里 dfs(root.left, level+1, ret); // 递归处理下一层的左子树和右子树 dfs(root.right, level+1, ret); &#125; /** * 求二叉树中的节点个数递归解法： O(n) * （1）如果二叉树为空，节点个数为0 * （2）如果二叉树不为空，二叉树节点个数 = 左子树节点个数 + 右子树节点个数 + 1 */ public static int getNodeNumRec(TreeNode root) &#123; if (root == null) &#123; return 0; &#125; else &#123; return getNodeNumRec(root.left) + getNodeNumRec(root.right) + 1; //左 右节点加上主节点1为总数 &#125; &#125; /** * 求二叉树的深度（高度） 递归解法： O(n) * （1）如果二叉树为空，二叉树的深度为0 * （2）如果二叉树不为空，二叉树的深度 = max(左子树深度， 右子树深度) + 1 * * * maxDepth() 1. 如果树为空，那么返回0 2. 否则 (a) 递归得到左子树的最大高度 例如，调用maxDepth( tree-&gt; left-subtree ) (b) 递归得到右子树的最大高度 例如，调用maxDepth( tree-&gt; right-subtree ) (c) 对于当前节点，取左右子树高度的最大值并加1。 max_depth = max(左子树的最大高度, 右子树的最大高度) + 1 (d) 返回max_depth */ public static int getDepthRec(TreeNode root) &#123; if (root == null) &#123; return 0; &#125; int leftDepth = getDepthRec(root.left); int rightDepth = getDepthRec(root.right); return Math.max(leftDepth, rightDepth) + 1; // +1是因为根节点已经是一层了,否则root==null直接是0了 &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125; 翻转链表1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.lifeibigdata.offer;import java.util.Stack;/** * 从尾到头打印链表 * Created by lifei on 16/11/13. */public class PrintListReverse &#123; private static class Node &#123; int val; Node next; public Node(int val) &#123; this.val = val; &#125; &#125; public static void main(String[] args) &#123; Node n1 = new Node(1); Node n2 = new Node(2);Node n3 = new Node(3); Node n4 = new Node(4);Node n5 = new Node(5); n1.next = n2;n2.next = n3;n3.next = n4;n4.next = n5; printListReverse2(n1); &#125; static void printListReverse1(Node head)&#123; Stack&lt;Node&gt; stack = new Stack(); Node tmpNode = head; while (tmpNode != null)&#123; stack.push(tmpNode); tmpNode = tmpNode.next; &#125; while (!stack.isEmpty())&#123; System.out.println(stack.pop().val); &#125; &#125; /** * 栈的本质是递归,要实现反过来输出链表,我们访问到一个节点的时候,先递归输出后面的节点,再输入该节点本身,这样链表的输出结果就反过来了 * @param head */ static void printListReverse2(Node head)&#123; if (head != null)&#123; if (head.next != null)&#123; printListReverse2(head.next); &#125; System.out.println(head.val); &#125; &#125;&#125;]]></content>
      <categories>
        <category>recursion</category>
      </categories>
      <tags>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ml 面试题目]]></title>
    <url>%2F2019%2F02%2F14%2Fml%2F</url>
    <content type="text"><![CDATA[ALS算法原理？答：对于user-product-rating数据，als会建立一个稀疏的评分矩阵，其目的就是通过一定的规则填满这个稀疏矩阵。als会对稀疏矩阵进行分解，分为用户-特征值，产品-特征值，一个用户对一个产品的评分可以由这两个矩阵相乘得到。通过固定一个未知的特征值，计算另外一个特征值，然后交替反复进行最小二乘法，直至差平方和最小，即可得想要的矩阵。 kmeans算法原理？随机初始化中心点范围，计算各个类别的平均值得到新的中心点。重新计算各个点到中心值的距离划分，再次计算平均值得到新的中心点，直至各个类别数据平均值无变化。 canopy算法原理？根据两个阈值来划分数据，以随机的一个数据点作为canopy中心。计算其他数据点到其的距离，划入t1、t2中，划入t2的从数据集中删除，划入t1的其他数据点继续计算，直至数据集中无数据。 朴素贝叶斯分类算法原理？对于待分类的数据和分类项，根据待分类数据的各个特征属性，出现在各个分类项中的概率判断该数据是属于哪个类别的。 关联规则挖掘算法apriori原理？一个频繁项集的子集也是频繁项集，针对数据得出每个产品的支持数列表，过滤支持数小于预设值的项，对剩下的项进行全排列，重新计算支持数，再次过滤，重复至全排列结束，可得到频繁项和对应的支持数。]]></content>
      <categories>
        <category>面试题目</category>
      </categories>
      <tags>
        <tag>面试题目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试题目]]></title>
    <url>%2F2019%2F02%2F14%2F%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[简答说一下hadoop的map-reduce编程模型首先map task会从本地文件系统读取数据，转换成key-value形式的键值对集合 使用的是hadoop内置的数据类型，比如longwritable、text等 将键值对集合输入mapper进行业务处理过程，将其转换成需要的key-value在输出 之后会进行一个partition分区操作，默认使用的是hashpartitioner，可以通过重写hashpartitioner的getpartition方法来自定义分区规则 之后会对key进行进行sort排序，grouping分组操作将相同key的value合并分组输出，在这里可以使用自定义的数据类型，重写WritableComparator的Comparator方法来自定义排序规则，重写RawComparator的compara方法来自定义分组规则 之后进行一个combiner归约操作，其实就是一个本地段的reduce预处理，以减小后面shufle和reducer的工作量 reduce task会通过网络将各个数据收集进行reduce处理，最后将数据保存或者显示，结束整个job hadoop的TextInputFormat作用是什么，如何自定义实现InputFormat会在map操作之前对数据进行两方面的预处理1是getSplits，返回的是InputSplit数组，对数据进行split分片，每片交给map操作一次2是getRecordReader，返回的是RecordReader对象，对每个split分片进行转换为key-value键值对格式传递给map 常用的InputFormat是TextInputFormat，使用的是LineRecordReader对每个分片进行键值对的转换，以行偏移量作为键，行内容作为值 自定义类继承InputFormat接口，重写createRecordReader和isSplitable方法在createRecordReader中可以自定义分隔符 hadoop和spark的都是并行计算，那么他们有什么相同和区别两者都是用mr模型来进行并行计算，hadoop的一个作业称为job，job里面分为map task和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束 spark用户提交的任务成为application，一个application对应一个sparkcontext，一个application中存在多个job，每触发一次action操作就会产生一个job,这些job可以并行或串行执行，每个job中有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行，executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算 hadoop的job只有map和reduce操作，表达能力比较欠缺而且在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系 spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作如join，groupby等，而且通过DAG图可以实现良好的容错 为什么要用flume导入hdfs，hdfs的构架是怎样的flume可以实时的导入数据到hdfs中，当hdfs上的文件达到一个指定大小的时候会形成一个文件，或者超过指定时间的话也形成一个文件 文件都是存储在datanode上面的，namenode记录着datanode的元数据信息，而namenode的元数据信息是存在内存中的，所以当文件切片很小或者很多的时候会卡死 简单说一下hadoop和spark的shuffle过程hadoop：map端保存分片数据，通过网络收集到reduce端spark：spark的shuffle是在DAGSchedular划分Stage的时候产生的，TaskSchedule要分发Stage到各个worker的executor 减少shuffle可以提高性能 Hive中存放是什么？表。存的是和hdfs的映射关系，hive是逻辑上的数据仓库，实际操作的都是hdfs上的文件，HQL就是用sql语法来写的mr程序。 Flume工作机制是什么？核心概念是agent，里面包括source、chanel和sink三个组件。source运行在日志收集节点进行日志采集，之后临时存储在chanel中，sink负责将chanel中的数据发送到目的地。只有成功发送之后chanel中的数据才会被删除。首先书写flume配置文件，定义agent、source、chanel和sink然后将其组装，执行flume-ng命令。 Sqoop工作原理是什么？hadoop生态圈上的数据传输工具。可以将关系型数据库的数据导入非结构化的hdfs、hive或者bbase中，也可以将hdfs中的数据导出到关系型数据库或者文本文件中。使用的是mr程序来执行任务，使用jdbc和关系型数据库进行交互。import原理：通过指定的分隔符进行数据切分，将分片传入各个map中，在map任务中在每行数据进行写入处理没有reduce。export原理：根据要操作的表名生成一个java类，并读取其元数据信息和分隔符对非结构化的数据进行匹配，多个map作业同时执行写入关系型数据库 Hbase行健列族的概念，物理模型，表的设计原则？行健：是hbase表自带的，每个行健对应一条数据。列族：是创建表时指定的，为列的集合，每个列族作为一个文件单独存储，存储的数据都是字节数组，其中的数据可以有很多，通过时间戳来区分。物理模型：整个hbase表会拆分为多个region，每个region记录着行健的起始点保存在不同的节点上，查询时就是对各个节点的并行查询，当region很大时使用.META表存储各个region的起始点，-ROOT又可以存储.META的起始点。rowkey的设计原则：各个列簇数据平衡，长度原则、相邻原则，创建表的时候设置表放入regionserver缓存中，避免自动增长和时间，使用字节数组代替string，最大长度64kb，最好16字节以内，按天分表，两个字节散列，四个字节存储时分毫秒。列族的设计原则：尽可能少（按照列族进行存储，按照region进行读取，不必要的io操作），经常和不经常使用的两类数据放入不同列族中，列族名字尽可能短。 Spark Streaming和Storm有何区别？一个实时毫秒一个准实时亚秒，不过storm的吞吐率比较低 mllib支持的算法？大体分为四大类，分类、聚类、回归、协同过滤。 Hadoop平台集群配置、环境变量设置？zookeeper：修改zoo.cfg文件，配置dataDir，和各个zk节点的server地址端口，tickTime心跳时间默认是2000ms，其他超时的时间都是以这个为基础的整数倍，之后再dataDir对应目录下写入myid文件和zoo.cfg中的server相对应。 hadoop：修改hadoop-env.sh配置java环境变量core-site.xml配置zk地址，临时目录等hdfs-site.xml配置nn信息，rpc和http通信地址，nn自动切换、zk连接超时时间等yarn-site.xml配置resourcemanager地址mapred-site.xml配置使用yarnslaves配置节点信息格式化nn和zk。 hbase：修改hbase-env.sh配置java环境变量和是否使用自带的zkhbase-site.xml配置hdfs上数据存放路径，zk地址和通讯超时时间、master节点regionservers配置各个region节点zoo.cfg拷贝到conf目录下 spark：安装Scala修改spark-env.sh配置环境变量和master和worker节点配置信息 环境变量的设置：直接在/etc/profile中配置安装的路径即可，或者在当前用户的宿主目录下，配置在.bashrc文件中，该文件不用source重新打开shell窗口即可，配置在.bash_profile的话只对当前用户有效。 Hadoop性能调优？调优可以通过系统配置、程序编写和作业调度算法来进行。hdfs的block.size可以调到128/256（网络很好的情况下，默认为64）调优的大头：mapred.map.tasks、mapred.reduce.tasks设置mr任务数（默认都是1）mapred.tasktracker.map.tasks.maximum每台机器上的最大map任务数mapred.tasktracker.reduce.tasks.maximum每台机器上的最大reduce任务数mapred.reduce.slowstart.completed.maps配置reduce任务在map任务完成到百分之几的时候开始进入这个几个参数要看实际节点的情况进行配置，reduce任务是在33%的时候完成copy，要在这之前完成map任务，（map可以提前完成）mapred.compress.map.output,mapred.output.compress配置压缩项，消耗cpu提升网络和磁盘io合理利用combiner注意重用writable对象 spark有哪些组件？（1）master：管理集群和节点，不参与计算。（2）worker：计算节点，进程本身不参与计算，和master汇报。（3）Driver：运行程序的main方法，创建spark context对象。（4）spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件。（5）client：用户提交程序的入口。 spark工作机制？用户在client端提交作业后，会由Driver运行main方法并创建spark context上下文。执行add算子，形成dag图输入dagscheduler，按照add之间的依赖关系划分stage输入task scheduler。task scheduler会将stage划分为task set分发到各个节点的executor中执行。 spark的优化怎么做？通过spark-env文件、程序中sparkconf和set property设置。（1）计算量大，形成的lineage过大应该给已经缓存了的rdd添加checkpoint，以减少容错带来的开销。（2）小分区合并，过小的分区造成过多的切换任务开销，使用repartition。 kafka工作原理？producer向broker发送事件，consumer从broker消费事件。事件由topic区分开，每个consumer都会属于一个group。相同group中的consumer不能重复消费事件，而同一事件将会发送给每个不同group的consumer。 转：https://www.cnblogs.com/jchubby/p/5449379.html]]></content>
      <categories>
        <category>面试题目</category>
      </categories>
      <tags>
        <tag>面试题目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark算子优化]]></title>
    <url>%2F2019%2F02%2F14%2Fspark%E7%AE%97%E5%AD%90%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1、使用MapPartitions代替map2、使用coalesce对过滤后的Rdd进行重新分区和压缩3、使用foreachPartition替代foreach4、使用repartition进行调整并行度5、使用reduceByKey进行本地聚合 参考：https://www.cnblogs.com/lifeone/p/6439130.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark参数优化]]></title>
    <url>%2F2019%2F02%2F14%2Fspark%E5%B8%B8%E8%A7%84%E4%BC%98%E5%8C%96%E7%82%B9%2F</url>
    <content type="text"><![CDATA[spark.default.parallelism默认的并发数spark中有partition的概念（和slice是同一个概念，在spark1.2中官网已经做出了说明），一般每个partition对应一个task。在我的测试过程中，如果没有设置spark.default.parallelism参数，spark计算出来的partition非常巨大，与我的cores非常不搭。我在两台机器上（8cores 2 +6g 2）上，spark计算出来的partition达到2.8万个，也就是2.9万个tasks，每个task完成时间都是几毫秒或者零点几毫秒，执行起来非常缓慢。在我尝试设置了 spark.default.parallelism 后，任务数减少到10，执行一次计算过程从minute降到20second。 问题: reduce task数目不合适 解决方案：需要根据实际情况调整默认配置，调整方式是修改参数spark.default.parallelism。通常的，reduce数目设置为core数目的2-3倍。数量太大，造成很多小任务，增加启动任务的开销；数目太小，任务运行缓慢。所以要合理修改reduce的task数目即spark.default.parallelism 参考：https://blog.csdn.net/bbaiggey/article/details/51984753 spark.files.maxPartitionBytes = 128 M（默认）代表着rdd的一个分区能存放数据的最大字节数，如果一个400m的文件，只分了两个区，则在action时会发生错误。当一个spark应用程序执行时，生成spark.context，同时会生成两个参数，由上面得到的spark.default.parallelism推导出这两个参数的值12sc.defaultParallelism = spark.default.parallelismsc.defaultMinPartitions = min(spark.default.parallelism,2) 当sc.defaultParallelism和sc.defaultMinPartitions最终确认后，就可以推算rdd的分区数了。 spark.local.dir这个看起来很简单，就是Spark用于写中间数据，如RDD Cache，Shuffle，Spill等数据的位置，那么有什么可以注意的呢。 首先，最基本的当然是我们可以配置多个路径（用逗号分隔）到多个磁盘上增加整体IO带宽，这个大家都知道。 其次，目前的实现中，Spark是通过对文件名采用hash算法分布到多个路径下的目录中去，如果你的存储设备有快有慢，比如SSD+HDD混合使用，那么你可以通过在SSD上配置更多的目录路径来增大它被Spark使用的比例，从而更好地利用SSD的IO带宽能力。当然这只是一种变通的方法，终极解决方案还是应该像目前HDFS的实现方向一样，让Spark能够感知具体的存储设备类型，针对性的使用。 需要注意的是，在Spark 1.0 以后，SPARK_LOCAL_DIRS(Standalone, Mesos) or LOCAL_DIRS (YARN)参数会覆盖这个配置。比如Spark On YARN的时候，Spark Executor的本地路径依赖于Yarn的配置，而不取决于这个参数。 spark.shuffle.consolidateFiles为true问题：map|reduce数量大，造成shuffle小文件数目多解决方案： 通过设置spark.shuffle.consolidateFiles为true，来合并shuffle中间文件，此时文件数为reduce tasks数目； spark.serializer问题：序列化时间长、结果大 解决方案： spark默认使用JDK 自带的ObjectOutputStream，这种方式产生的结果大、CPU处理时间长，可以通过设置spark.serializer为org.apache.spark.serializer.KeyoSerializer。 另外如果结果已经很大，那就最好使用广播变量方式了，结果你懂得。 mapPartition问题：单条记录消耗大 解决方案： 使用mapPartition替换map，mapPartition是对每个Partition进行计算，而map是对partition中的每条记录进行计算； collect输出大量结果时速度慢解决方案： collect源码中是把所有的结果以一个Array的方式放在内存中，可以直接输出到分布式的文件系统，然后查看文件系统中的内容； spark.speculation问题: 任务执行速度倾斜 解决方案： 如果数据倾斜，一般是partition key取得不好，可以考虑其他的并行处理方式，并在中间加上aggregation操作；如果是Worker倾斜，例如在某些Worker上的executor执行缓慢，可以通过设置spark.speculation=true 把那些持续慢的节点去掉； repartition问题: 通过多步骤的RDD操作后有很多空任务或者小任务产生 解决方案： 使用coalesce或者repartition去减少RDD中partition数量； spark.streaming.concurrentJobs问题:Spark Streaming吞吐量不高 可以设置spark.streaming.concurrentJobs Spark Streaming 运行速度突然下降了，经常会有任务延迟和阻塞解决方案： 这是因为我们设置job启动interval时间间隔太短了，导致每次job在指定时间无法正常执行完成，换句话说就是创建的windows窗口时间间隔太密集了； 使用KryoSerializer进行序列化使用KryoSerializer序列化的好处默认情况，spark使用的是java的序列化机制，ObjectOutputStream / ObjectInputStream，对象输入输出流机制，来进行序列化。 该序列化的好处是方便使用，但必须实现Serializable接口，缺点是效率低，速度慢，序列化后的占用空间大 KryoSerializer序列化机制，效率高，速度快，占用空间小（只有java序列化的1/10），可以减少网络传输 使用方法1234//配置使用KryoSerializer进行序列化conf.set(&quot;spark.serializer&quot;,&quot;org.apache.spark.serializer.KryoSerializer&quot;)//（为了使序列化效果达到最优）注册自定义的类型使KryoSerializer序列化 .registerKryoClasses(new Class[]&#123;ExtractSession.class,FilterCount.class,SessionDetail.class,Task.class,Top10Session.class,Top10.class,VisitAggr.class&#125;); 使用KryoSerializer序列化的场景 算子函数中使用到的外部变量，使用KryoSerializer后，可以优化网络传输效率，优化集群中内存的占用和消耗持久化Rdd,优化内存占用，task过程中创建对象，减少GC次数shuffle过程，优化网络的传输性能 将每个task中都使用的大的外部变量作为广播变量没有使用广播变量的缺点 默认情况，task使用到了外部变量，每个task都会获取一份外部变量的副本，会占用不必要的内存消耗，导致在Rdd持久化时不能写入到内存，只能持久化到磁盘中，增加了IO读写操作。 同时，在task创建对象时，内存不足，进行频繁的GC操作，降低效率 使用广播变量的好处 广播变量不是每个task保存一份，而是每个executor保存一份。 广播变量初始化时，在Driver上生成一份副本，task运行时需要用到广播变量中的数据，首次使用会在本地的Executor对应的BlockManager中尝试获取变量副本；如果本地没有，那么就会从Driver远程拉取变量副本，并保存到本地的BlockManager中；此后这个Executor中的task使用到的数据都从本地的BlockManager中直接获取。 Executor中的BlockManager除了从远程的Driver中拉取变量副本，也可能从其他节点的BlockManager中拉取数据，距离越近越好。 将rdd进行持久化持久化的原则 Rdd的架构重构和优化 尽量复用Rdd，差不多的Rdd进行抽象为一个公共的Rdd，供后面使用 公共Rdd一定要进行持久化 对应对次计算和使用的Rdd，一定要进行持久化 持久化是可以序列化的 首先采用纯内存的持久化方式，如果出现OOM异常，则采用纯内存+序列化的方法，如果依然存在OOM异常，使用内存+磁盘，以及内存+磁盘+序列化的方法 为了数据的高可靠性，而且内存充足时，可以使用双副本机制进行持久化 持久化的代码实现 .persist(StorageLevel.MEMORY_ONLY()) 持久化等级 StorageLevel.MEMORY_ONLY() 纯内存 等效于 .cache() 序列化的：后缀带有_SER 如：StorageLevel.MEMORY_ONLY_SER() 内存+序列化 后缀带有_DISK 表示磁盘，如：MEMORY_AND_DISK() 内存+磁盘 后缀带有_2表示副本数，如：MEMORY_AND_DISK_2() 内存+磁盘且副本数为2 开启map端输出文件的合并机制为什么要开启map端输出文件的合并机制 默认情况下，map端的每个task会为reduce端的每个task生成一个输出文件，reduce段的每个task拉取map端每个task生成的相应文件 开启后，map端只会在并行执行的task生成reduce端task数目的文件，下一批map端的task执行时，会复用首次生成的文件 如何开启12//开启map端输出文件的合并机制conf.set(&quot;spark.shuffle.consolidateFiles&quot;, &quot;true&quot;); 调节map端内存缓冲区为什么要调节map端内存缓冲区默认情况下，shuffle的map task,输出的文件到内存缓冲区，当内存缓冲区满了，才会溢写spill操作到磁盘，如果该缓冲区比较小，而map端输出文件又比较大，会频繁的出现溢写到磁盘，影响性能。如何调整12//设置map 端内存缓冲区大小（默认32k）conf.set(&quot;spark.shuffle.file.buffer&quot;, &quot;64k&quot;); 调节reduce端内存占比为什么要调节reduce端内存占比reduce task 在进行汇聚，聚合等操作时，实际上使用的是自己对应的executor内存，默认情况下executor分配给reduce进行聚合的内存比例是0.2，如果拉取的文件比较大，会频繁溢写到本地磁盘，影响性能。12//设置reduce端内存占比conf.set(&quot;spark.shuffle.memoryFraction&quot;, &quot;0.4&quot;); 修改shuffle管理器有哪些shuffle管理器HashShuffleManager：1.2.x版本前的默认选择SortShuffleManager：1.2.x版本之后的默认选择，会对每个task要处理的数据进行排序；同时，可以避免像 HashShuffleManager那么默认去创建多份磁盘文件，而是一个task只会写入一个磁盘文件，不同reduce task需要的的数据使用offset来进行划分。tungsten-sort（钨丝）：1.5.x之后的出现，和SortShuffleManager相似，但是它本事实现了一套内存管理机制，性能有了很大的提高，而且避免了shuffle过程中产生大量的OOM、GC等相关问题。 如何选择如果不需要排序，建议使用HashShuffleManager以提高性能如果需要排序，建议使用SortShuffleManager如果不需要排序，但是希望每个task输出的文件都合并到一个文件中，可以去调节bypassMergeThreshold这个阀值（默认为200），因为在合并文件的时候会进行排序，所以应该让该阀值大于reduce task数量。如果需要排序，而且版本在1.5.x或者更高，可以尝试使用 tungsten-sort 在项目中如何使用12345//设置spark shuffle manager (hash,sort,tungsten-sort)conf.set(&quot;spark.shuffle.manager&quot;, &quot;tungsten-sort&quot;); //设置文件合并的阀值conf.set(&quot;spark.shuffle.sort.bypassMergeThreshold&quot;, &quot;550&quot;); spark.reducer.maxSizeInFlight调节reduce端缓冲区大小避免OOM异常 为什么要调节reduce端缓冲区大小 对于map端不断产生的数据，reduce端会不断拉取一部分数据放入到缓冲区，进行聚合处理； 当map端数据特别大时，reduce端的task拉取数据是可能全部的缓冲区都满了，此时进行reduce聚合处理时创建大量的对象，导致OOM异常； 如何调节reduce端缓冲区大小 当由于以上的原型导致OOM异常出现是，可以通过减小reduce端缓冲区大小来避免OOM异常的出现 但是如果在内存充足的情况下，可以适当增大reduce端缓冲区大小，从而减少reduce端拉取数据的次数，提供性能。12//调节reduce端缓存的大小(默认48M)conf.set(&quot;spark.reducer.maxSizeInFlight&quot;, &quot;24&quot;); 解决JVM GC导致的shuffle文件拉取失败问题描述 下一个stage的task去拉取上一个stage的task的输出文件时，如果正好上一个stage正处在full gc的情况下（所有线程后停止运行），它们之间是通过netty进行通信的，就会出现很长时间拉取不到数据，此时就会报shuffle file not found的错误；但是下一个stage又重新提交task就不会出现问题了。 如何解决 调节最大尝试拉取次数：spark.shuffle.io.maxRetries 默认为3次 调节每次拉取最大的等待时长：spark.shuffle.io.retryWait 默认为5秒12345//调节拉取文件的最大尝试次数(默认3次)conf.set(&quot;spark.shuffle.io.maxRetries&quot;, &quot;60&quot;); //调节每次拉取数据时最大等待时长(默认为5s)conf.set(&quot;spark.shuffle.io.retryWait&quot;, &quot;5s&quot;); yarn-cluster模式的JVM内存溢出无法执行的问题 问题描述 有些spark作业，在yarn-client模式下是可以运行的，但在yarn-cluster模式下，会报出JVM的PermGen(永久代)的内存溢出，OOM. 出现以上原因是：yarn-client模式下，driver运行在本地机器上，spark使用的JVM的PermGen的配置，是本地的默认配置128M； 但在yarn-cluster模式下，driver运行在集群的某个节点上，spark使用的JVM的PermGen是没有经过默认配置的，默认是82M，故有时会出现PermGen Out of Memory error log. 如何处理在spark-submit脚本中设置PermGen--conf spark.driver.extraJavaOptions=&quot;-XX:PermSize=128M -XX:MaxPermSize=256M&quot;(最小128M，最大256M) 如果使用spark sql，sql中使用大量的or语句，可能会报出jvm stack overflow,jvm栈内存溢出，此时可以把复杂的sql简化为多个简单的sql进行处理即可。 checkpoint的使用checkpoint的作用 默认持久化的Rdd会保存到内存或磁盘中，下次使用该Rdd时直接冲缓存中获取，不需要重新计算；如果内存或者磁盘中文件丢失，再次使用该Rdd时需要重新进行。 如果将持久化的Rdd进行checkpoint处理，会把内存写入到hdfs文件系统中，此时如果再次使用持久化的Rdd，但文件丢失后，会从hdfs中获取Rdd并重新进行缓存。 如何使用 首先设置checkpoint目录12//设置checkpoint目录javaSparkContext.checkpointFile(&quot;hdfs://hadoop-senior.ibeifeng.com:8020/user/yanglin/spark/checkpoint/UserVisitSessionAnalyzeSpark&quot;); 将缓存后的Rdd进行checkpoint处理12//将缓存后的Rdd进行checkpointsessionRowPairRdd.checkpoint(); 参考：https://blog.csdn.net/stark_summer/article/details/42981201https://www.cnblogs.com/lifeone/p/6428885.htmlhttps://www.cnblogs.com/lifeone/p/6434840.htmlhttps://www.jianshu.com/p/da7d5edfb61f]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark jvm优化]]></title>
    <url>%2F2019%2F02%2F14%2Fspark-jvm%E4%BC%98%E5%8C%96%E7%82%B9%2F</url>
    <content type="text"><![CDATA[关于JVM内存的深入知识在这里不赘述，请大家自行对相关知识进行补充。好，说回Spark，运行Spark作业的时候，JVM对会对Spark作业产生什么影响呢？答案很简单，如果数据量过大，一定会导致JVM内存不足。在Spark作业运行时，会创建出来大量的对象，每一次将对象放入JVM时，首先将创建的对象都放入到eden区域和其中一个survivor区域中；当eden区域和一个survivor区域放满了以后，这个时候会触发minor gc，把不再使用的对象全部清除，而剩余的对象放入另外一个servivor区域中。JVM中默认的eden，survivor1，survivor2的内存占比为8:1:1。当存活的对象在一个servivor中放不下的时候，就会将这些对象移动到老年代。如果JVM的内存不够大的话，就会频繁的触发minor gc，这样会导致一些短生命周期的对象进入到老年代，老年代的对象不断的囤积，最终触发full gc。一次full gc会使得所有其他程序暂停很长时间。最终严重影响我们的Spark的性能和运行速度。 降低cache操作的内存占比spark中，堆内存又被划分成了两块儿，一块儿是专门用来给RDD的cache、persist操作进行RDD数据缓存用的；另外一块儿，就是我们刚才所说的，用来给spark算子函数的运行使用的，存放函数中自己创建的对象。默认情况下，给RDD cache操作的内存占比是0.6，即60%的内存都给了cache操作了。但是问题是，如果某些情况下cache占用的内存并不需要占用那么大，这个时候可以将其内存占比适当降低。怎么判断在什么时候调整RDD cache的内存占用比呢？其实通过Spark监控平台就可以看到Spark作业的运行情况了，如果发现task频繁的gc，就可以去调整cache的内存占用比了。通过SparkConf.set(&quot;spark.storage.memoryFraction&quot;,&quot;0.3&quot;)来设定。0.6 -&gt; 0.5 -&gt; 0.4 -&gt; 0.2大家可以自己去调，然后观察spark作业的运行统计针对该情况，大家可以 在 spark webui观察。yarn 去运行的话，那么就通过yarn的界面，去查看你的spark作业的 运行统计。很简单， 大家一层一层点击进去就好。 可以看到每个stage 的运行情况。 包括每个task的运行时间统计。gc时间。 如果发现gc太贫乏。时间太长，那么就可以适当调整这个比例。 降低cache操作的内存占比，大不了用persist 操作。选择一部分缓存的rdd数据写入磁盘，或者序列化的的方式。配合kryo序列化类。减少rdd缓存的内存占用，降低cache操作内存占比。 对应的 算子函数的内存占比，就提升了。这个时候，可能就减少minor gc 的频率，同时减少full gc的频率，对性能的 提升 有一定帮助的。 堆外内存和连接等待时长的调整其实这两个参数主要是为了解决一些Spark作业运行时候出现的一些错误信息而进行调整的 堆外内存a) 问题提出有时候，如果你的spark作业处理的数据量特别特别大，几亿数据量；然后spark作业一运行就会出现类似shuffle file cannot find，executor、task lost，out of memory（内存溢出）等这样的错误。这是因为可能是说executor的堆外内存不太够用，导致executor在运行的过程中，可能会内存溢出；然后可能导致后续的stage的task在运行的时候，可能要从一些executor中去拉取shuffle map output文件，但是executor可能已经挂掉了，关联的blockmanager也没有了；所以可能会报shuffle output file not found；resubmitting task；executor lost 这样的错误；最终导致spark作业彻底崩溃。 上述情况下，就可以去考虑调节一下executor的堆外内存。也许就可以避免报错；此外，有时，堆外内存调节的比较大的时候，对于性能来说，也会带来一定的提升。b) 解决方案：--conf spark.yarn.executor.memoryOverhead=2048在spark-submit脚本里面添加如上配置。默认情况下，这个堆外内存上限大概是300多M；我们通常项目中真正处理大数据的时候，这里都会出现问题导致spark作业反复崩溃无法运行；此时就会去调节这个参数，到至少1G或者更大的内存。通常这个参数调节上去以后，就会避免掉某些OOM的异常问题，同时呢，会让整体spark作业的性能，得到较大的提升。 连接等待时长的调整a) 问题提出：由于JVM内存过小，导致频繁的Minor gc，有时候更会触犯full gc，一旦出发full gc；此时所有程序暂停，导致无法建立网络连接；spark默认的网络连接的超时时长是60s；如果卡住60s都无法建立连接的话，那么就宣告失败了。碰到一种情况，有时候报错信息会出现一串类似file id not found，file lost的错误。这种情况下，很有可能是task需要处理的那份数据的executor在正在进行gc。所以拉取数据的时候，建立不了连接。然后超过默认60s以后，直接宣告失败。几次都拉取不到数据的话，可能会导致spark作业的崩溃。也可能会导致DAGScheduler，反复提交几次stage。TaskScheduler，反复提交几次task。大大延长我们的spark作业的运行时间。b) 解决方案：--conf spark.core.connection.ack.wait.timeout=300在spark-submit脚本中添加如上参数，调节这个值比较大以后，通常来说，可以避免部分的偶尔出现的某某文件拉取失败，某某文件lost掉的错误。 查看gcspark-env.sh: JAVA_OPTS=” -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps” https://www.cnblogs.com/jcchoiling/p/6494652.html参考:https://www.jianshu.com/p/e4557bf9186bhttps://www.cnblogs.com/lifeone/p/6434356.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RDD]]></title>
    <url>%2F2019%2F02%2F14%2FRDD%2F</url>
    <content type="text"><![CDATA[特点Resillient Distributed Dataset，即弹性分布式数据集RDD的内部属性 通过RDD的内部属性，用户可以获取相应的元数据信息。通过这些信息可以支持更复杂的算法或优化。 1）分区列表：通过分区列表可以找到一个RDD中包含的所有分区及其所在地址。 2）计算每个分片的函数：通过函数可以对每个数据块进行RDD需要进行的用户自定义函数运算。 3）对父RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖。 4）可选：key-value型的RDD是根据哈希来分区的，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce。 5）可选：每一个分片的优先计算位置（preferred locations），比如HDFS的block的所在位置应该是优先计算的位置。(存储的是一个表，可以将处理的分区“本地化”) 分区— partitions计算函数— computer(p,context)依赖— dependencies()分区策略(Pair RDD)– partitioner()本地性策略— preferredLocations(p) 12345678910//只计算一次 protected def getPartitions: Array[Partition] //对一个分片进行计算，得出一个可遍历的结果 def compute(split: Partition, context: TaskContext): Iterator[T] //只计算一次，计算RDD对父RDD的依赖 protected def getDependencies: Seq[Dependency[_]] = deps //可选的，分区的方法，针对第4点，类似于mapreduce当中的Paritioner接口，控制key分到哪个reduce @transient val partitioner: Option[Partitioner] = None //可选的，指定优先位置，输入参数是split分片，输出结果是一组优先的节点位置 protected def getPreferredLocations(split: Partition): Seq[String] = Nil 1）RDD的特点 1）创建：只能通过转换 ( transformation ，如map/filter/groupBy/join 等，区别于动作 action) 从两种数据源中创建 RDD 1 ）稳定存储中的数据； 2 ）其他 RDD。 2）只读：状态不可变，不能修改。 3）分区：支持使 RDD 中的元素根据那个 key 来分区 ( partitioning ) ，保存到多个结点上。还原时只会重新计算丢失分区的数据，而不会影响整个系统。 4）路径：在 RDD 中叫世族或血统 ( lineage ) ，即 RDD 有充足的信息关于它是如何从其他 RDD 产生而来的。 5）持久化：支持将会被重用的 RDD 缓存 ( 如 in-memory 或溢出到磁盘 )。 6）延迟计算： Spark 也会延迟计算 RDD ，使其能够将转换管道化 (pipeline transformation)。 7）操作：丰富的转换（transformation）和动作 ( action ) ， count/reduce/collect/save 等。 执行了多少次transformation操作，RDD都不会真正执行运算（记录lineage），只有当action操作被执行时，运算才会触发。https://blog.csdn.net/guohecang/article/details/51736572 本地性策略一直在用spark但是没有考虑过优化的看过来。分布式计算系统的精粹在于移动计算而非移动数据，但是在实际的计算过程中，总存在着移动数据的情况，除非是在集群的所有节点上都保存数据的副本。移动数据，将数据从一个节点移动到另一个节点进行计算，不但消耗了网络IO，也消耗了磁盘IO，降低了整个计算的效率。为了提高数据的本地性，除了优化算法（也就是修改spark内存，难度有点高），就是合理设置数据的副本。设置数据的副本，这需要通过配置参数并长期观察运行状态才能获取的一个经验值。Spark中的数据本地性有三种： PROCESS_LOCAL：进程本地化，代码和数据在同一个进程中，也就是在同一个executor中；计算数据的task由executor执行，数据在executor的BlockManager中；性能最好NODE_LOCAL：节点本地化，代码和数据在同一个节点中；比如说，数据作为一个HDFS block块，就在节点上，而task在节点上某个executor中运行；或者是，数据和task在一个节点上的不同executor中；数据需要在进程间进行传输NO_PREF：对于task来说，数据从哪里获取都一样，没有好坏之分,比如从数据库中获取数据RACK_LOCAL：机架本地化，数据和task在一个机架的两个节点上；数据需要通过网络在节点之间进行传输ANY：数据和task可能在集群中的任何地方，而且不在一个机架中，性能最差 通常读取数据PROCESS_LOCAL&gt;NODE_LOCAL&gt;ANY，尽量使数据以PROCESS_LOCAL或NODE_LOCAL方式读取。其中PROCESS_LOCAL还和cache有关，如果RDD经常用的话将该RDD cache到内存中，注意，由于cache是lazy的，所以必须通过一个action的触发，才能真正的将该RDD cache到内存中。 如何配置Locality呢？可以统一采用spark.locality.wait来设置（例如设置5000ms）。当然可以分别设置spark.locality.wait.process、spark.locality.wait.node、spark.locality.wait.rack等；一般的具体设置是Locality优先级越高则可以设置越高的等待超时时间。12new SparkConf() .set(&quot;spark.locality.wait&quot;, &quot;10&quot;) 如果数据是PROCESS_LOCAL，但是此时并没有空闲的Core来运行我们的Task，此时Task就要等待，例如等待3000ms，3000ms内如果能够运行待运行的Task则直接运行，如果超过了3000ms，此时数据本地性就要退而求其次采用NODE_LOCAL的方式。同样的道理，NODE_LOCAL也会有等待的超时时间，以此类推…对于ANY的情况，默认情况状态下性能会非常低下，此时强烈建议使用Tachyon，例如在百度云上，为了确保计算速度，就在计算集群和存储集群之间加入了Tachyon,通过Tachyon来从远程抓取数据，而Spark基于Tachyon来进行计算，这就更好的满足了数据本地性。 数据本地性任务分配的源码在 taskSetManager.scala 。https://blog.csdn.net/oitebody/article/details/80137721数据本地性的副作用https://www.jianshu.com/p/a1d0824053d8 参考:https://www.cnblogs.com/yourarebest/p/5122372.htmlhttps://www.cnblogs.com/jackie2016/p/5643100.htmlhttp://blog.sina.com.cn/s/blog_9ca9623b0102w8pc.htmlhttps://bit1129.iteye.com/blog/2186084https://www.jianshu.com/p/a1d0824053d8https://blog.csdn.net/u013939918/article/details/60897191]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark算子]]></title>
    <url>%2F2019%2F02%2F13%2Fspark%E7%AE%97%E5%AD%90%2F</url>
    <content type="text"><![CDATA[sortByKey该函数会对原始RDD中的数据进行Shuffle操作，从而实现排序。这个函数中,传入两个參数,ascending表示是升序还是降序,默认true表示升序.第二个參数是运行排序使用的partition的个数,默认是当前RDD的partition个数. groupByKey、reduceByKey与sortByKeygroupByKey把相同的key的数据分组到一个集合序列当中： [(“hello”,1), (“world”,1), (“hello”,1), (“fly”,1), (“hello”,1), (“world”,1)] –&gt; [(“hello”,(1,1,1)),(“word”,(1,1)),(“fly”,(1))] reduceByKey把相同的key的数据聚合到一起并进行相应的计算： [(“hello”,1), (“world”,1), (“hello”,1), (“fly”,1), (“hello”,1), (“world”,1)] add–&gt; [(“hello”,3）,(“word”,2),(“fly”,1)] sortByKey按key的大小排序，默认为升序排序： [(3,”hello”）,(2,”word”),(1,”fly”)] –&gt; [(1,”fly”)，(2,”word”)，(3,”hello”)] 123456789101112131415161718192021222324252627282930313233343536373839 from pyspark import SparkConf, SparkContextfrom operator import addconf = SparkConf()sc = SparkContext(conf=conf)def func_by_key(): data = [ &quot;hello world&quot;, &quot;hello fly&quot;, &quot;hello world&quot;, &quot;hello fly&quot;, &quot;hello fly&quot;, &quot;hello fly&quot; ] data_rdd = sc.parallelize(data) word_rdd = data_rdd.flatMap(lambda s: s.split(&quot; &quot;)).map(lambda x: (x, 1)) group_by_key_rdd = word_rdd.groupByKey() print(&quot;groupByKey:&#123;&#125;&quot;.format(group_by_key_rdd.mapValues(list).collect())) print(&quot;groupByKey mapValues(len):&#123;&#125;&quot;.format( group_by_key_rdd.mapValues(len).collect() )) reduce_by_key_rdd = word_rdd.reduceByKey(add) print(&quot;reduceByKey:&#123;&#125;&quot;.format(reduce_by_key_rdd.collect())) print(&quot;sortByKey:&#123;&#125;&quot;.format(reduce_by_key_rdd.map( lambda x: (x[1], x[0]) ).sortByKey().map(lambda x: (x[0], x[1])).collect()))func_by_key()sc.stop()&quot;&quot;&quot;result：groupByKey:[(&apos;fly&apos;, [1, 1, 1, 1]), (&apos;world&apos;, [1, 1]), (&apos;hello&apos;, [1, 1, 1, 1, 1, 1])]groupByKey mapValues(len):[(&apos;fly&apos;, 4), (&apos;world&apos;, 2), (&apos;hello&apos;, 6)]reduceByKey:[(&apos;fly&apos;, 4), (&apos;world&apos;, 2), (&apos;hello&apos;, 6)]sortByKey:[(2, &apos;world&apos;), (4, &apos;fly&apos;), (6, &apos;hello&apos;)]&quot;&quot;&quot; 从结果可以看出，groupByKey对分组后的每个key的value做mapValues(len)后的结果与reduceByKey的结果一致，即：如果分组后要对每一个key所对应的值进行操作则应直接用reduceByKey；sortByKey是按key排序，如果要对value排序，可以交换key与value的位置，再排序。https://www.cnblogs.com/FG123/p/9746830.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive 排序与分发的各种By]]></title>
    <url>%2F2019%2F02%2F13%2Fhive-%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[排序与分发的各种By与传统关系型数据库最大的区别就是处理数据的能力这种能力最大的体现就是排序与分发的原理order by 是全局排序，只有一个reduce，数据量多时速度慢sort by 是随机分发到一个reduce然后reduce内部排序，一般不会单独使用;，也就是说保证了局部有序（每个reducer出来的数据是有序的，但是不能保证所有的数据是有序的，除非只有一个reducer）好处是：执行了局部排序之后可以为接下去的全局排序提高不少的效率（其实就是做一次归并排序就可以做到全局排序了）distribute by 是根据 distribute by 的字段把相应的记录分发到那个reduce cluster by是distribute by + sort by的简写]]></content>
  </entry>
  <entry>
    <title><![CDATA[beeline]]></title>
    <url>%2F2019%2F02%2F13%2Fbeeline%2F</url>
    <content type="text"><![CDATA[1beeline -u &quot;jdbc:hive2://xxxxx;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=datahiveserver2_zk&quot; -n username -p &quot;password&quot; --color=true --silent=false --showHeader=false --outputformat=csv 格式参数:–outputformat[table/vertical/csv/tsv/dsv/csv2/tsv2]为了便于输出结果解析，建议把输出格式设置成普通文本，否则输出格式默认为table参考：https://my.oschina.net/guol/blog/875875]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拉链表]]></title>
    <url>%2F2019%2F01%2F29%2F%E6%8B%89%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[1、历史全量表2、每日更新表所谓拉链，就是记录历史。记录一个事物从开始，一直到当前状态的所有变化的信息。要确定拉链表的时间粒度，比如说拉链表每天只取一个状态，也就是说如果一天有3个状态变更，我们只取最后一个状态，这种天粒度的表其实已经能解决大部分的问题了；流水表存放的是一个用户的变更记录，比如在一张流水表中，一天的数据中，会存放一个用户的每条修改记录，但是在拉链表中只有一条记录。这是拉链表设计时需要注意的一个粒度问题。我们当然也可以设置的粒度更小一些，一般按天就足够（如果一个订单在一天内有多次状态变化，则只会记录最后一个状态的历史）。查询性能拉链表当然也会遇到查询性能的问题，比如说我们存放了5年的拉链数据，那么这张表势必会比较大，当查询的时候性能就比较低了，个人认为两个思路来解决：1.在一些查询引擎中，我们对start_date和end_date做索引，这样能提高不少性能。2.保留部分历史数据，比如说我们一张表里面存放全量的拉链表数据，然后再对外暴露一张只提供近3个月数据的拉链表。 每日的用户增量假设我们每天都会获得一份切片数据，我们可以通过取两天切片数据的不同来作为每日更新表，这种情况下我们可以对所有的字段先进行concat，再取md5，这样就ok了12345678910111213141516171819202122INSERT OVERWRITE TABLE dws.user_hisSELECT * FROM( SELECT A.user_num, A.mobile, A.reg_date, A.t_start_time, CASE WHEN A.t_end_time = &apos;9999-12-31&apos; AND B.user_num IS NOT NULL THEN &apos;2017-01-01&apos; //昨天是历史最新，今天有数据，则昨天为历史 ELSE A.t_end_time //保留昨天的状态 END AS t_end_time FROM dws.user_his AS A LEFT JOIN ods.user_update AS B ON A.user_num = B.user_numUNION SELECT C.user_num, C.mobile, C.reg_date, &apos;2017-01-02&apos; AS t_start_time, &apos;9999-12-31&apos; AS t_end_time FROM ods.user_update AS C) AS T 增量表：每天的新增数据，增量数据是上次导出之后的新数据(如果只是某些字段被更新而不是新增记录则不被包括)。拉链表：维护历史状态，以及最新状态数据的一种表，拉链表根据拉链粒度的不同，实际上相当于快照，只不过做了优化，去除了一部分不变的记录而已,通过拉链表可以很方便的还原出拉链时点的客户记录。每日更新表：假设我们每天都会获得一份切片数据，我们可以通过取两天切片数据的不同来作为每日更新表，这种情况下我们可以对所有的字段先进行concat，再取md5，这样就ok了拉链表比每日更新表更容易得到最新有效的final记录https://www.cnblogs.com/zuifangxiu/articles/6475179.html拉链表回滚]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[留存]]></title>
    <url>%2F2019%2F01%2F29%2F%E7%95%99%E5%AD%98%2F</url>
    <content type="text"><![CDATA[方法一：推荐1234567891011121314次日，7日内，30日内留存：selecta.uid,casewhen diff(b.dt,a.dt) = 1 then &apos;2,7,30&apos;when diff(b.dt,a.dt) &lt; 7 then &apos;7,30&apos;when diff(b.dt,a.dt) &lt;30 then &apos;30&apos;end as diffsfrom(select uid,dt from current_table where dt&gt;=’2018-06-02’and dt&lt;=’2018-07-02’) ainner join (select uid,dt from coming_table where dt&gt;=’2018-06-02’and dt&lt;=’2018-07-02’) bon a.uid = b.uid然后行转列 方法二：不推荐1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980select a.dt, count(distinct uid) `宫格区域`, count(distinct uid1) `次日留存`, concat(round(count(distinct uid1)/count(distinct uid) * 100,2) ,&apos;%&apos;) `次日留存率`, count(distinct if((uid1 is not null or uid2 is not null or uid3 is not null),uid,null)) `3日内留存`, concat(round(count(distinct if((uid1 is not null or uid2 is not null or uid3 is not null),uid,null))/count(distinct uid) * 100,2) ,&apos;%&apos;) `3日内留存率`, count(distinct if((uid1 is not null or uid2 is not null or uid3 is not null or uid4 is not null or uid5 is not null or uid6 is not null or uid7 is not null),uid,null)) `7日内留存`, concat(round(count(distinct if((uid1 is not null or uid2 is not null or uid3 is not null or uid4 is not null or uid5 is not null or uid6 is not null or uid7 is not null),uid,null))/count(distinct uid) * 100,2) ,&apos;%&apos;) `7日内留存率` from (select distinct dt,uid from ods_client_behavior_qav_set where dt&gt;=date_add(&apos;$&#123;yyyy-MM-dd&#125;&apos;,-14) and dt&lt;=date_add(&apos;$&#123;yyyy-MM-dd&#125;&apos;,-1) and pid=&apos;10010&apos; and action like &apos;%CARD_ENTRANCE_START####%&apos; and (action like &apos;%&quot;&quot;page&quot;&quot;：&quot;&quot;mine&quot;&quot;%&apos;or action like &apos;%&quot;&quot;page&quot;&quot;：&quot;&quot;minern&quot;&quot;%&apos; ) and (action like &apos;%&quot;&quot;module&quot;&quot;：&quot;&quot;answerquestion&quot;&quot;%&apos;and action like &apos;%&quot;&quot;operType&quot;&quot;：&quot;&quot;enter&quot;&quot;%&apos; ) aleft join (select distinct date_sub(dt,1) dt,uid uid1 from ods_client_behavior_qav_set where dt&gt;=date_add(&apos;$&#123;yyyy-MM-dd&#125;&apos;,-13) and dt&lt;=date_add(&apos;$&#123;yyyy-MM-dd&#125;&apos;,-1) and pid=&apos;10010&apos; and action like &apos;%CARD_ENTRANCE_START####%&apos; and (action like &apos;%&quot;&quot;page&quot;&quot;：&quot;&quot;mine&quot;&quot;%&apos;or action like &apos;%&quot;&quot;page&quot;&quot;：&quot;&quot;minern&quot;&quot;%&apos; ) and (action like &apos;%&quot;&quot;module&quot;&quot;：&quot;&quot;answerquestion&quot;&quot;%&apos;and action like &apos;%&quot;&quot;operType&quot;&quot;：&quot;&quot;enter&quot;&quot;%&apos;) b1 on a.dt=b1.dt and a.uid=b1.uid1left join (select distinct date_sub(dt,2) dt,uid uid2 from ods_client_behavior_qav_set where dt&gt;=date_add(&apos;$&#123;yyyy-MM-dd&#125;&apos;,-12) and dt&lt;=date_add(&apos;$&#123;yyyy-MM-dd&#125;&apos;,-1) and pid=&apos;10010&apos; and action like &apos;%CARD_ENTRANCE_START####%&apos; and (action like &apos;%&quot;&quot;page&quot;&quot;：&quot;&quot;mine&quot;&quot;%&apos;or action like &apos;%&quot;&quot;page&quot;&quot;：&quot;&quot;minern&quot;&quot;%&apos; ) and (action like &apos;%&quot;&quot;module&quot;&quot;：&quot;&quot;answerquestion&quot;&quot;%&apos;and action like &apos;%&quot;&quot;operType&quot;&quot;：&quot;&quot;enter&quot;&quot;%&apos;) b2 on a.dt=b2.dt and a.uid=b2.uid2left join (select distinct date_sub(dt,3) dt,uid uid3 from ods_client_behavior_qav_set where dt&gt;=date_add(&apos;$&#123;yyyy-MM-dd&#125;&apos;,-11) and dt&lt;=date_add(&apos;$&#123;yyyy-MM-dd&#125;&apos;,-1) and pid=&apos;10010&apos; and action like &apos;%CARD_ENTRANCE_START####%&apos; and (action like &apos;%&quot;&quot;page&quot;&quot;：&quot;&quot;mine&quot;&quot;%&apos;or action like &apos;%&quot;&quot;page&quot;&quot;：&quot;&quot;minern&quot;&quot;%&apos; ) and (action like &apos;%&quot;&quot;module&quot;&quot;：&quot;&quot;answerquestion&quot;&quot;%&apos;)and action like &apos;%&quot;&quot;operType&quot;&quot;：&quot;&quot;enter&quot;&quot;%&apos; ) b3 on a.dt=b3.dt and a.uid=b3.uid3left join (select distinct date_sub(dt,4) dt,uid uid4 from ods_client_behavior_qav_set where dt&gt;=date_add(&apos;$&#123;yyyy-MM-dd&#125;&apos;,-10) and dt&lt;=date_add(&apos;$&#123;yyyy-MM-dd&#125;&apos;,-1) and pid=&apos;10010&apos; and action like &apos;%CARD_ENTRANCE_START####%&apos; and (action like &apos;%&quot;&quot;page&quot;&quot;：&quot;&quot;mine&quot;&quot;%&apos;or action like &apos;%&quot;&quot;page&quot;&quot;：&quot;&quot;minern&quot;&quot;%&apos; ) and (action like &apos;%&quot;&quot;module&quot;&quot;：&quot;&quot;answerquestion&quot;&quot;%&apos;and action like &apos;%&quot;&quot;operType&quot;&quot;：&quot;&quot;enter&quot;&quot;%&apos;) b4 on a.dt=b4.dt and a.uid=b4.uid4left join (select distinct date_sub(dt,5) dt,uid uid5 from ods_client_behavior_qav_set where dt&gt;=date_add(&apos;$&#123;yyyy-MM-dd&#125;&apos;,-9) and dt&lt;=date_add(&apos;$&#123;yyyy-MM-dd&#125;&apos;,-1) and pid=&apos;10010&apos; and action like &apos;%CARD_ENTRANCE_START####%&apos; and (action like &apos;%&quot;&quot;page&quot;&quot;：&quot;&quot;mine&quot;&quot;%&apos;or action like &apos;%&quot;&quot;page&quot;&quot;：&quot;&quot;minern&quot;&quot;%&apos; ) and (action like &apos;%&quot;&quot;module&quot;&quot;：&quot;&quot;answerquestion&quot;&quot;%&apos;and action like &apos;%&quot;&quot;operType&quot;&quot;：&quot;&quot;enter&quot;&quot;%&apos;) b5 on a.dt=b5.dt and a.uid=b5.uid5left join (select distinct date_sub(dt,6) dt,uid uid6 from ods_client_behavior_qav_set where dt&gt;=date_add(&apos;$&#123;yyyy-MM-dd&#125;&apos;,-8) and dt&lt;=date_add(&apos;$&#123;yyyy-MM-dd&#125;&apos;,-1) and pid=&apos;10010&apos; and action like &apos;%CARD_ENTRANCE_START####%&apos; and (action like &apos;%&quot;&quot;page&quot;&quot;：&quot;&quot;mine&quot;&quot;%&apos;or action like &apos;%&quot;&quot;page&quot;&quot;：&quot;&quot;minern&quot;&quot;%&apos; ) and (action like &apos;%&quot;&quot;module&quot;&quot;：&quot;&quot;answerquestion&quot;&quot;%&apos;)and action like &apos;%&quot;&quot;operType&quot;&quot;：&quot;&quot;enter&quot;&quot;%&apos; ) b6 on a.dt=b6.dt and a.uid=b6.uid6left join (select distinct date_sub(dt,7) dt,uid uid7 from ods_client_behavior_qav_set where dt&gt;=date_add(&apos;$&#123;yyyy-MM-dd&#125;&apos;,-7) and dt&lt;=date_add(&apos;$&#123;yyyy-MM-dd&#125;&apos;,-1) and pid=&apos;10010&apos; and action like &apos;%CARD_ENTRANCE_START####%&apos; and (action like &apos;%&quot;&quot;page&quot;&quot;：&quot;&quot;mine&quot;&quot;%&apos;or action like &apos;%&quot;&quot;page&quot;&quot;：&quot;&quot;minern&quot;&quot;%&apos; ) and (action like &apos;%&quot;&quot;module&quot;&quot;：&quot;&quot;answerquestion&quot;&quot;%&apos;and action like &apos;%&quot;&quot;operType&quot;&quot;：&quot;&quot;enter&quot;&quot;%&apos;) b7 on a.dt=b7.dt and a.uid=b7.uid7group by a.dt]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive Update/Insert/Delete原理]]></title>
    <url>%2F2019%2F01%2F29%2Fhive-Update-Insert-Delete%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Ｈive从0.14开始支持ACID。也就是支持了Update Insert Delete及一些流式的API。也就是这个原因，Hive把0.14.1 Bug Fixes版本改成了 Hive 1.0，也就是认为功能基本稳定和健全了。 由于HDFS是不支持本地文件更改的，同时在写的时候也不支持读。表或者分区内的数据作为基础数据。事务产生的新数据如Insert/Update/Flume/Storm等会存储在增量文件（Delta Files）中。读取这个文件的时候，通常是Table Scan阶段，会合并更改，使读出的数据一致。 Hive Metastore上面增加了若干个线程，会周期性地合并并合并删除这些增量文件。 具体可以实现参考这个网页。 https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[层次桥接表]]></title>
    <url>%2F2019%2F01%2F29%2F%E5%B1%82%E6%AC%A1%E6%A1%A5%E6%8E%A5%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[初版： 扁平化: 层次桥接表 下钻 上钻er图：]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[桥接表]]></title>
    <url>%2F2019%2F01%2F28%2F%E6%A1%A5%E6%8E%A5%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[采用桥接表有利有弊，桥接表增加了误用模式的可能性，可能会大致不准确的结果，而拒绝桥接表则严重影响了维度解决方案应有的分析能力。所以只有了解它，才能在各种情况下为平衡能力与风险做出正确的抉择。 标准的一对多关系： 多值维度：如果每个订单有多个销售人员。面对多值维度，有两个基本选择：位置设计或桥接表设计。1、事实表中多个键值，位置设计是指预留空值字段用于新增字段，缺点是难以扩展，很快就会发现表的列用光了，要增加新列很困难。仅支持数量有限的关系。2、利用桥接表消除无法扩展和存在大量空值的情况，但是生成的表。 可能会导致双重或多重计算。3、为了解决查询复杂的情况，有时不需要新建桥接表，直接将多个维度用字符串分割，然后存储解决。如存|Ann|Henry| 区域是销售salesrep表的字段。 是关系型数据库的问题。在设计大数据数仓时可以考虑其他这种方式规避。问题解决：1、不提倡2、增加分配因子。不提倡多数情况下，分配因子不存在，可用性较低。3、使用主成员补充桥接表，桥接表不直接对外。不能最终解决问题。不提倡。4、缓慢变化维的问题。5、交叉，同样是对一些关系型分析工具的妥协。不提倡 多值属性理解上，是利用支架表和桥接表，解决方式和问题与多值维度雷同。 参考：star schema完全参考手册https://blog.csdn.net/u012073449/article/list/3?t=1http://book.51cto.com/art/201207/346044.htm多对多维度或多值维度维度表和事实表之间的标准关系是一对多关系，这意味着维度表中的一行记录会连接事实表中的多行记录，但是事实表中的一行记录在维度表中只关联一行记录。这种关系很重要，因为它防止了重复计数。幸运的是，在大多数情况下都是这种一对多关系。在现实世界中还存在比一对多关系更复杂的两种常见情况：事实表和维度表之间的多对多关系。维度表之间的多对多关系。这两种情况本质是相同的，但事实表和维度表之间的多对多关系少了唯一描述事实和维度组的中间维度。对于这两种情况，我们介绍一种称为桥接表的中间表，以支持更复杂的多对多关系。 1、事实表和维度表之间的多对多关系在多个维度表的值可以赋给单个事实事务时，事实表和维度表之间通常是多对多关系。一个常见的示例是多个销售代表可以参与给定的销售事务，这种情形经常发生在涉及大宗交易的复杂销售事件中(例如计算机系统)。精确地处理这种情况需要创建一个桥接表，将销售代表组合成一个组。SalesRepGroup桥接表如图2-4所示。 ETL过程需要针对每条引入的事实表记录中的销售代表组合，在桥接表中查找相应的销售代表组键。如果该销售代表组键不存在，就添加一个新的销售代表组。注意图2-4所示的桥接表有重复计数的风险。如果按照销售代表累加销售量，那么每个销售代表都会对总销售量做出贡献。对某些分析而言结果是正确的，但对于其他情况仍会有重复计数的问题。要解决这个问题，可以向桥接表中添加加权因子列。加权因子是一个分数值，所有的销售代表组的加权因子累加起来为1。将加权因子和累加事实相乘，以按照每个销售代表在分组中的比重分配事实。注意可能需要在Orders和SalesRepGroup之间添加一个SalesRepGroupKey表，以支持真正的主键-外键关系。这会把这个事实-维度实例变成维度-维度实例。但是不提倡使用加权因子，因为如果新增分类，历史数据不好恢复，而且如果加权因子没有达成共识，就没办法设置了。 2、维度之间的多对多关系从分析的角度来看，维度之间的多对多关系是一个很重要的概念，大多数维度都不是完全相互独立的。维度之间的独立性是连续的，而不是有或没有这两种截然不同的状态。例如在连续的一端，零售店这条链状关系的库存维度和产品维度是相对独立的，而不是绝对独立的。一些库存方式不适合某些产品。其他维度之间的关系则紧密得多，但是由于存在多对多关系，因此很难将其组合成单个维度。例如在银行系统中，账户和顾客之间有直接关系，但不是一对一的关系。一个账户可以有一个或多个签名确认的顾客，一个顾客也可有多个账户。银行通常从账户的角度来处理数据；MonthAccountSnapshot(月账快照)是金融机构中常见的一种事实表。因为账户和顾客之间存在的多对多关系，这种更多关注账户的系统就很难按照顾客来查看账户。一种方法是创建CustomerGroup桥接表来连接事实表，例如前面多对多示例中的SalesRepGroup表。较好的方法是利用账户和顾客之间的关系，如图2-5所示。 账户和顾客维度之间的AccountToCustomer桥接表可以捕获多对多关系，并且有几个显著的优点。首先，源系统中的关系是已知的，因此创建桥接表比手动构建SalesRepGroup维度表更容易。其次，账户-顾客关系自身就非常有趣。AccountToCustomer桥接表可以回答诸如”每个顾客的平均账户数量是多少？”这样的问题，而无须连接任何事实表。桥接表经常是底层业务过程的标志，特别是在需要跟踪桥接表随时间而产生的变化(即关系本身是类型2变化)时。对顾客和账户来说，业务过程可能称为账户维护，其中一项事务可能称作”添加签名人”。如果3个顾客与同一个账户关联，在源系统中该账户就会有3个”Add(添加)”事务。通常这些事务和它们表示的业务过程还不是很重要，不需要在DW/BI系统中通过它们自身的事实表来跟踪。然而，这些关系和它们产生的变化对分析业务来说是相当重要的。我们在维度模型中把它们包含为渐变维度，在一些情况下包含为桥接表。]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive默认分号提交问题]]></title>
    <url>%2F2019%2F01%2F28%2Fhive%E9%BB%98%E8%AE%A4%E5%88%86%E5%8F%B7%E6%8F%90%E4%BA%A4%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[将分号改为\0731234567891011121314151617181920212223CREATE EXTERNAL TABLE `table`( `a` string COMMENT &apos;&apos;, `b` string COMMENT &apos;&apos;, `c` string COMMENT &apos;&apos;, `d` int COMMENT &apos;&apos;, `e` string COMMENT &apos;&apos;, `f` string COMMENT &apos;&apos;, `g` string COMMENT &apos;&apos;, `h` int COMMENT &apos;&apos;)PARTITIONED BY ( `dt` string)ROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.RegexSerDe&apos;WITH SERDEPROPERTIES ( &apos;input.regex&apos;=&apos;(\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125;\\s*\\d&#123;2&#125;:\\d&#123;2&#125;:\\d&#123;2&#125;).\\d&#123;3&#125;\\s*[A-Z]&#123;3,5&#125;\\s*L:([a-zA-Z0-9_\\-]&#123;1,50&#125;),D:([A-Z0-9_]&#123;1,5&#125;),Mod:([0-9]&#123;1,2&#125;),([a-zA-Z0-9_]&#123;10,30&#125;):([A-Z]&#123;1&#125;),ClientCode:([a-zA-z\\d\\-_\\.\\|]&#123;1,100&#125;)(?:,RuleVersion:(\\d&#123;1,3&#125;))\073&apos;, &apos;output.format.string&apos;=&apos;%1$s %2$s %3$s %4$s %5$s %6$s %7$s &apos; )STORED AS INPUTFORMAT &apos;org.apache.hadoop.mapred.TextInputFormat&apos;OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&apos;LOCATION &apos;hdfs://qunarcluster/user/abtest/log/abtest_sdk_log_daycombine&apos;]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支架表]]></title>
    <url>%2F2019%2F01%2F28%2F%E6%94%AF%E6%9E%B6%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[支架表看起来和雪花模式有些类似。采用支架表的原因是：1、支架表中的维度没有基本维度表中的分析价值大，使用也不是很频繁。2、其次如果有维度在基本维度中有很大的冗余(基本维度记录中都含有重复字段)，那么将不常用的放到支架表中可以节省空间。备注：可以使用支架表，但是只可偶尔为之，不要常用。如果您的设计包含大量的支架表，就应该拉响警报。您可能会陷入过度规范化设计的麻烦之中。]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[横表和纵表]]></title>
    <url>%2F2019%2F01%2F28%2F%E6%A8%AA%E8%A1%A8%E5%92%8C%E7%BA%B5%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[https://blog.csdn.net/zhangzeyuaaa/article/details/50675058]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里onedata]]></title>
    <url>%2F2019%2F01%2F28%2F%E9%98%BF%E9%87%8Conedata%2F</url>
    <content type="text"><![CDATA[OneData体系分为：1、数据规范定义体系2、数据模型规范设计3、ETL规范研发以及支撑整个体系从方法到实施的工具体系。 落地实现数据规范定义将此前个性化的数据指标进行规范定义，抽象成：原子指标、时间周期、其他修饰词等三个要素。例如，以往业务方提出的需求是：最近7天的成交。而实际上，这个指标在规范定义中，应该结构化分解成为： 原子指标（支付订单金额 ）+修饰词-时间周期（最近7天）+修饰词-卖家类型（淘宝） 上文只有抽象指标的方法，应该还包括字段规范，表名规范派生指标的定义也很重要，因为数据运营人员经常会查最近7，最近一月的指标，每次都从基础事实表中，查询的量会很大。 数据模型架构 理解为数据仓库的分层设计 将数据分为ODS（操作数据）层、CDM（公共维度模型）层、ADS（应用数据）层。 其中：ODS层主要功能 这里先介绍一下阿里云数加大数据计算服务MaxCompute，是一种快速、完全托管的TB/PB级数据仓库解决方案，适用于多种数据处理场景，如日志分析，数据仓库，机器学习，个性化推荐和生物信息等。 同步：结构化数据增量或全量同步到数加MaxCompute（原ODPS）；结构化：非结构化(日志)结构化处理并存储到MaxCompute（原ODPS）；累积历史、清洗：根据数据业务需求及稽核和审计要求保存历史数据、数据清洗； CDM层主要功能 CDM层又细分为DWD层和DWS层，分别是明细宽表层和公共汇总数据层，采取维度模型方法基础，更多采用一些维度退化手法，减少事实表和维度表的关联，容易维度到事实表强化明细事实表的易用性；同时在汇总数据层，加强指标的维度退化，采取更多宽表化的手段构建公共指标数据层，提升公共指标的复用性，减少重复的加工。ADS层主要功能 个性化指标加工：不公用性；复杂性（指数型、比值型、排名型指标）基于应用的数据组装：大宽表集市、横表转纵表、趋势指标串 其模型架构图如上图，阿里通过构建全域的公共层数据，极大的控制了数据规模的增长趋势，同时在整体的数据研发效率，成本节约、性能改进方面都有不错的结果。 研发流程和工具落地实现将OneData体系贯穿于整个研发流程的每个环节中，并通过研发工具来进行保障。 实施效果1、数据标准统一：数据指标口径一致，各种场景下看到的数据一致性得到保障2、支撑多个业务，极大扩展性：服务了集团内部45个BU的业务，满足不同业务的个性化需求3、统一数据服务：建立了统一的数据服务层，其中离线数据日均调用次数超过22亿；实时数据调用日均超过11亿4、计算、存储成本：指标口径复用性强，将原本30000多个指标精简到3000个；模型分层、粒度清晰，数据表从之前的25000张精简到不超过3000张。5、研发成本：通过数据分域、模型分层，强调工程师之间的分工和协作，不再需要从头到尾每个细节都了解一遍，节省了工程师的时间和精力。参考：https://yq.aliyun.com/articles/67011 明细宽表层-面向业务建模过程： &emsp;&emsp;事务型事实宽表，周期性快照事实宽表，累计快照事实宽表公共汇总款表层-面向分析主题建模: &emsp;&emsp;流量，订单，商品，用户 上面两层都是基于公共维度表的基础上 维度退化手法指标维度退化大宽表集市、横表转纵表、趋势指标串个性化指标加工：不公用性；复杂性（指数型、比值型、排名型指标）]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hiveserver2]]></title>
    <url>%2F2019%2F01%2F28%2Fhiveserver2%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185import org.apache.log4j.Logger;import java.io.BufferedWriter;import java.io.FileWriter;import java.io.IOException;import java.sql.*;public class HiveJdbcCli &#123; //网上写 org.apache.hadoop.hive.jdbc.HiveDriver ,新版本不能这样写 private static String driverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;; //这里是hive2，网上其他人都写hive,在高版本中会报错// private static String url = &quot;jdbc:hive2://master:10000/default&quot;; private static String url = &quot;jdbc:hive2://hdpm5.corp.qunar.com:2181,hdpm6.corp.qunar.com:2181,hdpm7.corp.qunar.com:2181/intercarhive;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=datahiveserver2_zk&quot;; private static String user = &quot;intercarhive&quot;; private static String password = &quot;intercarhive@qunar.com&quot;; private static String sql = &quot;&quot;; private static ResultSet res; private static final Logger log = Logger.getLogger(HiveJdbcCli.class); public static void main(String[] args) &#123; Connection conn = null; Statement stmt = null; try &#123; conn = getConn(); stmt = conn.createStatement(); // 第一步:存在就先删除 String tableName = dropTable(stmt); stmt.close(); stmt = conn.createStatement(); // 第二步:不存在就创建 createTable(stmt, tableName); stmt.close(); stmt = conn.createStatement(); // 第三步:查看创建的表 showTables(stmt, tableName); stmt.close(); stmt = conn.createStatement(); // 执行describe table操作 describeTables(stmt, tableName); stmt.close(); stmt = conn.createStatement(); selectData2File(stmt, &quot; pf_test.hdfs_fsimage_file_orc&quot;, &quot;/d/test/fsimage.txt&quot;); stmt.close(); stmt = conn.createStatement();// // 执行load data into table操作// loadData(stmt, tableName);//// // 执行 select * query 操作// selectData(stmt, tableName);//// // 执行 regular hive query 统计操作// countData(stmt, tableName); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); log.error(driverName + &quot; not found!&quot;, e); System.exit(1); &#125; catch (SQLException e) &#123; e.printStackTrace(); log.error(&quot;Connection error!&quot;, e); System.exit(1); &#125; finally &#123; try &#123; if (conn != null) &#123; conn.close(); conn = null; &#125; if (stmt != null) &#123; stmt.close(); stmt = null; &#125; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125; private static void countData(Statement stmt, String tableName) throws SQLException &#123; sql = &quot;select count(1) from &quot; + tableName; System.out.println(&quot;Running:&quot; + sql); res = stmt.executeQuery(sql); System.out.println(&quot;执行“regular hive query”运行结果:&quot;); while (res.next()) &#123; System.out.println(&quot;count ------&gt;&quot; + res.getString(1)); &#125; &#125; private static void selectData(Statement stmt, String tableName) throws SQLException &#123; sql = &quot;select * from &quot; + tableName + &quot; limit 100&quot;; System.out.println(&quot;Running:&quot; + sql); res = stmt.executeQuery(sql); System.out.println(&quot;执行 select * query 运行结果:&quot;); while (res.next()) &#123; System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2)); &#125; &#125; private static void selectData2File(Statement stmt, String tableName, String fileName) &#123; sql = &quot;select * from &quot; + tableName + &quot; limit 10000000&quot;; System.out.println(&quot;Running:&quot; + sql); try &#123; res = stmt.executeQuery(sql); System.out.println(&quot;执行 select * query 运行结果:&quot;); BufferedWriter bw = new BufferedWriter(new FileWriter(fileName)); ResultSetMetaData rsmd = res.getMetaData(); int columnsNumber = rsmd.getColumnCount(); while (res.next()) &#123;// System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2)); for (int i = 1; i &lt;= columnsNumber; i++) &#123; bw.write(res.getString(i) + &quot;\t&quot;); &#125; bw.write(&quot;\n&quot;); &#125; bw.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; private static void loadData(Statement stmt, String tableName) throws SQLException &#123; //目录 ，我的是hive安装的机子的虚拟机的home目录下 String filepath = &quot;user.txt&quot;; sql = &quot;load data local inpath &apos;&quot; + filepath + &quot;&apos; into table &quot; + tableName; System.out.println(&quot;Running:&quot; + sql); stmt.execute(sql); &#125; private static void describeTables(Statement stmt, String tableName) throws SQLException &#123; sql = &quot;describe &quot; + tableName; System.out.println(&quot;Running:&quot; + sql); res = stmt.executeQuery(sql); System.out.println(&quot;执行 describe table 运行结果:&quot;); while (res.next()) &#123; System.out.println(res.getString(1) + &quot;\t&quot; + res.getString(2)); &#125; &#125; private static void showTables(Statement stmt, String tableName) throws SQLException &#123; sql = &quot;show tables &apos;&quot; + tableName + &quot;&apos;&quot;; System.out.println(&quot;Running:&quot; + sql); res = stmt.executeQuery(sql); System.out.println(&quot;执行 show tables 运行结果:&quot;); if (res.next()) &#123; System.out.println(res.getString(1)); &#125; &#125; private static void createTable(Statement stmt, String tableName) throws SQLException &#123; sql = &quot;create table &quot; + tableName + &quot; (key int, value string) row format delimited fields terminated by &apos;\t&apos;&quot;; stmt.execute(sql); &#125; private static String dropTable(Statement stmt) throws SQLException &#123; // 创建的表名 String tableName = &quot;testHive&quot;; sql = &quot;drop table if exists &quot; + tableName; stmt.execute(sql); return tableName; &#125; private static Connection getConn() throws ClassNotFoundException, SQLException &#123; Class.forName(driverName); Connection conn = DriverManager.getConnection(url, user, password); return conn; &#125;&#125; 1234567891011121314151617&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.45&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive使用和优化]]></title>
    <url>%2F2019%2F01%2F28%2Fhive%E4%BD%BF%E7%94%A8%E5%92%8C%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[使用complex type array 和 map 类型创建表时指定类型和分隔符 create table test1(f1 array,f2 map&lt;string,int&gt;)ROW FORMAT DELIMITEDCOLLECTION ITEMS TERMINATED BY ‘|’MAP KEYS TERMINATED BY ‘:’;文件中按照表定义的分隔符来分割，字段的分隔符默认为^A，可以在建表时通过 FIELDS TERMINATED BY ‘^A’ 指定 cat /home/hadoop/hivetest1.txt 1|2|3^Aa:1|b:211|12|13^Aaa:1|bb:2把文件 load 到表中 load data local inpath ‘/home/hadoop/hivetest1.txt’ into table test1;在查询中使用array 和 map 类型 select f1[0],f2[‘aa’] from test1;select * from test1 where f1[0]=11 and f2[‘aa’]=1;处理json字符串 create table test(json_str string);test表的json_str字段的内容为： {“a”:”1”,”b”:”2”,”c”:[“aa”,”11”,{“aaa”:”111”,”bbb”:”222”}]} select get_json_object(json_str,’$.b’) from test;select get_json_object(json_str,’$.c[0]’) from test;select get_json_object(json_str,’$.c[2].aaa’) from test;动态分区 set hive.exec.dynamic.partition=true;set hive.exec.dynamic.partition.mode=nonstrict;另外，如果默认值不够大，根据需要调大以下三个参数，否则会报错 set hive.exec.max.dynamic.partitions=2000set hive.exec.max.dynamic.partitions.pernode=1000set hive.exec.max.created.files=110000;这样，从另外一个表把数据导入到目标表时，就会自动按照指定字段分区。例如： insert overwrite table A partition(reportdate) select * from B;表A要先创建，并且以reportdate作为分区的字段。在上面sql中不用指定reportdate的值（如果是非动态分区则需要指定）。表B最后一个字段作为分区的字段，会自动根据最后一个字段的值自动分区。 map join 优化将hive.auto.convert.join设置为true set hive.auto.convert.join=true;设置进行mapjoin的小表的阈值（即当小表的大小 少于 一定该阈值时 hive 会做 mapjoin 优化，如果不设置，则采用默认值） set hive.mapjoin.smalltable.filesize=25000000;hive.mapjoin.bucket.cache.size=100每一个key有多少个value值缓存在内存中hive.mapjoin.cache.numrows=25000有多少行cache在内存中 insert into/overwrite多表插入的优化通过对原表扫描一遍，插入到不同的目的表中 from table insert overwrite table dest1 select where col=condition ; insert overwrite table dest2 select where col=condition ; insert overwrite table dest3 select * where col=condition ;对limit语句的优化 hive.limit.optimize.enable=falsehive.limit.row.max.size=100000hive.limit.optimize.limit.file=10hive.limit.optimize.fetch.max=50000对reduce个数的设置hive默认是根据输入的大小来设置，每个reducer处理的数据量是1G。如果有10G的输入数据，则hive自动生成10个reducer。默认情况下一个reducer处理1G的数据，这样一个reducer处理的数据量太大，可以改小一些。 hiive.exec.reducers.bytes.per.reducer=1Ghive.exec.reducers.max=999mapred.reduce.tasks数据倾斜优化有编译器的优化和运行期的优化 hive.groupby.skewindata=falsehive.optimize.skewjoin.compiletime=falsehive.optimize.skewjoin=falsehive.skewjoin.key=100000文件压缩对中间结果的压缩和最终结果的压缩 hive.exec.compress.output=falsehive.exec.compress.intermediate=false并行执行一个hive sql语句编译成多个MR作业，没有依赖关系的作业可以并行执行。 hive.exec.parallel=falsehive.exec.parallel.thread.number=8合并结果的小文件对于hive的结果中是小文件的，会再起一个MR作业合并小文件。 hive.merge.mapfiles=truehive.merge.mapredfiles=falsehive.merge.size.per.task=256000000hive.merge.smallfiles.avgsize=16000000推断执行 hive.mapred.reduce.tasks.speculative.execution=true设置reduce个数显示设置reduce的个数，或者每个reduce处理的数据的大小（默认1G的值很多时候有些大，可以设置小一些，同时reduce的内存也要相应小一些，提高并行度）。 mapred.reduce.tasks=-1hive.exec.reducers.bytes.per.reducer=1000000000（1G）hive.exec.reducers.max=999map端聚合 hive.map.aggr=truegroup by语句时现在map聚合一次，减少传输到reduce的数据，就是mapreduce的combiner。默认是打开的。 列剪裁列剪裁优化，只对需要的列进行处理，忽略其他的列，减少数据的处理量。默认是打开的。 hive.optimize.cp=true本地模式一些sql处理的数据量比较少，或者计算量比较少，可以在本地运行而不是MR作业运行，这样性能会更好些，也节约hadoop集群的资源。 hive.exec.mode.local.auto=falsehive.fetch.task.conversion=minimal测试模式 hive.test.mode=falsehive.test.mode.prefix=test_hive.test.mode.samplefreq=32hive.test.mode.nosamplelist=table1,table2,table3用于测试，通过采样减少输入的数据，结果表前面加前缀“test_” 严格模式 hive.mapred.mode=nonstrict通过严格模式，使一些不严格的用法不通过，防止潜在的错误。No partition being picked up for a query.Comparing bigints and strings.Comparing bigints and doubles.Orderby without limit. jvm重用JVM重用是hadoop调优参数的内容，对hive的性能具有非常大的 影响，特别是对于很难避免小文件的场景或者task特别多的场景，这类场景大多数执行时间都很短。hadoop默认配置是使用派生JVM来执行map和 reduce任务的，这是jvm的启动过程可能会造成相当大的开销，尤其是执行的job包含有成千上万个task任务的情况。 JVM重用可以使得JVM实例在同一个JOB中重新使用N次，N的值可以在Hadoop的mapre-site.xml文件中进行设置 1mapred.job.reuse.jvm.num.tasks 也可在hive的执行设置：1set mapred.job.reuse.jvm.num.tasks=10; JVM的一个缺点是，开启JVM重用将会一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡“的job中有几个 reduce task 执行的时间要比其他reduce task消耗的时间多得多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。 hive参数优化之默认启用本地模式启动hive本地模式参数，一般建议将其设置为true，即时刻启用：12hive (chavin)&gt; set hive.exec.mode.local.auto; hive.exec.mode.local.auto=false 推测执行相关配置1234567891011hive (default)&gt; set mapred.map.tasks.speculative.execution;mapred.map.tasks.speculative.execution=truehive (default)&gt; set mapred.reduce.tasks.speculative.execution;mapred.reduce.tasks.speculative.execution=truehive (default)&gt; set hive.mapred.reduce.tasks.speculative.execution;hive.mapred.reduce.tasks.speculative.execution=true 单个mapreduce中运行多个group by参数hive.multigroupby.singlemr控制师徒将查询中的多个group by组装到单个mapreduce任务中。如果启用这个优化，那么需要一组常用的group by键： 聚合优化：启用参数：hive.map.aggr=true,默认开启 参数hive.fetch.task.conversion的调优：默认值：hive.fetch.task.conversion=minimal 建议值：set hive.fetch.task.conversion=more;参考：https://www.cnblogs.com/duanxingxing/p/4535842.html]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive-site.xml 参数优化]]></title>
    <url>%2F2019%2F01%2F28%2Fhive-site-xml-%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[强烈建议按照规定设置Hive参数，提高资源利用率！123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117&lt;!-- 任务输出小文件后，自动启动任务进行合并 --&gt;&lt;property&gt; &lt;name&gt;hive.merge.mapfiles&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Merge small files at the end of a map-only job&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.merge.mapredfiles&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Merge small files at the end of a map-reduce job&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.merge.tezfiles&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Merge small files at the end of a Tez DAG&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.merge.sparkfiles&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Merge small files at the end of a Spark DAG Transformation&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.merge.size.per.task&lt;/name&gt; &lt;value&gt;2560000000&lt;/value&gt; &lt;description&gt;Size of merged files at the end of the job&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.merge.smallfiles.avgsize&lt;/name&gt; &lt;value&gt;32000000&lt;/value&gt; &lt;description&gt; When the average output file size of a job is less than this number, Hive will start an additional map-reduce job to merge the output files into bigger files. This is only done for map-only jobs if hive.merge.mapfiles is true, and for map-reduce jobs if hive.merge.mapredfiles is true. &lt;/description&gt;&lt;/property&gt; &lt;!— 新建表默认格式 —&gt;&lt;property&gt; &lt;name&gt;hive.default.fileformat&lt;/name&gt; &lt;value&gt;orc&lt;/value&gt; &lt;description&gt; Expects one of [textfile, sequencefile, rcfile, orc]. Default file format for CREATE TABLE statement. Users can explicitly override it by CREATE TABLE ... STORED AS [FORMAT] &lt;/description&gt;&lt;/property&gt; &lt;!— blocksize —&gt;&lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;268435456&lt;/value&gt; &lt;description&gt;&lt;/description&gt;&lt;/property&gt; &lt;!— 数据倾斜优化参数 —&gt;&lt;property&gt; &lt;name&gt;hive.optimize.skewjoin.compiletime&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.optimize.skewjoin&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.optimize.groupby&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;&lt;/description&gt;&lt;/property&gt; &lt;!— 压缩参数设置 —&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.compress.output&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec&lt;/value&gt;&lt;/property&gt;&lt;!— 多个小文件用使用一个map —&gt;&lt;property&gt; &lt;name&gt;hive.input.format&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.io.CombineHiveInputFormat&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.input.fileinputformat.split.maxsize&lt;/name&gt; &lt;value&gt;1073741824&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.input.fileinputformat.split.minsize&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.input.fileinputformat.split.minsize.per.node&lt;/name&gt; &lt;value&gt;536870912&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.input.fileinputformat.split.minsize.per.rack&lt;/name&gt; &lt;value&gt;536870912&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.reducers.bytes.per.reducer&lt;/name&gt; &lt;value&gt;2147483648&lt;/value&gt;&lt;/property&gt;]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink vs spark]]></title>
    <url>%2F2019%2F01%2F28%2Fflink-vs-spark%2F</url>
    <content type="text"><![CDATA[概念在 spark 中， 批处理使用 RDD， 流处理使用 DStream（内部使用RDD实现）， 所有底层统一都使用 RDD的抽象实现在 flink 中， 批处理使用 Dataset， 流处理使用 DataStreams， 听起来类似 RDD 和 DStreams， 但其实不是， 不同点在于 1、flink 中的 Dataset 代表着执行计划， 而spark 的 RDD 仅仅是一个 java 对象，spark中的dataframes 才有执行计划， flink 和 spark 两者最基础的东西， Dataset 和 RDD， 是不同的。 一个是经过优化器优化的， 一个没有。 flink 中的 Dataset 类似 spark 中经过优化的 Dataframe 概念， spark 1.6 中引入的dataset（跟 flink 中的 Dataset 重名了，两者类似） 最终应该会代替RDD的抽象吧。 2、在 spark 中， DStream 和 Dataframe 等都是基于 RDD 的封装， 然而 flink 中的 Dataset 和 DataStream 则是独立实现的， 尽管两者间尽量保持相同的 API， 但是你很难统一起来， 至少没有 spark 中那样优雅， 这个大方向， flink 能不能做到就难说了。我们不能统一 DataSet 和 DataStreams， 尽管 flink 有和 spark 相同的抽象，但是内部实现是不同的。 内存管理spark 1.5 之前， spark 一直都是使用 java jvm 堆来保存对象， 虽然有利于启动项目， 但是经常会产生 OOM 和 gc 问题， 所以 1.5 开始， spark 开始引入 自己管理内存的 tungsten 项目。 Flink 从第一天起就自己管理内存， spark 就是从这学的吧， 不仅保存数据使用 binary data， 而且可以直接在binary data 上进行操作。 spark 1.5 开始也可以直接在binary data 进行 dataframe API 提供的操作了。 自己管理内存， 直接使用分配的binary data而不是JVM objects 可以得到更高的性能 和 更好的资源利用。 流式处理在 spark 的眼里， streaming 是特殊的 batch， 在 flink 眼里， batch 是特殊的 streaming， 主要的区别在于1、实时 vs 准实时 ，flink 提供 event 事件级别的延迟， 可以认为是实时的， 类似于 storm 的模型， 而 spark 中， 是微批处理， 不能提供事件级别的延迟， 我们可以称之为准实时。2、flink 则可以灵活的支持窗口， 支持带有事件时间的窗口（Window）操作是flink 的亮点， 你可以选择使用处理时间还是事件时间进行窗口操作， 这种灵活性是 spark 所不如的。3、spark 来自于 Map/Reduce 时代， 崇尚 运算追着数据走， 数据可以是内存中的数组， 也可以是磁盘中的文件， 可以进行很好的容错。Flink 的模型中， 数据是追着运算走的， 算子位于节点上， 数据从中流过， 类似于 akka-streams 中的概念。]]></content>
      <categories>
        <category>flink</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streamin、kafka、redis、 Exactly once]]></title>
    <url>%2F2019%2F01%2F28%2FSpark-Streamin%E3%80%81kafka%E3%80%81redis%E3%80%81-Exactly-once%2F</url>
    <content type="text"><![CDATA[转：http://lxw1234.com/archives/2018/02/901.htm]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[状态计算]]></title>
    <url>%2F2019%2F01%2F28%2F%E7%8A%B6%E6%80%81%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[spark流计算的数据是以窗口的形式，源源不断的流过来的。如果每个窗口之间的数据都有联系的话，那么就需要对前一个窗口的数据做状态管理。spark有提供了两种模型来达到这样的功能，一个是updateStateByKey，另一个是mapWithState ，后者属于Spark1.6之后的版本特性，性能是前者的数十倍。基本的wordcount123456789101112131415161718192021222324252627282930313233343536package com.scala.testimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;object WC &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() //程序在运行时receiver会独占一个线程,所以streaming程序至少要两个线程,防止starvation scenario conf.setAppName(&quot;WordCount&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) //所有流功能的主要入口 val smc : StreamingContext = new StreamingContext(sc, Seconds(5)) //指定从TCP源数据流的离散流,接收到的每一行数据都是一行文本 val lines : ReceiverInputDStream[String] = smc.socketTextStream(&quot;localhost&quot;,6666) //将接收到的文本压平,转换,聚合 val dStream : DStream[(String, Int)] = lines.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_) dStream.print() // Spark Streaming 只有建立在启动时才会执行计算，在它已经开始之后，并没有真正地处理// -------------------------- //启动计算 smc.start(); //等待计算终止 smc.awaitTermination(); //true 会把内部的sparkcontext同时停止 //false 只会停止streamingcontext 不会停sparkcontext smc.stop(true); &#125;&#125; updateStateByKey1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.scala.testimport org.apache.spark.streaming.dstream.&#123;DStream, ReceiverInputDStream&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;import scala.util.parsing.json.JSON/** * * https://blog.csdn.net/cyony/article/details/79653357 * nc -lk 6666 * 样例数据 * &#123;&quot;name&quot;:&quot;cyony1&quot;,&quot;score&quot;:&quot;90&quot;,&quot;sex&quot;:&quot;1&quot;&#125; * * &#123;&quot;name&quot;:&quot;cyony2&quot;,&quot;score&quot;:&quot;76&quot;,&quot;sex&quot;:&quot;0&quot;&#125; * * updateStateByKey这种模型，每次窗口触发，都会将两个RDD执行cogroup操作，，非常的耗时。而且checkpoint dir也会很大 * */object WC_stateful &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() //程序在运行时receiver会独占一个线程,所以streaming程序至少要两个线程,防止starvation scenario conf.setAppName(&quot;WordCount&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) //所有流功能的主要入口 val smc : StreamingContext = new StreamingContext(sc, Seconds(5)) //定义checkpoint目录 smc.checkpoint(&quot;./wc_stateful&quot;) //指定从TCP源数据流的离散流,接收到的每一行数据都是一行文本 val lines : ReceiverInputDStream[String] = smc.socketTextStream(&quot;localhost&quot;,6666) //定义updateStateByKey更新函数 val updateFunc = (currentValue:Seq[Int],preValue:Option[Int]) =&gt; &#123; Some(currentValue.sum + preValue.getOrElse(0)) &#125; //将接收到的文本压平,转换,聚合 lines.map(JSON.parseFull(_).get.asInstanceOf[Map[String,String]]) .map(map =&gt; (map.get(&quot;sex&quot;).get.toInt,map.get(&quot;score&quot;).get.toInt)) .reduceByKey(_+_) .updateStateByKey(updateFunc) .print()// dStream.print() // Spark Streaming 只有建立在启动时才会执行计算，在它已经开始之后，并没有真正地处理// -------------------------- //启动计算 smc.start(); //等待计算终止 smc.awaitTermination(); //true 会把内部的sparkcontext同时停止 //false 只会停止streamingcontext 不会停sparkcontext smc.stop(true); &#125;&#125; mapWithState123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package com.scala.testimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.streaming.&#123;Seconds, State, StateSpec, StreamingContext&#125;import org.apache.spark.streaming.dstream.ReceiverInputDStreamimport scala.util.parsing.json.JSON/** * * https://blog.csdn.net/cyony/article/details/79653357 * nc -lk 6666 * 样例数据 * &#123;&quot;name&quot;:&quot;cyony1&quot;,&quot;score&quot;:&quot;90&quot;,&quot;sex&quot;:&quot;1&quot;&#125; * * &#123;&quot;name&quot;:&quot;cyony2&quot;,&quot;score&quot;:&quot;76&quot;,&quot;sex&quot;:&quot;0&quot;&#125; * * 如果当前窗口期没有新的数据过来，mapstate方式是根本不会触发状态更新操作的，但是updateState方式就会触发更新操作。 * 这个和他的模型原理有关，进一步佐证了updateState方式会每次都执行cogroup操作RDD，生成新的RDD。 * * https://www.jianshu.com/p/1463bc1d81b5 * https://blog.csdn.net/zangdaiyang1991/article/details/84099722 * http://www.cnblogs.com/DT-Spark/articles/5616560.html * */object WC_mapWithState &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() //程序在运行时receiver会独占一个线程,所以streaming程序至少要两个线程,防止starvation scenario conf.setAppName(&quot;WordCount&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) //所有流功能的主要入口 val smc : StreamingContext = new StreamingContext(sc, Seconds(5)) //定义checkpoint目录 smc.checkpoint(&quot;./wc_mapWithState&quot;) //指定从TCP源数据流的离散流,接收到的每一行数据都是一行文本 val lines : ReceiverInputDStream[String] = smc.socketTextStream(&quot;localhost&quot;,6666) //定义MapWithState更新函数 val mappingFun = (sex: Int, score: Option[Int], state: State[Int]) =&gt; &#123; val sum = score.getOrElse(0) + state.getOption().getOrElse(0) state.update(sum) (sex, sum) &#125; //将接收到的文本压平,转换,聚合 lines.map(JSON.parseFull(_).get.asInstanceOf[Map[String, String]]) .map(map =&gt; (map.get(&quot;sex&quot;).get.toInt, map.get(&quot;score&quot;).get.toInt)).reduceByKey(_ + _) .mapWithState(StateSpec.function(mappingFun)) .print() // dStream.print() // Spark Streaming 只有建立在启动时才会执行计算，在它已经开始之后，并没有真正地处理 // -------------------------- //启动计算 smc.start(); //等待计算终止 smc.awaitTermination(); //true 会把内部的sparkcontext同时停止 //false 只会停止streamingcontext 不会停sparkcontext smc.stop(true); &#125;&#125;]]></content>
      <categories>
        <category>spark编程</category>
      </categories>
      <tags>
        <tag>spark编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实时uv方案]]></title>
    <url>%2F2019%2F01%2F28%2F%E5%AE%9E%E6%97%B6uv%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[将spark streaming按window将uv明细数据入到hbase中，然后开启小时，天，周的定时任务，利用hive读取hbase计算每小时，每周，每天的数据，同时hbase开启ttl，减少存储量。分钟级的可以利用状态计算来解决。]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[幂等操作]]></title>
    <url>%2F2019%2F01%2F28%2F%E5%B9%82%E7%AD%89%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[最近很多人都在谈论幂等性，好吧，这回我也来聊聊这个话题，光看着俩字，一开始的确有点一头雾水，语文不好嘛，词太专业嘛，对吧 现如今我们的系统大多拆分为分布式SOA，或者微服务，一套系统中包含了多个子系统服务，而一个子系统服务往往会去调用另一个服务，而服务调用服务无非就是使用RPC通信或者restful，既然是通信，那么就有可能再服务器处理完毕后返回结果的时候挂掉，这个时候用户端发现很久没有反应，那么就会多次点击按钮，这样请求有多次，那么处理数据的结果是否要统一呢？那是肯定的！尤其再支付场景。幂等性：就是用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用。举个最简单的例子，那就是支付，用户购买商品使用约支付，支付扣款成功，但是返回结果的时候网络异常，此时钱已经扣了，用户再次点击按钮，此时会进行第二次扣款，返回结果成功，用户查询余额返发现多扣钱了，流水记录也变成了两条．．．在以前的单应用系统中，我们只需要把数据操作放入事务中即可，发生错误立即回滚，但是再响应客户端的时候也有可能出现网络中断或者异常等等。在增删改查4个操作中，尤为注意就是增加或者修改，查询对于结果是不会有改变的，删除只会进行一次，用户多次点击产生的结果一样修改在大多场景下结果一样增加在重复提交的场景下会出现那么如何设计接口才能做到幂等呢？ 方法一、单次支付请求，也就是直接支付了，不需要额外的数据库操作了，这个时候发起异步请求创建一个唯一的ticketId，就是门票，这张门票只能使用一次就作废，具体步骤如下：异步请求获取门票调用支付，传入门票根据门票ID查询此次操作是否存在，如果存在则表示该操作已经执行过，直接返回结果；如果不存在，支付扣款，保存结果返回结果到客户端如果步骤4通信失败，用户再次发起请求，那么最终结果还是一样的 方法二、分布式环境下各个服务相互调用这边就要举例我们的系统了，我们支付的时候先要扣款，然后更新订单，这个地方就涉及到了订单服务以及支付服务了。用户调用支付，扣款成功后，更新对应订单状态，然后再保存流水。而在这个地方就没必要使用门票ticketId了，因为会比较闲的麻烦（支付状态：未支付，已支付）步骤：1、查询订单支付状态2、如果已经支付，直接返回结果3、如果未支付，则支付扣款并且保存流水4、返回支付结果如果步骤4通信失败，用户再次发起请求，那么最终结果还是一样的对于做过支付的朋友，幂等，也可以称之为冲正，保证客户端与服务端的交易一致性，避免多次扣款。 最后来看一下我们的订单流程，虽然不是很复杂，但是最后在支付环境是一定要实现幂等性的 可以简单理解成upsert操作，有则更新，无则插入参考https://www.cnblogs.com/leechenxiang/p/6626629.html]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Streaming如何应对 Exactly once 语义（kafka）]]></title>
    <url>%2F2019%2F01%2F28%2FSpark-Streaming%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9-Exactly-once-%E8%AF%AD%E4%B9%89%2F</url>
    <content type="text"><![CDATA[Spark Streaming（以下简写SS）Exactly once语义（以下简写EO） 首先EO表示可以精准控制到某一条记录，但由于SS是基于rdd和batch的，所以SS的EO可以认为是针对一个批次的的精准控制(控制各个批次间是否重复和漏读)。涉及到三部分都保证 exactly once 的语义。1、(数据源)上游是否EO到SS2、(数据处理)SS作为整体是否保证了EO3、(数据存储)SS是否将数据EO地写出到了下游 涉及到 上游是否EO到SS对于 接收数据，主要取决于上游数据源的特性。例如，从 HDFS 这类支持容错的文件系统中读取文件，能够直接支持 Exactly-once 语义。如果上游消息系统支持 ACK（如RabbitMQ），我们就可以结合 Spark 的 Write Ahead Log 特性来实现 At-least-once 语义。对于非可靠的数据接收器（如 socketTextStream），当 Worker 或 Driver 节点发生故障时就会产生数据丢失，提供的语义也是未知的。而 Kafka 消息系统是基于偏移量（Offset）的，它的 Direct API 可以提供 Exactly-once 语义。 官方在创建 DirectKafkaInputStream(Kafka direct api)时只需要输入消费 Kafka 的 From Offset，然后其自行获取本次消费的 End Offset，也就是当前最新的 Offset。保存的 Offset 是本批次的 End Offset，下次消费从上次的 End Offset 开始消费。 当程序宕机或重启任务后，这其中存在一些问题。如果在数据处理完成前存储 Offset，则可能存在作业处理数据失败与作业宕机等情况，重启后会无法追溯上次处理的数据导致数据出现丢失。如果在数据处理完成后存储 Offset，但是存储 Offset 过程中发生失败或作业宕机等情况，则在重启后会重复消费上次已经消费过的数据。 而且此时又无法保证重启后消费的数据与宕机前的数据量相同数据相当，这又会引入另外一个问题，如果是基于聚合统计指标作更新操作，这会带来无法判断上次数据是否已经更新成功。 参考文章中给出的解决方案是，保证在创建 DirectKafkaInputStream 可以同时输入 From Offset 与 End Offset，并且我们在存储 Kafka Offset 的时候保存了每个批次的起始Offset 与结束 Offset。这样的设计使得后面用户在后面对于第一个批次的数据处理非常灵活可变：1、如果用户直接忽略第一个批次的数据，那此时保证的是 at most once 的语义，因为我们无法获知重启前的最后一个批次数据操作是否有成功完成。2、如果用户依照原有逻辑处理第一个批次的数据，不对其做去重操作，那此时保证的是 at least once 的语义，最终结果中可能存在重复数据；3、最后如果用户想要实现 exactly once，muise spark core 提供了根据topic、partition 与 offset 生成 UID 的功能。只要确保两个批次消费的 Offset 相同，则最终生成的 UID 也相同，用户可以根据此 UID 作为判断上个批次数据是否有存储成功的依据。下面简单的给出了重启后第一个批次操作的行为。 参考:http://www.10tiao.com/html/522/201809/2651425155/1.html实际上如果是聚合操作，完全可以引入状态计算，而不需要修改源码。或者将Offset，存储到redis中，每次存redis中读取,见其他文章https://blog.csdn.net/qq_32252917/article/details/78827126 。本文只对EO做讨论，状态计算在其他文章做介绍。EO只要明确保证能拿到上次成功结束的offset就可以了(保证数据零丢失)，至于后面是否会被重复计算部分，可以根据业务做不同的处理(根据输出做幂等设计)。 控制offset实际上是解决数据丢失如下的主要场景：SS在使用Receiver收到数据时(非Kafka direct api)，通过Driver的调度，Executor开始计算数据的时候如果Driver突然奔溃（导致Executor会被Kill掉），此时Executor会被Kill掉，那么Executor中的数据就会丢失，此时就必须通过例如WAL机制让所有的数据通过类似HDFS的方式进行安全性容错处理，从而解决Executor被Kill掉后导致数据丢失可以通过WAL机制恢复回来。此时数据可以零丢失，但并不能保证Exactly Once，如果Receiver接收且保存起来后没来得及更新updateOffsets时，就会导致数据被重复处理。所以本文讨论的EO是用户自己能精准控制offset而非交给框架去处理。 SS处理数据是否保证了EO在使用 Spark RDD 对数据进行 转换或汇总 时，我们可以天然获得 Exactly-once 语义，因为 RDD 本身就是一种具备容错性、不变性、以及计算确定性的数据结构。只要数据来源是可用的，且处理过程中没有副作用（Side effect），我们就能一直得到相同的计算结果。 SS内部的实现机制是spark core基于RDD模型的，RDD为保证计算过程中数据不丢失使用了checkpoint机制，也就是说其计算逻辑是RDD的变换过程，也就是DAG，可以在计算过程中的任何一个阶段（也就是这个阶段的RDD）上使用checkpoint方法，就可以保证当后续计算失败，可以从这个checkpoint重新算起，使得计算延续下去。当Spark Streaming场景下，其天然会进行batch操作，也就是说kafka过来的数据，每秒（一个固定batch的时间周期）会对当前kafka中的数据产生一个RDD，那么后续计算就是在这个RDD上进行的。只需要在kafkaRDD这个位置合理使用了checkpoint（这一点在前面已经讲过，可以保证）就能保证SS内部的Exactly once。 SS是否将数据EO地写出到了下游结果输出 默认符合 At-least-once 语义，因为 foreachRDD 方法可能会因为 Worker 节点失效而执行多次，从而重复写入外部存储。我们有两种方式解决这一问题，幂等更新和事务更新。下面我们将深入探讨这两种方式。参考：https://blog.csdn.net/qq_32252917/article/details/78827126首先输出操作是具有At-least Once语义的，也就是说SS可以保证需要输出的数据一定会输出出去，只不过由于失败等原因可能会输出多次。那么如何保证Exactly once？第一种“鸵鸟做法”，就是期望下游（数据）具有幂等特性。比如相同数据写 hdfs 同一个文件，这本身就是幂等操作，保证了多次操作最终获取的值还是相同；HBase、ElasticSearch 与 redis 等都能够实现幂等操作。对于关系型数据库的操作一般都是能够支持事务性操作。第二种使用事务更新，简要代码如下：1234567dstream.foreachRDD &#123; (rdd, time) =&gt; rdd.foreachPartition &#123; partitionIterator =&gt; val partitionId = TaskContext.get.partitionId() val uniqueId = generateUniqueId(time.milliseconds, partitionId) // use this uniqueId to transactionally commit the data in partitionIterator &#125;&#125; 这样保证同一个partition要么一起更新成功，要么一起失败，通过uniqueId来标识这一次的更新，这就要求下游支持事务机制。如果不采用幂等或者事务。可以采用如下方案，除了数据主动(重启服务)出错外，还会遇到如下问题。关于Spark Streaming数据输出多次重写及解决方案： 为什么会有这个问题，因为SparkStreaming在计算的时候基于SparkCore，SparkCore天生会做以下事情导致SparkStreaming的结果（部分）重复输出: 1.Task重试； 2.慢任务推测； 3.Stage重复； 4.Job重试；会导致数据的丢失。对应的解决方案： 1.一个任务失败就是job 失败，设置spark.task.maxFailures次数为1； 2.设置spark.speculation为关闭状态（因为慢任务推测其实非常消耗性能，所以关闭后可以显著的提高Spark Streaming处理性能） 3.Spark streaming on kafka的话，假如job失败后可以设置kafka的auto.offset.reset为largest的方式会自动恢复job的执行。参考：https://zybuluo.com/marlin/note/486917http://spark.apache.org/docs/latest/streaming-programming-guide.html#semantics-of-output-operationshttps://blog.cloudera.com/blog/2015/03/exactly-once-spark-streaming-from-apache-kafka/]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive小文件合并]]></title>
    <url>%2F2019%2F01%2F26%2FHive%E5%B0%8F%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6%2F</url>
    <content type="text"><![CDATA[HDFS中的文件、目录和块都映射为一个对象，存储在NameNode服务器内存中，通常占用150个字节。 如果有1千万个文件，就需要消耗大约3G的内存空间。如果是10亿个文件呢，简直不可想象。所以我们要了解一下,hadoop 处理小文件的各种方案，然后选择一种适合的方案来解决本的小文件问题。 此外，HDFS读写小文件时也会更加耗时，因为每次都需要从NameNode获取元信息，并与对应的DataNode建立连接。对于MapReduce程序来说，小文件还会增加Mapper的个数，job作为一个独立的jvm实例，每个job只处理很少的数据，其开启和停止的开销可能会大大超过实际的任务处理时间，浪费了大量的调度时间。当然，这个问题可以通过使用CombinedInputFile和JVM重用来解决. 参考:https://blog.csdn.net/WYpersist/article/details/80043816 输入文件合并输入合并。即在Map前合并小文件。这个方法即可以解决之前小文件数太多，导致mapper数太多的问题；还可以防止输出小文件合数太多的问题（因为mr只有map时，mapper数就是输出的文件个数）。文件合并失效，且job只有map时，map的个数就是文件个数；通过控制map大小控制map个数，以控制输出文件个数。set hive.hadoop.supports.splittable.combineinputformat=true; 开关set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 执行Map前进行小文件合并set mapred.max.split.size=2048000000; 2G 每个Map最大输入大小set mapred.min.split.size.per.node=2048000000; 一个节点上split的至少的大小 ，决定了多个data node上的文件是否需要合并set mapred.min.split.size.per.rack=2048000000; 一个交换机下split的至少的大小，决定了多个交换机上的文件是否需要合并MR-Job 默认的输入格式 FileInputFormat 为每一个小文件生成一个切片。CombineFileInputFormat 通过将多个“小文件”合并为一个”切片”（在形成切片的过程中也考虑同一节点、同一机架的数据本地性），让每一个 Mapper 任务可以处理更多的数据，从而提高 MR 任务的执行速度。 解读：CombineFileInputFormat类：https://www.cnblogs.com/skyl/p/4754999.html 输出文件合并输出合并。即在输出结果的时候合并小文件动态分区好用，但是会产生很多小文件。原因就在于，假设初始有N个mapper,最后生成了m个分区，最终会有多少个文件生成呢？答案是N*m,是的，每一个mapper会生成m个文件，就是每个分区都会对应一个文件，这样的话你算一下。所以小文件就会成倍的产生。怎么解决这个问题，通常处理方式也是像上面那样，让数据尽量聚到少量reducer里面。但是有时候虽然动态分区不会产生reducer,但是也就意味着最后没有进行文件合并,我们也可以用distribute by rand()这句来保证数据聚类到相同的reducer。参考：https://www.iteblog.com/archives/1533.html 但是如果是多层分区呢，且二层分区数据量差异很大，虽然也可以使用上面的方式但仍然有可能不均匀，此时需要扩展，采用如下方式distribute by if(productType in (&#39;h&#39;,&#39;f&#39;,&#39;t&#39;),floor(rand() * 10),1) 其他合并方式，配置参数set hive.merg.xxxx在文件合并 和 压缩 并存时会失效，即只对text或者seq文件生效，对压缩格式如orc等会不适用，就有些鸡肋参考：https://meihuakaile.github.io/2018/10/19/hive%E5%B0%8F%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6/ 上面的方式只是解决使用和规避问题，如果是已经有很多小文件，那么只有压缩一条路可走了，问题就转变为如何更快更优的解决各种格式的压缩问题了。]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[awk]]></title>
    <url>%2F2019%2F01%2F26%2Fawk%2F</url>
    <content type="text"><![CDATA[awk是行处理器: 相比较屏幕处理的优点，在处理庞大文件时不会出现内存溢出或是处理缓慢的问题，通常用来格式化文本信息awk处理过程: 依次对每一行进行处理，然后输出 awk命令形式:awk [-F|-f|-v] ‘BEGIN{} // {command1; command2} END{}’ file [-F|-f|-v] 大参数，-F指定分隔符，-f调用脚本，-v定义变量 var=value‘ ‘ 引用代码块BEGIN 初始化代码块，在对每一行进行处理之前，初始化代码，主要是引用全局变量，设置FS分隔符// 匹配代码块，可以是字符串或正则表达式{} 命令代码块，包含一条或多条命令； 多条命令使用分号分隔END 结尾代码块，在对每一行进行处理之后再执行的代码块，主要是进行最终计算或输出结尾摘要信息特殊要点: $0 表示整个当前行$1 每行第一个字段 NF 字段数量变量；即每行的字段个数，Number FieldNR 每行的记录号，多文件记录递增；即行号 Number Record FNR 与NR类似，不过多文件记录不递增，每个文件都从1开始 \t 制表符\n 换行符 FS BEGIN时定义分隔符RS输入的记录分隔符， 默认为换行符(即文本是按一行一行输入) ~ 匹配，与==相比不是精确比较!~ 不匹配，不精确比较 == 等于，必须全部相等，精确比较!= 不等于，精确比较 &amp;&amp; 逻辑与|| 逻辑或 + 匹配时表示1个或1个以上 /[0-9][0-9]+/ 两个或两个以上数字/[0-9][0-9]*/ 一个或一个以上数字 FILENAME 文件名 OFS输出字段分隔符， 默认也是空格，可以改为制表符等ORS 输出的记录分隔符，默认为换行符,即处理结果也是一行一行输出到屏幕 -F’[:#/]’ 定义三个分隔符 IF语句 必须用在{}中，且比较内容用()扩起来 awk -F: ‘{if($1~/mail/) print $1}’ /etc/passwd //简写 awk -F: ‘{if($1~/mail/) {print $1}}’ /etc/passwd //全写 awk -F: ‘{if($1~/mail/) {print $1} else {print $2}}’ /etc/passwd //if…else… 原文链接 : http://blog.chinaunix.net/uid-23302288-id-3785105.html]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell统计]]></title>
    <url>%2F2019%2F01%2F26%2Fshell%E7%BB%9F%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[统计 列出当天访问次数最多的IP前20个命令：cut -d- -f 1 /usr/local/apache2/logs/access_log |uniq -c | sort -rn | head -20 查看当天有多少个IP访问：awk ‘{print $1}’ log_file|sort|uniq|wc -l 查看某一个页面被访问的次数;grep “/index.php” log_file | wc -l 查看每一个IP访问了多少个页面：awk ‘{++S[$1]} END {for (a in S) print a,S[a]}’ log_file 将每个IP访问的页面数进行从小到大排序：awk ‘{++S[$1]} END {for (a in S) print S[a],a}’ log_file | sort -n 查看某一个IP访问了哪些页面：grep ^111.111.111.111 log_file| awk ‘{print $1,$7}’ 去掉搜索引擎统计当天的页面：awk ‘{print $12,$1}’ log_file | grep ^\”Mozilla | awk ‘{print $2}’ |sort | uniq | wc -l 查看2009年6月21日14时这一个小时内有多少IP访问：awk ‘{print $4,$1}’ log_file | grep 21/Jun/2009:14 | awk ‘{print $2}’| sort | uniq | wc -l 统计访问日志里每个ip访问次数[root@qunar logs]# cat a.sh#!/bin/bash#将28/Jan/2015全天的访问日志放到a.txt文本cat access.log |sed -rn ‘/28\/Jan\/2015/p’ &gt; a.txt #统计a.txt里面有多少个ip访问cat a.txt |awk ‘{print $1}’|sort |uniq &gt; ipnum.txt#通过shell统计每个ip访问次数for i in `cat ipnum.txt`doiptj=`cat access.log |grep $i | grep -v 400 |wc -l`echo “ip地址”$i”在2015-01-28日全天(24小时)累计成功请求”$iptj”次，平均每分钟请求次数为：”$(($iptj/1440)) &gt;&gt; result.txtdone 把100天前的文件打包并且删除find [path] -type f -mtime +100 -exec tar rvf tmp.tar –remove-files {} \; 查找find . -name ‘.sh’ | xargs grep -in ‘48 ‘ 查找所有”.h”文件find /PATH -name “*.h” 查找所有”.h”文件中的含有”helloworld”字符串的文件find /PATH -name “.h” -exec grep -in “helloworld” {} \;find /PATH -name “.h” | xargs grep -in “helloworld” 查找所有”.h”和”.c”文件中的含有”helloworld”字符串的文件find /PATH /( -name “.h” -or -name “.c” /) -exec grep -in “helloworld” {} \; 查找非备份文件中的含有”helloworld”字符串的文件find /PATH /( -not -name “*~” /) -exec grep -in “helloworld” {} \;注：/PATH为查找路径，默认为当前路径。带-exec参数时必须以\;结尾，否则会提示“find: 遗漏“-exec”的参数”。 运维 lsof |grep deleted注：这个deleted表示该已经删除了的文件，但是文件句柄未释放,这个命令会把所有的未释放文件句柄的进程列出来 swap使用排序 1for i in $( cd /proc;ls |grep &quot;^[0-9]&quot;|awk &apos; $0 &gt;100&apos;) ;do sudo awk &apos;/^Swap:/&#123;a=a+$2&#125;END&#123;print &apos;&quot;$i&quot;&apos;,a/1024&quot;M&quot;&#125;&apos; /proc/$i/smaps 2&gt;/dev/null ; done | sort -k2nr | head -10 curl -O下载 正则bizType=([^&amp;]+)bizType=(.*?)[&amp;|$]]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Top*]]></title>
    <url>%2F2019%2F01%2F26%2FTop%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sed操作]]></title>
    <url>%2F2019%2F01%2F26%2Fsed%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[一次性给文件多行加注释在vim 视图模式下：2,5 s/^/#/或者直接使用sed，命令如下：sed -i ‘2,5s/^/#/‘ filename 注释取消反之，将2~5行带#注释取消：：2,5 s/^#//或者sed -i ‘2,5s/^#//‘ filename 去掉空行sed -i ‘/^$/d’ df.txt 将每一行拖尾的“空白字符”（空格，制表符）删除sed ‘s/ *$//‘ df.txt &gt;cwm.txt 将每一行中的前导和拖尾的空白字符删除sed ‘s/^ //;s/ $//‘ df.txt &gt;cwm.txt vi下全文替换:%s/1/2/g 全文替换“1,20” ：表示从第1行到20行；“%” ：表示整个文件，同“1,$”；“. ,$” ：从当前行到文件尾；]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有效电话号码]]></title>
    <url>%2F2019%2F01%2F26%2F%E6%9C%89%E6%95%88%E7%94%B5%E8%AF%9D%E5%8F%B7%E7%A0%81%2F</url>
    <content type="text"><![CDATA[给定一个包含电话号码列表（一行一个电话号码）的文本文件 file.txt，写一个 bash 脚本输出所有有效的电话号码。 你可以假设一个有效的电话号码必须满足以下两种格式： (xxx) xxx-xxxx 或 xxx-xxx-xxxx。（x 表示一个数字） 你也可以假设每行前后没有多余的空格字符。 示例: 假设 file.txt 内容如下： 987-123-4567123 456 7890(123) 456-7890你的脚本应当输出下列有效的电话号码： 987-123-4567(123) 456-7890 1egrep -o &quot;(^[0-9]&#123;3&#125;-[0-9]&#123;3&#125;-[0-9]&#123;4&#125;$)|(^\([0-9][0-9][0-9]\)\s[0-9]&#123;3&#125;-[0-9]&#123;4&#125;$)&quot; file.txt]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[转置文件]]></title>
    <url>%2F2019%2F01%2F26%2F%E8%BD%AC%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[给定一个文件 file.txt，转置它的内容。 你可以假设每行列数相同，并且每个字段由 ‘ ‘ 分隔. 示例: 假设 file.txt 文件内容如下： name agealice 21ryan 30应当输出： name alice ryanage 21 30 1cat file.txt | awk &apos;&#123;for(i=1;i&lt;=NF;i++)&#123;if(NR==1)&#123;res[i]=$i&#125;else&#123;res[i]=res[i]&quot; &quot;$i&#125;&#125;&#125;END&#123;for(i=1;i&lt;=NF;i++)&#123;print res[i]&#125;&#125;&apos; awk是一行一行处理数据，if(NR==1){res[i]=$i}优先初始化第一行的res[]数组，长度为第一行的元素格式，后面每行在对应位置元素后面追加。]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第十行]]></title>
    <url>%2F2019%2F01%2F26%2F%E7%AC%AC%E5%8D%81%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[给定一个文本文件 file.txt，请只打印这个文件中的第十行。 示例: 假设 file.txt 有如下内容： Line 1Line 2Line 3Line 4Line 5Line 6Line 7Line 8Line 9Line 10你的脚本应当显示第十行： Line 10说明: 如果文件少于十行，你应当输出什么？ 至少有三种不同的解法，请尝试尽可能多的方法来解题。 12tail -n +10 file.txt | head -n 1awk &apos;NR==10&#123;print $0&#125;&apos; file.txt]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计词频]]></title>
    <url>%2F2019%2F01%2F26%2F%E7%BB%9F%E8%AE%A1%E8%AF%8D%E9%A2%91%2F</url>
    <content type="text"><![CDATA[cat words.txt | tr -s &quot; &quot; &quot;\n&quot; |sort |uniq -c | sort -r -n | awk &#39;{print $2,$1}&#39;tr -s 将重复出现字符串压缩为一个字符串“ “ “\n” 将空格替换为换行uniq 命令删除文件中的重复行。uniq 命令读取由 InFile 参数指定的标准输入或文件。该命令首先比较相邻的行，然后除去第二行和该行的后续副本。重复的行一定相邻。（所以一定要在发出 uniq 命令之前，请使用 sort 命令使所有重复行相邻。）sort -n按照数值排序sort -r降序排序sort file.txt | uniq -c -c或–count在每列旁边显示该行重复出现的次数 另:删除字符asdtr -d ‘asd’删除空行tr -s ‘\n’sort -u 去重,如果只有sort是不会去重sort file.txt | uniq -u -u或——unique：仅显示出一次的行列；只显示单一行sort file.txt | uniq -d -d或–repeated：仅显示重复出现的行列]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构]]></title>
    <url>%2F2019%2F01%2F26%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[天下武功唯快不破，快是基本功，那么算法中的基本功首推递归递归，迭代相互转换动态规划深度优先广度优先贪心算法回溯算法二分查找分治算法双指针排序 数组链表栈队列堆树图]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[left semi join]]></title>
    <url>%2F2019%2F01%2F25%2Fhive-LeftSemiJoin%2F</url>
    <content type="text"><![CDATA[参考:https://blog.csdn.net/happyrocking/article/details/79885071]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[join倾斜]]></title>
    <url>%2F2019%2F01%2F25%2Fhive-join%E5%80%BE%E6%96%9C%2F</url>
    <content type="text"><![CDATA[map join使用 map join 在 map 端就先进行 join ，免得到reduce 时卡住 Auto Map Join还记得原理中提到的物理优化器？Physical Optimizer么？它的其中一个功能就是把Join优化成Auto Map Join 图上左边是优化前的，右边是优化后的 优化过程是把Join作业前面加上一个条件选择器ConditionalTask和一个分支。左边的分支是MapJoin，右边的分支是Common Join(Reduce Join) 看看左边的分支是不是和我们上上一张图很像？ 这个时候，我们在执行的时候，就由这个Conditional Task 进行实时路径选择，遇到小于25兆走左边，大于25兆走右边。所谓，男的走左边，女的走右边，人妖走中间。 在比较新版的Hive中，Auto Mapjoin是默认开启的。如果没有开启，可以使用一个开关， set hive.auto.convert.join=true 开启。当然，Join也会遇到和上面的Group By一样的倾斜问题。 Ｈive 也可以通过像Group By一样两道作业的模式单独处理一行或者多行倾斜的数据。即采用下面的方式。 hive.optimize.skewjoinhive 中设定set hive.optimize.skewjoin = true;set hive.skewjoin.key = skew_key_threshold （default = 100000）可以就按官方默认的1个reduce 只处理1G 的算法，那么skew_key_threshold= 1G/平均行长.或者默认直接设成250000000 (差不多算平均行长4个字节) 其原理是就在Reduce Join过程，把超过十万条的倾斜键的行写到文件里，回头再起一道Join单行的Map Join作业来单独收拾它们。最后把结果取并集就是了 对full outer join无效。 其他方法如果A表关联B表在某个key上倾斜，B是码表，那么可以将B表放大1000倍，然后对A表的key字段加上一个1000以内hash后缀，然后和放大后的b表关联，可以解决问题。 参考:https://blog.csdn.net/lw_ghy/article/details/51469753]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[group by数据倾斜]]></title>
    <url>%2F2019%2F01%2F25%2Fhive_groupby%E5%80%BE%E6%96%9C%2F</url>
    <content type="text"><![CDATA[group by数据倾斜倾斜原因：select count(distinct name) from user时 使用distinct会将所有的name值都shuffle到一个reducer里面。特别的有select uid, count(distinct name) from user group by uid; 即count distinct + （group by）的情况。 优化：（1）主要是把count distinct改变成group by。改变上面的sql为select uid, count(name) from (select uid, name from user group by uid, name)t group by uid.（2）给group by 字段加随机数打散，聚合，之后把随机数去掉，再次聚合（有点类似下面的参数SET hive.groupby.skewindata=true;）1234567891011121314151617181920212223242526272829select split(uid, &apos;_&apos;)[0] uid, sum(names) from( select concat_ws(&apos;-&apos;, uid, substr(rand()*10, 1, 1)) uid, count(name) names from ( select uid, name ---此处去重，且不会倾斜 from user group by uid, name )a group by concat_ws(&apos;-&apos;, uid, substr(rand()*10, 1, 1)))bgroup by split(uid, &apos;_&apos;)[0]等同于下面select split(uid, &apos;_&apos;)[0] uid, sum(names) from( select uid,count(name) names from ( select concat_ws(&apos;-&apos;, uid, substr(rand()*10, 1, 1)) uid, name from ( select uid, name from user group by uid, name )a ) c group by uid )bgroup by split(uid, &apos;_&apos;)[0] (3)SET hive.groupby.skewindata=true; 当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，该Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作. set hive.map.aggr=true; 在mapper端部分聚合，相当于Combiner 。而Map-Side聚合，一般在聚合函数sum,count时使用。 无论你使用Map端，或者两道作业。其原理都是通过部分聚合来来减少数据量。能不能部分聚合，部分聚合能不能有效减少数据量，通常与UDAF，也就是聚合函数有关。也就是只对代数聚合函数有效，对整体聚合函数无效。所谓代数聚合函数，就是由部分结果可以汇总出整体结果的函数，如count，sum。 所谓整体聚合函数，就是无法由部分结果汇总出整体结果的函数，如avg，mean。 比如，sum, count，知道部分结果可以加和得到最终结果。 而对于，mean，avg，知道部分数据的中位数或者平均数，是求不出整体数据的中位数和平均数的。 set hive.groupby.mapaggr.checkinterval=100000；–这个是group的键对应的记录条数超过这个值则会进行分拆,值根据具体数据量设置。hive.map.aggr.hash.min.reduction=0.5(默认)预先取100000条数据聚合,如果聚合后的条数/100000&gt;0.5，则不再聚合 使用Hive的过程中，我们习惯性用set hive.groupby.skewindata=true来避免因数据倾斜造成的计算效率问题，但是每个设置都是把双刃剑，最近调研了下相关问题，现总结如下： 从下表可以看出，skewindata配置真正发生作用，只会在以下三种情况下，能够将1个job转化为2个job：select count distinct … from …select a,count() from … group by a 只针对单列有效select count(),count(distinct …) from 此处没有group by如下sql会报错select count(*),count(distinct …) from … group by a 此处有group by需要改为select a,sum(1),count(distinct …) from … group by a 参考：https://meihuakaile.github.io/2018/10/19/hive%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/https://blog.csdn.net/dxl342/article/details/77886577https://blog.csdn.net/lw_ghy/article/details/51469753]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[维度建模]]></title>
    <url>%2F2018%2F02%2F14%2F%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1%2F</url>
    <content type="text"><![CDATA[概念维度表：说明数据，维度是指可指定不同值的对象的描述性属性或特征。例如，地理位置的维度可以包括“纬度”、“经度”或“城市名称”。“城市名称”维度的值可以为“旧金山”、“柏林”或“新加坡”。维度是现实世界中的对象或者概念。 事实表：其实质就是通过一些指标值和各种维度外键来确定一个事实的。发生在现实世界中的操作型事件，其所产生的可度量数值，存储在事实表中。从最低的粒度级别来看，事实表行对应一个度量事件，反之亦然。事实表的关键是度量值数据仓库架构中的中央表，它包含联系事实与维度表的数字度量值和键。 事务事实表/原子事实表/交易事实表聚合事实表周期快照事实表累积快照事实表参考:https://blog.csdn.net/weixin_42478648/article/details/85290881 维度模型也有着一些缺点，比如数据的一致性很难保证，数据的冗余，大量的维度信息处理等，但这些相对于有点来讲都是可接受的，而且也可以通过其他方式避免和简化的。但是有了总线架构和一致性维度、一致性事实就不一样了以维表为总线，事实表以维表为基础的总线矩阵，意味着建设出来的架构正是总线式架构。 总线架构一致性维度就好比企业范围内的一组总线，不同数据集市的事实的就好比插在这组总线上的元件。这也是称之为总线架构的原因。实际设计过程中，我们通常把总线架构列表成矩阵的形式，其中列为一致性维度，行为不同的业务处理过程，即事实，在交叉点上打上标记表示该业务处理过程与该维度相关。这个矩阵也称为总线矩阵（Bus Matrix）。 总线矩阵的每一行代表一个业务过程，并且至少定义了一个事实表和相应的维度。通常，总线矩阵的一行会产生几个相关的事实表，由此可以从不同角度跟踪业务过程。 总线矩阵是企业商业智能数据的路标，必须为任何企业范围的DW/BI工作创建总线矩阵。对数据建模人员和数据管理员来说，创建得到企业认可的一致性维度是一项艰巨的挑战。使用单个维度表来描述企业的产品、顾客或设备，意味着公司必须认同每个维度表的定义方式，包括属性列表、属性名称、体系结构及定义和派生表中每个属性所需的业务规则。从企业战略来看，这是艰巨的工作，其难度与员工人数和部门数量成正比。但是，这一过程是必需的。 一致性维度在多维体系结构中，没有物理上的数据仓库，由物理上的数据集市组合成逻辑上的数据仓库。而且数据集市的建立是可以逐步完成的，最终组合在一起，成为一个数据仓库。如果分步建立数据集市的过程出现了问题，数据集市就会变成孤立的集市，不能组合成数据仓库，而一致性维度的提出正式为了解决这个问题。一致性维度的范围是总线架构中的维度，即可能会在多个数据集市中都存在的维度，这个范围的选取需要架构师来决定。一致性维度的内容和普通维度并没有本质上区别，都是经过数据清洗和整合后的结果。 一致性维度建立的地点是多维体系结构的后台（Back Room），即数据准备区。在多维体系结构的数据仓库项目组内需要有专门的维度设计师，他的职责就是建立维度和维护维度的一致性。在后台建立好的维度同步复制到各个数据集市。这样所有数据集市的这部分维度都是完全相同的。建立新的数据集市时，需要在后台进行一致性维度处理，根据情况来决定是否新增和修改一致性维度，然后同步复制到各个数据集市。这是不同数据集市维度保持一致的要点。在同一个集市内，一致性维度的意思是两个维度如果有关系，要么就是完全一样的，要么就是一个维度在数学意义上是另一个维度的子集。例如，如果建立月维度话，月维度的各种描述必须与日期维度中的完全一致，最常用的做法就是在日期维度上建立视图生成月维度。这样月维度就可以是日期维度的子集，在后续钻取等操作时可以保持一致。如果维度表中的数据量较大，出于效率的考虑，应该建立物化视图或者实际的物理表。这样，维度保持一致后，事实就可以保存在各个数据集市中。虽然在物理上是独立的，但在逻辑上由一致性维度使所有的数据集市是联系在一起，随时可以进行交叉探察等操作，也就组成了数据仓库。 一致性事实在建立多个数据集市时，完成一致性维度的工作就已经完成了一致性的80%－90%的工作量。余下的工作就是建立一致性事实。 一致性事实和一致性维度有些不同，一致性维度是由专人维护在后台（Back Room），发生修改时同步复制到每个数据集市，而事实表一般不会在多个数据集市间复制。需要查询多个数据集市中的事实时，一般通过交叉探查（drill across）来实现。 为了能在多个数据集市间进行交叉探查，一致性事实主要需要保证两点。第一个是KPI的定义及计算方法要一致，第二个是事实的单位要一致性。如果业务要求或事实上就不能保持一致的话，建议不同单位的事实分开建立字段保存。 这样，一致性维度将多个数据集市结合在一起，一致性事实保证不同数据集市间的事实数据可以交叉探查，一个分布式的数据仓库就建成了。 多维体系结构Multidimensional Architecture（MD）有三个关键性概念：总线架构（Bus Architecture），一致性维度（Conformed Dimension）和一致性事实（Conformed Fact）。 多维体系结构主要包括后台（Back Room）和前台（Front Room）两部分。1、后台也称为数据准备区（Staging Area），是MD架构的最为核心的部件。在后台，是一致性维度的产生、保存和分发的场所。同时，代理键也在后台产生。2、前台是MD架构对外的接口，包括两种主要的数据集市，一种是原子数据集市，另一种是聚集数据集市。原子数据集市保存着最低粒度的细节数据，数据以星型结构来进行数据存储。聚集数据集市的粒度通常比原子数据集市要高，和原子数据集市一样，聚集数据集市也是以星型结构来进行数据存储。前台还包括像查询管理、活动监控等为了提供数据仓库的性能和质量的服务。在多维体系结构（MD） 的数据仓库架构中，主导思想是分步建立数据仓库，由数据集市组合成企业的数据仓库。但是，在建立第一个数据集市前，架构师首先要做的就是设计出在整个企业 内具有统一解释的标准化的维度和事实，即一致性维度和一致性事实。而开发团队必须严格的按照这个体系结构来进行数据集市的迭代开发。 维度模型设计过程选择业务过程，即选择主题声明粒度确定维度确定事实 星座模型星座模型是星型模型延伸而来，星型模型是基于一张事实表的，而星座模型是基于多张事实表的，而且共享维度信息。 通过构建一致性维度，来建设星座模型，也是很好的选择。比如同一主题的细节表和汇总表共享维度，不同主题的事实表，可以通过在维度上互相补充来生成可以共享的维度。以维表为总线，事实表以维表为基础的总线矩阵，意味着建设出来的架构正是总线式架构。]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark 二次排序]]></title>
    <url>%2F2018%2F02%2F13%2Fspark-%E4%BA%8C%E6%AC%A1%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[数据：40 2040 1040 3040 530 3030 2030 1030 4050 2050 5050 1050 601234567891011121314151617181920212223242526272829303132333435363738394041package com.scala.test.core.secondsortimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object SecondarySort &#123; def main(args:Array[String]): Unit =&#123; val conf = new SparkConf().setAppName(&quot;SecondarySort&quot;).setMaster(&quot;local[*]&quot;) val sc = new SparkContext(conf) val line = sc.textFile(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/secondarysort/sort.txt&quot;) val data = line.map(x =&gt; &#123; val line : Array[String] = x.split(&quot; &quot;) (line(0),line(1)) &#125;) val rdd = data.groupByKey().sortByKey()//todo groupbykey和sortByKey// (40,CompactBuffer(20, 10, 30, 5))// (50,CompactBuffer(20, 50, 10, 60))// (30,CompactBuffer(30, 20, 10, 40)) //todo 之前多条现在变成3条了,改变了原来的条数// val result = rdd.map(item =&gt; (item._1, item._2.toList.sortWith(_.toInt&lt;_.toInt)))//todo values转list并排序 //todo 不改变原来的条数 val result = rdd.flatMap(item =&gt; &#123; val list = item._2.toList.sortWith(_.toInt&lt;_.toInt) list.map(x =&gt; (item._1,x)) &#125;)// 保存// result.saveAsTextFile(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/secondarysort/out&quot;)// 打印 result.collect().foreach(println) sc.stop() &#125;&#125;``` 上面的方式，groupbykey之后把相同的key聚合在一起排序，实际上是内存排序，这种方法可能导致归约器耗尽内存。如果数量很少可以用。和mapreduce的setGroupingComparatorClass方式不同,mapreduce在reduce中没有排序操作，是用框架中的排序进行。 使用repartitionAndSortWithinPartitions： package com.scala.test.core.secondsort import org.apache.spark.{HashPartitioner, Partitioner, SparkConf, SparkContext} object SecondarySortRepartition { def main(args: Array[String]): Unit = { implicit val caseInsensitiveOrdering = new Ordering[Int] { override def compare(a: Int, b: Int) = b.compareTo(a) } import org.apache.spark.Partitioner class KeyBasePartitioner(partitions: Int) extends Partitioner { require(partitions &gt;= 0, s&quot;Number of partitions ($partitions) cannot be negative.&quot;) override def numPartitions: Int = partitions override def getPartition(key: Any): Int = { val k = key.asInstanceOf[SecondarySort] Math.abs(k.one.hashCode() % numPartitions) } } val conf = new SparkConf().setAppName(&quot;SecondarySortRepartition&quot;).setMaster(&quot;local[*]&quot;) val sc = new SparkContext(conf) val line = sc.textFile(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/secondarysort/sort.txt&quot;) class SecondarySort(val one :Int,val two : Int) extends Ordered[SecondarySort] with Serializable { override def compare(that: SecondarySort): Int = { if(this.one-that.one != 0){ this.one-that.one }else{ this.two-that.two } } } line.map(x =&gt; { val xy=x.split(&quot; &quot;) (new SecondarySort(Integer.parseInt(xy(0)),Integer.parseInt(xy(1)).toInt),x) } ) .repartitionAndSortWithinPartitions(new KeyBasePartitioner(3)) .saveAsTextFile(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/secondarysort/out&quot;) // .foreach(println) sc.stop() }}`当spark的分区数大于线程数时，spark仍会按照一个一个分区单独处理，而不会像MapReduce设置setGroupingComparatorClass。可以通过设置.setMaster(“local[2]”)和new KeyBasePartitioner(3)来验证。 spark 1.2之后引入了一个高质量的算子 repartitionAndSortWithinPartitions?。该算子为spark的Shuffle增加了sort。假如，后面再跟mapPartitions算子的话，其算子就是针对已经按照key排序的分区，这就有点像mr的意思了。与groupbykey不同的是，数据不会一次装入内存，而是使用迭代器一次一条记录从磁盘加载。这种方式最小化了内存压力。 定义partitioner的方式可以参考https://www.bbsmax.com/A/LPdoVrl253/repartitionAndSortWithinPartitions算子比先分区在排序效率高https://blog.csdn.net/luofazha2012/article/details/80587128 sortbykey VS repartitionAndSortWithinPartitionshttps://blog.csdn.net/wyqwilliam/article/details/81627603]]></content>
      <categories>
        <category>spark编程</category>
      </categories>
      <tags>
        <tag>spark编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark wordcount]]></title>
    <url>%2F2018%2F02%2F12%2Fspark-wordcount%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718package com.scala.test.coreimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object WordCount &#123; def main(args : Array[String]): Unit = &#123; val conf = new SparkConf().setAppName(&quot;wordCount&quot;).setMaster(&quot;local[*]&quot;) val sc = new SparkContext(conf) val line = sc.textFile(&quot;/Users/lifei/Desktop/ab/a.txt&quot;);//todo wholeTextFiles 可以读取目录// 直接打印// line.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).collect().foreach(println)// 保存到文件 line.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).saveAsTextFile(&quot;/Users/lifei/Desktop/ab/lala&quot;) sc.stop(); &#125;&#125;]]></content>
      <categories>
        <category>spark编程</category>
      </categories>
      <tags>
        <tag>spark编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive级联报表查询]]></title>
    <url>%2F2018%2F02%2F12%2Fhive%E7%BA%A7%E8%81%94%E6%8A%A5%E8%A1%A8%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021A,2015-01,5A,2015-01,15B,2015-01,5A,2015-01,8B,2015-01,25A,2015-01,5C,2015-01,10C,2015-01,20A,2015-02,4A,2015-02,6C,2015-02,30C,2015-02,10B,2015-02,10B,2015-02,5A,2015-03,14A,2015-03,6B,2015-03,20B,2015-03,25C,2015-03,10C,2015-03,20``` create table t_access_times(username string,month string,counts int)row format delimited fields terminated by ‘,’;1![upload successful](/images/pasted-52.png) – 窗口分析函数– 窗口的定义：– 针对窗口中数据的操作– 哪些数据select uid, month, amount,sum (amount) over(partition by uid order by month rows between unbounded preceding and current row) as accumulatefrom t_access_amount; `参考：https://www.cnblogs.com/arjenlee/p/9692312.html#auto_id_81]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive模型]]></title>
    <url>%2F2018%2F02%2F12%2Fhive%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[Jobtracker是hadoop1.x中的组件，它的功能相当于： Resourcemanager+MRAppMaster TaskTracker 相当于： Nodemanager + yarnchild hive里面有两种服务模式一种是cli模式，一种是hiveserver2，分别对应的启动入口 cli：hive-cli/org.apache.hadoop.hive.cli.CliDriver.javahiveserver2：hiveservice/org.apache.hive.service.server.HiveServer2.java,直接用debug或者run运行调试 hive sql 的执行顺序from… where…. select…group by… having … order by… 源码 compiler包括parser解释器和SemanticAnalyzer语义解析器。 无论使用CLI、Thrift Server、JDBC还是自定义的提交工具，最终的HQL都会传给Driver实例，执行Driver.run()方法。从这种设计也可以看出，如果您要开发一套自定义的Hive作业提交工具，最好的方式是引用Driver实例，调用相关方法进行开发。 而Driver.run()方法，获得了这样一个HQL，则会执行两个重要的步骤：编译和执行，即Driver.complie()和Driver.execute()。对于Driver.comile()来说，其实就是调用parse和optimizer包中的相关模块，执行语法解析、语义分析、优化；对于Driver.run()来说，其实就是调用exec包中的相关模块，将解析后的执行计划执行，如果解析后的结果是一个查询计划，那么通常的作法就是提交一系列的MapReduce作业。 以查询的执行为例，整个Hive的流程是非常简单的一条直线，由上到下进行。 Join操作左边为小表应该将条目少的表/子查询放在 Join 操作符的左边原因是在 Join 操作的 Reduce 阶段，位于 Join 操作符左边的表的内容会被加载进内存，将条目少的表放在左边，可以有效减少发生 OOM 错误的几率 参考：https://blog.csdn.net/dante_003/article/details/73789910https://www.jianshu.com/p/892cc8985c9chttps://blog.csdn.net/wf1982/article/details/9122543https://blog.csdn.net/wzq6578702/article/details/71250081]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[join]]></title>
    <url>%2F2018%2F02%2F11%2Fjoin%2F</url>
    <content type="text"><![CDATA[测试数据user.txt (用户id,用户名)1 用户12 用户23 用户3 more post.txt (用户id,帖子id,标题)1 1 贴子11 2 贴子22 3 帖子34 4 贴子45 5 贴子55 6 贴子65 7 贴子7123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149package com.qunar.mr.join;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.ArrayList;import java.util.List;/** * * user.txt 用户表(用户id,用户名) * * post.txt 帖子表(用户id,帖子id,标题) */public class Join &#123;// 类型 U表示用户,P表示帖子 public static class UserMap extends Mapper&lt;Object, Text,Text,Text&gt; &#123; public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; String[] line = value.toString().split(&quot;,&quot;); context.write(new Text(line[0]),new Text(&quot;U,&quot;+value.toString())); &#125; &#125; public static class PostMap extends Mapper&lt;Object, Text,Text,Text&gt; &#123; public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; String[] line = value.toString().split(&quot;,&quot;); context.write(new Text(line[0]),new Text(&quot;P,&quot;+value.toString())); &#125; &#125; public static class Reduce extends Reducer&lt;Text,Text,Text,Text&gt; &#123; private List&lt;String&gt; users = new ArrayList&lt;String&gt;(); private List&lt;String&gt; posts = new ArrayList&lt;String&gt;(); private String joinType; @Override protected void setup(Context context) throws IOException, InterruptedException &#123; super.setup(context); joinType = context.getConfiguration().get(&quot;joinType&quot;); &#125; public void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; //todo users.clear(); posts.clear(); for (Text v:values) &#123; if (v.toString().contains(&quot;U,&quot;))&#123; users.add(v.toString().substring(2)); &#125; else &#123; posts.add(v.toString().substring(2)); &#125; &#125; if (joinType.equals(&quot;innerJoin&quot;))&#123; if (users.size() &gt; 0 &amp;&amp; posts.size() &gt;0)&#123; for (String user:users)&#123; for (String post:posts)&#123; context.write(new Text(user),new Text(post)); &#125; &#125; &#125; &#125; if (joinType.equals(&quot;leftOuter&quot;))&#123; for (String user:users) &#123; if (posts.size() &gt; 0)&#123;//todo for (String post:posts) &#123; context.write(new Text(user),new Text(post)); &#125; &#125; else &#123; context.write(new Text(user),new Text(&quot;,,&quot;)); &#125; &#125; &#125; if (joinType.equals(&quot;rightOuter&quot;))&#123; for (String post:posts) &#123; if (users.size() &gt; 0)&#123; for (String user:users) &#123; context.write(new Text(user),new Text(post)); &#125; &#125; else &#123; context.write(new Text(&quot;,&quot;),new Text(post)); &#125; &#125; &#125; if (joinType.equals(&quot;allOuter&quot;))&#123; if (users.size() &gt; 0)&#123; for (String user:users) &#123; if (posts.size() &gt; 0)&#123; for (String post:posts) &#123; context.write(new Text(user),new Text(post)); &#125; &#125; else &#123; context.write(new Text(user),new Text(&quot;,,&quot;)); &#125; &#125; &#125; else &#123; for (String post:posts) &#123; context.write(new Text(&quot;,&quot;),new Text(post)); &#125; &#125; &#125; if (joinType.equals(&quot;anti&quot;))&#123; if (users.size() == 0 || posts.size() == 0)&#123;//todo for (String user:users) &#123; context.write(new Text(user),new Text(&quot;,,&quot;)); &#125; for (String post:posts) &#123; context.write(new Text(&quot;,&quot;),new Text(post)); &#125; &#125; &#125; &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;Join&quot;); job.setJarByClass(Join.class); //设置连接类型 //innerJoin,leftOuter,rightOuter,allOuter,anti job.getConfiguration().set(&quot;joinType&quot;,&quot;allOuter&quot;);// job.setMapperClass(Map.class); //todo 不能单独设置 job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); //todo MultipleInputs.addInputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/join/in/user.txt&quot;), TextInputFormat.class,UserMap.class); MultipleInputs.addInputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/join/in/post.txt&quot;), TextInputFormat.class,PostMap.class); Path outpath = new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/join/out&quot;); FileSystem fs = outpath.getFileSystem(job.getConfiguration()); if (fs.exists(outpath))&#123; fs.delete(outpath,true); &#125; FileOutputFormat.setOutputPath(job,outpath); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;]]></content>
      <categories>
        <category>mapreduce编程</category>
      </categories>
      <tags>
        <tag>mapreduce编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[倒排]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%80%92%E6%8E%92%2F</url>
    <content type="text"><![CDATA[题意hdfs 上有三个文件，内容下上面左面框中所示。右框中为处理完成后的结果文件。倒排索引（Inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档检索系统中最常用的数据结构。通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。倒排索引主要由两个部分组成：“单词词典”和“倒排文件”这个任务与传统的倒排索引任务不同的地方是加上了每个文件中的频数。 实现思路首先关注结果中有文件名称，这个我们有两种方式处理：1、自定义InputFormat，在其中的自定义RecordReader中，直接通过InputSplit得到Path，继而得到FileName;2、在Mapper中，通过上下文可以取到Split，也可以得到fileName。这个任务中我们使用第二种方式，得到filename.在mapper中，得到filename 及 word，封装到一个自定义keu中。value 使用IntWritable。在map 中直接输出值为1的IntWritable对象。对进入reduce函数中的key进行分组控制，要求按word相同的进入同一次reduce调用。所以需要自定义GroupingComparator。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160package com.qunar.mr.invertedsort;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.*;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import org.apache.log4j.Logger;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import java.util.LinkedHashMap;/** * http://blog.itpub.net/30066956/viewspace-2120238/ */public class InvertedSort &#123; static class WordKey implements WritableComparable&lt;WordKey&gt; &#123; private String fileName; private String word; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(fileName); out.writeUTF(word); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.fileName = in.readUTF(); this.word = in.readUTF(); &#125; @Override public int compareTo(WordKey key) &#123; int r = word.compareTo(key.word); if(r==0) r = fileName.compareTo(key.fileName); return r; &#125; public String getFileName() &#123; return fileName; &#125; public void setFileName(String fileName) &#123; this.fileName = fileName; &#125; public String getWord() &#123; return word; &#125; public void setWord(String word) &#123; this.word = word; &#125; &#125; public static class IndexInvertedMapper extends Mapper&lt;LongWritable, Text,WordKey, IntWritable&gt; &#123; private WordKey newKey = new WordKey(); private IntWritable ONE = new IntWritable(1); private String fileName ; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; newKey.setFileName(fileName); String words [] = value.toString().split(&quot; &quot;); for(String w:words)&#123; newKey.setWord(w); context.write(newKey, ONE); &#125; &#125; @Override protected void setup(Context context) throws IOException, InterruptedException &#123; FileSplit inputSplit = (FileSplit) context.getInputSplit(); fileName = inputSplit.getPath().getName(); &#125; &#125; public static class IndexInvertedReducer extends Reducer&lt;WordKey,IntWritable,Text,Text&gt; &#123; private Text outputKey = new Text(); @Override protected void reduce(WordKey key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; outputKey.set(key.getWord()); LinkedHashMap&lt;String,Integer&gt; map = new LinkedHashMap&lt;String,Integer&gt;(); for(IntWritable v :values)&#123; if(map.containsKey(key.getFileName()))&#123; map.put(key.getFileName(), map.get(key.getFileName())+ v.get()); &#125; else&#123; map.put(key.getFileName(), v.get()); &#125; &#125; StringBuilder sb = new StringBuilder(); sb.append(&quot;&#123;&quot;); for(String k: map.keySet())&#123; sb.append(&quot;(&quot;).append(k).append(&quot;,&quot;).append(map.get(k)).append(&quot;)&quot;).append(&quot;,&quot;); &#125; sb.deleteCharAt(sb.length()-1).append(&quot;&#125;&quot;); context.write(outputKey, new Text(sb.toString())); &#125; &#125; public static class IndexInvertedGroupingComparator extends WritableComparator &#123; Logger log = Logger.getLogger(getClass()); public IndexInvertedGroupingComparator()&#123; super(WordKey.class,true); &#125; @Override public int compare(WritableComparable a, WritableComparable b) &#123; WordKey key1 = (WordKey) a; WordKey key2 = (WordKey) b; log.info(&quot;==============key1.getWord().compareTo(key2.getWord()):&quot;+key1.getWord().compareTo(key2.getWord())); return key1.getWord().compareTo(key2.getWord()); &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;IndexInvertedJob&quot;); job.setJarByClass(InvertedSort.class); Path in = new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/invertedsort/in&quot;); Path out = new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/invertedsort/out&quot;); FileSystem.get(conf).delete(out,true); FileInputFormat.setInputPaths(job, in); FileOutputFormat.setOutputPath(job, out); job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); job.setMapperClass(IndexInvertedMapper.class); job.setMapOutputKeyClass(WordKey.class); job.setMapOutputValueClass(IntWritable.class); job.setReducerClass(IndexInvertedReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); job.setGroupingComparatorClass(IndexInvertedGroupingComparator.class); System.exit(job.waitForCompletion(true)? 0 : 1); &#125;&#125; 参考:http://blog.itpub.net/30066956/viewspace-2120238/]]></content>
      <categories>
        <category>mapreduce编程</category>
      </categories>
      <tags>
        <tag>mapreduce编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二次排序]]></title>
    <url>%2F2018%2F02%2F11%2F%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[输入文件 sort.txt 内容为 40 20 40 10 40 30 40 5 30 30 30 20 30 10 30 40 50 20 50 50 50 10 50 60 输出文件的内容（从小到大排序）如下 30 10 30 20 30 30 30 40 -------- 40 5 40 10 40 20 40 30 -------- 50 10 50 20 50 50 50 60 从输出的结果可以看出Key实现了从小到大的排序，同时相同Key的Value也实现了从小到大的排序，这就是二次排序的结果 在本例中要比较两次。先按照第一字段排序，然后再对第一字段相同的按照第二字段排序。根据这一点，我们可以构造一个复合类IntPair ，它有两个字段，先利用分区对第一字段排序，再利用分区内的比较对第二字段排序。二次排序的流程分为以下几步。 1、自定义 key 2、自定义分区 3、Key的比较类 4、定义分组类函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159 package com.qunar.mr.secondarysort;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.*;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Partitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import java.util.*;public class SecondarySort &#123;// 1、自定义 key public static class IntPair implements WritableComparable&lt;IntPair&gt; &#123; private int first = 0; private int second = 0; /** * Read the two integers. * Encoded as: MIN_VALUE -&amp;gt; 0, 0 -&amp;gt; -MIN_VALUE, MAX_VALUE-&amp;gt; -1 */ @Override public void readFields(DataInput in) throws IOException &#123; first = in.readInt(); second = in.readInt(); &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(first); out.writeInt(second); &#125; @Override public int hashCode() &#123; return first * 157 + second; &#125; @Override public boolean equals(Object right) &#123; if (right instanceof IntPair) &#123; IntPair r = (IntPair) right; return r.first == first &amp;&amp; r.second == second; &#125; else &#123; return false; &#125; &#125; @Override public int compareTo(IntPair o) &#123; if (first != o.first) &#123; return first &lt; o.first ? -1 : 1; &#125; else if (second != o.second) &#123; return second &lt; o.second ? -1 : 1; &#125; else &#123; return 0; &#125; &#125; /** * Set the left and right values. */ public void set(int left, int right) &#123; first = left; second = right; &#125; public int getFirst() &#123; return first; &#125; public int getSecond() &#123; return second; &#125; &#125;// 2、自定义分区 public static class FirstPartitioner extends Partitioner&lt;IntPair,IntWritable&gt; &#123; @Override public int getPartition(IntPair key, IntWritable value, int numPartitions) &#123; return Math.abs(key.getFirst() * 127) % numPartitions; &#125; &#125;// 3、Key的比较类 public static class FirstGroupingComparator implements RawComparator&lt;IntPair&gt; &#123; @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123; return WritableComparator.compareBytes(b1, s1, Integer.SIZE/8, b2, s2, Integer.SIZE/8); &#125; @Override public int compare(IntPair o1, IntPair o2) &#123; int l = o1.getFirst(); int r = o2.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); &#125; &#125; public static class MapClass extends Mapper&lt;Object, Text,IntPair, IntWritable&gt;&#123; private final IntPair key = new IntPair(); private final IntWritable value = new IntWritable(); @Override public void map(Object inKey, Text inValue, Context context) throws IOException, InterruptedException &#123; StringTokenizer itr = new StringTokenizer(inValue.toString()); int left = 0; int right = 0; if (itr.hasMoreTokens()) &#123; left = Integer.parseInt(itr.nextToken()); if (itr.hasMoreTokens()) &#123; right = Integer.parseInt(itr.nextToken()); &#125; key.set(left, right); value.set(right); context.write(key, value); &#125; &#125; &#125; public static class Reduce extends Reducer&lt;IntPair,IntWritable,Text,IntWritable&gt;&#123; private static final Text SEPARATOR = new Text(&quot;------------------------------------------------&quot;); private final Text first = new Text(); public void reduce(IntPair key,Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; context.write(SEPARATOR, null); first.set(Integer.toString(key.getFirst())); for(IntWritable value: values) &#123; context.write(first, value); &#125; &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf, &quot;secondary sort&quot;); job.setJarByClass(SecondarySort.class); job.setMapperClass(MapClass.class); job.setReducerClass(Reduce.class); // group and partition by the first int in the pair job.setPartitionerClass(FirstPartitioner.class); job.setGroupingComparatorClass(FirstGroupingComparator.class); // the map output is IntPair, IntWritable job.setMapOutputKeyClass(IntPair.class); job.setMapOutputValueClass(IntWritable.class); // the reduce output is Text, IntWritable job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/secondarysort/sort.txt&quot;)); FileOutputFormat.setOutputPath(job, new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/secondarysort/out&quot;)); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;]]></content>
      <categories>
        <category>mapreduce编程</category>
      </categories>
      <tags>
        <tag>mapreduce编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全排序]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%85%A8%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[错误写法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.qunar.mr.datasort;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;/** * http://www.aboutyun.com/thread-7046-1-1.html * * 这个实例仅仅要求对输入数据进行排序，熟悉MapReduce过程的读者会很快想到在MapReduce过程中就有排序，是否可以利用这个默认的排序，而不需要自己再实现具体的排序呢？ * 答案是肯定的。 * * 但是在使用之前首先需要了解它的默认排序规则。它是按照key值进行排序的， * 如果key为封装int的IntWritable类型，那么MapReduce按照数字大小对key排序， * 如果key为封装为String的Text类型，那么MapReduce按照字典顺序对字符串排序。 * * 了解了这个细节，我们就知道应该使用封装int的IntWritable型数据结构了。 * 也就是在map中将读入的数据转化成IntWritable型，然后作为key值输出（value任意）。 * reduce拿到&lt;key，value-list&gt;之后，将输入的key作为value输出，并根据value-list中元素的个数决定输出的次数。 * 输出的key（即代码中的linenum）是一个全局变量，它统计当前key的位次。 * * 需要注意的是这个程序中没有配置Combiner，也就是在MapReduce过程中不使用Combiner。 * 这主要是因为使用map和reduce就已经能够完成任务了。 * * * https://blog.csdn.net/evo_steven/article/details/17139123 */public class DataSort &#123; public static class Map extends Mapper&lt;Object, Text, IntWritable,Text&gt; &#123;// private static IntWritable data = null;//todo 此处不能这样写，否则会报错 public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; String line = value.toString(); context.write(new IntWritable(Integer.parseInt(line)),new Text(&quot;&quot;)); &#125; &#125; public static class Reduce extends Reducer&lt;IntWritable,Text,IntWritable,IntWritable&gt; &#123;// 每个reduce中也只是单独的全局变量，并非整个集群的全局变量，// 一旦加入job.setNumReduceTasks(2);就会有两个文件，会出现错误结果 private static IntWritable count = new IntWritable(1); //todo IntWritable public void reduce(IntWritable key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException &#123; for (Text val:values) &#123; context.write(count,key); count = new IntWritable(count.get() + 1); &#125; &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;DataSort&quot;); job.setJarByClass(DataSort.class); job.setNumReduceTasks(2); //todo 如果有此处，实际环境中会报错 job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(IntWritable.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/datasort/in&quot;)); FileOutputFormat.setOutputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/datasort/out&quot;)); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 有问题的做法，缺少setPartitionerClass设计模式中的全排序实现思路是 两次JOB，第一次做分区，第二次做排序，也用到了setPartitionerClass，reduce只负责输出Partitinoner的作用除了快速找到key对应的reducer，更重要的一点是：这个Partitioner控制了排序的总体有序！ 正确做法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package com.qunar.mr.datasort;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.*;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.partition.InputSampler;import org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner;import java.io.IOException;public class DataSortNew &#123; public static class Map extends Mapper&lt;Text, Text, Text,IntWritable&gt; &#123; public void map(Text key,Text value,Context context) throws IOException, InterruptedException &#123; context.write(key,new IntWritable(Integer.parseInt(key.toString())));//todo key是Text,value也是key &#125; &#125; public static class Reduce extends Reducer&lt;Text,IntWritable,IntWritable, NullWritable&gt; &#123;//todo 此处的key也一定要跟着改为Text public void reduce(Text key,Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; for (IntWritable val:values) &#123; context.write(val,NullWritable.get()); &#125; &#125; &#125; public static class KeyComparator extends WritableComparator &#123; protected KeyComparator() &#123; super(Text.class, true); &#125; @Override public int compare(WritableComparable w1, WritableComparable w2) &#123; int v1 = Integer.parseInt(w1.toString()); int v2 = Integer.parseInt(w2.toString()); return v1 - v2; &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); conf.set(&quot;mapreduce.totalorderpartitioner.naturalorder&quot;, &quot;false&quot;); Job job = Job.getInstance(conf,&quot;DataSortNew&quot;); job.setJarByClass(DataSortNew.class); FileInputFormat.addInputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/datasort/in&quot;)); FileOutputFormat.setOutputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/datasort/out&quot;)); job.setInputFormatClass(KeyValueTextInputFormat.class); job.setSortComparatorClass(KeyComparator.class);//todo job.setNumReduceTasks(100); //todo job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); job.setOutputValueClass(NullWritable.class);// 在新版本的Hadoop中，内置了三个采样器： SplitSampler，RandomSampler和IntervalSampler。这三个采样器都是InputSampler类的静态内部类，并且都实现了InputSampler类的内部接口Sample// https://flyingdutchman.iteye.com/blog/1878962// 0.01-----------------每个样本被抽到的概率// 1000------------------样本数// 100--------------------分区数 String partitionPath=&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/datasort/ss/sampler&quot;; TotalOrderPartitioner.setPartitionFile(job.getConfiguration(),new Path(partitionPath)); InputSampler.RandomSampler&lt;Text,Text&gt; sampler =new InputSampler.RandomSampler&lt;&gt;(0.01,1000,100); InputSampler.writePartitionFile(job,sampler); job.setPartitionerClass(TotalOrderPartitioner.class);//todo job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 参考:https://www.iteblog.com/archives/2146.htmlhttps://www.iteblog.com/archives/2147.htmlhttps://flyingdutchman.iteye.com/blog/1878962]]></content>
      <categories>
        <category>mapreduce编程</category>
      </categories>
      <tags>
        <tag>mapreduce编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[去重]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%8E%BB%E9%87%8D%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.qunar.mr.removeduplicate;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;/** * http://www.aboutyun.com/thread-7041-1-1.html */public class RemoveDuplicate &#123; public static class Map extends Mapper&lt;Object,Text, Text,Text&gt;&#123; private static Text line = new Text(); public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123;//todo line = value; context.write(line,new Text(&quot;&quot;)); &#125; &#125; public static class Reduce extends Reducer&lt;Text,Text,Text,Text&gt;&#123; public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException &#123;//todo context.write(key,new Text(&quot;&quot;)); &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;//todo Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;RemoveDuplicate&quot;);//todo job.setJarByClass(RemoveDuplicate.class); job.setMapperClass(Map.class); job.setCombinerClass(Reduce.class);//todo job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.addInputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/removeduplicate/in&quot;)); FileOutputFormat.setOutputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/removeduplicate/out&quot;)); System.exit(job.waitForCompletion(true) ? 0: 1); &#125;&#125;]]></content>
      <categories>
        <category>mapreduce编程</category>
      </categories>
      <tags>
        <tag>mapreduce编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[共同好友]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%85%B1%E5%90%8C%E5%A5%BD%E5%8F%8B%2F</url>
    <content type="text"><![CDATA[题意给出A-O个人中每个人的好友列表，求出哪些人两两之间有共同好友，以及他们的共同好友都有谁 原始文件：A:B,C,D,F,E,OB:A,C,E,KC:F,A,D,ID:A,E,F,LE:B,C,D,M,LF:A,B,C,D,E,O,MG:A,C,D,E,FH:A,C,D,E,OI:A,OJ:B,OK:A,C,DL:D,E,FM:E,F,GO:A,H,I,J输出格式：A-B: C,E… 思路此题旨在求两人之间的共同好友，原信息是&lt;人，该人的所有好友&gt;，因此首先以好友为键，人为值，交给reduce找出拥有此好友的所有人；再将这些人中两两配对作为键，之前的键（好友）作为值交给reduce去合并简而言之我打算分成两个步骤，两次迭代1）求出每一个人都是哪些人的共同好友2）把这些人（用共同好友的人）作为key,其好友作为value输出 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131package com.qunar.mr.sharefriends;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.Arrays;import java.util.HashSet;import java.util.Set;public class ShareFriends &#123; public static class Map1 extends Mapper&lt;Object, Text,Text,Text&gt;&#123; /** * 第一阶段的map函数主要完成以下任务 * 1.遍历原始文件中每行&lt;所有朋友&gt;信息 * 2.遍历“朋友”集合，以每个“朋友”为键，原来的“人”为值 即输出&lt;朋友,人&gt; */ public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; String lines = value.toString(); String cur = lines.split(&quot;:&quot;)[0]; String[] friends = lines.split(&quot;:&quot;)[1].split(&quot;,&quot;); for (String f:friends) &#123; context.write(new Text(f),new Text(cur)); &#125; &#125; &#125; /** * 第一阶段的reduce函数主要完成以下任务 * 1.对所有传过来的&lt;朋友，list(人)&gt;进行拼接，输出&lt;朋友,拥有这名朋友的所有人&gt; */ public static class Reduce1 extends Reducer&lt;Text,Text,Text,Text&gt;&#123; public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException &#123; StringBuffer sb = new StringBuffer(); for (Text v:values) &#123; sb.append(v.toString()).append(&quot;,&quot;); &#125; sb.deleteCharAt(sb.length() - 1); context.write(key,new Text(sb.toString())); &#125; &#125; /** * * //todo 如果A是B的好友，B也一定是A的好友，则不需要第一层job * * 第二阶段的map函数主要完成以下任务 * 1.将上一阶段reduce输出的&lt;朋友,拥有这名朋友的所有人&gt;信息中的 “拥有这名朋友的所有人”进行排序 ，以防出现B-C C-B这样的重复 * 2.将 “拥有这名朋友的所有人”进行两两配对，并将配对后的字符串当做键，“朋友”当做值输出，即输出&lt;人-人，共同朋友&gt; * */ public static class Map2 extends Mapper&lt;Object, Text,Text,Text&gt;&#123; public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] friend_persons = line.split(&quot;\t&quot;); String friend = friend_persons[0];//todo String[] persons = friend_persons[1].split(&quot;,&quot;); Arrays.sort(persons); //todo 排序,以防出现B-C C-B这样的重复 //两两配对 for(int i=0;i&lt;persons.length-1;i++)&#123; for(int j=i+1;j&lt;persons.length;j++)&#123; context.write(new Text(persons[i]+&quot;-&quot;+persons[j]+&quot;:&quot;), new Text(friend));//todo friend值作为共同好友value &#125; &#125; &#125; &#125; /** * 第二阶段的reduce函数主要完成以下任务 * 1.&lt;人-人，list(共同朋友)&gt; 中的“共同好友”进行拼接 最后输出&lt;人-人，两人的所有共同好友&gt; */ public static class Reduce2 extends Reducer&lt;Text,Text,Text,Text&gt;&#123; public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException &#123; Set&lt;String&gt; set = new HashSet&lt;String&gt;(); StringBuffer sb = new StringBuffer();//todo for(Text friend : values)&#123; if(!set.contains(friend.toString())) set.add(friend.toString()); &#125; for(String friend : set)&#123; sb.append(friend.toString()).append(&quot;,&quot;); &#125; sb.deleteCharAt(sb.length()-1); context.write(key, new Text(sb.toString())); &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); //第一阶段 Job job1 = Job.getInstance(conf,&quot;shareFriends&quot;); job1.setJarByClass(ShareFriends.class); job1.setMapperClass(Map1.class); job1.setReducerClass(Reduce1.class); job1.setOutputKeyClass(Text.class); job1.setOutputValueClass(Text.class); FileInputFormat.setInputPaths(job1, new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/sharefriends/in&quot;)); FileOutputFormat.setOutputPath(job1, new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/sharefriends/out/01&quot;)); boolean res1 = job1.waitForCompletion(true); //第二阶段 Job job2 = Job.getInstance(conf); job2.setMapperClass(Map2.class); job2.setReducerClass(Reduce2.class); job2.setOutputKeyClass(Text.class); job2.setOutputValueClass(Text.class); FileInputFormat.setInputPaths(job2, new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/sharefriends/out/01&quot;)); FileOutputFormat.setOutputPath(job2, new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/sharefriends/out/02&quot;)); boolean res2 = job2.waitForCompletion(true); System.exit(res2?0:1); &#125;&#125; 参考：https://blog.csdn.net/u012808902/article/details/77513188]]></content>
      <categories>
        <category>mapreduce编程</category>
      </categories>
      <tags>
        <tag>mapreduce编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用单表关联在父子关系中求解祖孙关系]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%88%A9%E7%94%A8%E5%8D%95%E8%A1%A8%E5%85%B3%E8%81%94%E5%9C%A8%E7%88%B6%E5%AD%90%E5%85%B3%E7%B3%BB%E4%B8%AD%E6%B1%82%E8%A7%A3%E7%88%B7%E5%AD%99%E5%85%B3%2F</url>
    <content type="text"><![CDATA[首先是有如下数据，设定左边是右边的儿子，右边是左边的父母 Tom LucyTom JackJone LucyJone JackLucy MaryLucy BenJack AliceJack JesseTerry AliceTerry JessePhilip TerryPhilip AlmaMark TerryMark Alma要求输出如下所示的爷孙关系，左边是右边的孙子，右边是左边的祖父母：Tom JesseTom AliceJone JesseJone AliceJone BenJone MaryTom BenTom MaryPhilip AlicePhilip JesseMark AliceMark Jesse 要利用Mapreduce解决这个问题，主要思想如下：1、在Map阶段，将父子关系与相反的子父关系，同时在各个value前补上前缀-与+标识此key-value中的value是正序还是逆序产生的，之后进入context。 下图通过其中的Jone-Lucy Lucy-Mary，求解出Jone-Mary来举例2、MapReduce会自动将同一个key的不同的value值，组合在一起，推到Reduce阶段。在value数组中，跟住前缀，我们可以轻松得知，哪个是爷，哪个是孙。 因此对各个values数组中各个项的前缀进行输出。可以看得出，整个过程Key一直被作为连接的桥梁来用。形成一个单表关联的运算。因此代码如下，根据上面的思想很容易得出如下的代码了，需要注意的是，输出流输入流都要设置成Text，因为全程都是在对Text而不是IntWritable进行操作：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package com.qunar.mr.Grandson;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.ArrayList;import java.util.List;/** * 求祖父母，外祖父母四个人，或者更多人 */public class GrandChildren &#123; public static class Map extends Mapper&lt;Object,Text,Text, Text&gt; &#123; public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; String[] record = value.toString().split(&quot; &quot;); context.write(new Text(record[0]),new Text(record[1]+&quot;,Father&quot;)); context.write(new Text(record[1]),new Text(record[0]+&quot;,Son&quot;)); &#125; &#125; public static class Reduce extends Reducer&lt;Text,Text,Text,Text&gt;&#123; public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException &#123; List&lt;String&gt; grandpa = new ArrayList&lt;&gt;(); List&lt;String&gt; grandson = new ArrayList&lt;&gt;(); for (Text v:values) &#123; if (v.toString().contains(&quot;Father&quot;))&#123; grandpa.add(v.toString().split(&quot;,&quot;)[0]); &#125; else if (v.toString().contains(&quot;Son&quot;))&#123; grandson.add(v.toString().split(&quot;,&quot;)[0]); &#125; &#125; for (String s:grandson) &#123; for (String g:grandpa) &#123; context.write(new Text(s),new Text(g)); &#125; &#125; &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;Grandson&quot;); job.setJarByClass(GrandChildren.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.addInputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/Grandson/in&quot;)); FileOutputFormat.setOutputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/Grandson/out&quot;)); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;]]></content>
      <categories>
        <category>mapreduce编程</category>
      </categories>
      <tags>
        <tag>mapreduce编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用job嵌套，多重Mapreduce，求解二度人脉]]></title>
    <url>%2F2018%2F02%2F11%2F%E5%88%A9%E7%94%A8job%E5%B5%8C%E5%A5%97%EF%BC%8C%E5%A4%9A%E9%87%8DMapreduce%EF%BC%8C%E6%B1%82%E8%A7%A3%E4%BA%8C%E5%BA%A6%E4%BA%BA%E8%84%89%2F</url>
    <content type="text"><![CDATA[题意Tom LucyTom JackJone LucyJone JackLucy MaryLucy BenJack AliceJack JesseTerry AliceTerry JessePhilip TerryPhilip AlmaMark TerryMark Alma 只是这次是假设一个没有重复，也就是不会出现Tom Lucy-Lucy Tom这样的好友关系表。任务是求其其中的二度人脉、潜在好友，也就是如下图：比如I认识C、G、H，但C不认识G，那么C-G就是一对潜在好友，但G-H早就认识了，因此不算为潜在好友。最终得出的是，如下的一个，没有重复的，首字母接近A再前的，也就Tom Mary-MaryTom只输出Mary Tom，因为M比T更接近A： Alice JesseAlice JoneAlice MarkAlice PhilipAlice TomAlma TerryBen JoneBen MaryBen TomJack LucyJack TerryJesse JoneJesse MarkJesse PhilipJesse TomJone MaryJone TomMark PhilipMary Tom 思路如下首先，我们进行第一个MapReduce：1、一个输入行，产生一对互逆的关系，压入context 例如Tom Lucy这个输入行就在Map阶段搞出Tom Lucy-Lucy Tom这样的互逆关系。 2、之后Map-reduce会自动对context中相同的key合并在一起。 例如由于存在Tom Lucy、Tom Jack，显然会产生一个Tom:{Lucy,Jack} 这是Reduce阶段以开始的键值对。 3、这个键值对相当于Tom所认识的人。先进行如下的输出，1代表Tom的一度人脉 Tom Lucy 1Tom Jack 1 潜在好友显然会在{Lucy,Jack}这个Tom所认识的人产生，对这个数组做笛卡尔乘积，形成关系：{&lt;Lucy,Lucy&gt;,&lt;Jack,Jack&gt;,&lt;Lucy,Jack&gt;,&lt;Jack,Lucy&gt;} 将存在自反性，前项首字母大于后项剔除，也就是&lt;Lucy,Lucy&gt;这类无意义的剔除，&lt;Lucy,Jack&gt;,&lt;Jack,Lucy&gt;认定为一个关系，将剩余关系进行如下的输出，其中2代表Tom的二度人脉，也就是所谓的潜在好友： Lucy Jack 2 此时，第一个MapReduce，输出如下： Alice Jack 1Alice Terry 1Jack Terry 2Alma Philip 1Alma Mark 1Mark Philip 2Ben Lucy 1Jack Jesse 1Jack Tom 1Jack Jone 1Alice Jack 1Jesse Tom 2Jesse Jone 2Jone Tom 2Alice Jesse 2Alice Tom 2Alice Jone 2Jesse Terry 1Jack Jesse 1Jack Terry 2Jack Jone 1Jone Lucy 1Jack Lucy 2Ben Lucy 1Jone Lucy 1Lucy Tom 1Lucy Mary 1Ben Jone 2Ben Tom 2Ben Mary 2Jone Tom 2Jone Mary 2Mary Tom 2Alma Mark 1Mark Terry 1Alma Terry 2Lucy Mary 1Philip Terry 1Alma Philip 1Alma Terry 2Philip Terry 1Alice Terry 1Mark Terry 1Jesse Terry 1Alice Philip 2Alice Mark 2Alice Jesse 2Mark Philip 2Jesse Philip 2Jesse Mark 2Jack Tom 1Lucy Tom 1Jack Lucy 2这时，形式已经很明朗了 再进行第二个Mapreduce任务是剔除本身就存在的关系，也就是在潜在好友中剔除本身就认识的关系。 将上述第一个Mapreduce的输出，关系作为key，后面的X度人脉这个1、2值作为value，进行Mapreduce的处理。 那么例如这个关系，之所以会被认定为潜在好友，是因为它所对应的值数组，里面一个1都没有，全是2，也就是它们本来不是一度人脉，而这对，不能成为潜在好友，因为他们所对应的值数组，里面有1，存在任意一对一度人脉、直接认识，就绝对不能被认定为二度人脉。 将被认定为二度人脉的关系输出，就得到最终结果。 代码package com.qunar.mr.deg2friend; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import java.io.IOException; import java.util.ArrayList; import java.util.List; import java.util.Random; //https://www.cnblogs.com/hxsyl/p/6127843.html public class Deg2Friend { public static class Map extends Mapper&lt;Object,Text,Text,Text&gt; { public void map(Object key,Text value,Context context) throws IOException, InterruptedException { String[] line = value.toString().split(&quot;,&quot;); context.write(new Text(line[0]),new Text(line[1])); context.write(new Text(line[1]),new Text(line[0])); } } public static class Reduce extends Reducer&lt;Text,Text,Text,Text&gt;{ public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException { List&lt;String&gt; potentialFriends = new ArrayList&lt;&gt;(); //这里一定要用string存，而不是用Text，Text只是一个类似指针，一个MapReduce的引用 //换成用Text来存，你会惊讶，为何我存的值都一模一样的？ for (Text v:values) { potentialFriends.add(v.toString()); if (key.toString().compareTo(v.toString()) &lt; 0){// 确保首字母大者再前，如Tom Alice则输出Alice Tom context.write(new Text(key + &quot;\t&quot; + v),new Text(&quot;1&quot;)); } else { context.write(new Text(v + &quot;\t&quot; + key),new Text(&quot;1&quot;)); } } for (String p1:potentialFriends) { for (String p2:potentialFriends) { if (p1.compareTo(p2) &lt; 0){//todo 将存在自反性，前项首字母大于后项的关系剔除 context.write(new Text(p1 +&quot;\t&quot; + p2),new Text(&quot;2&quot;)); } } } } } public static class Map2 extends Mapper&lt;Object,Text,Text,Text&gt;{ public void map(Object key,Text value,Context context) throws IOException, InterruptedException { String[] line = value.toString().split(&quot;\t&quot;);//输入文件，键值对的分隔符为\t context.write(new Text(line[0] + &quot;\t&quot; + line[1]), new Text(line[2]));//关系作为key，后面的X度人脉这个1、2值作为value } } public static class Reduce2 extends Reducer&lt;Text, Text, Text, Text&gt; { public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException, InterruptedException { //检查合并之后是否存在任意一对一度人脉 boolean isPotentialFriend = true; for (Text v:values) { if (v.toString().equals(&quot;1&quot;)){ isPotentialFriend = false; break; } } if (isPotentialFriend){ String[] potentialFriends = key.toString().split(&quot;\t&quot;); context.write(new Text(potentialFriends[0]),new Text(potentialFriends[1])); } } } public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException { Configuration conf = new Configuration(); // 判断output文件夹是否存在，如果存在则删除 Path outPath = new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/deg2friend/out&quot;); FileSystem fs = outPath.getFileSystem(conf); if (fs.exists(outPath)){ fs.delete(outPath,true);// true的意思是，就算output有东西，也一带删除 } Job job = Job.getInstance(conf,&quot;deg2friend&quot;); job.setJarByClass(Deg2Friend.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setOutputKeyClass(Text.class);//todo keyClass job.setOutputValueClass(Text.class);//todo valueClass // 原来当mapper与reducer的输出类型一致时可以用 job.setOutputKeyClass(theClass)与job.setOutputValueClass //(theClass)这两个进行配置就行，但是当mapper用于reducer两个的输出类型不一致的时候就需要分别进行配置了。 // job.setMapOutputKeyClass(Text.class); //todo 测试只有map的情况 // job.setMapOutputValueClass(Text.class); FileInputFormat.addInputPath(job,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/deg2friend/in&quot;)); Path tempDir = new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/deg2friend/out/&quot; + Integer.toString(new Random().nextInt(Integer.MAX_VALUE)) ); FileOutputFormat.setOutputPath(job, tempDir); if (job.waitForCompletion(true)){ Job job2 = Job.getInstance(conf,&quot;job2&quot;); job2.setMapperClass(Map2.class); job2.setReducerClass(Reduce2.class); job2.setOutputKeyClass(Text.class); job2.setOutputValueClass(Text.class); FileInputFormat.addInputPath(job2,tempDir); FileOutputFormat.setOutputPath(job2,new Path(&quot;/Users/lifei/qunarproject/hiveudf/src/main/java/com/qunar/mr/deg2friend/out/final&quot;)); System.exit(job2.waitForCompletion(true) ? 0 : 1); } } } 参考:https://blog.csdn.net/yongh701/article/details/50630498]]></content>
      <categories>
        <category>mapreduce编程</category>
      </categories>
      <tags>
        <tag>mapreduce模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wordcount]]></title>
    <url>%2F2018%2F02%2F11%2Fwordcount%2F</url>
    <content type="text"><![CDATA[最基本的，需要手动写出。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.qunar.mr.wordcount;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.StringTokenizer;/** * StringTokenizer * https://www.cnblogs.com/gnivor/p/4509268.html * * * http://hadoop.apache.org/docs/r1.0.4/cn/mapred_tutorial.html#Map%2FReduce+-+%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2 * http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Payload * */public class WordCount &#123; public static class TokenizerMapper extends Mapper&lt;Object, Text,Text, IntWritable&gt;&#123; private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key,Text value,Context context) throws IOException, InterruptedException &#123; StringTokenizer itr = new StringTokenizer(value.toString());//StringTokenizer while (itr.hasMoreTokens())&#123; word.set(itr.nextToken()); context.write(word,one); &#125; &#125; &#125; public static class IntSumReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt;&#123; private IntWritable result = new IntWritable(); public void reduce(Text key,Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val:values) &#123; sum += val.get(); &#125; result.set(sum); context.write(key,result); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf,&quot;word count&quot;); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job,new Path(&quot;/Users/lifei/Desktop/ab/a.txt&quot;)); FileOutputFormat.setOutputPath(job,new Path(&quot;/Users/lifei/Desktop/ab_out&quot;)); System.exit(job.waitForCompletion(true)? 0 : 1); &#125;&#125;]]></content>
      <categories>
        <category>mapreduce编程</category>
      </categories>
      <tags>
        <tag>mapreduce编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch]]></title>
    <url>%2F2017%2F02%2F16%2FElasticsearch%2F</url>
    <content type="text"><![CDATA[建立倒排索引 标准化规则(normalization)并不会单纯直接建立的倒排索引，会使用到标准化规则(normalization) dog、dogs fox foxes单复数问题只保留一个，有共同词根Quick、quick 不区分大小写问题jump和leap意思相近，只保留一个 分词器介绍以及内置分词器 配置中文分词器重新启动elasticsearch在日志中可以看到。 CRUD新建索引,默认配置 PUT lib2查看索引 GET _all/_settings指定文档id用put,不指定文档id用post 获取docGET /lib2/user/1GET /lib2/user/1?_source=age,about 修改文档修改文档全覆盖用PUT，先把原有文档标记为deleted(es会在合适的时间把它删除掉),然后会创建一个新的文档修改具体字段用post两种修改方式的区别是： post方式对并发问题处理，可以增加一个重试参数 删除文档DELETE /lib/user/1 DELETE /lib/user 先把文档标记为deleted，es会在合适的时间把它删除mget,bulk 版本控制外部的版本号需要大于内存的版本号才能成功，相等也不行 mapping当我们添加一个文档的时候，elasticsearch就会默认给我们创建一个mapping,指定字段的数据类型。 数值，日期类型必须是完全一样才能查出来，即精确查询，没有分词；但是text类型默认进行分词，只需要包含就可以查到自定义mapping除了规定字段的类型，最大作用就是可以指定字段的属性是否分词,是否存储，是否索引等 字段类型 支持的属性 查询term、terms只会从倒排索引中查询，如果倒排索引中没有就不会查到数据terms表示包含其中一个即可。 基本查询 中文查询 Filter建立倒排索引时，会默认转为小写的，所以查大写字符的时，查询条件要转为小写，termId为id100127 exists相当于数据库的is not null将query转化为filter进行优化 聚合查询 cardinality相当于数据库的distinct constant_score由于不查评分，效率会高一些 原理解析es分布式结构特点作为应用程序的使用方目前只需要关心怎么添加文档和搜索文档。这就是隐藏特性垂直扩容是原集群节点数不变，新采购的机器配置高于旧机器。 路由相关到有相关数据的其他节点，就是将请求转发到其他节点，该及诶单将结果再转发回原始请求节点，最终返回给客户端(即应用程序)。 分片和副本机制新增节点。副本可以处理查询请求 水平扩容的过程 容错机制 文档核心元数据从6开始一个index下只能有一个type GUID算法可以保证在分布式并发情况下生成的id不会冲突 基于grovvy脚本实现partial update 文档数据路由原理 文档增删改内部原理 写一致原理和qurom机制 文档查询内部原理 分页查询中的deep paging问题mysql也可能会出现 copyto 字符串排序问题 如何计算相关度分数 DocValues解析默认开启的es对非字符串类型(日期，数字等)除建立倒排索引外，还会建立一个正排索引 基于scroll技术滚动搜索大量数据 dynamic mapping策略 english分词器会把is,a,an等停用词去掉，这些词不会建立倒排索引。而standard会建立。 重建索引 索引不可变的原因]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql千万级分页]]></title>
    <url>%2F2016%2F02%2F17%2Fmysql%E5%8D%83%E4%B8%87%E7%BA%A7%E5%88%86%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[Mysql数据库最简单，是利用mysql的LIMIT函数,LIMIT [offset,] rows从数据库表中M条记录(不包含m条)开始检索N条记录的语句为：1SELECT * FROM 表名称 LIMIT M,N 其中limit为可选项，例如我们有个student表，我们选取前5条记录可以使用下面的sql语句1select * from student limit 5; 问题：千万级的表查询分页，怎么优化查询12select * from user limit 10000000,10select * from user where name=&quot;a&quot; limit 10000000,10 为了能更加优化查询，建立联合索引是一个好的方法，where 的条件 和主键id 作为索引 serach(name,id) 第一次建立索引时候 是id 在前 name在后，这样确实也解决这样的问题，都变成0.5s左右 ，但是还不是我想要的 ，最后上网搜索答案，发现建立联合索引顺序不同，性能也就大大提高，于是把建立索引顺序变成是id 在后 name在前。结果打出意料 ，速度变成了0.05秒 。性能大幅度提升. 开始的select id from collect order by id limit 90000,10;这么快就是因为走了索引，可是如果加了where 就不走索引了。抱着试试看的想法加了 search(vtype,id) 这样的索引。然后测试select id from collect where vtype=1 limit 90000,10; 非常快！0.04秒完成！再测试: select id ,title from collect where vtype=1 limit 90000,10; 非常遗憾，8-9秒，没走search索引！再测试：search(id,vtype)，还是select id 这个语句，也非常遗憾，0.5秒。综上：如果对于有where 条件，又想走索引用limit的，必须设计一个索引，将where放第一位，limit用到的主键放第2位，而且只能select 主键！即search(vtype,id) 这样的索引,如下查询select id from collect where vtype=1 limit 90000,10可以快速返回id，然后根据id in (xxxx)的方式去实现 参考:https://blog.csdn.net/m0_37922390/article/details/81976014https://blog.csdn.net/li772030428/article/details/52839987/]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapreduce模型]]></title>
    <url>%2F2016%2F01%2F26%2Fmapreduce%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[graph TD A[Map] -->|partition,kvBuffer,sort,spill与combiner,merge| B{Shuffle} B -->|copy,cache_sort_merge,spill,merge,file,GroupingComparator| C[reduce] 从JVM的角度看Map和Reduce Map阶段包括：第一读数据：从HDFS读取数据1、问题:读取数据产生多少个Mapper？？ Mapper数据过大的话，会产生大量的小文件，由于Mapper是基于虚拟机的，过多的Mapper创建和初始化及关闭虚拟机都会消耗大量的硬件资源； Mapper数太小，并发度过小，Job执行时间过长，无法充分利用分布式硬件资源； 2、Mapper数量由什么决定？？ （1）输入文件数目 （2）输入文件的大小 （3）配置参数 所以说map数是可以控制的这三个因素决定的。 涉及参数：mapreduce.input.fileinputformat.split.minsize //启动map最小的split size大小，默认0 mapreduce.input.fileinputformat.split.maxsize //启动map最大的split size大小，默认256Mdfs.block.size//block块大小，默认64M计算公式：splitSize = Math.max(minSize, Math.min(maxSize, blockSize)); 例如默认情况下：例如一个文件800M，Block大小是128M，那么Mapper数目就是7个。6个Mapper处理的数据是128M，1个Mapper处理的数据是32M；再例如一个目录下有三个文件大小分别为：5M10M 150M 这个时候其实会产生四个Mapper处理的数据分别是5M，10M，128M，22M。 Mapper是基于文件自动产生的，如果想要自己控制Mapper的个数？？？ 就如上面，5M，10M的数据很快处理完了，128M要很长时间；这个就需要通过参数的控制来调节Mapper的个数。 减少Mapper的个数的话，就要合并小文件，这种小文件有可能是直接来自于数据源的小文件，也可能是Reduce产生的小文件。 设置合并器：（set都是在hive脚本，也可以配置Hadoop） 设置合并器本身：set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;set hive.merge.mapFiles=true;set hive.merge.mapredFiles=true;set hive.merge.size.per.task=256000000;//每个Mapper要处理的数据，就把上面的5M10M……合并成为一个 一般还要配合一个参数：set mapred.max.split.size=256000000 // mapred切分的大小set mapred.min.split.size.per.node=128000000//低于128M就算小文件，数据在一个节点会合并，在多个不同的节点会把数据抓过来进行合并。 Hadoop中的参数：可以通过控制文件的数量控制mapper数量mapreduce.input.fileinputformat.split.minsize（default：0），小于这个值会合并mapreduce.input.fileinputformat.split.maxsize 大于这个值会切分 第二处理数据：Partition说明对于map输出的每一个键值对，系统都会给定一个partition，partition值默认是通过计算key的hash值后对Reduce task的数量取模获得。如果一个键值对的partition值为1，意味着这个键值对会交给第一个Reducer处理。 自定义partitioner的情况：1、我们知道每一个Reduce的输出都是有序的，但是将所有Reduce的输出合并到一起却并非是全局有序的，如果要做到全局有序，我们该怎么做呢？最简单的方式，只设置一个Reduce task，但是这样完全发挥不出集群的优势，而且能应对的数据量也很受限。最佳的方式是自己定义一个Partitioner，用输入数据的最大值除以系统Reduce task数量的商作为分割边界，也就是说分割数据的边界为此商的1倍、2倍至numPartitions-1倍，这样就能保证执行partition后的数据是整体有序的。2、解决数据倾斜：另一种需要我们自己定义一个Partitioner的情况是各个Reduce task处理的键值对数量极不平衡。对于某些数据集，由于很多不同的key的hash值都一样，导致这些键值对都被分给同一个Reducer处理，而其他的Reducer处理的键值对很少，从而拖延整个任务的进度。当然，编写自己的Partitioner必须要保证具有相同key值的键值对分发到同一个Reducer。3、自定义的Key包含了好几个字段，比如自定义key是一个对象，包括type1，type2，type3,只需要根据type1去分发数据，其他字段用作二次排序。 环形缓冲区 Map的输出结果是由collector处理的，每个Map任务不断地将键值对输出到在内存中构造的一个环形数据结构中。使用环形数据结构是为了更有效地使用内存空间，在内存中放置尽可能多的数据。 这个数据结构其实就是个字节数组，叫Kvbuffer，名如其义，但是这里面不光放置了数据，还放置了一些索引数据，给放置索引数据的区域起了一个Kvmeta的别名，在Kvbuffer的一块区域上穿了一个IntBuffer（字节序采用的是平台自身的字节序）的马甲。数据区域和索引数据区域在Kvbuffer中是相邻不重叠的两个区域，用一个分界点来划分两者，分界点不是亘古不变的，而是每次Spill之后都会更新一次。初始的分界点是0，数据的存储方向是向上增长，索引数据的存储方向是向下增长Kvbuffer的存放指针bufindex是一直闷着头地向上增长，比如bufindex初始值为0，一个Int型的key写完之后，bufindex增长为4，一个Int型的value写完之后，bufindex增长为8。 索引是对在kvbuffer中的键值对的索引，是个四元组，包括：value的起始位置、key的起始位置、partition值、value的长度，占用四个Int长度，Kvmeta的存放指针Kvindex每次都是向下跳四个“格子”，然后再向上一个格子一个格子地填充四元组的数据。比如Kvindex初始位置是-4，当第一个键值对写完之后，(Kvindex+0)的位置存放value的起始位置、(Kvindex+1)的位置存放key的起始位置、(Kvindex+2)的位置存放partition的值、(Kvindex+3)的位置存放value的长度，然后Kvindex跳到-8位置，等第二个键值对和索引写完之后，Kvindex跳到-12位置。 第三写数据到磁盘Mapper中的Kvbuffer的大小默认100M，可以通过mapreduce.task.io.sort.mb（default：100）参数来调整。可以根据不同的硬件尤其是内存的大小来调整，调大的话，会减少磁盘spill的次数此时如果内存足够的话，一般都会显著提升性能。spill一般会在Buffer空间大小的80%开始进行spill（因为spill的时候还有可能别的线程在往里写数据，因为还预留空间，有可能有正在写到Buffer中的数据），可以通过mapreduce.map.sort.spill.percent（default：0.80）进行调整，Map Task在计算的时候会不断产生很多spill文件，在Map Task结束前会对这些spill文件进行合并，这个过程就是merge的过程。 mapreduce.task.io.sort.factor（default：10），代表进行merge的时候最多能同时merge多少spill，如果有100个spill个文件，此时就无法一次完成整个merge的过程，这个时候需要调大mapreduce.task.io.sort.factor（default：10）来减少merge的次数，从而减少磁盘的操作； Spill这个重要的过程是由Spill线程承担，Spill线程从Map任务接到“命令”之后就开始正式干活，干的活叫SortAndSpill，原来不仅仅是Spill，在Spill之前还有个颇具争议性的Sort。 Combiner存在的时候，此时会根据Combiner定义的函数对map的结果进行合并，什么时候进行Combiner操作呢？？？和Map在一个JVM中，是由min.num.spill.for.combine的参数决定的，默认是3，也就是说spill的文件数在默认情况下由三个的时候就要进行combine操作，最终减少磁盘数据； 减少磁盘IO和网络IO还可以进行：压缩，对spill，merge文件都可以进行压缩。中间结果非常的大，IO成为瓶颈的时候压缩就非常有用，可以通过mapreduce.map.output.compress（default：false）设置为true进行压缩，数据会被压缩写入磁盘，读数据读的是压缩数据需要解压，在实际经验中Hive在Hadoop的运行的瓶颈一般都是IO而不是CPU，压缩一般可以10倍的减少IO操作，压缩的方式Gzip，Lzo,BZip2,Lzma等，其中Lzo是一种比较平衡选择，mapreduce.map.output.compress.codec（default：org.apache.hadoop.io.compress.DefaultCodec）参数设置。但这个过程会消耗CPU，适合IO瓶颈比较大。 Shuffle和Reduce阶段包括：一、Copy1、由于job的每一个map都会根据reduce(n)数将数据分成map 输出结果分成n个partition，所以map的中间结果中是有可能包含每一个reduce需要处理的部分数据的。所以，为了优化reduce的执行时间，hadoop中是等job的第一个map结束后，所有的reduce就开始尝试从完成的map中下载该reduce对应的partition部分数据，因此map和reduce是交叉进行的，其实就是shuffle。Reduce任务通过HTTP向各个Map任务拖取（下载）它所需要的数据（网络传输），Reducer是如何知道要去哪些机器取数据呢？一旦map任务完成之后，就会通过常规心跳通知应用程序的Application Master。reduce的一个线程会周期性地向master询问，直到提取完所有数据（如何知道提取完？）数据被reduce提走之后，map机器不会立刻删除数据，这是为了预防reduce任务失败需要重做。因此map输出数据是在整个作业完成之后才被删除掉的。2、reduce进程启动数据copy线程(Fetcher)，通过HTTP方式请求maptask所在的TaskTracker获取maptask的输出文件。由于map通常有许多个，所以对一个reduce来说，下载也可以是并行的从多个map下载，那到底同时到多少个Mapper下载数据？？这个并行度是可以通过mapreduce.reduce.shuffle.parallelcopies(default5）调整。默认情况下，每个Reducer只会有5个map端并行的下载线程在从map下数据，如果一个时间段内job完成的map有100个或者更多，那么reduce也最多只能同时下载5个map的数据，所以这个参数比较适合map很多并且完成的比较快的job的情况下调大，有利于reduce更快的获取属于自己部分的数据。 在Reducer内存和网络都比较好的情况下，可以调大该参数；3、reduce的每一个下载线程在下载某个map数据的时候，有可能因为那个map中间结果所在机器发生错误，或者中间结果的文件丢失，或者网络瞬断等等情况，这样reduce的下载就有可能失败，所以reduce的下载线程并不会无休止的等待下去，当一定时间后下载仍然失败，那么下载线程就会放弃这次下载，并在随后尝试从另外的地方下载（因为这段时间map可能重跑）。reduce下载线程的这个最大的下载时间段是可以通过mapreduce.reduce.shuffle.read.timeout（default180000秒）调整的。如果集群环境的网络本身是瓶颈，那么用户可以通过调大这个参数来避免reduce下载线程被误判为失败的情况。一般情况下都会调大这个参数，这是企业级最佳实战。 二、MergeSort1、这里的merge和map端的merge动作类似，只是数组中存放的是不同map端copy来的数值。Copy过来的数据会先放入内存缓冲区中，然后当使用内存达到一定量的时候才spill磁盘。这里的缓冲区大小要比map端的更为灵活，它基于JVM的heap size设置。这个内存大小的控制就不像map一样可以通过io.sort.mb来设定了，而是通过另外一个参数 mapreduce.reduce.shuffle.input.buffer.percent（default 0.7f 源码里面写死了） 来设置，这个参数其实是一个百分比，意思是说，shuffile在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of reduce task。JVM的heapsize的70%。内存到磁盘merge的启动门限可以通过mapreduce.reduce.shuffle.merge.percent（default0.66）配置。也就是说，如果该reduce task的最大heap使用量（通常通过mapreduce.admin.reduce.child.java.opts来设置，比如设置为-Xmx1024m）的一定比例用来缓存数据。默认情况下，reduce会使用其heapsize的70%来在内存中缓存数据。假设 mapreduce.reduce.shuffle.input.buffer.percent 为0.7，reducetask的max heapsize为1G，那么用来做下载数据缓存的内存就为大概700MB左右。这700M的内存，跟map端一样，也不是要等到全部写满才会往磁盘刷的，而是当这700M中被使用到了一定的限度（通常是一个百分比），就会开始往磁盘刷（刷磁盘前会先做sortMerge）。这个限度阈值也是可以通过参数 mapreduce.reduce.shuffle.merge.percent（default0.66）来设定。与map 端类似，这也是溢写的过程，这个过程中如果你设置有Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。这种merge方式一直在运行，直到没有map端的数据时才结束，然后启动磁盘到磁盘的merge方式生成最终的那个文件。 这里需要强调的是，merge有三种形式：1)内存到内存（memToMemMerger）2）内存中Merge（inMemoryMerger）3）磁盘上的Merge（onDiskMerger）具体包括两个：（一）Copy过程中磁盘合并（二）磁盘到磁盘。（1）内存到内存Merge（memToMemMerger） Hadoop定义了一种MemToMem合并，这种合并将内存中的map输出合并，然后再写入内存。这种合并默认关闭，可以通过mapreduce.reduce.merge.memtomem.enabled(default:false) 打开，当map输出文件达到mapreduce.reduce.merge.memtomem.threshold时，触发这种合并。（2）内存中Merge（inMemoryMerger）：当缓冲中数据达到配置的阈值时，这些数据在内存中被合并、写入机器磁盘。阈值有2种配置方式： 配置内存比例：前面提到reduceJVM堆内存的一部分用于存放来自map任务的输入，在这基础之上配置一个开始合并数据的比例。假设用于存放map输出的内存为500M，mapreduce.reduce.shuffle.merge.percent配置为0.66，则当内存中的数据达到330M的时候，会触发合并写入。配置map输出数量： 通过mapreduce.reduce.merge.inmem.threshold配置。在合并的过程中，会对被合并的文件做全局的排序。如果作业配置了Combiner，则会运行combine函数，减少写入磁盘的数据量。（3）磁盘上的Merge（onDiskMerger）： （3.1）Copy过程中磁盘Merge：在copy过来的数据不断写入磁盘的过程中，一个后台线程会把这些文件合并为更大的、有序的文件。如果map的输出结果进行了压缩，则在合并过程中，需要在内存中解压后才能给进行合并。这里的合并只是为了减少最终合并的工作量，也就是在map输出还在拷贝时，就开始进行一部分合并工作。合并的过程一样会进行全局排序。（3.2）最终磁盘中Merge：当所有map输出都拷贝完毕之后，所有数据被最后合并成一个整体有序的文件，作为reduce任务的输入。这个合并过程是一轮一轮进行的，最后一轮的合并结果直接推送给reduce作为输入，节省了磁盘操作的一个来回。最后（所以map输出都拷贝到reduce之后）进行合并的map输出可能来自合并后写入磁盘的文件，也可能来及内存缓冲，在最后写入内存的map输出可能没有达到阈值触发合并，所以还留在内存中。 每一轮合并不一定合并平均数量的文件数，指导原则是使用整个合并过程中写入磁盘的数据量最小，为了达到这个目的，则需要最终的一轮合并中合并尽可能多的数据，因为最后一轮的数据直接作为reduce的输入，无需写入磁盘再读出。因此我们让最终的一轮合并的文件数达到最大，即合并因子的值，通过mapreduce.task.io.sort.factor（default：10）来配置。 如上图：Reduce阶段中一个Reduce过程 可能的合并方式为：假设现在有20个map输出文件，合并因子配置为5，则需要4轮的合并。最终的一轮确保合并5个文件，其中包括2个来自前2轮的合并结果，因此原始的20个中，再留出3个给最终一轮。 三、Reduce函数调用（用户自定义业务逻辑）1、当reduce将所有的map上对应自己partition的数据下载完成后，就会开始真正的reduce计算阶段。reducetask真正进入reduce函数的计算阶段，由于reduce计算时肯定也是需要消耗内存的，而在读取reduce需要的数据时，同样是需要内存作为buffer，这个参数是控制，reducer需要多少的内存百分比来作为reduce读已经sort好的数据的buffer大小？？默认用多大内存呢？？默认情况下为0，也就是说，默认情况下，reduce是全部从磁盘开始读处理数据。可以用mapreduce.reduce.input.buffer.percent（default 0.0）(源代码MergeManagerImpl.java：674行)来设置reduce的缓存。如果这个参数大于0，那么就会有一定量的数据被缓存在内存并输送给reduce，当reduce计算逻辑消耗内存很小时，可以分一部分内存用来缓存数据，可以提升计算的速度。所以默认情况下都是从磁盘读取数据，如果内存足够大的话，务必设置该参数让reduce直接从缓存读数据，这样做就有点Spark Cache的感觉；2、Reduce在这个阶段，框架为已分组的输入数据中的每个 &lt;key, (list of values)&gt;对调用一次 reduce(WritableComparable,Iterator, OutputCollector, Reporter)方法。Reduce任务的输出通常是通过调用 OutputCollector.collect(WritableComparable,Writable)写入文件系统的。Reducer的输出是没有排序的,按照reduce的输出顺序，所以用户可以在reduce中自己实现排序。 性能调优如果能够根据情况对shuffle过程进行调优，对于提供MapReduce性能很有帮助。相关的参数配置列在后面的表格中。 一个通用的原则是给shuffle过程分配尽可能大的内存，当然你需要确保map和reduce有足够的内存来运行业务逻辑。因此在实现Mapper和Reducer时，应该尽量减少内存的使用，例如避免在Map中不断地叠加。 运行map和reduce任务的JVM，内存通过mapred.child.java.opts属性来设置，尽可能设大内存。容器的内存大小通过mapreduce.map.memory.mb和mapreduce.reduce.memory.mb来设置，默认都是1024M。 map优化 在map端，避免写入多个spill文件可能达到最好的性能，一个spill文件是最好的。通过估计map的输出大小，设置合理的mapreduce.task.io.sort.*属性，使得spill文件数量最小。例如尽可能调大mapreduce.task.io.sort.mb。 map端相关的属性如下表： reduce优化 在reduce端，如果能够让所有数据都保存在内存中，可以达到最佳的性能。通常情况下，内存都保留给reduce函数，但是如果reduce函数对内存需求不是很高，将mapreduce.reduce.merge.inmem.threshold（触发合并的map输出文件数）设为0，mapreduce.reduce.input.buffer.percent（用于保存map输出文件的堆内存比例）设为1.0，可以达到很好的性能提升。在2008年的TB级别数据排序性能测试中，Hadoop就是通过将reduce的中间数据都保存在内存中胜利的。 reduce端相关属性： 通用优化Hadoop默认使用4KB作为缓冲，这个算是很小的，可以通过io.file.buffer.size来调高缓冲池大小。]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L897_IncreasingBST]]></title>
    <url>%2F2016%2F01%2F26%2FL897-IncreasingBST%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package LeetCode.topic.tree;/** * 给定一个树，按中序遍历重新排列树，使树中最左边的结点现在是树的根，并且每个结点没有左子结点，只有一个右子结点。 * * * * 示例 ： * * 输入：[5,3,6,2,4,null,8,1,null,null,null,7,9] * * 5 * / \ * 3 6 * / \ \ * 2 4 8 * / / \ * 1 7 9 * * 输出：[1,null,2,null,3,null,4,null,5,null,6,null,7,null,8,null,9] * * 1 * \ * 2 * \ * 3 * \ * 4 * \ * 5 * \ * 6 * \ * 7 * \ * 8 * \ * 9 * * * 提示： * * 给定树中的结点数介于 1 和 100 之间。 * 每个结点都有一个从 0 到 1000 范围内的唯一整数值。 */public class L897_IncreasingBST &#123; TreeNode dummy = new TreeNode(-1); TreeNode res = dummy; public TreeNode increasingBST(TreeNode root) &#123; helper(root); return res.right; &#125; private void helper(TreeNode root) &#123;// if (root == null) return; if (root.left != null) helper(root.left); dummy.right = new TreeNode(root.val); dummy = dummy.right; if (root.right != null) helper(root.right); &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L965_IsUnivalTree]]></title>
    <url>%2F2016%2F01%2F26%2FL965-IsUnivalTree%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425package LeetCode.topic.tree;public class L965_IsUnivalTree &#123; public boolean isUnivalTree(TreeNode root) &#123; if(root == null)&#123; return true; &#125; return helper(root,root.val); &#125; public boolean helper(TreeNode node,int val)&#123; if(node == null)return true; if(node.val != val)return false; if(node.left != null &amp;&amp; node.left.val != val) return false; if(node.right != null &amp;&amp; node.right.val != val) return false; return helper(node.left,val) &amp;&amp; helper(node.right,val); &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L700_SearchBST]]></title>
    <url>%2F2016%2F01%2F26%2FL700-SearchBST%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425package LeetCode.topic.tree;public class L700_SearchBST &#123; public TreeNode searchBST(TreeNode root, int val) &#123; if(root == null) return null; if(root.val == val) return root; TreeNode left = searchBST(root.left,val); TreeNode right = searchBST(root.right,val); if(left != null)&#123; return left; &#125; if(right != null)&#123; return right; &#125; return null; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L687_LongestUnivaluePath]]></title>
    <url>%2F2016%2F01%2F26%2FL687-LongestUnivaluePath%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041package LeetCode.topic.tree;/** * * 解题思路类似于124题, 对于任意一个节点, 如果最长同值路径包含该节点, 那么只可能是两种情况: * 1. 其左右子树中加上该节点后所构成的同值路径中较长的那个继续向父节点回溯构成最长同值路径 * 2. 左右子树加上该节点都在最长同值路径中, 构成了最终的最长同值路径 * 需要注意因为要求同值, 所以在判断左右子树能构成的同值路径时要加入当前节点的值作为判断依据 */public class L687_LongestUnivaluePath &#123; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125; private int max = 0; public int longestUnivaluePath(TreeNode root) &#123; if (root == null)return 0; helper(root,root.val); return max; &#125; private int helper(TreeNode node, int val) &#123; if (node == null)return 0; int left = helper(node.left,node.val); int right = helper(node.right,node.val); max = Math.max(max,left + right);// 路径长度为节点数减1所以此处不加1 if (node.val == val) return Math.max(left,right) + 1; return 0; &#125;// 下面是自己作物的思路// if (node.val == val)&#123;// return Math.max(helper(node.left,node.val,count+1),helper(node.right,node.val,count+1));// &#125; else &#123;// return Math.max(helper(node.left,node.val,count),helper(node.right,node.val,count));// &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L671_BSTFindSecondMinimumValue]]></title>
    <url>%2F2016%2F01%2F26%2FL671-BSTFindSecondMinimumValue%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package LeetCode.topic.tree;/** * 给定一个非空特殊的二叉树，每个节点都是正数，并且每个节点的子节点数量只能为 2 或 0。如果一个节点有两个子节点的话，那么这个节点的值不大于它的子节点的值。 * * 给出这样的一个二叉树，你需要输出所有节点中的第二小的值。如果第二小的值不存在的话，输出 -1 。 * * 示例 1: * * 输入: * 2 * / \ * 2 5 * / \ * 5 7 * * 输出: 5 * 说明: 最小的值是 2 ，第二小的值是 5 。 * 示例 2: * * 输入: * 2 * / \ * 2 2 * * 输出: -1 * 说明: 最小的值是 2, 但是不存在第二小的值。 */public class L671_BSTFindSecondMinimumValue &#123; int second = -1; public int findSecondMinimumValue(TreeNode root) &#123; if (root == null) return -1; helper(root,root.val); return second; &#125; private void helper(TreeNode root, int min) &#123; if (root == null)return; if (root.val == min)&#123; helper(root.left,min); helper(root.right,min); return; &#125; if (second == -1 || root.val &lt; second)&#123; second = root.val; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L654_ConstructMaximumBinaryTree]]></title>
    <url>%2F2016%2F01%2F26%2FL654-ConstructMaximumBinaryTree%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package LeetCode.topic.tree;/** * 给定一个不含重复元素的整数数组。一个以此数组构建的最大二叉树定义如下： * * 二叉树的根是数组中的最大元素。 * 左子树是通过数组中最大值左边部分构造出的最大二叉树。 * 右子树是通过数组中最大值右边部分构造出的最大二叉树。 * 通过给定的数组构建最大二叉树，并且输出这个树的根节点。 * * Example 1: * * 输入: [3,2,1,6,0,5] * 输入: 返回下面这棵树的根节点： * * 6 * / \ * 3 5 * \ / * 2 0 * \ * 1 * 注意: * * 给定的数组的大小在 [1, 1000] 之间。 */public class L654_ConstructMaximumBinaryTree &#123; public TreeNode constructMaximumBinaryTree(int[] nums) &#123; return buildTree(nums,0,nums.length -1); &#125; public TreeNode buildTree(int[] nums, int start, int end)&#123; if (start &gt; end)&#123; return null; &#125; int maxIndex = maxIndex(nums, start, end); TreeNode root = new TreeNode(nums[maxIndex]); root.left = buildTree(nums,start,maxIndex - 1); root.right = buildTree(nums,maxIndex + 1,end); return root; &#125; public int maxIndex(int[] arr,int start,int end)&#123; int maxIndex = start; for(int i = start ;i &lt;= end; i++)&#123; if (arr[i] &gt; arr[maxIndex])&#123; maxIndex = i; &#125; &#125; return maxIndex; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L572_IsSubtree]]></title>
    <url>%2F2016%2F01%2F26%2FL572-IsSubtree%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package LeetCode.topic.tree;/** * 给定两个非空二叉树 s 和 t，检验 s 中是否包含和 t 具有相同结构和节点值的子树。s 的一个子树包括 s 的一个节点和这个节点的所有子孙。s 也可以看做它自身的一棵子树。 * * 示例 1: * 给定的树 s: * * 3 * / \ * 4 5 * / \ * 1 2 * 给定的树 t： * * 4 * / \ * 1 2 * 返回 true，因为 t 与 s 的一个子树拥有相同的结构和节点值。 * * 示例 2: * 给定的树 s： * * 3 * / \ * 4 5 * / \ * 1 2 * / * 0 * 给定的树 t： * * 4 * / \ * 1 2 * 返回 false。 */public class L572_IsSubtree &#123; public boolean isSubtree(TreeNode s, TreeNode t) &#123; if (s == null &amp;&amp; t != null)return false; return isSame(s,t) || isSubtree(s.left,t) || isSubtree(s.right,t); &#125; private boolean isSame(TreeNode s, TreeNode t) &#123; if (s!= null &amp;&amp; t != null)&#123; return s.val == t.val &amp;&amp; isSame(s.left,t.left) &amp;&amp; isSame(s.right,t.right); &#125; else if (s == null &amp;&amp; t == null)&#123; return true; &#125; else &#123; return false; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L617_MergeTrees]]></title>
    <url>%2F2016%2F01%2F26%2FL617-MergeTrees%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243package LeetCode.topic.tree;/** * 给定两个二叉树，想象当你将它们中的一个覆盖到另一个上时，两个二叉树的一些节点便会重叠。 * * 你需要将他们合并为一个新的二叉树。合并的规则是如果两个节点重叠，那么将他们的值相加作为节点合并后的新值，否则不为 NULL 的节点将直接作为新二叉树的节点。 * * 示例 1: * * 输入: * Tree 1 Tree 2 * 1 2 * / \ / \ * 3 2 1 3 * / \ \ * 5 4 7 * 输出: * 合并后的树: * 3 * / \ * 4 5 * / \ \ * 5 4 7 * 注意: 合并必须从两个树的根节点开始。 */public class L617_MergeTrees &#123; public TreeNode mergeTrees(TreeNode t1, TreeNode t2) &#123; if (t1 == null) return t2; if (t2 == null) return t1; TreeNode cur = new TreeNode(t1.val + t2.val); cur.left = mergeTrees(t1.left,t2.left); cur.right = mergeTrees(t1.right,t2.right); return cur; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L543_DiameterOfBinaryTree]]></title>
    <url>%2F2016%2F01%2F26%2FL543-DiameterOfBinaryTree%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041package LeetCode.topic.tree;/** * 给定一棵二叉树，你需要计算它的直径长度。一棵二叉树的直径长度是任意两个结点路径长度中的最大值。这条路径可能穿过根结点。 * * 示例 : * 给定二叉树 * * 1 * / \ * 2 3 * / \ * 4 5 * 返回 3, 它的长度是路径 [4,2,1,3] 或者 [5,2,1,3]。 * * 注意：两结点之间的路径长度是以它们之间边的数目表示。 */public class L543_DiameterOfBinaryTree &#123; private int rst = 0; public int diameterOfBinaryTree(TreeNode root) &#123; if (root == null) return 0; helper(root); return rst; &#125; private int helper(TreeNode root) &#123; if (root == null) return 0; int left = helper(root.left); int right = helper(root.right); if (rst &lt; left + right) rst = left + right; return Math.max(left,right) + 1; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L530_BSTgetMinimumDifference]]></title>
    <url>%2F2016%2F01%2F26%2FL530-BSTgetMinimumDifference%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637package LeetCode.topic.tree;public class L530_BSTgetMinimumDifference &#123; /** * //二叉查找树中，中间节点的值一定是其左右节点值的中间数，因此最小差别一定是在中间节点与左右节点之间 * //中序遍历二叉查找树，每次比较当前节点与前一节点差值的绝对值与目前result中保存的最小值的大小，将较小的保存在result中 * * @param root * @return */ private int result = Integer.MAX_VALUE; private TreeNode preNode = null; public int getMinimumDifference(TreeNode root) &#123; getMin(root); return result; &#125; private void getMin(TreeNode root) &#123; if(root == null)&#123; return; &#125; getMin(root.left); if(preNode != null)//todo &#123; result = Math.min(Math.abs(root.val - preNode.val), result); &#125; preNode = root; getMin(root.right); &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L501_BSTFindMode]]></title>
    <url>%2F2016%2F01%2F26%2FL501-BSTFindMode%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package LeetCode.topic.tree;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;/** * 给定一个有相同值的二叉搜索树（BST），找出 BST 中的所有众数（出现频率最高的元素）。 * * 假定 BST 有如下定义： * * 结点左子树中所含结点的值小于等于当前结点的值 * 结点右子树中所含结点的值大于等于当前结点的值 * 左子树和右子树都是二叉搜索树 * 例如： * 给定 BST [1,null,2,2], * * 1 * \ * 2 * / * 2 * 返回[2]. * * 提示：如果众数超过1个，不需考虑输出顺序 * * 进阶：你可以不使用额外的空间吗？（假设由递归产生的隐式调用栈的开销不被计算在内） * * 使用中序遍历 * https://blog.csdn.net/qq_38959715/article/details/82682383 * */public class L501_BSTFindMode &#123; TreeNode pre = null;//todo must全局 Integer curTimes = 1,maxTimes = 0;//todo must全局 public int[] findMode(TreeNode root) &#123; if (root == null) return new int[]&#123;&#125;; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); inOrder(root,list); int[] result = new int[list.size()]; for(int i = 0;i&lt;list.size();i++)&#123; result[i] = list.get(i); &#125; return result; &#125; private void inOrder(TreeNode root, List&lt;Integer&gt; list) &#123; if (root == null) return; inOrder(root.left,list); if (pre != null)&#123; curTimes = (root.val == pre.val) ? curTimes + 1 : 1; &#125; if (curTimes == maxTimes)&#123; list.add(root.val); &#125; else if (curTimes &gt; maxTimes)&#123; list.clear(); list.add(root.val); maxTimes = curTimes; &#125; pre = root; inOrder(root.right,list); &#125; public static void main(String[] args) &#123; TreeNode r = new TreeNode(1); TreeNode t1 = new TreeNode(2); TreeNode t2 = new TreeNode(2); r.right = t1; t1.left = t2; L501_BSTFindMode t = new L501_BSTFindMode(); int[] mode = t.findMode(r); for (int x:mode) &#123; System.out.println(x); &#125; &#125; public static class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L437_TreePathSum]]></title>
    <url>%2F2016%2F01%2F26%2FL437-TreePathSum%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package LeetCode.topic.tree;import java.util.Stack;/** * 给定一个二叉树，它的每个结点都存放着一个整数值。 * * 找出路径和等于给定数值的路径总数。 * * 路径不需要从根节点开始，也不需要在叶子节点结束，但是路径方向必须是向下的（只能从父节点到子节点）。 * * 二叉树不超过1000个节点，且节点数值范围是 [-1000000,1000000] 的整数。 * * 示例： * * root = [10,5,-3,3,2,null,11,3,-2,null,1], sum = 8 * * 10 * / \ * 5 -3 * / \ \ * 3 2 11 * / \ \ * 3 -2 1 * * 返回 3。和等于 8 的路径有: * * 1. 5 -&gt; 3 * 2. 5 -&gt; 2 -&gt; 1 * 3. -3 -&gt; 11 */public class L437_TreePathSum &#123; public int pathSum(TreeNode root, int sum) &#123; int count = 0; if(root == null) return 0; Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;(); stack.add(root); while(!stack.isEmpty())&#123; TreeNode node = stack.pop(); count += helper(node, sum); if(node.right != null) stack.push(node.right); if(node.left != null) stack.push(node.left); &#125; return count; &#125; public int helper(TreeNode root, int sum) &#123; int count = 0; if(root == null) return 0; if(sum == root.val)&#123; count++; &#125; count += helper(root.left, sum-root.val); count += helper(root.right, sum-root.val); return count; &#125;// public int pathSum(TreeNode root, int sum) &#123;// if(root == null) return 0;// int res = helper(root,sum);// res += pathSum(root.left,sum);// res += pathSum(root.right,sum);// return res;// &#125;// public int helper(TreeNode node,int num)&#123;// if(node == null)&#123;// return 0;// &#125;// int res = 0;// if(node.val == num)&#123;// res++;// &#125;// res += helper(node.left,num - node.val);// res += helper(node.right,num - node.val);// return res;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L429_TreeLevelOrder]]></title>
    <url>%2F2016%2F01%2F26%2FL429-TreeLevelOrder%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142package LeetCode.topic.tree;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;public class L429_TreeLevelOrder &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; levelOrder(Node root) &#123; if(root == null)return new ArrayList&lt;&gt;(); LinkedList&lt;Node&gt; queue = new LinkedList&lt;Node&gt;(); queue.add(root); while(!queue.isEmpty())&#123; int count = queue.size(); List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); while(count &gt; 0)&#123; Node cur = queue.pop(); list.add(cur.val); if(cur.children != null)&#123; for(Node n : cur.children)&#123; queue.add(n); &#125; &#125; count--; &#125; res.add(list); &#125; return res; &#125; class Node &#123; public int val; public List&lt;Node&gt; children; public Node() &#123;&#125; public Node(int _val,List&lt;Node&gt; _children) &#123; val = _val; children = _children; &#125; &#125;;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L297_DeserializeSerializeTree]]></title>
    <url>%2F2016%2F01%2F26%2FL297-DeserializeSerializeTree%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package LeetCode.topic.tree;import java.util.ArrayDeque;import java.util.Arrays;import java.util.Queue;/** * 序列化是将一个数据结构或者对象转换为连续的比特位的操作，进而可以将转换后的数据存储在一个文件或者内存中，同时也可以通过网络传输到另一个计算机环境，采取相反方式重构得到原数据。 * * 请设计一个算法来实现二叉树的序列化与反序列化。这里不限定你的序列 / 反序列化算法执行逻辑，你只需要保证一个二叉树可以被序列化为一个字符串并且将这个字符串反序列化为原始的树结构。 * * 示例: * * 你可以将以下二叉树： * * 1 * / \ * 2 3 * / \ * 4 5 * * 序列化为 &quot;[1,2,3,null,null,4,5]&quot; * 提示: 这与 LeetCode 目前使用的方式一致，详情请参阅 LeetCode 序列化二叉树的格式。你并非必须采取这种方式，你也可以采用其他的方法解决这个问题。 * * 说明: 不要使用类的成员 / 全局 / 静态变量来存储状态，你的序列化和反序列化算法应该是无状态的。 */public class L297_DeserializeSerializeTree &#123; // Encodes a tree to a single string. public String serialize(TreeNode root) &#123; if (root == null) &#123; return &quot;$,&quot;; &#125; return root.val + &quot;,&quot; + serialize(root.left) + serialize(root.right); &#125; // Decodes your encoded data to tree. public TreeNode deserialize(String data) &#123; String[] strings = data.split(&quot;,&quot;); Queue&lt;String&gt; queue = new ArrayDeque&lt;&gt;(Arrays.asList(strings)); return func(queue); &#125; private TreeNode func(Queue&lt;String&gt; strings) &#123; String string = strings.remove(); if (&quot;$&quot;.equals(string)) &#123; return null; &#125; TreeNode newNode = new TreeNode(Integer.parseInt(string)); newNode.left = func(strings); newNode.right = func(strings); return newNode; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L404_TreeSumOfLeftLeaves]]></title>
    <url>%2F2016%2F01%2F26%2FL404-TreeSumOfLeftLeaves%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940package LeetCode.topic.tree;public class L404_TreeSumOfLeftLeaves &#123; /** * 本质还是二叉树的遍历 * * 1. 求所有左叶子节点的值的和，那就是求当前节点的左节点和右节点的所有左叶子节点的和 * * 2. 不是左叶子节点返回值为0，是左叶子节点放回叶子的值 * * 3. 递归求和即可 * * @param root * @return */ public int sumOfLeftLeaves(TreeNode root) &#123; return dfs(root, false); &#125; public int dfs(TreeNode node, Boolean isLeft) &#123; if (node == null) return 0; if (node.left == null &amp;&amp; node.right == null) &#123; if (isLeft == true) return node.val; else return 0; &#125; int leftVal = dfs(node.left, true); int rightVal = dfs(node.right, false); return leftVal + rightVal; &#125; class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L257_BinaryTreePaths]]></title>
    <url>%2F2016%2F01%2F26%2FL257-BinaryTreePaths%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;/** * 给定一个二叉树，返回所有从根节点到叶子节点的路径。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例: * * 输入: * * 1 * / \ * 2 3 * \ * 5 * * 输出: [&quot;1-&gt;2-&gt;5&quot;, &quot;1-&gt;3&quot;] * * 解释: 所有根节点到叶子节点的路径为: 1-&gt;2-&gt;5, 1-&gt;3 */public class L257_BinaryTreePaths &#123; List&lt;String&gt; res = new ArrayList&lt;&gt;(); public List&lt;String&gt; binaryTreePaths(TreeNode root) &#123; if (root == null)return new ArrayList&lt;&gt;(); ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); helper(root,list); return res; &#125; private void helper(TreeNode root, ArrayList&lt;Integer&gt; list) &#123; if (root == null) return; if (root.left == null &amp;&amp; root.right == null)&#123; list.add(root.val); String s = &quot;&quot;; for (Integer node:list) &#123; if (s.equals(&quot;&quot;))&#123; s += node+&quot;&quot;; &#125; else &#123; s = s + &quot;-&gt;&quot;+ node; &#125; &#125; res.add(s); list.remove(list.size() - 1); return; &#125; list.add(root.val); helper(root.right,list); helper(root.left,list); list.remove(list.size() - 1); &#125;// List&lt;String&gt; res = new ArrayList&lt;&gt;();// public List&lt;String&gt; binaryTreePaths(TreeNode root) &#123;// if (root == null)return new ArrayList&lt;&gt;();// ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;();// list.add(root.val);// helper(root,list);// return res;// &#125;//// private void helper(TreeNode root, ArrayList&lt;Integer&gt; list) &#123;// if (root.left == null &amp;&amp; root.right == null)&#123;// String s = &quot;&quot;;// for (Integer node:list) &#123;// if (s.equals(&quot;&quot;))&#123;// s += node+&quot;&quot;;// &#125; else &#123;// s = s + &quot;-&gt;&quot;+ node;// &#125;// &#125;// res.add(s);// &#125; else if (root.left == null)&#123;// list.add(root.right.val);// helper(root.right,list);// &#125; else if (root.right == null)&#123;// list.add(root.left.val);// helper(root.left,list);// &#125; else &#123;// ArrayList&lt;Integer&gt; mycopy=new ArrayList&lt;Integer&gt;();// mycopy=(ArrayList&lt;Integer&gt;) list.clone();// list.add(root.right.val);// helper(root.right,list);// mycopy.add(root.left.val);// helper(root.left,mycopy);// &#125;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L236_TreeLowestCommonAncestor]]></title>
    <url>%2F2016%2F01%2F26%2FL236-TreeLowestCommonAncestor%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package LeetCode.topic.tree;/** * 给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。 * * 百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。” * * 例如，给定如下二叉树: root = [3,5,1,6,2,0,8,null,null,7,4] * * * * * * 示例 1: * * 输入: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 1 * 输出: 3 * 解释: 节点 5 和节点 1 的最近公共祖先是节点 3。 * 示例 2: * * 输入: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 4 * 输出: 5 * 解释: 节点 5 和节点 4 的最近公共祖先是节点 5。因为根据定义最近公共祖先节点可以为节点本身。 * * * 说明: * * 所有节点的值都是唯一的。 * p、q 为不同节点且均存在于给定的二叉树中。 * * LCA 问题，查阅相关资料 */public class L236_TreeLowestCommonAncestor &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if (root == null) return null; if (root == q || root == p)&#123; return root;//找到待查询的值，一层一层向外传递 &#125; TreeNode left = lowestCommonAncestor(root.left,p,q); TreeNode right = lowestCommonAncestor(root.right,p,q); if (left != null &amp;&amp; right != null)&#123; return root; &#125; else if (left != null)&#123; return left; &#125; else if (right != null)&#123; return right; &#125; return null; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L235_BSTLowestCommonAncestor]]></title>
    <url>%2F2016%2F01%2F26%2FL235-BSTLowestCommonAncestor%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package LeetCode.topic.tree;/** * 给定一个二叉搜索树, 找到该树中两个指定节点的最近公共祖先。 * * 百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。” * * 例如，给定如下二叉搜索树: root = [6,2,8,0,4,7,9,null,null,3,5] * * * * * * 示例 1: * * 输入: root = [6,2,8,0,4,7,9,null,null,3,5], p = 2, q = 8 * 输出: 6 * 解释: 节点 2 和节点 8 的最近公共祖先是 6。 * 示例 2: * * 输入: root = [6,2,8,0,4,7,9,null,null,3,5], p = 2, q = 4 * 输出: 2 * 解释: 节点 2 和节点 4 的最近公共祖先是 2, 因为根据定义最近公共祖先节点可以为节点本身。 * * * 说明: * * 所有节点的值都是唯一的。 * p、q 为不同节点且均存在于给定的二叉搜索树中。 * * * //todo 可以利用二叉搜索树的特性 */public class L235_BSTLowestCommonAncestor &#123; TreeNode res = null; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if (root == null) return null; helper(root,p,q); return res; &#125; private void helper(TreeNode root, TreeNode p, TreeNode q) &#123; if(root == null)return; if ((p.val - root.val)*(q.val - root.val) &lt;= 0)&#123;//一定在最近公共祖先的两侧 res = root; &#125; else if (p.val &gt; root.val &amp;&amp; q.val &gt; root.val)&#123; helper(root.right,q,p); &#125; else &#123; helper(root.left,p,q); &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L230_TreeKthSmallest]]></title>
    <url>%2F2016%2F01%2F26%2FL230-TreeKthSmallest%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package LeetCode.topic.tree;import java.util.LinkedList;import java.util.Stack;/** * 给定一个二叉搜索树，编写一个函数 kthSmallest 来查找其中第 k 个最小的元素。 * * 说明： * 你可以假设 k 总是有效的，1 ≤ k ≤ 二叉搜索树元素个数。 * * 示例 1: * * 输入: root = [3,1,4,null,2], k = 1 * 3 * / \ * 1 4 * \ * 2 * 输出: 1 * 示例 2: * * 输入: root = [5,3,6,2,4,null,null,1], k = 3 * 5 * / \ * 3 6 * / \ * 2 4 * / * 1 * 输出: 3 * * 中序遍历，是升序的 */public class L230_TreeKthSmallest &#123; public int kthSmallest(TreeNode root, int k) &#123; Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); int i = 0; TreeNode cur = root; while (true)&#123;//重要 while (cur != null)&#123; stack.push(cur); cur = cur.left; &#125; if (stack.isEmpty())&#123; break; &#125; TreeNode node = stack.pop(); if (i == k - 1)&#123; return node.val; &#125; i++; cur = node.right; &#125; return -1; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L226_InvertTree]]></title>
    <url>%2F2016%2F01%2F26%2FL226-InvertTree%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package LeetCode.topic.tree;/** * 翻转一棵二叉树。 * * 示例： * * 输入： * * 4 * / \ * 2 7 * / \ / \ * 1 3 6 9 * 输出： * * 4 * / \ * 7 2 * / \ / \ * 9 6 3 1 * 备注: * 这个问题是受到 Max Howell 的 原问题 启发的 ： * * 谷歌：我们90％的工程师使用您编写的软件(Homebrew)，但是您却无法在面试时在白板上写出翻转二叉树这道题，这太糟糕了。 */public class L226_InvertTree &#123; public TreeNode invertTree(TreeNode root) &#123; if (root == null)return null; helper(root); return root; &#125; private void helper(TreeNode root) &#123; if (root == null)return; // todo 必须在首位 helper(root.left); helper(root.right); //下面的代码，可在最前，也可以在最后 TreeNode tmp = root.left; root.left = root.right; root.right = tmp; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L222_TreeCountNodes]]></title>
    <url>%2F2016%2F01%2F26%2FL222-TreeCountNodes%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package LeetCode.topic.tree;/** * 给出一个完全二叉树，求出该树的节点个数。 * * 说明： * * 完全二叉树的定义如下：在完全二叉树中，除了最底层节点可能没填满外，其余每层节点数都达到最大值，并且最下面一层的节点都集中在该层最左边的若干位置。若最底层为第 h 层，则该层包含 1~ 2h 个节点。 * * 示例: * * 输入: * 1 * / \ * 2 3 * / \ / * 4 5 6 * * 输出: 6 */public class L222_TreeCountNodes &#123; public int countNodes(TreeNode root) &#123; if(root == null) return 0; return countNodes(root.left) + countNodes(root.right) + 1; &#125;// public int countNodes(TreeNode root) &#123;// /**// 完全二叉树的高度可以直接通过不断地访问左子树就可以获取// 判断左右子树的高度:// 如果相等说明左子树是满二叉树, 然后进一步判断右子树的节点数(最后一层最后出现的节点必然在右子树中)// 如果不等说明右子树是深度小于左子树的满二叉树, 然后进一步判断左子树的节点数(最后一层最后出现的节点必然在左子树中)// **/// if (root==null) return 0;// int ld = getDepth(root.left);// int rd = getDepth(root.right);// if(ld == rd)// return (1 &lt;&lt; ld) + countNodes(root.right); // 1(根节点) + (1 &lt;&lt; ld)-1(左完全左子树节点数) + 右子树节点数量// else// return (1 &lt;&lt; rd) + countNodes(root.left); // 1(根节点) + (1 &lt;&lt; rd)-1(右完全右子树节点数) + 左子树节点数量//// &#125;//// private int getDepth(TreeNode r) &#123;// int depth = 0;// while(r != null) &#123;// depth++;// r = r.left;// &#125;// return depth;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L199_TreeRightSideView]]></title>
    <url>%2F2016%2F01%2F26%2FL199-TreeRightSideView%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package LeetCode.topic.tree;import java.util.ArrayDeque;import java.util.ArrayList;import java.util.List;/** * 给定一棵二叉树，想象自己站在它的右侧，按照从顶部到底部的顺序，返回从右侧所能看到的节点值。 * * 示例: * * 输入: [1,2,3,null,5,null,4] * 输出: [1, 3, 4] * 解释: * * 1 &lt;--- * / \ * 2 3 &lt;--- * \ \ * 5 4 &lt;--- */public class L199_TreeRightSideView &#123; public List&lt;Integer&gt; rightSideView(TreeNode root) &#123; List&lt;Integer&gt; res = new ArrayList&lt;Integer&gt;(); if(root == null) return res; ArrayDeque&lt;TreeNode&gt; queue = new ArrayDeque&lt;TreeNode&gt;(); queue.add(root); while(!queue.isEmpty())&#123; int cont = queue.size(); TreeNode node = queue.peek();//todo 队列是先进先出的，因为下面优先遍历的是右子树 res.add(node.val); while(cont &gt; 0)&#123; node = queue.poll(); if(node.right != null) queue.add(node.right);//todo here if(node.left != null) queue.add(node.left); cont--; &#125; &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L129_TreeSumNumbers]]></title>
    <url>%2F2016%2F01%2F26%2FL129-TreeSumNumbers%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109package LeetCode.topic.tree;import java.util.ArrayList;import java.util.Arrays;import java.util.List;/** * 给定一个二叉树，它的每个结点都存放一个 0-9 的数字，每条从根到叶子节点的路径都代表一个数字。 * * 例如，从根到叶子节点路径 1-&gt;2-&gt;3 代表数字 123。 * * 计算从根到叶子节点生成的所有数字之和。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例 1: * * 输入: [1,2,3] * 1 * / \ * 2 3 * 输出: 25 * 解释: * 从根到叶子节点路径 1-&gt;2 代表数字 12. * 从根到叶子节点路径 1-&gt;3 代表数字 13. * 因此，数字总和 = 12 + 13 = 25. * 示例 2: * * 输入: [4,9,0,5,1] * 4 * / \ * 9 0 * / \ * 5 1 * 输出: 1026 * 解释: * 从根到叶子节点路径 4-&gt;9-&gt;5 代表数字 495. * 从根到叶子节点路径 4-&gt;9-&gt;1 代表数字 491. * 从根到叶子节点路径 4-&gt;0 代表数字 40. * 因此，数字总和 = 495 + 491 + 40 = 1026. */public class L129_TreeSumNumbers &#123; private int total; public int sumNumbers(TreeNode root) &#123; if (root == null) return 0; ArrayList&lt;Integer&gt; sum = new ArrayList&lt;&gt;(); helper(root,sum); return total; &#125; private void helper(TreeNode cur,ArrayList&lt;Integer&gt; sum) &#123; if (cur == null) return; if (cur.left == null &amp;&amp; cur.right == null)&#123; sum.add(0,cur.val);//todo int s = 0; for (int i = sum.size() -1 ;i&gt;=0;i--)&#123; s += (int) (Math.pow(10,Double.parseDouble(i+&quot;&quot;))*sum.get(i).intValue()); &#125; total += s; sum.remove(0);//todo return; &#125; sum.add(0,cur.val); helper(cur.left,sum); helper(cur.right,sum); sum.remove(0);//todo &#125;// private int total;// public int sumNumbers(TreeNode root) &#123;// if (root == null) return 0;// ArrayList&lt;Integer&gt; sum = new ArrayList&lt;&gt;();// sum.add(root.val);// helper(root,sum);// return total;// &#125;//// private void helper(TreeNode cur,ArrayList&lt;Integer&gt; sum) &#123;// if (cur.left == null &amp;&amp; cur.right == null)&#123;// int s = 0;// for (int i = sum.size() -1 ;i&gt;=0;i--)&#123;// s += (int) (Math.pow(10,Double.parseDouble(i+&quot;&quot;))*sum.get(i).intValue());// &#125;// total += s;// &#125; else if (cur.left == null)&#123;// sum.add(0,cur.right.val);// helper(cur.right,sum);// &#125; else if (cur.right == null)&#123;// sum.add(0,cur.left.val);// helper(cur.left,sum);// &#125; else &#123;// ArrayList&lt;Integer&gt; mycopy=new ArrayList&lt;Integer&gt;();// mycopy=(ArrayList&lt;Integer&gt;) sum.clone();// sum.add(0,cur.right.val);// helper(cur.right,sum);// mycopy.add(0,cur.left.val);// helper(cur.left,mycopy);// &#125;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L144_PreorderTraversal]]></title>
    <url>%2F2016%2F01%2F26%2FL144-PreorderTraversal%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;import java.util.Stack;/** * 给定一个二叉树，返回它的 前序 遍历。 * * 示例: * * 输入: [1,null,2,3] * 1 * \ * 2 * / * 3 * * 输出: [1,2,3] * 进阶: 递归算法很简单，你可以通过迭代算法完成吗？ */public class L144_PreorderTraversal &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); stack.push(root); while (!stack.isEmpty())&#123; TreeNode cur = stack.pop(); res.add(cur.val); if (cur.right != null)&#123; stack.push(cur.right); &#125; if (cur.left != null)&#123; stack.push(cur.left); &#125; &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L124_TreeMaxPathSum]]></title>
    <url>%2F2016%2F01%2F26%2FL124-TreeMaxPathSum%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package LeetCode.topic.tree;/** * 给定一个非空二叉树，返回其最大路径和。 * * 本题中，路径被定义为一条从树中任意节点出发，达到任意节点的序列。该路径至少包含一个节点，且不一定经过根节点。 * * 示例 1: * * 输入: [1,2,3] * * 1 * / \ * 2 3 * * 输出: 6 * 示例 2: * * 输入: [-10,9,20,null,null,15,7] * * -10 * / \ * 9 20 * / \ * 15 7 * * 输出: 42 */public class L124_TreeMaxPathSum &#123; private int ret = Integer.MIN_VALUE; public int maxPathSum(TreeNode root) &#123; /** 对于任意一个节点, 如果最大和路径包含该节点, 那么只可能是两种情况: 1. 其左右子树中所构成的和路径值较大的那个加上该节点的值后向父节点回溯构成最大路径 2. 左右子树都在最大路径中, 加上该节点的值构成了最终的最大路径 **/ getMax(root); return ret; &#125; private int getMax(TreeNode r) &#123; if(r == null) return 0; int left = Math.max(0, getMax(r.left)); // 如果子树路径和为负则应当置0表示最大路径不包含子树 int right = Math.max(0, getMax(r.right)); ret = Math.max(ret, r.val + left + right); // 判断在该节点包含左右子树的路径和是否大于当前最大路径和 return Math.max(left, right) + r.val; &#125;// private int res = 0;// public int maxPathSum(TreeNode root) &#123;// res = root.val;// helper(root);// return res;// &#125;//// private int helper(TreeNode cur) &#123;// int sum;// if (cur.left != null &amp;&amp; cur.right != null)&#123;// sum = cur.val;// &#125; else if (cur.left == null)&#123;// int right = helper(cur.right);// sum = right &gt;0 ? right + cur.val : cur.val;// &#125; else if (cur.right == null)&#123;// int left = helper(cur.left);// sum = left &gt; 0 ? left + cur.val : cur.val;// &#125; else &#123;// int left = helper(cur.left);// int right = helper(cur.right);// res = Math.max(res,left + right + cur.val);//走当前节点//// int max = Math.max(left,right);//不走当前节点// sum = max &gt; 0 ? max + cur.val : cur.val;// &#125;// res = Math.max(res,sum);// return res;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L116_TreeConnectOne]]></title>
    <url>%2F2016%2F01%2F26%2FL116-TreeConnectOne%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package LeetCode.topic.tree;import java.util.LinkedList;/** * 给定一个二叉树 * * struct TreeLinkNode &#123; * TreeLinkNode *left; * TreeLinkNode *right; * TreeLinkNode *next; * &#125; * 填充它的每个 next 指针，让这个指针指向其下一个右侧节点。如果找不到下一个右侧节点，则将 next 指针设置为 NULL。 * * 初始状态下，所有 next 指针都被设置为 NULL。 * * 说明: * * 你只能使用额外常数空间。 * 使用递归解题也符合要求，本题中递归程序占用的栈空间不算做额外的空间复杂度。 * 你可以假设它是一个完美二叉树（即所有叶子节点都在同一层，每个父节点都有两个子节点）。 * 示例: * * 给定完美二叉树， * * 1 * / \ * 2 3 * / \ / \ * 4 5 6 7 * 调用你的函数后，该完美二叉树变为： * * 1 -&gt; NULL * / \ * 2 -&gt; 3 -&gt; NULL * / \ / \ * 4-&gt;5-&gt;6-&gt;7 -&gt; NULL */public class L116_TreeConnectOne &#123; public class TreeLinkNode &#123; int val; TreeLinkNode left, right, next; TreeLinkNode(int x) &#123; val = x; &#125; &#125; public void connect(TreeLinkNode root) &#123; if (root == null) return; LinkedList&lt;TreeLinkNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while (!queue.isEmpty())&#123; int count = queue.size(); TreeLinkNode last = null; while (count &gt; 0)&#123; TreeLinkNode cur = queue.pop(); if (cur.left != null)&#123; queue.add(cur.left); if (last == null)&#123; last = cur.left; &#125; else &#123; last.next = cur.left; last = last.next;//传递 &#125; &#125; if (cur.right != null)&#123; queue.add(cur.right); if (last == null)&#123; last = cur.right; &#125; else &#123; last.next = cur.right; last = last.next;//传递 &#125; &#125; count--; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L114_BstFlattenList]]></title>
    <url>%2F2016%2F01%2F26%2FL114-BstFlattenList%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package LeetCode.topic.tree;/** * 给定一个二叉树，**原地**将它展开为链表。 * * 例如，给定二叉树 * * 1 * / \ * 2 5 * / \ \ * 3 4 6 * 将其展开为： * * 1 * \ * 2 * \ * 3 * \ * 4 * \ * 5 * \ * 6 * 用了递归的思路，把左子树作为右子树，并把 原右子树(temp) 拼接在 现右子树 的最右端 * * * * 1 * / \ * 2 5 * \ \ * (3) 6 * \ * 4 * * * * 1 * \ * (2,3,4) * \ * 5 * \ * 6 */public class L114_BstFlattenList &#123; public void flatten(TreeNode root) &#123; if (root == null) return; flatten(root.left); flatten(root.right); if (root.left != null)&#123;//注意，一定要判断左子树是否为空 TreeNode right = root.right;//记录右节点 root.right = root.left; root.left = null;//将左节点置空 TreeNode cur = root.right;//到左节点的最后一个节点 while (cur.right != null)&#123;//把左子树作为右子树 cur = cur.right; &#125; cur.right = right; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L115_PostorderTraversal]]></title>
    <url>%2F2016%2F01%2F26%2FL115-PostorderTraversal%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;import java.util.Stack;/** * 给定一个二叉树，返回它的 后序 遍历。 * * 示例: * * 输入: [1,null,2,3] * 1 * \ * 2 * / * 3 * * 输出: [3,2,1] * 进阶: 递归算法很简单，你可以通过迭代算法完成吗？ */public class L115_PostorderTraversal &#123; public List&lt;Integer&gt; postorderTraversal(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); Stack&lt;TreeNode&gt; outputStack = new Stack&lt;&gt;(); TreeNode cur = root; stack.push(root); while (!stack.isEmpty())&#123; cur = stack.pop(); outputStack.push(cur); if (cur.left != null)&#123; stack.push(cur.left); &#125; if (cur.right != null)&#123; stack.push(cur.right); &#125; &#125; List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); while (!outputStack.isEmpty())&#123; res.add(outputStack.pop().val); &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L113_PathSumTree]]></title>
    <url>%2F2016%2F01%2F26%2FL113-PathSumTree%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package LeetCode.topic.tree;import java.util.ArrayList;import java.util.Arrays;import java.util.List;/** * 给定一个二叉树和一个目标和，找到所有从根节点到叶子节点路径总和等于给定目标和的路径。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例: * 给定如下二叉树，以及目标和 sum = 22， * * 5 * / \ * 4 8 * / / \ * 11 13 4 * / \ / \ * 7 2 5 1 * 返回: * * [ * [5,4,11,2], * [5,8,4,5] * ] */public class L113_PathSumTree &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; pathSum(TreeNode root, int sum) &#123; if (root == null) return new ArrayList&lt;&gt;(); helper(root,sum,new ArrayList&lt;Integer&gt;()); return res; &#125; private void helper(TreeNode root, int sum, ArrayList&lt;Integer&gt; list) &#123; if (root == null) return; if (root.left == null &amp;&amp; root.right == null &amp;&amp; sum - root.val == 0)&#123; list.add(root.val);//todo 一定要添加 res.add(new ArrayList&lt;&gt;(list));//todo 重点，此处一定要copy一个list list.remove(list.size() - 1);//todo return; &#125; list.add(root.val); helper(root.left,sum - root.val,list); helper(root.right,sum - root.val,list); list.remove(list.size() - 1);//todo &#125;// List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;();// public List&lt;List&lt;Integer&gt;&gt; pathSum(TreeNode root, int sum) &#123;// if (root == null) return new ArrayList&lt;&gt;();// helper(root,sum,0,new ArrayList&lt;Integer&gt;());// return res;// &#125;//// private boolean helper(TreeNode root, int sum, int tempSum, ArrayList&lt;Integer&gt; list) &#123;// if (root.left == null &amp;&amp; root.right == null)&#123;// if (tempSum + root.val == sum)&#123;// list.add(root.val);// res.add(list);// return true;// &#125;// return false;// &#125; else &#123;// tempSum += root.val;// list.add(root.val);// if (root.left == null)&#123;// return helper(root.right,sum,tempSum,list);// &#125; else if (root.right == null)&#123;// return helper(root.left,sum,tempSum,list);// &#125; else &#123;// ArrayList&lt;Integer&gt; mycopy=new ArrayList&lt;Integer&gt;();// mycopy=(ArrayList&lt;Integer&gt;) list.clone();// helper(root.left,sum,tempSum,list);// helper(root.right,sum,tempSum,mycopy);// &#125;// &#125;// return false;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L112_HasPathSumTree]]></title>
    <url>%2F2016%2F01%2F26%2FL112-HasPathSumTree%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package LeetCode.topic.tree;/** * 给定一个二叉树和一个目标和，判断该树中是否存在根节点到叶子节点的路径，这条路径上所有节点值相加等于目标和。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例: * 给定如下二叉树，以及目标和 sum = 22， * * 5 * / \ * 4 8 * / / \ * 11 13 4 * / \ \ * 7 2 1 * 返回 true, 因为存在目标和为 22 的根节点到叶子节点的路径 5-&gt;4-&gt;11-&gt;2。 */public class L112_HasPathSumTree &#123; public boolean hasPathSum(TreeNode root, int sum) &#123; if (root == null) return false; return helper(root,sum,0); &#125; private boolean helper(TreeNode node, int sum, int tempSum) &#123; if (node.left == null &amp;&amp; node.right == null)&#123; return tempSum + node.val == sum; &#125; else &#123; tempSum += node.val; if (node.left == null)&#123; return helper(node.right,sum,tempSum); &#125; else if (node.right == null)&#123; return helper(node.left,sum,tempSum); &#125; else &#123; return helper(node.right,sum,tempSum) || helper(node.left,sum,tempSum); &#125; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L110_IsBalancedTree]]></title>
    <url>%2F2016%2F01%2F26%2FL110-IsBalancedTree%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package LeetCode.topic.tree;/** * 给定一个二叉树，判断它是否是高度平衡的二叉树。 * * 本题中，一棵高度平衡二叉树定义为： * * 一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过1。 * * 示例 1: * * 给定二叉树 [3,9,20,null,null,15,7] * * 3 * / \ * 9 20 * / \ * 15 7 * 返回 true 。 * * 示例 2: * * 给定二叉树 [1,2,2,3,3,null,null,4,4] * * 1 * / \ * 2 2 * / \ * 3 3 * / \ * 4 4 * 返回 false 。 */public class L110_IsBalancedTree &#123; public boolean isBalanced(TreeNode root) &#123; if (root == null)return true; int left = getDepth(root.left); int right = getDepth(root.right); if (Math.abs(left - right) &gt; 1)&#123; return false; &#125; return isBalanced(root.left) &amp;&amp; isBalanced(root.right); &#125; private int getDepth(TreeNode node) &#123; if (node == null)return 0; return Math.max(1 + getDepth(node.right),1 + getDepth(node.left)); &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L111_MinDepthTree]]></title>
    <url>%2F2016%2F01%2F26%2FL111-MinDepthTree%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738package LeetCode.topic.tree;/** * 给定一个二叉树，找出其最小深度。 * * 最小深度是从根节点到最近叶子节点的最短路径上的节点数量。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例: * * 给定二叉树 [3,9,20,null,null,15,7], * * 3 * / \ * 9 20 * / \ * 15 7 * 返回它的最小深度 2. */public class L111_MinDepthTree &#123;// 错误的写法；如果根节点的一侧为空，另一侧不为空；此时求的是不为空的一侧的最小深度 public int minDepth(TreeNode root) &#123; if (root == null) return 0; if (root.left == null) return 1 + minDepth(root.right); if (root.right == null) return 1 + minDepth(root.left); return Math.min(minDepth(root.left), minDepth(root.right)) + 1; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L108_SortedArrayToBST]]></title>
    <url>%2F2016%2F01%2F26%2FL108-SortedArrayToBST%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445package LeetCode.topic.tree;/** * 将一个按照升序排列的有序数组，转换为一棵高度平衡二叉搜索树。 * * 本题中，一个高度平衡二叉树是指一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过 1。 * * 示例: * * 给定有序数组: [-10,-3,0,5,9], * * 一个可能的答案是：[0,-3,9,-10,null,5]，它可以表示下面这个高度平衡二叉搜索树： * * 0 * / \ * -3 9 * / / * -10 5 * * 左右等分建立左右子树，中间节点作为子树根节点，递归该过程 */public class L108_SortedArrayToBST &#123; public TreeNode sortedArrayToBST(int[] nums) &#123; if (nums == null || nums.length == 0)return null; return helper(nums,0,nums.length - 1); &#125; private TreeNode helper(int[] nums, int low, int high) &#123; if (low &gt; high) return null; if (low == high) return new TreeNode(nums[low]);//low是序号，nums[low]才是其值 int mid = (low + high)&gt;&gt;1; TreeNode cur = new TreeNode(nums[mid]);//mid是序号，nums[low]才是其值 cur.left = helper(nums,low,mid-1); cur.right = helper(nums,mid+1,high); return cur; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L107_levelOrderBottomTraversal]]></title>
    <url>%2F2016%2F01%2F26%2FL107-levelOrderBottomTraversal%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package LeetCode.topic.tree;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;/** * 给定一个二叉树，返回其节点值自底向上的层次遍历。 （即按从叶子节点所在层到根节点所在的层，逐层从左向右遍历） * * 例如： * 给定二叉树 [3,9,20,null,null,15,7], * * 3 * / \ * 9 20 * / \ * 15 7 * 返回其自底向上的层次遍历为： * * [ * [15,7], * [9,20], * [3] * ] */public class L107_levelOrderBottomTraversal &#123; public List&lt;List&lt;Integer&gt;&gt; levelOrderBottom(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); LinkedList&lt;TreeNode&gt; quene = new LinkedList&lt;&gt;(); quene.add(root); while (!quene.isEmpty())&#123; int count = quene.size(); List&lt;Integer&gt; innerList = new ArrayList&lt;&gt;(); while (count &gt; 0)&#123; TreeNode cur = quene.pop(); innerList.add(cur.val); if (cur.left != null)&#123; quene.add(cur.left); &#125; if (cur.right != null)&#123; quene.add(cur.right); &#125; count--; &#125; res.add(0,innerList); &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L106_BuildTreeByMiddlePost]]></title>
    <url>%2F2016%2F01%2F26%2FL106-BuildTreeByMiddlePost%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;/** * 根据一棵树的中序遍历与后序遍历构造二叉树。 * * 注意: * 你可以假设树中没有重复的元素。 * * 例如，给出 * * 中序遍历 inorder = [9,3,15,20,7] * 后序遍历 postorder = [9,15,7,20,3] * 返回如下的二叉树： * * 3 * / \ * 9 20 * / \ * 15 7 * * * 后序遍历最后一个是根 */public class L106_BuildTreeByMiddlePost &#123; public TreeNode buildTree(int[] inorder, int[] postorder) &#123; TreeNode root = null; if (inorder.length != 0 &amp;&amp; postorder.length != 0)&#123; List&lt;Integer&gt; inorderList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; inorder.length; i++) &#123; inorderList.add(inorder[i]); &#125; List&lt;Integer&gt; postorderList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; postorder.length; i++) &#123; postorderList.add(postorder[i]); &#125; return helper(inorderList,postorderList); &#125; return root; &#125; private TreeNode helper(List&lt;Integer&gt; inorderList, List&lt;Integer&gt; postorderList) &#123; TreeNode root = null; List&lt;Integer&gt; leftInorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; rightInorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; leftPostorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; rightPostorderList = new ArrayList&lt;&gt;(); if (inorderList.size() != 0 &amp;&amp; postorderList.size() != 0)&#123; root = new TreeNode(postorderList.get(postorderList.size() - 1)); int rootInorderPos = inorderList.indexOf(root.val); leftInorderList = inorderList.subList(0,rootInorderPos); rightInorderList = inorderList.subList(rootInorderPos+1,inorderList.size()); int leftInorderSize =leftInorderList.size(); leftPostorderList = postorderList.subList(0,leftInorderSize); rightPostorderList = postorderList.subList(leftInorderSize,postorderList.size() - 1); root.left = helper(leftInorderList,leftPostorderList); root.right = helper(rightInorderList,rightPostorderList); &#125; return root; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L105_BuildTreeByPreMiddle]]></title>
    <url>%2F2016%2F01%2F26%2FL105-BuildTreeByPreMiddle%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package LeetCode.topic.tree;import java.util.ArrayList;import java.util.Arrays;import java.util.List;/** * 根据一棵树的前序遍历与中序遍历构造二叉树。 * * 注意: * 你可以假设树中没有重复的元素。 * * 例如，给出 * * 前序遍历 preorder = [3,9,20,15,7] * 中序遍历 inorder = [9,3,15,20,7] * 返回如下的二叉树： * * 3 * / \ * 9 20 * / \ * 15 7 */public class L105_BuildTreeByPreMiddle &#123; public TreeNode buildTree(int[] preorder, int[] inorder) &#123; TreeNode root = null; if (preorder.length != 0 &amp;&amp; inorder.length != 0)&#123; //todo 先转化为list处理 List&lt;Integer&gt; preorderList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; preorder.length; i++) &#123; preorderList.add(preorder[i]); &#125; List&lt;Integer&gt; inorderList = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; inorder.length; i++) &#123; inorderList.add(inorder[i]); &#125; return helper(preorderList,inorderList); &#125; return root; &#125; private TreeNode helper(List&lt;Integer&gt; preorderList, List&lt;Integer&gt; inorderList) &#123; TreeNode root = null; List&lt;Integer&gt; leftPreorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; rightPreorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; leftInorderList = new ArrayList&lt;&gt;(); List&lt;Integer&gt; rightInorderList = new ArrayList&lt;&gt;(); if (preorderList.size() != 0 &amp;&amp; inorderList.size() != 0)&#123; root = new TreeNode(preorderList.get(0)); int inOrderpos = inorderList.indexOf(root.val);//根节点 leftInorderList = inorderList.subList(0,inOrderpos); rightInorderList = inorderList.subList(inOrderpos+1,inorderList.size()); int leftInorderSize = leftInorderList.size(); leftPreorderList = preorderList.subList(1,leftInorderSize+1); rightPreorderList = preorderList.subList(leftInorderSize+1,preorderList.size()); //todo 重点 root.left = helper(leftPreorderList,leftInorderList); root.right = helper(rightPreorderList,rightInorderList); &#125; return root; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L104_TreeMaxDepth]]></title>
    <url>%2F2016%2F01%2F26%2FL104-TreeMaxDepth%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132package LeetCode.topic.tree;/** * 给定一个二叉树，找出其最大深度。 * * 二叉树的深度为根节点到最远叶子节点的最长路径上的节点数。 * * 说明: 叶子节点是指没有子节点的节点。 * * 示例： * 给定二叉树 [3,9,20,null,null,15,7]， * * 3 * / \ * 9 20 * / \ * 15 7 * 返回它的最大深度 3 。 */public class L104_TreeMaxDepth &#123; public int maxDepth(TreeNode root) &#123; if (root == null) return 0; return 1 + Math.max(maxDepth(root.left),maxDepth(root.right)); &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L103_ZigzagLevelOrder]]></title>
    <url>%2F2016%2F01%2F26%2FL103-ZigzagLevelOrder%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package LeetCode.topic.tree;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;/** * 给定一个二叉树，返回其节点值的锯齿形层次遍历。（即先从左往右，再从右往左进行下一层遍历，以此类推，层与层之间交替进行）。 * * 例如： * 给定二叉树 [3,9,20,null,null,15,7], * * 3 * / \ * 9 20 * / \ * 15 7 * 返回锯齿形层次遍历如下： * * [ * [3], * [20,9], * [15,7] * ] */public class L103_ZigzagLevelOrder &#123; public List&lt;List&lt;Integer&gt;&gt; zigzagLevelOrder(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); LinkedList&lt;TreeNode&gt; quene = new LinkedList&lt;&gt;(); quene.add(root); int depth = 0; while (!quene.isEmpty())&#123; int count = quene.size(); List&lt;Integer&gt; innerList = new ArrayList&lt;&gt;(); while (count &gt; 0)&#123; TreeNode cur = quene.poll(); if (depth % 2 == 1)&#123;//todo 判断奇偶层，从0开始计数 innerList.add(0,cur.val);//todo 如果是奇数层，就倒排 &#125; else &#123; innerList.add(cur.val); &#125; if (cur.left != null)&#123; quene.add(cur.left); &#125; if (cur.right != null)&#123; quene.add(cur.right);//.add 和 .push方法不同 &#125; count--; &#125; depth++; res.add(innerList); &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L102_LevelorderTraversal]]></title>
    <url>%2F2016%2F01%2F26%2FL102-LevelorderTraversal%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package LeetCode.topic.tree;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;import java.util.Queue;/** * 给定一个二叉树，返回其按层次遍历的节点值。 （即逐层地，从左到右访问所有节点）。 * * 例如: * 给定二叉树: [3,9,20,null,null,15,7], * * 3 * / \ * 9 20 * / \ * 15 7 * 返回其层次遍历结果： * * [ * [3], * [9,20], * [15,7] * ] */public class L102_LevelorderTraversal &#123; public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;();// if (root == null) return new ArrayList&lt;&gt;(); LinkedList&lt;TreeNode&gt; quene = new LinkedList&lt;&gt;(); quene.add(root); while (!quene.isEmpty())&#123; int count = quene.size();// List&lt;Integer&gt; innerList = new ArrayList&lt;&gt;();// while (count &gt; 0)&#123;// TreeNode cur = quene.pop(); innerList.add(cur.val);// System.out.println(cur.val); if (cur.left != null)&#123; quene.add(cur.left); &#125; if (cur.right != null)&#123; quene.add(cur.right); &#125; count--; &#125; res.add(innerList); &#125; return res; &#125;// public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123;//// LinkedList&lt;TreeNode&gt; quene = new LinkedList&lt;&gt;();// quene.add(root);//// while (!quene.isEmpty())&#123;// TreeNode cur = quene.pop();// System.out.println(cur.val);// if (cur.left != null)&#123;// quene.add(cur.left);// &#125;// if (cur.right != null)&#123;// quene.add(cur.right);// &#125;// &#125;// return null;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L101_IsSymmetric]]></title>
    <url>%2F2016%2F01%2F26%2FL101-IsSymmetric%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445package LeetCode.topic.tree;/** * 给定一个二叉树，检查它是否是镜像对称的。 * * 例如，二叉树 [1,2,2,3,4,4,3] 是对称的。 * * 1 * / \ * 2 2 * / \ / \ * 3 4 4 3 * 但是下面这个 [1,2,2,null,3,null,3] 则不是镜像对称的: * * 1 * / \ * 2 2 * \ \ * 3 3 * 说明: * * 如果你可以运用递归和迭代两种方法解决这个问题，会很加分。 */public class L101_IsSymmetric &#123; public boolean isSymmetric(TreeNode root) &#123; if (root == null)return true; return helper(root.left,root.right); &#125; private boolean helper(TreeNode leftNode, TreeNode rightNode) &#123; if (leftNode == null &amp;&amp; rightNode == null) return true; if ((leftNode != null &amp;&amp; rightNode != null &amp;&amp; leftNode.val == rightNode.val))&#123; return helper(leftNode.left,rightNode.right) &amp;&amp; helper(leftNode.right,rightNode.left); &#125; else &#123; return false; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L100_IsSameTree]]></title>
    <url>%2F2016%2F01%2F26%2FL100-IsSameTree%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package LeetCode.topic.tree;/** * 给定两个二叉树，编写一个函数来检验它们是否相同。 * * 如果两个树在结构上相同，并且节点具有相同的值，则认为它们是相同的。 * * 示例 1: * * 输入: 1 1 * / \ / \ * 2 3 2 3 * * [1,2,3], [1,2,3] * * 输出: true * 示例 2: * * 输入: 1 1 * / \ * 2 2 * * [1,2], [1,null,2] * * 输出: false * 示例 3: * * 输入: 1 1 * / \ / \ * 2 1 1 2 * * [1,2,1], [1,1,2] * * 输出: false */public class L100_IsSameTree &#123; public boolean isSameTree(TreeNode p, TreeNode q) &#123; if (p == null &amp;&amp; q == null)&#123; return true; &#125; else &#123; if (p == null || q == null)&#123; return false; &#125; else &#123; if (p.val == q.val)&#123; return isSameTree(p.left,q.left) &amp;&amp; isSameTree(p.right,q.right); &#125; else &#123; return false; &#125; &#125; &#125; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L99_RecoverTree]]></title>
    <url>%2F2016%2F01%2F26%2FL99-RecoverTree%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112package LeetCode.topic.tree;import java.util.Stack;/** * 二叉搜索树中的两个节点被错误地交换。 * * 请在不改变其结构的情况下，恢复这棵树。 * * 示例 1: * * 输入: [1,3,null,null,2] * * 1 * / * 3 * \ * 2 * * 输出: [3,1,null,null,2] * * 3 * / * 1 * \ * 2 * 示例 2: * * 输入: [3,1,4,null,null,2] * * 3 * / \ * 1 4 * / * 2 * * 输出: [2,1,4,null,null,3] * * 2 * / \ * 1 4 * / * 3 * 进阶: * * 使用 O(n) 空间复杂度的解法很容易实现。 * 你能想出一个只使用常数空间的解决方案吗？ * * 个人思路： * 中序遍历，是递增的，找到不一致的，然后进行交换 */public class L99_RecoverTree &#123; TreeNode first = null; TreeNode second = null; TreeNode prev = null; public void recoverTree(TreeNode root) &#123; if (root == null) return; helper(root); int tmp = first.val; first.val = second.val; second.val = tmp; &#125; private void helper(TreeNode root) &#123; if (root == null) return; helper(root.left); if (prev != null &amp;&amp; prev.val &gt;= root.val)&#123; if (first == null) first = prev; second = root;// &#125; prev =root;//如果正常继续轮转 helper(root.right); &#125;// public void recoverTree(TreeNode root) &#123;// if (root == null) return;// TreeNode first = null;// TreeNode second = null;// TreeNode prev = null;//// TreeNode cur = root;// Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();// while (!stack.isEmpty() || cur != null)&#123;// if (cur != null)&#123;// stack.push(cur);// cur = cur.left;// &#125; else &#123;// cur = stack.pop();// if (prev != null &amp;&amp; prev.val &gt;= cur.val)&#123;// if (first == null) first = prev;// second = cur;//// &#125;// prev = cur;// cur = cur.right;// &#125;// &#125;// int tmp = first.val;// first.val = second.val;// second.val = tmp;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L98_IsValidBST]]></title>
    <url>%2F2016%2F01%2F26%2FL98-IsValidBST%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package LeetCode.topic.tree;/** * 给定一个二叉树，判断其是否是一个有效的二叉搜索树。 * * 假设一个二叉搜索树具有如下特征： * * 节点的左子树只包含小于当前节点的数。 * 节点的右子树只包含大于当前节点的数。 * 所有左子树和右子树自身必须也是二叉搜索树。 * 示例 1: * * 输入: * 2 * / \ * 1 3 * 输出: true * 示例 2: * * 输入: * 5 * / \ * 1 4 * / \ * 3 6 * 输出: false * 解释: 输入为: [5,1,4,null,null,3,6]。 * 根节点的值为 5 ，但是其右子节点值为 4 。 * */public class L98_IsValidBST &#123; public boolean isValidBST(TreeNode root) &#123; if (root == null) return true; return helper(root,null,null); &#125; private boolean helper(TreeNode root, Integer min, Integer max) &#123; if (root == null) return true; if (min != null &amp;&amp; root.val &lt;= min) return false; if (max != null &amp;&amp; root.val &gt;= max) return false; return helper(root.left,min,root.val) &amp;&amp; helper(root.right,root.val,max); &#125;// double min = -Double.MAX_VALUE;// public boolean isValidBST(TreeNode root) &#123;// if (root == null) return true;// if (isValidBST(root.left))&#123;// if (root.val &gt; min)&#123;// min = root.val;// return isValidBST(root.right);// &#125;// &#125;// return false;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L96_NumTrees*]]></title>
    <url>%2F2016%2F01%2F26%2FL96-NumTrees%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839package LeetCode.topic.tree;/** * 给定一个整数 n，求以 1 ... n 为节点组成的二叉搜索树有多少种？ * * 示例: * * 输入: 3 * 输出: 5 * 解释: * 给定 n = 3, 一共有 5 种不同结构的二叉搜索树: * * 1 3 3 2 1 * \ / / / \ \ * 3 2 1 1 3 2 * / / \ \ * 2 1 2 3 * * n = 3 * root: 1 left:0 right:2 f(0)*f(2) * root: 2 left:1 right:1 f(1)*f(1) * root: 3 left:2 right:0 f(2)*f(0) * * f(n) = f(0)*f(n-1)+f(1)*(n-2)+...+ f(n-2)*f(1) + f(n-1)*f(0) * time:O(n) * space:O(n) */public class L96_NumTrees &#123; public int numTrees(int n) &#123; int[] res = new int[n + 1]; res[0] = 1; for (int i = 1; i &lt;= n; i++) &#123;//从1开始 for (int j = 0; j &lt; i; j++) &#123;//从0开始 res[i] += res[j] * res[i -j - 1];//res[j]是左子树 i是总个数，1是根节点，所以i - 1 -j是右子树 &#125; &#125; return res[n]; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L95_GenerateTrees*]]></title>
    <url>%2F2016%2F01%2F26%2FL95-GenerateTrees%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;/** * 给定一个整数 n，生成所有由 1 ... n 为节点所组成的二叉搜索树。 * * 示例: * * 输入: 3 * 输出: * [ * [1,null,3,2], * [3,2,null,1], * [3,1,null,null,2], * [2,1,3], * [1,null,2,null,3] * ] * 解释: * 以上的输出对应以下 5 种不同结构的二叉搜索树： * * 1 3 3 2 1 * \ / / / \ \ * 3 2 1 1 3 2 * / / \ \ * 2 1 2 3 */public class L95_GenerateTrees &#123; public List&lt;TreeNode&gt; generateTrees(int n) &#123; if (n ==0) return new ArrayList&lt;&gt;(); return generateTrees(1,n); &#125; private List&lt;TreeNode&gt; generateTrees(int start, int end) &#123; List&lt;TreeNode&gt; res = new ArrayList&lt;&gt;(); if (start &gt; end)&#123; res.add(null); return res; &#125; // 每一个i作为根 // start～～i-1为左子树 // i+1～～end为右子树 for (int i = start;i &lt;= end;i++)&#123; List&lt;TreeNode&gt; subLeftTree = generateTrees(start,i - 1);//todo i-1 List&lt;TreeNode&gt; subRightTree = generateTrees(i + 1,end);//todo i+1 for (TreeNode left : subLeftTree)&#123; for (TreeNode right: subRightTree) &#123; TreeNode node = new TreeNode(i);//todo i node.left = left; node.right = right; res.add(node); &#125; &#125; &#125; return res; &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
        <tag>recursion</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L94_InorderTraversal]]></title>
    <url>%2F2016%2F01%2F26%2FL94-InorderTraversal%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package LeetCode.topic.tree;import java.util.ArrayList;import java.util.List;import java.util.Stack;/** * 给定一个二叉树，返回它的中序 遍历。 * * 示例: * * 输入: [1,null,2,3] * 1 * \ * 2 * / * 3 * * 输出: [1,3,2] */public class L94_InorderTraversal &#123; public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123; if (root == null) return new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode cur = root; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); while (cur != null || !stack.isEmpty())&#123;//变形 while (cur != null)&#123; stack.push(cur); cur = cur.left; &#125;// if (stack.isEmpty())&#123;// break;// &#125; cur = stack.pop(); list.add(cur.val); cur = cur.right; &#125; return list; &#125;// public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123;// if (root == null) return new ArrayList&lt;&gt;();// Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();// TreeNode cur = root;// List&lt;Integer&gt; list = new ArrayList&lt;&gt;();// while (true)&#123;// while (cur != null)&#123;// stack.push(cur);// cur = cur.left;// &#125;// if (stack.isEmpty())&#123;// break;// &#125;// cur = stack.pop();// list.add(cur.val);// cur = cur.right;// &#125;// return list;// &#125; public class TreeNode &#123; int val; TreeNode left; TreeNode right; TreeNode(int x) &#123; val = x; &#125; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GroupComparator原理]]></title>
    <url>%2F2016%2F01%2F25%2Fmr-GroupComparator%2F</url>
    <content type="text"><![CDATA[分析最近看dadoop中关于辅助排序（SecondarySort）的实现，说到了三个东西要设置：1. partioner；2. Key Comparator；3. Group Comparator。前两个都比较容易理解，但是关于group的概念我一直理解不了：一，有了partioner，所有的key已经放到一个分区了，每个分区对应一个reducer，而且key也可以排序了，那么不是实现了整个数据集的全排序了吗？第二，mapper产生的中间结果经过shuffle和sort后，每个key整合成一个记录(集合)，每次reduce方法调用处理这个记录(集合)，但是group的目的是让一次reduce调用处理多条记录(将该集合进行内部分组)，这不是矛盾吗，找了好久一直都没找到这个问题的清晰解释。 后来找到一本书，《Pro Hadoop》，里面有一部分内容详细解释了这个问题，看后终于明白了，和大家分享一下。reduce方法每次是读一条记录(集合)，读到相应的key，但是处理value集合时，处理完当前记录的value后，还会判断下一条记录是不是和当前的key是不是同一个组，如果是的话，会继续读取这些记录的值，而这个记录也会被认为已经处理了，直到记录不是当前组，这次reduce调用才结束，这样一次reduce调用就会处理掉一个组中的所有记录，而不仅仅是一条完整的记录(集合)了。 这个有什么用呢？如果不用分组，那么同一组的记录就要在多次reduce方法中独立处理(所有的数据都在同一组中)，那么有些状态数据就要传递了，就会增加复杂度，在一次调用中处理的话，这些状态只要用方法内的变量就可以的。比如查找最大值，只要读第一个值就可以了。 参考：https://blog.csdn.net/qq_20641565/article/details/52770872 源码分析目标：弄明白，我们配置的GroupComparator是如何对进入reduce函数中的key Iterable 进行影响。如下是一个配置了GroupComparator 的reduce 函数。具体影响是我们可以在自定义的GroupComparator 中确定哪儿些value组成一组，进入一个reduce函数 123456789101112131415161718192021public static class DividendGrowthReducer extends Reducer&lt;Stock, DoubleWritable, NullWritable, DividendChange&gt; &#123; private NullWritable outputKey = NullWritable.get(); private DividendChange outputValue = new DividendChange(); @Override protected void reduce(Stock key, Iterable&lt;DoubleWritable&gt; values, Context context) throws IOException, InterruptedException &#123; double previousDividend = 0.0; for(DoubleWritable dividend : values) &#123; double currentDividend = dividend.get(); double growth = currentDividend - previousDividend; if(Math.abs(growth) &gt; 0.000001) &#123; outputValue.setSymbol(key.getSymbol()); outputValue.setDate(key.getDate()); outputValue.setChange(growth); context.write(outputKey, outputValue); previousDividend = currentDividend; &#125; &#125; &#125; &#125; 着先我们找到向上找，是谁调用了我们写的这个reduce函数。 Reducer类的run 方法。通过如下代码，可以看到是在run方法中，对于每个key，调用一次reduce函数。此处传入reduce函数的都是对象引用。1234567891011121314/** * Advanced application writers can use the * &#123;@link #run(org.apache.hadoop.mapreduce.Reducer.Context)&#125; method to * control how the reduce task works. */ public void run(Context context) throws IOException, InterruptedException &#123; ..... while (context.nextKey()) &#123; reduce(context.getCurrentKey(), context.getValues(), context); ..... &#125; ..... &#125;&#125; 结合我们写的reduce函数，key是在遍历value的时候会对应变化。那我们继续跟踪context.getValues 得到的迭代器的next方法。context 此处是ReduceContext.java （接口）. 对应的实现类为ReduceContextImpl.java123456789101112131415161718protected class ValueIterable implements Iterable&lt;VALUEIN&gt; &#123; private ValueIterator iterator = new ValueIterator(); @Override public Iterator&lt;VALUEIN&gt; iterator() &#123; return iterator; &#125; &#125; /** * Iterate through the values for the current key, reusing the same value * object, which is stored in the context. * @return the series of values associated with the current key. All of the * objects returned directly and indirectly from this method are reused. */ public Iterable&lt;VALUEIN&gt; getValues() throws IOException, InterruptedException &#123; return iterable; &#125; 直接返回了一个iterable。继续跟踪ValueIterable 类型的iterable。那明白了，在reduce 函数中进行Iterable的遍历，其实调用的是ValueIterable的next方法。下面看一下next的实现。 1234567@Override public VALUEIN next() &#123; ……………… nextKeyValue(); return value; ……………… &#125; 再继续跟踪nextKeyValue()方法。终于找了一个comparator。 这个就是我们配置的GroupingComparator.1234567891011121314151617@Overridepublic boolean nextKeyValue() throws IOException, InterruptedException &#123; …………………………………… if (hasMore) &#123; nextKey = input.getKey(); nextKeyIsSame = comparator.compare(currentRawKey.getBytes(), 0, currentRawKey.getLength(), nextKey.getData(), nextKey.getPosition(), nextKey.getLength() - nextKey.getPosition() ) == 0; &#125; else &#123; nextKeyIsSame = false; &#125; inputValueCounter.increment(1); return true;&#125; 为了证明这个就是我们配置的GroupingComparator。 跟踪ReduceContextImpl的构造调用者。 ReduceTask的run方法。12345678@Override @SuppressWarnings(&quot;unchecked&quot;) public void run(JobConf job, final TaskUmbilicalProtocol umbilical)&#123; ……………………………… RawComparator comparator = job.getOutputValueGroupingComparator(); runNewReducer(job, umbilical, reporter, rIter, comparator, keyClass, valueClass); &#125; 下面把runNewReducer 的代码也贴出来。1234567891011121314151617void runNewReducer(JobConf job, final TaskUmbilicalProtocol umbilical, final TaskReporter reporter, RawKeyValueIterator rIter, RawComparator&lt;INKEY&gt; comparator, Class&lt;INKEY&gt; keyClass, Class&lt;INVALUE&gt; valueClass ) &#123; org.apache.hadoop.mapreduce.Reducer.Context reducerContext = createReduceContext(reducer, job, getTaskID(), rIter, reduceInputKeyCounter, reduceInputValueCounter, trackedRW, committer, reporter, comparator, keyClass, valueClass); 好吧，关于自定义GroupingComparator如何起做用的代码分析，就到此吧。参考：http://blog.itpub.net/30066956/viewspace-2095520/]]></content>
      <categories>
        <category>mapreduce</category>
      </categories>
      <tags>
        <tag>mapreduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L1_twosum]]></title>
    <url>%2F2016%2F01%2F23%2Ftwosum%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829/** * 给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。 * * 你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。 * * 示例: * * 给定 nums = [2, 7, 11, 15], target = 9 * * 因为 nums[0] + nums[1] = 2 + 7 = 9 * 所以返回 [0, 1] */public class L1_twoSum &#123; public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); int[] rst = new int[2]; for (int i = 0; i &lt; nums.length; i++) &#123; int diff = target - nums[i]; if (map.containsKey(diff))&#123; rst[0] = map.get(diff); rst[1] = i; break; &#125; map.put(nums[i],i); &#125; return rst; &#125;&#125;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[equal和hashcode]]></title>
    <url>%2F2015%2F02%2F15%2Fequal%E5%92%8Chashcode%2F</url>
    <content type="text"><![CDATA[1、 为什么要重载equal方法？答案：因为Object的equal方法默认是两个对象的引用的比较，意思就是指向同一内存地址则相等，否则不相等；如果你现在需要利用对象里面的值来判断是否相等，则重载equal方法。 2、 为什么重载hashCode方法？答案：一般的地方不需要重载hashCode，只有当类需要放在HashTable、HashMap、HashSet等等hash结构的集合时才会 重载hashCode，那么为什么要重载hashCode呢？就HashMap来说，好比HashMap就是一个大内存块，里面有很多小内存块，小内存块 里面是一系列的对象，可以利用hashCode来查找小内存块hashCode%size(小内存块数量-即容量)，所以当equal相等时，hashCode必须相等，而且如果是object对象，必须重载hashCode和equal方法。 3、 为什么equals()相等，hashCode就一定要相等，而hashCode相等，却不要求equals相等?答案：1、因为是按照hashCode来访问小内存块，所以hashCode必须相等。2、HashMap获取一个对象是比较key的hashCode相等和equal为true。之所以hashCode相等，却可以equal不等，就比如ObjectA和ObjectB他们都有属性name，那么hashCode都以name计算，所以hashCode一样，但是两个对象属于不同类型，所以equal为false。 4、 为什么需要hashCode?1、 通过hashCode可以很快的查到小内存块。2、 通过hashCode比较比equal方法快，当get时先比较hashCode，如果hashCode不同，直接返回false。 ==比较符只会比较地址,如果地址不同就返回falsejava中任何类都可以重写equals()方法来实现自己的比较方式,String类重写了equals()方法. String类的equals()方法不仅会比较两个对象的地址,还会比较他们的字符串的内容.如果被比较的两个引用指向不同的地址,但是两个地址中的字符串的内容是相同的String的equals()方法仍然会返回true. 哈希码(HashCode)哈希码产生的依据：哈希码并不是完全唯一的，它是一种算法，让同一个类的对象按照自己不同的特征尽量的有不同的哈希码，但不表示不同的对象哈希码完全不同。也有相同的情况，看程序员如何写哈希码的算法。 什么是哈希码(HashCode)在Java中，哈希码代表对象的特征。例如对象String str1 = “aa”, str1.hashCode= 3104String str2 = “bb”, str2.hashCode= 3106String str3 = “aa”, str3.hashCode= 3104根据HashCode由此可得出str1!=str2,str1==str3下面给出几个常用的哈希码的算法。1：Object类的hashCode.返回对象的内存地址经过处理后的结构，由于每个对象的内存地址都不一样，所以哈希码也不一样。2：String类的hashCode.根据String类包含的字符串的内容，根据一种特殊算法返回哈希码，只要字符串所在的堆空间相同，返回的哈希码也相同。3：Integer类，返回的哈希码就是Integer对象里所包含的那个整数的数值，例如Integer i1=new Integer(100),i1.hashCode的值就是100 。由此可见，2个一样大小的Integer对象，返回的哈希码也一样。 案例分析https://www.cnblogs.com/keyi/p/7119825.html 总结1、equals方法用于比较对象的内容是否相等（覆盖以后）2、hashcode方法只有在集合中用到3、当覆盖了equals方法时，比较对象是否相等将通过覆盖后的equals方法进行比较（判断对象的内容是否相等）。4、将对象放入到集合中时，首先判断要放入对象的hashcode值与集合中的任意一个元素的hashcode值是否相等，如果不相等直接将该对象放入集合中。如果hashcode值相等，然后再通过equals方法判断要放入对象与集合中的任意一个对象是否相等，如果equals判断不相等，直接将该元素放入到集合中，否则不放入。5、将元素放入集合的流程图：]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二进制与16进制]]></title>
    <url>%2F2015%2F02%2F15%2F%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%8E16%E8%BF%9B%E5%88%B6%2F</url>
    <content type="text"><![CDATA[首先呢，先要看看十六位数的表示方法 再来掌握二进制数与十六进制数之间的对应关系表 二进制转换成十六进制的方法是，取四合一法，即从二进制的小数点为分界点，向左（或向右）每四位取成一位举例:组分好以后，对照二进制与十六进制数的对应表（如图2中所示），将四位二进制按权相加，得到的数就是一位十六进制数，然后按顺序排列，小数点的位置不变哦，最后得到的就是十六进制数哦]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hashmap]]></title>
    <url>%2F2015%2F02%2F14%2FHashmap%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[哈希(hash)Hash，一般翻译做“散列”，也有直接音译为“哈希”的，就是把任意长度的输入，通过散列算法，变换成固定长度的输出，该输出就是散列值。这种转换是一种压缩映射，也就是，散列值的空间通常远小于输入的空间，不同的输入可能会散列成相同的输出，所以不可能从散列值来唯一的确定输入值。简单的说就是一种将任意长度的消息压缩到某一固定长度的消息摘要的函数。所有散列函数都有如下一个基本特性：根据同一散列函数计算出的散列值如果不同，那么输入值肯定也不同。但是，根据同一散列函数计算出的散列值如果相同，输入值不一定相同。两个不同的输入值，根据同一散列函数计算出的散列值相同的现象叫做碰撞。常见的Hash函数有以下几个：1234567891011121314151617直接定址法：直接以关键字k或者k加上某个常数（k+c）作为哈希地址。数字分析法：提取关键字中取值比较均匀的数字作为哈希地址。除留余数法：用关键字k除以某个不大于哈希表长度m的数p，将所得余数作为哈希表地址。分段叠加法：按照哈希表地址位数将关键字分成位数相等的几部分，其中最后一部分可以比较短。然后将这几部分相加，舍弃最高进位后的结果就是该关键字的哈希地址。平方取中法：如果关键字各个部分分布都不均匀的话，可以先求出它的平方值，然后按照需求取中间的几位作为哈希地址。伪随机数法：采用一个伪随机数当作哈希函数。 上面介绍过碰撞。衡量一个哈希函数的好坏的重要指标就是发生碰撞的概率以及发生碰撞的解决方案。任何哈希函数基本都无法彻底避免碰撞，常见的解决碰撞的方法有以下几种：1234567891011开放定址法：开放定址法就是一旦发生了冲突，就去寻找下一个空的散列地址，只要散列表足够大，空的散列地址总能找到，并将记录存入。链地址法将哈希表的每个单元作为链表的头结点，所有哈希地址为i的元素构成一个同义词链表。即发生冲突时就把该关键字链在以该单元为头结点的链表的尾部。再哈希法当哈希地址发生冲突用其他的函数计算另一个哈希函数地址，直到冲突不再产生为止。建立公共溢出区将哈希表分为基本表和溢出表两部分，发生冲突的元素都放入溢出表中。 HashMapHashMap是Array和Linkedlist的结合体 我们可以从上图看到，左边很明显是个数组，数组的每个成员是一个链表。该数据结构所容纳的所有元素均包含一个指针，用于元素间的链接。我们根据元素的自身特征把元素分配到不同的链表中去，反过来我们也正是通过这些特征找到正确的链表，再从链表中找出正确的元素。其中，根据元素特征计算元素数组下标的方法就是哈希算法，即本文的主角hash()函数（当然，还包括indexOf()函数）。 源码解析首先，在同一个版本的Jdk中，HashMap、HashTable以及ConcurrentHashMap里面的hash方法的实现是不同的。再不同的版本的JDK中（Java7 和 Java8）中也是有区别的。我会尽量全部介绍到。相信，看文这篇文章，你会彻底理解hash方法。 在上代码之前，我们先来做个简单分析。我们知道，hash方法的功能是根据Key来定位这个K-V在链表数组中的位置的。也就是hash方法的输入应该是个Object类型的Key，输出应该是个int类型的数组下标。如果让你设计这个方法，你会怎么做？ 其实简单，我们只要调用Object对象的hashCode()方法，该方法会返回一个整数，然后用这个数对HashMap或者HashTable的容量进行取模就行了。没错，其实基本原理就是这个，只不过，在具体实现上，由两个方法int hash(Object k)和int indexFor(int h, int length)来实现。但是考虑到效率等问题，HashMap的实现会稍微复杂一点。 12hash ：该方法主要是将Object转换成一个整型。indexFor ：该方法主要是将hash生成的整型转换成链表数组中的下标。 HashMap In Java 71234567891011121314final int hash(Object k) &#123; int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; indexFor方法前面我说过，indexFor方法其实主要是将hash生成的整型转换成链表数组中的下标。那么return h &amp; (length-1);是什么意思呢？其实，他就是取模。Java之所有使用位运算(&amp;)来代替取模运算(%)，最主要的考虑就是效率。位运算(&amp;)效率要比代替取模运算(%)高很多，主要原因是位运算直接对内存数据进行操作，不需要转成十进制，因此处理速度非常快。 那么，为什么可以使用位运算(&amp;)来实现取模运算(%)呢？这实现的原理如下：123456789X % 2^n = X &amp; (2^n – 1)2^n表示2的n次方，也就是说，一个数对2^n取模 == 一个数和(2^n – 1)做按位与运算 。假设n为3，则2^3 = 8，表示成2进制就是1000。2^3 -1 = 7 ，即0111。此时X &amp; (2^3 – 1) 就相当于取X的2进制的最后三位数。从2进制角度来看，X / 8相当于 X &gt;&gt; 3，即把X右移3位，此时得到了X / 8的商，而被移掉的部分(后三位)，则是X % 8，也就是余数。 上面的解释不知道你有没有看懂，没看懂的话其实也没关系，你只需要记住这个技巧就可以了。或者你可以找几个例子试一下。126 % 8 = 6 ，6 &amp; 7 = 610 &amp; 8 = 2 ，10 &amp; 7 = 2 所以，return h &amp; (length-1);只要保证length的长度是2^n的话，就可以实现取模运算了。而HashMap中的length也确实是2的倍数，初始值是16，之后每次扩充为原来的2倍。扩容因子是0.75。 为了推断HashMap的默认长度为什么是1612长度16或者其他2的幂,length - 1的值是所有二进制位全为1,这种情况下,index的结果等同于hashcode后几位的值只要输入的hashcode本身分布均匀,hash算法的结果就是均匀的 所以,HashMap的默认长度为16,是为了降低hash碰撞的几率https://blog.csdn.net/zjcjava/article/details/78495416 分析完indexFor方法后，我们接下来准备分析hash方法的具体原理和实现。在深入分析之前，至此，先做个总结。 HashMap的数据是存储在链表数组里面的。在对HashMap进行插入/删除等操作时，都需要根据K-V对的键值定位到他应该保存在数组的哪个下标中。而这个通过键值求取下标的操作就叫做哈希。HashMap的数组是有长度的，Java中规定这个长度只能是2的倍数，初始值为16。简单的做法是先求取出键值的hashcode，然后在将hashcode得到的int值对数组长度进行取模。为了考虑性能，Java总采用按位与操作实现取模操作。 hash方法接下来我们会发现，无论是用取模运算还是位运算都无法直接解决冲突较大的问题。比如：CA11 0000和0001 0000在对0000 1111进行按位与运算后的值是相等的。 两个不同的键值，在对数组长度进行按位与运算后得到的结果相同，这不就发生了冲突吗。那么如何解决这种冲突呢，来看下Java是如何做的。 其中的主要代码部分如下：1234h ^= k.hashCode();h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12);return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); 这段代码是为了对key的hashCode进行扰动计算，防止不同hashCode的高位不同但低位相同导致的hash冲突。简单点说，就是为了把高位的特征和低位的特征组合起来，降低哈希冲突的概率，也就是说，尽量做到任何一位的变化都能对最终得到的结果产生影响。 举个例子来说，我们现在想向一个HashMap中put一个K-V对，Key的值为“hollischuang”，经过简单的获取hashcode后，得到的值为“1011000110101110011111010011011”，如果当前HashTable的大小为16，即在不进行扰动计算的情况下，他最终得到的index结果值为11。由于15的二进制扩展到32位为“00000000000000000000000000001111”，所以，一个数字在和他进行按位与操作的时候，前28位无论是什么，计算结果都一样（因为0和任何数做与，结果都为0）。如下图所示。 可以看到，后面的两个hashcode经过位运算之后得到的值也是11 ，虽然我们不知道哪个key的hashcode是上面例子中的那两个，但是肯定存在这样的key，这就产生了冲突。 那么，接下来，我看看一下经过扰动的算法最终的计算结果会如何。 从上面图中可以看到，之前会产生冲突的两个hashcode，经过扰动计算之后，最终得到的index的值不一样了，这就很好的避免了冲突。 其实，使用位运算代替取模运算，除了性能之外，还有一个好处就是可以很好的解决负数的问题。1因为我们知道，hashcode的结果是int类型，而int的取值范围是-2^31 ~ 2^31 – 1，即[ -2147483648, 2147483647]；这里面是包含负数的，我们知道，对于一个负数取模还是有些麻烦的。如果使用二进制的位运算的话就可以很好的避免这个问题。首先，不管hashcode的值是正数还是负数。length-1这个值一定是个正数。那么，他的二进制的第一位一定是0（有符号数用最高位作为符号位，“0”代表“+”，“1”代表“-”），这样里两个数做按位与运算之后，第一位一定是个0，也就是，得到的结果一定是个正数。 HashTable In Java 7上面是Java 7中HashMap的hash方法以及indexOf方法的实现，那么接下来我们要看下，线程安全的HashTable是如何实现的，和HashMap有何不同，并试着分析下不同的原因。以下是Java 7中HashTable的hash方法的实现。1234private int hash(Object k) &#123; // hashSeed will be zero if alternative hashing is disabled. return hashSeed ^ k.hashCode();&#125; 我们可以发现，很简单，(1)相当于只是对k做了个简单的hash，取了一下其hashCode。(1)而HashTable中也没有indexOf方法，取而代之的是这段代码：int index = (hash &amp; 0x7FFFFFFF) % tab.length;。也就是说，HashMap和HashTable对于计算数组下标这件事，采用了两种方法。HashMap采用的是位运算，而HashTable采用的是直接取模。12为啥要把hash值和0x7FFFFFFF做一次按位与操作呢，主要是为了保证得到的index的第一位为0，也就是为了得到一个正数。因为有符号数第一位0代表正数，1代表负数。 0x7FFFFFFF 是long int的最大值 我们前面说过，HashMap之所以不用取模的原因是为了提高效率。有人认为，因为HashTable是个线程安全的类，本来就慢，所以Java并没有考虑效率问题，就直接使用取模算法了呢？但是其实并不完全是，Java这样设计还是有一定的考虑在的，虽然这样效率确实是会比HashMap慢一些。 其实，HashTable采用简单的取模是有一定的考虑在的。这就要涉及到HashTable的构造函数和扩容函数了。由于篇幅有限，这里就不贴代码了，直接给出结论：12345HashTable默认的初始大小为11，之后每次扩充为原来的2n+1。也就是说，HashTable的链表数组的默认大小是一个素数、奇数。之后的每次扩充结果也都是奇数。由于HashTable会尽量使用素数、奇数作为容量的大小。当哈希表的大小为素数时，简单的取模哈希的结果会更加均匀。（这个是可以证明出来的，由于不是本文重点，暂不详细介绍，可参考：http://zhaox.github.io/algorithm/2015/06/29/hash） 至此，我们看完了Java 7中HashMap和HashTable中对于hash的实现，我们来做个简单的总结。12345HashMap默认的初始化大小为16，之后每次扩充为原来的2倍。HashTable默认的初始大小为11，之后每次扩充为原来的2n+1。当哈希表的大小为素数时，简单的取模哈希的结果会更加均匀，所以单从这一点上看，HashTable的哈希表大小选择，似乎更高明些。因为hash结果越分散效果越好。在取模计算时，如果模数是2的幂，那么我们可以直接使用位运算来得到结果，效率要大大高于做除法。所以从hash计算的效率上，又是HashMap更胜一筹。但是，HashMap为了提高效率使用位运算代替哈希，这又引入了哈希分布不均匀的问题，所以HashMap为解决这问题，又对hash算法做了一些改进，进行了扰动计算。 ConcurrentHashMap In Java 71234567891011121314151617181920private int hash(Object k) &#123; int h = hashSeed; if ((0 != h) &amp;&amp; (k instanceof String)) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); // Spread bits to regularize both segment and index locations, // using variant of single-word Wang/Jenkins hash. h += (h &lt;&lt; 15) ^ 0xffffcd7d; h ^= (h &gt;&gt;&gt; 10); h += (h &lt;&lt; 3); h ^= (h &gt;&gt;&gt; 6); h += (h &lt;&lt; 2) + (h &lt;&lt; 14); return h ^ (h &gt;&gt;&gt; 16);&#125; int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; 上面这段关于ConcurrentHashMap的hash实现其实和HashMap如出一辙。都是通过位运算代替取模，然后再对hashcode进行扰动。区别在于，ConcurrentHashMap 使用了一种变种的Wang/Jenkins 哈希算法，其主要母的也是为了把高位和低位组合在一起，避免发生冲突。至于为啥不和HashMap采用同样的算法进行扰动，我猜这只是程序员自由意志的选择吧。至少我目前没有办法证明哪个更优。 HashMap In Java 8在Java 8 之前，HashMap和其他基于map的类都是通过链地址法解决冲突，它们使用单向链表来存储相同索引值的元素。在最坏的情况下，这种方式会将HashMap的get方法的性能从O(1)降低到O(n)。为了解决在频繁冲突时hashmap性能降低的问题，Java 8中使用平衡树来替代链表存储冲突的元素。这意味着我们可以将最坏情况下的性能从O(n)提高到O(logn)。关于HashMap在Java 8中的优化，我后面会有文章继续深入介绍。 如果恶意程序知道我们用的是Hash算法，则在纯链表情况下，它能够发送大量请求导致哈希碰撞，然后不停访问这些key导致HashMap忙于进行线性查找，最终陷入瘫痪，即形成了拒绝服务攻击（DoS）。 关于Java 8中的hash函数，原理和Java 7中基本类似。Java 8中这一步做了优化，只做一次16位右位移异或混合，而不是四次，但原理是不变的。1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的。以上方法得到的int的hash值，然后再通过h &amp; (table.length -1)来得到该对象在数据中保存的位置。 HashTable In Java 8在Java 8的HashTable中，已经不在有hash方法了。但是哈希的操作还是在的，比如在put方法中就有如下实现：12int hash = key.hashCode();int index = (hash &amp; 0x7FFFFFFF) % tab.length; 这其实和Java 7中的实现几乎无差别，就不做过多的介绍了 ConcurrentHashMap In Java 8Java 8 里面的求hash的方法从hash改为了spread。实现方式如下：123static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; Java 8的ConcurrentHashMap同样是通过Key的哈希值与数组长度取模确定该Key在数组中的索引。同样为了避免不太好的Key的hashCode设计，它通过如下方法计算得到Key的最终哈希值。不同的是，Java 8的ConcurrentHashMap作者认为引入红黑树后，即使哈希冲突比较严重，寻址效率也足够高，所以作者并未在哈希值的计算上做过多设计，只是将Key的hashCode值与其高16位作异或并保证最高位为0（从而保证最终结果为正整数）。 总结至此，我们已经分析完了HashMap、HashTable以及ConcurrentHashMap分别在Jdk 1.7 和 Jdk 1.8中的实现。我们可以发现，为了保证哈希的结果可以分散、为了提高哈希的效率，JDK在一个小小的hash方法上就有很多考虑，做了很多事情。当然，我希望我们不仅可以深入了解背后的原理，还要学会这种对代码精益求精的态度。 非线程安全问题解决方案HashMap为什么线程不安全1、在两个线程同时尝试扩容HashMap时，可能将一个链表形成环形的链表，所有的next都不为空，进入死循环2、在两个线程同时进行put时可能造成一个线程数据的丢失 我们可以使用CocurrentHashMap来代替HashTable吗？这是另外一个很热门的面试题，因为ConcurrentHashMap越来越多人用了。我们知道HashTable是synchronized的，但是ConcurrentHashMap同步性能更好，因为它仅仅根据同步级别对map的一部分进行上锁。ConcurrentHashMap当然可以代替HashTable，但是HashTable提供更强的线程安全性。 先来些简单的问题你用过HashMap吗？什么是HashMap？你为什么用到它？ 几乎每个人都会回答“是的”，然后回答HashMap的一些特性，譬如HashMap可以接受null键值和值，而HashTable则不能；HashMap是非synchronized;HashMap很快；以及HashMap储存的是键值对等等。这显示出你已经用过HashMap，而且对它相当的熟悉。但是面试官来个急转直下，从此刻开始问出一些刁钻的问题，关于HashMap的更多基础的细节。面试官可能会问出下面的问题： 你知道HashMap的工作原理吗？你知道HashMap的get()方法的工作原理吗？ 你也许会回答“我没有详查标准的Java API，你可以看看Java源代码或者Open JDK。”“我可以用Google找到答案。” 但一些面试者可能可以给出答案，“HashMap是基于hashing的原理，我们使用put(key, value)存储对象到HashMap中，使用get(key)从HashMap中获取对象。当我们给put()方法传递键和值时，我们先对键调用hashCode()方法，返回的hashCode用于找到bucket位置来储存Entry对象。”这里关键点在于指出，HashMap是在bucket中储存键对象和值对象，作为Map.Entry。这一点有助于理解获取对象的逻辑。如果你没有意识到这一点，或者错误的认为仅仅只在bucket中存储值的话，你将不会回答如何从HashMap中获取对象的逻辑。这个答案相当的正确，也显示出面试者确实知道hashing以及HashMap的工作原理。但是这仅仅是故事的开始，当面试官加入一些Java程序员每天要碰到的实际场景的时候，错误的答案频现。下个问题可能是关于HashMap中的碰撞探测(collision detection)以及碰撞的解决方法： 当两个对象的hashcode相同会发生什么？hashcode相同只会说明存在相同的bucket下。从这里开始，真正的困惑开始了，一些面试者会回答因为hashcode相同，所以两个对象是相等的，HashMap将会抛出异常，或者不会存储它们。然后面试官可能会提醒他们有equals()和hashCode()两个方法，并告诉他们两个对象就算hashcode相同，但是它们可能并不相等。一些面试者可能就此放弃，而另外一些还能继续挺进，他们回答“因为hashcode相同，所以它们的bucket位置相同，‘碰撞’会发生。因为HashMap使用LinkedList存储对象，这个Entry(包含有键值对的Map.Entry对象)会存储在LinkedList中 。”这个答案非常的合理，虽然有很多种处理碰撞的方法，这种方法是最简单的，也正是HashMap的处理方法。但故事还没有完结，面试官会继续问： 如果两个键的hashcode相同，你如何获取值对象？ 面试者会回答：当我们调用get()方法，HashMap会使用键对象的hashcode找到bucket位置，然后获取值对象。面试官提醒他如果有两个值对象储存在同一个bucket，他给出答案:将会遍历LinkedList直到找到值对象。面试官会问因为你并没有值对象去比较，你是如何确定确定找到值对象的？除非面试者直到HashMap在LinkedList中存储的是键值对，否则他们不可能回答出这一题 。 其中一些记得这个重要知识点的面试者会说，找到bucket位置之后，会调用keys.equals()方法去找到LinkedList中正确的节点，最终找到要找的值对象。完美的答案！ 许多情况下，面试者会在这个环节中出错，因为他们混淆了hashCode()和equals()方法。因为在此之前hashCode()屡屡出现，而equals()方法仅仅在获取值对象的时候才出现。一些优秀的开发者会指出使用不可变的、声明作final的对象，并且采用合适的equals()和hashCode()方法的话，将会减少碰撞的发生，提高效率。不可变性使得能够缓存不同键的hashcode，这将提高整个获取对象的速度，使用String，Interger这样的wrapper类作为键是非常好的选择。 如果你认为到这里已经完结了，那么听到下面这个问题的时候，你会大吃一惊。 如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？除非你真正知道HashMap的工作原理，否则你将回答不出这道题。默认的负载因子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其它集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程叫作rehashing，因为它调用hash方法找到新的bucket位置。 如果你能够回答这道问题，下面的问题来了： 你了解重新调整HashMap大小存在什么问题吗？你可能回答不上来，这时面试官会提醒你当多线程的情况下，可能产生条件竞争(race condition) 。 当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在LinkedList中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在LinkedList的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。这个时候，你可以质问面试官，为什么这么奇怪，要在多线程的环境下使用HashMap呢？ 热心的读者贡献了更多的关于HashMap的问题： 为什么String, Interger这样的wrapper类适合作为键？String, Interger这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的，也是final的，而且已经重写了equals()和hashCode()方法了。其他的wrapper类也有这个特点。不可变性是必要的，因为为了要计算hashCode()，就要防止键值改变，如果键值在放入时和获取时返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性还有其他的优点如线程安全。如果你可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法是非常重要的。如果两个不相等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能。 我们可以使用自定义的对象作为键吗？这是前一个问题的延伸。当然你可能使用任何对象作为键，只要它遵守了equals()和hashCode()方法的定义规则，并且当对象插入到Map中之后将不会再改变了。如果这个自定义对象时不可变的，那么它已经满足了作为键的条件，因为当它创建之后就已经不能改变了。我们可以使用CocurrentHashMap来代替HashTable吗？这是另外一个很热门的面试题，因为ConcurrentHashMap越来越多人用了。我们知道HashTable是synchronized的，但是ConcurrentHashMap同步性能更好，因为它仅仅根据同步级别对map的一部分进行上锁。ConcurrentHashMap当然可以代替HashTable，但是HashTable提供更强的线程安全性。 总结 HashMap的工作原理 HashMap基于hashing原理，我们通过put()和get()方法储存和获取对象。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，然后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用LinkedList来解决碰撞问题，当发生碰撞了，对象将会储存在LinkedList的下一个节点中。 HashMap在每个LinkedList节点中储存键值对对象。 当两个不同的键对象的hashcode相同时会发生什么？ 它们会储存在同一个bucket位置的LinkedList中。键对象的equals()方法用来找到键值对。 Jdk的源代码，每一行都很有意思，都值得花时间去钻研、推敲。参考:https://blog.csdn.net/skiof007/article/details/80253587]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
</search>
